Names,Categories,Categories_code,ScrapData
Absolute_zero,Chemistry,1,"
 Absolute zero is the lowest limit of the thermodynamic temperature scale, a state at which the enthalpy and entropy of a cooled ideal gas reach their minimum value, taken as zero kelvins. The fundamental particles of nature have minimum vibrational motion, retaining only quantum mechanical, zero-point energy-induced particle motion. The theoretical temperature is determined by extrapolating the ideal gas law; by international agreement, absolute zero is taken as −273.15° on the Celsius scale (International System of Units),[1][2] which equals −459.67° on the Fahrenheit scale (United States customary units or Imperial units).[3] The corresponding Kelvin and Rankine temperature scales set their zero points at absolute zero by definition.
 It is commonly thought of as the lowest temperature possible, but it is not the lowest enthalpy state possible, because all real substances begin to depart from the ideal gas when cooled as they approach the change of state to liquid, and then to solid; and the sum of the enthalpy of vaporization (gas to liquid) and enthalpy of fusion (liquid to solid) exceeds the ideal gas's change in enthalpy to absolute zero. In the quantum-mechanical description, matter (solid) at absolute zero is in its ground state, the point of lowest internal energy.
 The laws of thermodynamics indicate that absolute zero cannot be reached using only thermodynamic means, because the temperature of the substance being cooled approaches the temperature of the cooling agent asymptotically,[4] and a system at absolute zero still possesses quantum mechanical zero-point energy, the energy of its ground state at absolute zero. The kinetic energy of the ground state cannot be removed. 
 Scientists and technologists routinely achieve temperatures close to absolute zero, where matter exhibits quantum effects such as Bose–Einstein condensate, superconductivity and superfluidity.
"
Absorbance,Chemistry,1,"In optics, absorbance or decadic absorbance is the common logarithm of the ratio of incident to transmitted radiant power through a material, and spectral absorbance or spectral decadic absorbance is the common logarithm of the ratio of incident to transmitted spectral radiant power through a material.[1] Absorbance is dimensionless, and in particular is not a length, though it is a monotonically increasing function of path length, and approaches zero as the path length approaches zero. The use of the term ""optical density"" for absorbance is discouraged.[1]
In physics, a closely related quantity called ""optical depth"" is used instead of absorbance: the natural logarithm of the ratio of incident to transmitted radiant power through a material. The optical depth equals the absorbance times ln(10).
 The term absorption refers to the physical process of absorbing light, while absorbance does not always measure absorption: it measures attenuation (of transmitted radiant power). Attenuation can be caused by absorption, but also reflection, scattering, and other physical processes.
"
Absorption_(chemistry),Chemistry,1,"In chemistry, absorption is a physical or chemical phenomenon or a process in which atoms, molecules or ions enter some bulk phase – liquid or solid material. This is a different process from adsorption, since molecules undergoing absorption are taken up by the volume, not by the surface (as in the case for adsorption). A more general term is sorption, which covers absorption, adsorption, and ion exchange. Absorption is a condition in which something takes in another substance.[1] In many processes important in technology, the chemical absorption is used in place of the physical process, e.g., absorption of carbon dioxide by sodium hydroxide – such acid-base processes do not follow the Nernst partition law.
 For some examples of this effect, see liquid-liquid extraction. It is possible to extract a solute from one liquid phase to another  without a chemical reaction. Examples of such solutes are noble gases and osmium tetroxide.[1] The process of absorption means that a substance captures and transforms energy. The absorbent distributes the material it captures throughout whole and adsorbent only distributes it through the surface.
 The process of gas or liquid which penetrate into the body of adsorbent is commonly known as absorption.
 Absorption
"
Abundance_(chemistry),Chemistry,1,"In a chemical reaction, a reactant is considered to be in abundance if the quantity of that substance is high and virtually unchanged by the reaction.  Abundance differs from excess in that a reactant in excess is simply any reactant other than the limiting reagent;[1] the amount by which a reactant is in excess is often specified, such as with terms like ""twofold excess"", indicating that there is twice the amount of reactant necessary for the limiting reagent to be completely reacted.  In this case, should the reaction go to completion, the quantity of the reactant in excess will have halved.
 When performing kinetic or thermodynamic studies, it is often useful to have one or more reactants in abundance, as it allows their concentrations to be treated as constants (or parameters) rather than as variables.
"
Accuracy,Chemistry,1,"In measurement of a set, accuracy is closeness of the measurements to a specific value, while precision is the closeness of the measurements to each other.
 Accuracy has two definitions:
 Precision is a description of random errors, a measure of statistical variability.
 In simpler terms, given a set of data points from repeated measurements of the same quantity, the set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if the values are close to each other.  In the first, more common definition of ""accuracy"" above, the two concepts are independent of each other, so a particular set of data can be said to be either accurate, or precise, or both, or neither.
"
Acid,Chemistry,1,"
 An acid is a molecule or ion capable of donating a proton (hydrogen ion H+) (a Brønsted–Lowry acid), or, alternatively, capable of forming a covalent bond with an electron pair (a Lewis acid).[1] The first category of acids are the proton donors, or Brønsted–Lowry acids. In the special case of aqueous solutions, proton donors form the hydronium ion H3O+ and are known as Arrhenius acids. Brønsted and Lowry generalized the Arrhenius theory to include non-aqueous solvents. A Brønsted or Arrhenius acid usually contains a hydrogen atom bonded to a chemical structure that is still energetically favorable after loss of H+.
 Aqueous Arrhenius acids have characteristic properties which provide a practical description of an acid.[2] Acids form aqueous solutions with a sour taste, can turn blue litmus red, and react with bases and certain metals (like calcium) to form salts. The word acid is derived from the Latin acidus/acēre, meaning 'sour'.[3] An aqueous solution of an acid has a pH less than 7 and is colloquially also referred to as ""acid"" (as in ""dissolved in acid""), while the strict definition refers only to the solute.[1] A lower pH means a higher acidity, and thus a higher concentration of positive hydrogen ions in the solution. Chemicals or substances having the property of an acid are said to be acidic.
 Common aqueous acids include hydrochloric acid (a solution of hydrogen chloride which is found in gastric acid in the stomach and activates digestive enzymes), acetic acid (vinegar is a dilute aqueous solution of this liquid), sulfuric acid (used in car batteries), and citric acid (found in citrus fruits). As these examples show, acids (in the colloquial sense) can be solutions or pure substances, and can be derived from acids (in the strict[1] sense) that are solids, liquids, or gases. Strong acids and some concentrated weak acids are corrosive, but there are exceptions such as carboranes and boric acid.
 The second category of acids are Lewis acids, which form a covalent bond with an electron pair. An example is boron trifluoride (BF3), whose boron atom has a vacant orbital which can form a covalent bond by sharing a lone pair of electrons on an atom in a base, for example the nitrogen atom in ammonia (NH3). Lewis considered this as a generalization of the Brønsted definition, so that an acid is a chemical species that accepts electron pairs either directly or by releasing protons (H+) into the solution, which then accept electron pairs. However, hydrogen chloride, acetic acid, and most other Brønsted–Lowry acids cannot form a covalent bond with an electron pair and are therefore not Lewis acids.[4] Conversely, many Lewis acids are not Arrhenius or Brønsted–Lowry acids. In modern terminology, an acid is implicitly a Brønsted acid and not a Lewis acid, since chemists almost always refer to a Lewis acid explicitly as a Lewis acid.[4]"
Acid_anhydride,Chemistry,1,"An acid anhydride is a type of chemical compound derived by the removal of water molecules from an acid.
 In organic chemistry, organic acid anhydrides contain the functional group R(CO)O(CO)R'. Organic acid anhydrides often form when one equivalent of water is removed from two equivalents of an organic acid in a dehydration reaction.
 In inorganic chemistry, an acid anhydride refers to an acidic oxide, an oxide that reacts with water to form an oxyacid (an inorganic acid that contains oxygen or carbonic acid), or with a base to form a salt.
"
Acid_dissociation_constant,Chemistry,1,"An acid dissociation constant, Ka, (also known as acidity constant, or acid-ionization constant) is a quantitative measure of the strength of an acid in solution. It is the equilibrium constant for a chemical reaction
 known as dissociation in the context of acid–base reactions. The chemical species HA is an acid that dissociates into A−, the conjugate base of the acid and a hydrogen ion, H+.[note 1]  The system is said to be in equilibrium when the concentrations of its components will not change over time, because both forward and backward reactions are occurring at the same rate.[1] The dissociation constant is defined by[note 2] where quantities in square brackets represent the concentrations of the species at equilibrium.[note 3][2]"
Actinoid,Chemistry,1,"
 The actinoid /ˈæktɪnɔɪd/ (IUPAC nomenclature, also called actinide[1] /ˈæktɪnaɪd/) series encompasses the 15 metallic chemical elements with atomic numbers from 89 to 103, actinium through lawrencium. The actinoid series derives its name from the first element in the series, actinium. The informal chemical symbol An is used in general discussions of actinoid chemistry to refer to any actinoid.[2][3][4] Strictly speaking, actinium has been labeled as group 3 element, but is often included in any general discussion of the chemistry of the actinoid elements. Since ""actinoid"" means ""actinium-like"" (cf. humanoid or android), it has been argued due semantic reasons that actinium cannot logically be an actinoid, but IUPAC acknowledges its inclusion based on common usage.[5] All but one of the actinides are f-block elements, with the exception being either actinium or lawrencium. The series mostly corresponds to the filling of the 5f electron shell, although actinium and thorium lack any 5f electrons, and curium and lawrencium have the same number as the preceding element. In comparison with the lanthanides, also mostly f-block elements, the actinides show much more variable valence. They all have very large atomic and ionic radii and exhibit an unusually large range of physical properties. While actinium and the late actinides (from americium onwards) behave similarly to the lanthanides, the elements thorium, protactinium, and uranium are much more similar to transition metals in their chemistry, with neptunium and plutonium occupying an intermediate position.
 All actinides are radioactive and release energy upon radioactive decay; naturally occurring uranium and thorium, and synthetically produced plutonium are the most abundant actinides on Earth. These are used in nuclear reactors and nuclear weapons. Uranium and thorium also have diverse current or historical uses, and americium is used in the ionization chambers of most modern smoke detectors.
 Of the actinides, primordial thorium and uranium occur naturally in substantial quantities. The radioactive decay of uranium produces transient amounts of actinium and protactinium, and atoms of neptunium and plutonium are occasionally produced from transmutation reactions in uranium ores. The other actinides are purely synthetic elements.[2][6] Nuclear weapons tests have released at least six actinides heavier than plutonium into the environment; analysis of debris from a 1952 hydrogen bomb explosion showed the presence of americium, curium, berkelium, californium, einsteinium and fermium.[7] In presentations of the periodic table, the lanthanides and the actinides are customarily shown as two additional rows below the main body of the table,[2] with placeholders or else a selected single element of each series (either lanthanum or lutetium, and either actinium or lawrencium, respectively) shown in a single cell of the main table, between barium and hafnium, and radium and rutherfordium, respectively.  This convention is entirely a matter of aesthetics and formatting practicality; a rarely used wide-formatted periodic table inserts the lanthanide and actinide series in their proper places, as parts of the table's sixth and seventh rows (periods).
 Primordial  From decay  Synthetic Border shows natural occurrence of the element
"
Activated_complex,Chemistry,1,"In chemistry an activated complex is defined by the International Union of Pure and Applied Chemistry (IUPAC) as ""that assembly of atoms which corresponds to an arbitrary infinitesimally small region at or near the col (saddle point) of a potential energy surface"".[1] In other words, it refers to a collection of intermediate structures in a chemical reaction that persist while bonds are breaking and new bonds are forming.  It therefore represents not one defined state, but rather a range of transient configurations that a collection of atoms passes through in between clearly defined products and reactants.
 It is the subject of transition state theory - also known as activated complex theory - which studies the kinetics of reactions that pass through a defined intermediate state with standard Gibbs energy of activation ΔG°‡.[2] The state represented by the double dagger symbol is known as the transition state and represents the exact configuration that has an equal probability of forming either the reactants or products of the given reaction.[3] The activated complex is often confused with the transition state and is used interchangeably in many textbooks. However, it differs from the transition state in that the transition state represents only the highest potential energy configuration of the atoms during the reaction while the activated complex refers to a range of configurations near the transition state that the atoms pass through in the transformation from products to reactants. This can be visualized in terms of a reaction coordinate, where the transition state is the molecular configuration at the peak of the diagram while the activated complex can refer to any point near the maximum. Activated complex has partial reactant and partial product character.[4]"
Activation_energy,Chemistry,1,"In chemistry and physics, activation energy is the energy that must be provided to compounds to result in a chemical reaction.[1]The activation energy (Ea) of a reaction is measured in joules per mole (J/mol), kilojoules per mole (kJ/mol) or kilocalories per mole (kcal/mol).[2] Activation energy can be thought of as the magnitude of the potential barrier (sometimes called the energy barrier) separating minima of the potential energy surface pertaining to the initial and final thermodynamic state. For a chemical reaction to proceed at a reasonable rate, the temperature of the system should be high enough such that there exists an appreciable number of molecules with translational energy equal to or greater than the activation energy. The term Activation Energy was introduced in 1889 by the Swedish scientist Svante Arrhenius.[3]"
Activity_series,Chemistry,1,"In chemistry, a reactivity series (or activity series) is an empirical, calculated, and structurally analytical progression[1] of a series of metals, arranged by their ""reactivity"" from highest to lowest.[2][3][4] It is used to summarize information about the reactions of metals with acids and water, single displacement reactions and the extraction of metals from their ores.
"
Actual_yield,Chemistry,1,"
 In chemistry, yield, also referred to as reaction yield, is a measure of the quantity of moles of a product formed in relation to the reactant consumed, obtained in a chemical reaction, usually expressed as a percentage. [1]  Yield is one of the primary factors that scientists must consider in organic and inorganic chemical synthesis processes.[2] In chemical reaction engineering, ""yield"",  ""conversion"" and ""selectivity"" are terms used to describe ratios of how much of a reactant was consumed (conversion),  how much desired product was formed (yield) in relation to the undesired product (selectivity), represented as X, Y, and S.
"
Open-chain_compound,Chemistry,1,"In chemistry, an open-chain compound (also spelled as open chain compound) or acyclic compound (Greek prefix ""α"", without and ""κύκλος"", cycle) is a compound with a linear structure, rather than a cyclic one.[1]
An open-chain compound having no side chains is called a straight-chain compound (also spelled as straight chain compound).[2][3] Many of the simple molecules of organic chemistry, such as the alkanes and alkenes, have both linear and ring isomers, that is, both acyclic and cyclic, with the latter often classified as aromatic. For those with 4 or more carbons, the linear forms can have straight-chain or branched-chain isomers. The lowercase prefix n- denotes the straight-chain isomer; for example, n-butane is straight-chain butane, whereas i-butane is isobutane. Cycloalkanes are isomers of alkenes, not of alkanes, because the ring's closure involves a C-C bond. Having no rings (aromatic or otherwise), all open-chain compounds are aliphatic.
 Typically in biochemistry, some isomers are more prevalent than others. For example, in living organisms, the open-chain isomer of glucose usually exists only transiently, in small amounts; D-glucose is the usual isomer; and L-glucose is rare.
 Straight-chain molecules are often not literally straight, in the sense that their bond angles are often not 180°, but the name reflects that they are schematically straight. For example, the straight-chain alkanes are wavy or ""puckered"", as the models below show.
"
Addition_reaction,Chemistry,1,"An addition reaction, in organic chemistry, is in its simplest terms an organic reaction where two or more molecules combine to form a larger one (the adduct).[1][2] Addition reactions are limited to chemical compounds that have multiple bonds, such as molecules with carbon–carbon double bonds (alkenes), or with triple bonds (alkynes), and compounds that have rings, which are also considered points of unsaturation. Molecules containing carbon—hetero double bonds like carbonyl (C=O) groups, or imine (C=N) groups, can undergo addition, as they too have double-bond character.
 An addition reaction is the reverse of an elimination reaction. For instance, the hydration of an alkene to an alcohol is reversed by dehydration. 
 There are two main types of polar addition reactions: electrophilic addition and nucleophilic addition. Two non-polar addition reactions exist as well, called free-radical addition and cycloadditions.  Addition reactions are also encountered in polymerizations and called addition polymerization.  
 Depending on the product structure, it could promptly react further to eject a leaving group to give the addition–elimination reaction sequence.
"
Adhesion,Chemistry,1,"Adhesion is the tendency of dissimilar particles or surfaces to cling to one another (cohesion refers to the tendency of similar or identical particles/surfaces to cling to one another). 
 Note 1: Adhesion requires energy that can come from chemical and/or physicallinkages, the latter being reversible when enough energy is applied.
 Note 2: In biology, adhesion reflects the behavior of cells shortly after contactto the surface.
 The forces that cause adhesion and cohesion can be divided into several types. The intermolecular forces responsible for the function of various kinds of stickers and sticky tape fall into the categories of chemical adhesion, dispersive adhesion, and diffusive adhesion. In addition to the cumulative magnitudes of these intermolecular forces, there are also certain emergent mechanical effects.
"
Adsorption,Chemistry,1,"Adsorption is the adhesion of atoms, ions or molecules from a gas, liquid or dissolved solid to a surface.[1] This process creates a film of the adsorbate on the surface of the adsorbent. This process differs from absorption, in which a fluid (the absorbate) is dissolved by or permeates a liquid or solid (the absorbent), respectively.[2] Adsorption is a surface phenomenon, while absorption involves the whole volume of the material, although adsorption does often precede absorption.[3] The term sorption encompasses both processes, while desorption is the reverse of it.
 Note 1: Adsorption of proteins is of great importance when a material is in contact with blood or body fluids. In the case of blood, albumin, which is largely predominant, is generally adsorbed first, and then rearrangements occur in favor of other minor proteins according to surface affinity against mass law selection (Vroman effect).
 Similar to surface tension, adsorption is a consequence of surface energy. In a bulk material, all the bonding requirements (be they ionic, covalent or metallic) of the constituent atoms of the material are filled by other atoms in the material. However, atoms on the surface of the adsorbent are not wholly surrounded by other adsorbent atoms and therefore can attract adsorbates. The exact nature of the bonding depends on the details of the species involved, but the adsorption process is generally classified as physisorption (characteristic of weak van der Waals forces) or chemisorption (characteristic of covalent bonding). It may also occur due to electrostatic attraction.[5] Adsorption is present in many natural, physical, biological and chemical systems and is widely used in industrial applications such as heterogeneous catalysts,[6][7] activated charcoal, capturing and using waste heat to provide cold water for air conditioning and other process requirements (adsorption chillers), synthetic resins, increasing storage capacity of carbide-derived carbons and water purification. Adsorption, ion exchange and chromatography are sorption processes in which certain adsorbates are selectively transferred from the fluid phase to the surface of insoluble, rigid particles suspended in a vessel or packed in a column. Pharmaceutical industry applications, which use adsorption as a means to prolong neurological exposure to specific drugs or parts thereof,[citation needed] are lesser known.
 The word ""adsorption"" was coined in 1881 by German physicist Heinrich Kayser (1853–1940).[8]"
Aeration,Chemistry,1,"Aeration (also called aerification or aeriation) is the process by which air is circulated through, mixed with or dissolved in a liquid.
"
Alcohol,Chemistry,1,"
 
 In chemistry, alcohol is an organic compound that carries at least one hydroxyl functional group (−OH) bound to a saturated carbon atom.[2] The term alcohol originally referred to the primary alcohol ethanol (ethyl alcohol), which is used as a drug and is the main alcohol present in alcoholic beverages. An important class of alcohols, of which methanol and ethanol are the simplest members, includes all compounds for which the general formula is CnH2n+1OH. Simple monoalcohols that are the subject of this article include primary (RCH2OH), secondary (R2CHOH) and tertiary (R3COH) alcohols.
 The suffix -ol appears in the IUPAC chemical name of all substances where the hydroxyl group is the functional group with the highest priority. When a higher priority group is present in the compound, the prefix hydroxy- is used in its IUPAC name. The suffix -ol in non-IUPAC names (such as paracetamol or cholesterol) also typically indicates that the substance is an alcohol. However, many substances that contain hydroxyl functional groups (particularly sugars, such as glucose and sucrose) have names which include neither the suffix -ol, nor the prefix hydroxy-. 
"
Aldehyde,Chemistry,1,"Aldehydes, which are generally created by removing a hydrogen from an alcohol, are common in organic chemistry; the most well-known is formaldehyde.  As they are frequently strongly scented, many fragrances are or contain aldehydes. 
 Chemically, an aldehyde /ˈældɪhaɪd/ is a compound containing a functional group with the structure −CHO, consisting of a carbonyl center (a carbon double-bonded to oxygen) with the carbon atom also bonded to hydrogen and to any generic alkyl or side chain R group,[1]. The functional group itself (i.e. without the ""R"" side chain) is known as an aldehyde or formyl group.  
"
Alkali_metal,Chemistry,1,"
 The alkali metals consist of the chemical elements lithium (Li), sodium (Na), potassium (K),[note 1] rubidium (Rb), caesium (Cs),[note 2] and francium (Fr). Together with hydrogen they constitute group 1,[note 3] which lies in the s-block of the periodic table. All alkali metals have their outermost electron in an s-orbital: this shared electron configuration results in their having very similar characteristic properties.[note 4] Indeed, the alkali metals provide the best example of group trends in properties in the periodic table, with elements exhibiting well-characterised homologous behaviour. This family of elements is also known as the lithium family after its leading element.
 Legend
 The alkali metals are all shiny, soft, highly reactive metals at standard temperature and pressure and readily lose their outermost electron to form cations with charge +1. They can all be cut easily with a knife due to their softness, exposing a shiny surface that tarnishes rapidly in air due to oxidation by atmospheric moisture and oxygen (and in the case of lithium, nitrogen). Because of their high reactivity, they must be stored under oil to prevent reaction with air, and are found naturally only in salts and never as the free elements. Caesium, the fifth alkali metal, is the most reactive of all the metals. All the alkali metals react with water, with the heavier alkali metals reacting more vigorously than the lighter ones.
 All of the discovered alkali metals occur in nature as their compounds: in order of abundance, sodium is the most abundant, followed by potassium, lithium, rubidium, caesium, and finally francium, which is very rare due to its extremely high radioactivity; francium occurs only in minute traces in nature as an intermediate step in some obscure side branches of the natural decay chains. Experiments have been conducted to attempt the synthesis of ununennium (Uue), which is likely to be the next member of the group; none was successful. However, ununennium may not be an alkali metal due to relativistic effects, which are predicted to have a large influence on the chemical properties of superheavy elements; even if it does turn out to be an alkali metal, it is predicted to have some differences in physical and chemical properties from its lighter homologues.
 Most alkali metals have many different applications. One of the best-known applications of the pure elements is the use of rubidium and caesium in atomic clocks, of which caesium atomic clocks form the basis of the second. A common application of the compounds of sodium is the sodium-vapour lamp, which emits light very efficiently. Table salt, or sodium chloride, has been used since antiquity. Lithium finds use as a psychiatric medication and as an anode in lithium batteries. Sodium and potassium are also essential elements, having major biological roles as electrolytes, and although the other alkali metals are not essential, they also have various effects on the body, both beneficial and harmful.
"
Alkaline_earth_metal,Chemistry,1,"The alkaline earth metals are six chemical elements in group 2 of the periodic table. They are beryllium (Be), magnesium (Mg), calcium (Ca), strontium (Sr), barium (Ba), and radium (Ra).[1] The elements have very similar properties: they are all shiny, silvery-white, somewhat reactive metals at standard temperature and pressure.[2] Legend
 Structurally, they (together with helium) have in common an outer s-orbital which is full;[2][3][4]
that is, this orbital contains its full complement of two electrons, which the alkaline earth metals readily lose to form cations with charge +2, and an oxidation state of +2.[5] All the discovered alkaline earth metals occur in nature, although radium occurs only through the decay chain of uranium and thorium and not as a primordial element.[6] There have been experiments, all unsuccessful, to try to synthesize element 120, the next potential member of the group.
"
Alkane,Chemistry,1,"
 In organic chemistry, an alkane, or paraffin (a historical name that also has other meanings), is an acyclic saturated hydrocarbon. In other words, an alkane consists of hydrogen and carbon atoms arranged in a tree structure in which all the carbon–carbon bonds are single.[1] Alkanes have the general chemical formula CnH2n+2. The alkanes range in complexity from the simplest case of methane (CH4), where n = 1 (sometimes called the parent molecule), to arbitrarily large and complex molecules, like pentacontane (C50H102) or 6-ethyl-2-methyl-5-(1-methylethyl) octane, an isomer of tetradecane (C14H30).
 International Union of Pure and Applied Chemistry defines alkanes as ""acyclic branched or unbranched hydrocarbons having the general formula CnH2n+2, and therefore consisting entirely of hydrogen atoms and saturated carbon atoms"". However, some sources use the term to denote any saturated hydrocarbon, including those that are either monocyclic (i.e. the cycloalkanes) or polycyclic,[2] despite their having a distinct general formula (i.e. cycloalkanes are CnH2n).
 In an alkane, each carbon atom is sp3-hybridized with 4 sigma bonds (either C–C or C–H), and each hydrogen atom is joined to one of the carbon atoms (in a C–H bond). The longest series of linked carbon atoms in a molecule is known as its carbon skeleton or carbon backbone. The number of carbon atoms may be considered as the size of the alkane.
 One group of the higher alkanes are waxes, solids at standard ambient temperature and pressure (SATP), for which the number of carbon atoms in the carbon backbone is greater than about 17.
With their repeated –CH2 units, the alkanes constitute a homologous series of organic compounds in which the members differ in molecular mass by multiples of 14.03 u (the total mass of each such methylene-bridge unit, which comprises a single carbon atom of mass 12.01 u and two hydrogen atoms of mass ~1.01 u each).
 Methane is produced by methanogenic bacteria and some long-chain alkanes function as pheromones in certain animal species or as protective waxes in plants and fungi. Nevertheless, most alkanes do not have much biological activity.  They can be viewed as molecular trees upon which can be hung the more active/reactive functional groups of biological molecules.
 The alkanes have two main commercial sources: petroleum (crude oil)[3] and natural gas.
 An alkyl group is an alkane-based molecular fragment that bears one open valence for bonding. They are generally abbreviated with the symbol for any organyl group, R, although Alk is sometimes used to specifically symbolize an alkyl group (as opposed to an alkenyl group or aryl group).
"
Alkene,Chemistry,1,"
 In chemistry, an alkene is a hydrocarbon that contains a carbon–carbon double bond.[1] The term is often used as synonym of olefin, that is, any hydrocarbon containing one or more double bonds.[2] However, the IUPAC recommends using the name ""alkene"" only for acyclic hydrocarbons with just one double bond; alkadiene, alkatriene, etc., or polyene for acyclic hydrocarbons with two or more double bonds; cycloalkene, cycloalkadiene, etc. for cyclic ones; and ""olefin"" for the general class — cyclic or acyclic, with one or more double bonds.[3][4][5] Acyclic alkenes, with only one double bond and no other functional groups (also known as mono-enes) form a homologous series of hydrocarbons with the general formula CnH2n with n being 2 or more (which is two hydrogens less than the corresponding alkane). When n is four or more, there are multiple isomers with this formula, distinguished by the position and conformation of the double bond.
 Alkenes are generally colorless apolar compounds, somewhat similar to alkanes but more reactive. The first few members of the series are gases or liquids at room temperature.  The simplest alkene, ethylene (C2H4) (or ""ethene"" in the IUPAC nomenclature) is the organic compound produced on the largest scale industrially.[6] Aromatic compounds are often drawn as cyclic alkenes, but their structure and properties are sufficiently distinct that they are not classified as alkenes or olefins.[4]  Hydrocarbons with two overlapping double bonds (C=C=C) are called allenes after the simplest such compound, and those with three or more overlapping bonds (C=C=C=C, C=C=C=C=C, etc.) are called cumulenes.  Some authors do not consider allenes and cumulenes to be ""alkenes"".
"
Alkyl_group,Chemistry,1,"In organic chemistry, an alkyl substituent is an alkane missing one hydrogen.[1]
The term alkyl is intentionally unspecific to include many possible substitutions. 
An acyclic alkyl has the general formula of CnH2n+1. A cycloalkyl is derived from a cycloalkane by removal of a hydrogen atom from a ring and has the general formula CnH2n-1.[2]
Typically an alkyl is a part of a larger molecule. In structural formula, the symbol R is used to designate a generic (unspecified) alkyl group. The smallest alkyl group is methyl, with the formula CH3−.
[3]"
Alkyne,Chemistry,1,"In organic chemistry, an alkyne is an unsaturated hydrocarbon containing at least one carbon—carbon triple bond.[1] The simplest acyclic alkynes with only one triple bond and no other functional groups form a homologous series with the general chemical formula CnH2n−2. Alkynes are traditionally known as acetylenes, although the name acetylene also refers specifically to C2H2, known formally as ethyne using IUPAC nomenclature. Like other hydrocarbons, alkynes are generally hydrophobic.
"
Allomer,Chemistry,1,"Allomerism is the similarity in the crystalline structure of substances of different chemical composition.
"
Allotrope,Chemistry,1,"Allotropy or allotropism (from Ancient Greek  ἄλλος (allos) 'other', and  τρόπος (tropos) 'manner, form') is the property of some chemical elements to exist in two or more different forms, in the same physical state, known as allotropes of the elements. Allotropes are different structural modifications of an element;[1] the atoms of the element are bonded together in a different manner. For example, the allotropes of carbon include diamond (the carbon atoms are bonded together in a tetrahedral lattice arrangement), graphite (the carbon atoms are bonded together in sheets of a hexagonal lattice), graphene (single sheets of graphite), and fullerenes (the carbon atoms are bonded together in spherical, tubular, or ellipsoidal formations). The term allotropy is used for elements only, not for compounds. The more general term, used for any crystalline material, is polymorphism. Allotropy refers only to different forms of an element within the same phase (i.e.:  solid, liquid or gas states); the differences between these states would not alone constitute examples of allotropy.
 For some elements, allotropes have different molecular formulae despite difference in phase; for example, two allotropes of oxygen (dioxygen, O2, and ozone, O3) can both exist in the solid, liquid and gaseous states. Other elements do not maintain distinct allotropes in different phases; for example, phosphorus has numerous solid allotropes, which all revert to the same P4 form when melted to the liquid state.
"
Alloy,Chemistry,1,"
 An alloy is a combination of metals or metals combined with one or more other elements. For example, combining the metallic elements gold and copper produces red gold, gold and silver becomes white gold, and silver combined with copper produces sterling silver. Elemental iron, combined with non-metallic carbon or silicon, produces alloys called steel or silicon steel. The resulting mixture forms a substance with properties that often differ from those of the pure metals, such as increased strength or hardness. Unlike other substances that may contain metallic bases but do not behave as metals, such as aluminium oxide (sapphire), beryllium aluminium silicate (emerald) or sodium chloride (salt), an alloy will retain all the properties of a metal in the resulting material, such as electrical conductivity, ductility, opaqueness, and luster. Alloys are used in a wide variety of applications, from the steel alloys, used in everything from buildings to automobiles to surgical tools, to exotic titanium-alloys used in the aerospace industry, to beryllium-copper alloys for non-sparking tools. In some cases, a combination of metals may reduce the overall cost of the material while preserving important properties. In other cases, the combination of metals imparts synergistic properties to the constituent metal elements such as corrosion resistance or mechanical strength. Examples of alloys are steel, solder, brass, pewter, duralumin, bronze and amalgams.
 An alloy may be a solid solution of metal elements (a single phase, where all metallic grains (crystals) are of the same composition) or a mixture of metallic phases (two or more solutions, forming a microstructure of different crystals within the metal). Intermetallic compounds are alloys with a defined stoichiometry and crystal structure. Zintl phases are also sometimes considered alloys depending on bond types (see Van Arkel–Ketelaar triangle for information on classifying bonding in binary compounds).
 Alloys are defined by a metallic bonding character.[1] The alloy constituents are usually measured by mass percentage for practical applications, and in atomic fraction for basic science studies. Alloys are usually classified as substitutional or interstitial alloys, depending on the atomic arrangement that forms the alloy. They can be further classified as homogeneous (consisting of a single phase), or heterogeneous (consisting of two or more phases) or intermetallic.
"
Amalgam_(chemistry),Chemistry,1,"An amalgam is an alloy of mercury with another metal. It may be a liquid, a soft paste or a solid, depending upon the proportion of mercury. These alloys are formed through metallic bonding,[1] with the electrostatic attractive force of the conduction electrons working to bind all the positively charged metal ions together into a crystal lattice structure.[2] Almost all metals can form amalgams with mercury, the notable exceptions being iron, platinum, tungsten, and tantalum. Silver-mercury amalgams are important in dentistry, and gold-mercury amalgam is used in the extraction of gold from ore. Dentistry has used alloys of mercury  with metals such as silver, copper, indium, tin and zinc.
"
Amount_of_substance,Chemistry,1,"In chemistry, the amount of substance in a given sample of matter is defined as the number of discrete atomic-scale particles in it divided by the Avogadro constant NA. In a truly atomistic view, the amount of substance is simply the number of particles that constitute the substance.[1][2][3] The particles or entities may be molecules, atoms, ions, electrons, or other, depending on the context. The value of the Avogadro constant NA has been defined as 6.02214076×1023 mol−1. In the truly atomistic view, 1 mol = 6.02214076×1023 particles (the Avogadro number) [4] and therefore the conversion constant is simply NA = 1.[3] The amount of substance is sometimes referred to as the chemical amount.
 The mole (symbol: mol) is a unit of amount of substance in the International System of Units, defined (since 2019) by fixing the Avogadro constant at the given value.  Historically, the mole was defined as the amount of substance in 12 grams of the carbon-12 isotope. As a consequence, the mass of one mole of a chemical compound, in grams, is numerically equal (for all practical purposes) to the mass of one molecule of the compound, in daltons, and the molar mass of an isotope in grams per mole is equal to the mass number. For example, a molecule of water has a mass of about 18.015  daltons on average, whereas a mole of water (which contains 6.02214076×1023 water molecules) has a total mass of about 18.015 grams.
 In chemistry, because of the law of multiple proportions, it is often much more convenient to work with amounts of substances (that is, number of moles or of molecules) than with masses (grams) or volumes (liters).  For example, the chemical fact ""1 molecule of oxygen (O2) will react with 2 molecules of hydrogen (H2) to make 2 molecules of water (H2O)"" can also be stated as ""1 mole of O2 will react with 2 moles of H2 to form 2 moles of water"". The same chemical fact, expressed in terms of masses, would be ""32 g  (1 mole) of oxygen  will react with approximately 4.0304 g (2 moles of H2) hydrogen to make approximately 36.0304 g (2 moles) of water"" (and the numbers would depend on the isotopic composition of the reagents). In terms of volume, the numbers would depend on the pressure and temperature of the reagents and products.  For the same reasons, the concentrations of reagents and products in solution are often specified in moles per liter, rather than grams per liter.
 The amount of substance is also a convenient concept in thermodynamics.  For example, the pressure of a certain quantity of a noble gas in a recipient of a given volume, at a given temperature, is directly related to the number of molecules in the gas (through the ideal gas law), not to its mass.
 This technical sense of the term ""amount of substance"" should not be confused with the general sense of ""amount"" in the English language. The latter may refer to other measurements such as mass or volume,[5] rather than the number of particles. There are proposals to replace ""amount of substance"" with more easily-distinguishable terms, such as enplethy[6]  and stoichiometric amount.[5] The IUPAC recommends that ""amount of substance"" should be used instead of ""number of moles"", just as the quantity mass should not be called ""number of kilograms"".[7]"
Analyte,Chemistry,1,"
 An analyte, component (in clinical chemistry), or chemical species is a substance or chemical constituent that is of interest in an analytical procedure. The purest substances are referred to as analytes. Example : 24 karat gold, NaCl, water, etc. In reality, no substance has been found to be 100% pure in its quality, so we call a substance that is found to be most pure (for some metals, 99% after electrolysis) an analyte.[1]"
Analytical_chemistry,Chemistry,1,"Analytical chemistry studies and uses instruments and methods used to separate, identify, and quantify matter.[1] In practice, separation, identification or quantification may constitute the entire analysis or be combined with another method. Separation isolates analytes. Qualitative analysis identifies analytes, while quantitative analysis determines the numerical amount or concentration.
 Analytical chemistry consists of classical, wet chemical methods and modern, instrumental methods.[2] Classical qualitative methods use separations such as precipitation, extraction, and distillation. Identification may be based on differences in color, odor, melting point, boiling point, radioactivity or reactivity. Classical quantitative analysis uses mass or volume changes to quantify amount. Instrumental methods may be used to separate samples using chromatography, electrophoresis or field flow fractionation. Then qualitative and quantitative analysis can be performed, often with the same instrument and may use light interaction,  heat interaction, electric fields or magnetic fields. Often the same instrument can separate, identify and quantify an analyte.
 Analytical chemistry is also focused on improvements in experimental design, chemometrics, and the creation of new measurement tools. Analytical chemistry has broad applications to medicine, science and engineering.
"
Anion,Chemistry,1,"
 An ion (/ˈaɪɒn, -ən/)[1] is an particle,atom or molecule with a net electrical charge. 
 The charge of the electron is considered negative by convention.  The negative charge of an ion is equal and opposite to charged proton(s) considered positive by convention.  The net charge of an ion is non-zero due to its total number of electrons being unequal to its total number of protons. 
 A cation is a positively charged ion, with fewer electrons than protons, while an anion is negatively charged, with more electrons than protons. Because of their opposite electric charges, cations and anions attract each other and readily form ionic compounds.
 Ions consisting of only a single atom are termed atomic or monatomic ions, while two or more atoms form molecular ions or polyatomic ions. In the case of physical ionization in a fluid (gas or liquid), ""ion pairs"" are created by spontaneous molecule collisions, where each generated pair consists of a free electron and a positive ion.[2] Ions are also created by chemical interactions, such as the dissolution of a salt in liquids, or by other means, such as passing a direct current through a conducting solution, dissolving an anode via ionization.
"
Anode,Chemistry,1,"
 An anode is an electrode through which the conventional current enters into a polarized electrical device.  This contrasts with a cathode, an electrode through which conventional current leaves an electrical device.  A common mnemonic is ACID, for ""anode current into device"".[1] The direction of conventional current (the flow of positive charges) in a circuit is opposite to the direction of electron flow, so (negatively charged) electrons flow out the anode of a galvanic cell, into the outside circuit. In both a galvanic cell and an electrolytic cell, the anode is the electrode at which the oxidation reaction occurs.
 In an electrolytic cell, the anode is the wire or plate having excess positive charge.[2] Consequently, anions will tend to move towards the anode where they can undergo oxidation.
 Historically, the anode has also been known as the zincode.
"
Aqueous_solution,Chemistry,1,"An aqueous solution is a solution in which the solvent is water. It is mostly shown in chemical equations by appending (aq) to the relevant chemical formula. For example, a solution of table salt, or sodium chloride (NaCl), in water would be represented as Na+(aq) + Cl−(aq). The word aqueous (which comes from aqua) means pertaining to, related to, similar to, or dissolved in, water. As water is an excellent solvent and is also naturally abundant, it is a ubiquitous solvent in chemistry. Aqueous solution is water with a pH of 7.0 where the hydrogen ions (H+) and hydroxide ions (OH−) are in Arrhenius balance (10−7).
 A non-aqueous solution is a solution in which the solvent is a liquid, but is not water.[1] (See also Solvent and Inorganic nonaqueous solvent.)
 Substances that are hydrophobic ('water-fearing') do not dissolve well in water, whereas those that are hydrophilic ('water-friendly') do. An example of a hydrophilic substance is sodium chloride. Acids and bases are aqueous solutions, as part of their Arrhenius definitions.
 The ability of a substance to dissolve in water is determined by whether the substance can match or exceed the strong attractive forces that water molecules generate between themselves. If the substance lacks the ability to dissolve in water, the molecules form a precipitate.
 Reactions in aqueous solutions are usually metathesis reactions. Metathesis reactions are another term for double-displacement; that is, when a cation displaces to form an ionic bond with the other anion. The cation bonded with the latter anion will dissociate and bond with the other anion.
 Aqueous solutions that conduct electric current efficiently contain strong electrolytes, while ones that conduct poorly are considered to have weak electrolytes. Those strong electrolytes are substances that are completely ionized in water, whereas the weak electrolytes exhibit only a small degree of ionization in water.
 Nonelectrolytes are substances that dissolve in water yet maintain their molecular integrity (do not dissociate into ions). Examples include sugar, urea, glycerol, and methylsulfonylmethane (MSM).
 When writing the equations of aqueous reactions, it is essential to determine the precipitate. To determine the precipitate, one must consult a chart of solubility. Soluble compounds are aqueous, while insoluble compounds are the precipitate.  There may not always be a precipitate.
 When performing calculations regarding the reacting of one or more aqueous solutions, in general one must know the concentration, or molarity, of the aqueous solutions. Solution concentration is given in terms of the form of the solute prior to it dissolving.
 Aqueous solutions may contain, especially in alkaline zone or subjected to radiolysis, hydrated atomic hydrogen and hydrated electrons.
"
Aromaticity,Chemistry,1,"In chemistry, aromaticity is a property of cyclic (ring-shaped), planar (flat) structures with pi bonds in resonance (those containing delocalized electrons) that gives increased stability compared to other geometric or connective arrangements with the same set of atoms. Aromatic rings are very stable and do not break apart easily. Organic compounds that are not aromatic are classified as aliphatic compounds—they might be cyclic, but only aromatic rings have enhanced stability.
 Since the most common aromatic compounds are derivatives of benzene (an aromatic hydrocarbon common in petroleum and its distillates), the word aromatic occasionally refers informally to benzene derivatives, and so it was first defined. Nevertheless, many non-benzene aromatic compounds exist. In living organisms, for example, the most common aromatic rings are the double-ringed bases in RNA and DNA. An aromatic functional group or other substituent is called an aryl group.
 The earliest use of the term aromatic was in an article by August Wilhelm Hofmann in 1855.[1]  Hofmann used the term for a class  of benzene compounds, many of which have odors (aromas), unlike pure saturated hydrocarbons. Aromaticity as a chemical property bears no general relationship with the olfactory properties of such compounds (how they smell), although in 1855, before the structure of benzene or organic compounds was understood, chemists like Hofmann were beginning to understand that odiferous molecules from plants, such as terpenes, had chemical properties that we recognize today are similar to unsaturated petroleum hydrocarbons like benzene.
 In terms of the electronic nature of the molecule, aromaticity describes a conjugated system often made of alternating single and double bonds in a ring. This configuration allows for the electrons in the molecule's pi system to be delocalized around the ring, increasing the molecule's stability. The molecule cannot be represented by one structure, but rather a resonance hybrid of different structures, such as with the two resonance structures of benzene. These molecules cannot be found in either one of these representations, with the longer single bonds in one location and the shorter double bond in another (see Theory below). Rather, the molecule exhibits bond lengths in between those of single and double bonds. This commonly seen model of aromatic rings, namely the idea that benzene was formed from a six-membered carbon ring with alternating single and double bonds (cyclohexatriene), was developed by August Kekulé (see History below). The model for benzene consists of two resonance forms, which corresponds to the double and single bonds superimposing to produce six one-and-a-half bonds.  Benzene is a more stable molecule than would be expected without accounting for charge delocalization.
"
Arrow_pushing,Chemistry,1,"Arrow pushing or electron pushing is a technique used to describe the progression of organic chemistry reaction mechanisms.[1]  It was first developed by Sir Robert Robinson.[2] In using arrow pushing, ""curved arrows"" or ""curly arrows"" are superimposed over the structural formulae of reactants in a chemical equation to show the reaction mechanism. The arrows illustrate the movement of electrons as bonds between atoms are broken and formed.  Arrow pushing is also used to describe how positive and negative charges are distributed around organic molecules through resonance. It is important to remember, however, that arrow pushing is a formalism and electrons (or rather, electron density) do not move around so neatly and discretely in reality.
 Recently, arrow pushing has been extended to inorganic chemistry, especially to the chemistry of s- and p-block elements. It has been shown to work well for hypervalent compounds.[3]"
Aryl,Chemistry,1,"In the context of organic molecules, aryl is any functional group or substituent derived from an aromatic ring, usually an aromatic hydrocarbon, such as phenyl and naphthyl.[1] ""Aryl"" is used for the sake of abbreviation or generalization, and ""Ar"" is used as a placeholder for the aryl group in chemical structure diagrams, analogous to “R” used for any organic substituent. “Ar” is not to be confused with the elemental symbol for argon.
 A simple aryl group is phenyl (with the chemical formula C6H5), a group derived from benzene. Examples of other aryl groups consist of:
 Arylation is the process in which an aryl group is attached to a substituent.  It is typically achieved by cross-coupling reactions.
"
Atom,Chemistry,1,"
 An atom is the smallest unit of ordinary matter that forms a chemical element. Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms. Atoms are extremely small, typically around 100 picometers across. They are so small that accurately predicting their behavior using classical physics—as if they were tennis balls, for example—is not possible due to quantum effects.
 Every atom is composed of a nucleus and one or more electrons bound to the nucleus. The nucleus is made of one or more protons and a number of neutrons. Only the most common variety of hydrogen has no neutrons. More than 99.94% of an atom's mass is in the nucleus. The protons have a positive electric charge, the electrons have a negative electric charge, and the neutrons have no electric charge. If the number of protons and electrons are equal, then the atom is electrically neutral. If an atom has more or fewer electrons than protons, then it has an overall negative or positive charge, respectively – such atoms are called ions.
 The electrons of an atom are attracted to the protons in an atomic nucleus by the electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by the nuclear force. This force is usually stronger than the electromagnetic force that repels the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force. In this case, the nucleus splits and leaves behind different elements. This is a form of nuclear decay.
 The number of protons in the nucleus is the atomic number and it defines to which chemical element the atom belongs. For example, any atom that contains 29 protons is copper. The number of neutrons defines the isotope of the element. Atoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules or crystals. The ability of atoms to associate and dissociate is responsible for most of the physical changes observed in nature. Chemistry is the discipline that studies these changes.
"
Atomic_mass,Chemistry,1,"The atomic mass (ma or m) is the mass of an atom. Although the SI unit of mass is kilogram (symbol: kg), the atomic mass is often expressed in the non-SI unit dalton (symbol: Da, or u) where 1 dalton is defined as ​1⁄12 of the mass of a single carbon-12 atom, at rest.[1] The protons and neutrons of the nucleus account for nearly all of the total mass of atoms, with the electrons and nuclear binding energy making minor contributions. Thus, the numeric value of the atomic mass when expressed in daltons has nearly the same value as the mass number. Conversion between mass in kilograms and mass in daltons can be done using the atomic mass constant 




m


u



=



m
(





12


C


)


12



=
1
 


D
a




{  m_{\rm {u}}={{m({\rm {^{12}C}})} \over {12}}=1\ {\rm {Da}}}
.
 The formula used for conversion is:[2][3] where 




M


u





{  M_{\rm {u}}}
 is the molar mass constant, 




N


A





{  N_{\rm {A}}}
 is the Avogadro constant and 



M

(

12



C

)


{  M(^{12}\mathrm {C} )}
 is the experimentally determined molar mass of carbon-12.
 The relative isotopic mass (see section below) can be obtained by dividing the atomic mass ma of an isotope by the atomic mass constant mu yielding a dimensionless value. Thus, the atomic mass of a carbon-12 atom is 12 Da, but the relative isotopic mass of a carbon-12 atom is simply 12. The sum of relative isotopic masses of all atoms in a molecule is the relative molecular mass.
 The atomic mass of an isotope and the relative isotopic mass refers to a certain specific isotope of an element. Because substances are usually not isotopically pure, it is convenient to use the elemental atomic mass which is the average (mean) atomic mass of an element, weighted by the abundance of the isotopes. The dimensionless (standard) atomic weight is the weighted mean relative isotopic mass of a (typical naturally-occurring) mixture of isotopes.
 The atomic mass of atoms, ions, or atomic nuclei is slightly less than the sum of the masses of their constituent protons, neutrons, and electrons, due to binding energy mass loss (as per E = mc2).
"
Atomic_mass_unit,Chemistry,1,"The dalton or unified atomic mass unit (symbols: Da or u) is a unit of mass widely used in physics and chemistry. It is defined as 1/12 of the mass of an unbound neutral atom of carbon-12 in its nuclear and electronic ground state and at rest.[1][2] The atomic mass constant, denoted mu  is defined identically, giving mu = m(12C)/12 = 1 Da.[3] This unit is commonly used in physics and chemistry to express the mass of atomic-scale objects, such as atoms, molecules, and elementary particles, both for discrete instances and multiple types of ensemble averages. For example, an atom of helium-4 has a mass of 4.0026 Da. This is an intrinsic property of the isotope and all helium-4 have the same mass. Acetylsalicylic acid (aspirin), C9H8O4, has an average mass of approximately 180.157 Da. However, there are no acetylsalicylic acid molecules with this mass. The two most common masses of individual acetylsalicylic acid molecules are 180.04228 Da and 181.04565 Da. 
 The molecular masses of proteins, nucleic acids, and other large polymers are often expressed with the units kilodaltons (kDa), megadaltons (MDa), etc.[4] Titin, one of the largest known proteins, has a molecular mass of between 3 and 3.7 megadaltons.[5] The DNA of chromosome 1 in the human genome has about 249 million base pairs, each with an average mass of about 650 Da, or 156 GDa total.[6] The mole is a unit of amount of substance, widely used in chemistry and physics, which was originally defined so that the mass of one mole of a substance, measured in grams, would be numerically equal to the average mass of one of its constituent particles, measured in daltons. That is, the molar mass of a chemical compound was meant to be numerically equal to its average molecular mass.  For example, the average mass of one molecule of water is about 18.0153 daltons, and one mole of water is about 18.0153 grams.  A protein whose molecule has an average mass of 64 kDa would have a molar mass of 64 kg/mol. However, while this equality can be assumed for almost all practical purposes, it is now only approximate, because of the way mole was redefined on 20 May 2019.[4][1] In general, the mass in daltons of an atom is numerically close, but not exactly equal to the number of nucleons A contained in its nucleus. It follows that the molar mass of a compound (grams per mole) is numerically close to the average number of nucleons contained in each molecule. By definition, the mass of an atom of carbon-12 is 12 daltons, which corresponds with the number of nucleons that it has (6 protons and 6 neutrons). However, the mass of an atomic-scale object is affected by the binding energy of the nucleons in its atomic nuclei, as well as the mass and binding energy of its electrons.  Therefore, this equality holds only for the carbon-12 atom in the stated conditions, and will vary for other substances. For example, the mass of one unbound atom of the common hydrogen isotope (hydrogen-1, protium) is 1.007825032241(94) Da, the mass of one free neutron is 1.00866491595(49) Da,[7] and the mass of one hydrogen-2 (deuterium) atom is 2.014101778114(122) Da.[8]  In general, the difference (mass defect) is less than 0.1%; exceptions include hydrogen-1 (about 0.8%), helium-3 (0.5%), lithium (0.25%) and beryllium (0.15%). 
 The unified atomic mass unit and the dalton should not be confused with the unit of mass in the atomic units systems, which is instead the electron rest mass (me).
"
Atomic_number,Chemistry,1,"
 The atomic number or proton number (symbol Z) of a chemical element is the number of protons found in the nucleus of every atom of that element. The atomic number uniquely identifies a chemical element. It is identical to the charge number of the nucleus. In an uncharged atom, the atomic number is also equal to the number of electrons.
 The sum of the atomic number Z and the number of neutrons N gives the mass number A of an atom. Since protons and neutrons have approximately the same mass (and the mass of the electrons is negligible for many purposes) and the mass defect of nucleon binding is always small compared to the nucleon mass, the atomic mass of any atom, when expressed in unified atomic mass units (making a quantity called the ""relative isotopic mass""), is within 1% of the whole number A.
 Atoms with the same atomic number but different neutron numbers, and hence different mass numbers, are known as isotopes. A little more than three-quarters of naturally occurring elements exist as a mixture of isotopes (see monoisotopic elements), and the average isotopic mass of an isotopic mixture for an element (called the relative atomic mass) in a defined environment on Earth, determines the element's standard atomic weight. Historically, it was these atomic weights of elements (in comparison to hydrogen) that were the quantities measurable by chemists in the 19th century.
 The conventional symbol Z comes from the German word Zahl meaning number, which, before the modern synthesis of ideas from chemistry and physics, merely denoted an element's numerical place in the periodic table, whose order is approximately, but not completely, consistent with the order of the elements by atomic weights. Only after 1915, with the suggestion and evidence that this Z number was also the nuclear charge and a physical characteristic of atoms, did the word Atomzahl (and its English equivalent atomic number) come into common use in this context.
"
Atomic_orbital,Chemistry,1,"In atomic theory and quantum mechanics, an atomic orbital is a mathematical function describing the location and wave-like behavior of an electron in an atom.[1] This function can be used to calculate the probability of finding any electron of an atom in any specific region around the atom's nucleus. The term atomic orbital may also refer to the physical region or space where the electron can be calculated to be present, as predicted by the particular mathematical form of the orbital.[2] Each orbital in an atom is characterized by a unique set of values of the three quantum numbers n, ℓ, and m,[dubious  – discuss] which respectively correspond to the electron's energy, angular momentum, and an angular momentum vector component (the magnetic quantum number). Each such orbital can be occupied by a maximum of two electrons, each with its own spin quantum number s. The simple names s orbital, p orbital, d orbital, and f orbital refer to orbitals with angular momentum quantum number ℓ = 0, 1, 2, and 3 respectively. These names, together with the value of n, are used to describe the electron configurations of atoms. They are derived from the description by early spectroscopists of certain series of alkali metal spectroscopic lines as sharp, principal, diffuse, and fundamental. Orbitals for ℓ > 3 continue alphabetically, omitting j (g, h, i, k, ...)[3][4][5] because some languages do not distinguish between the letters ""i"" and ""j"".[6] Atomic orbitals are the basic building blocks of the atomic orbital model (alternatively known as the electron cloud or wave mechanics model), a modern framework for visualizing the submicroscopic behavior of electrons in matter. In this model the electron cloud of a multi-electron atom may be seen as being built up (in approximation) in an electron configuration that is a product of simpler hydrogen-like atomic orbitals. The repeating periodicity of the blocks of 2, 6, 10, and 14 elements within sections of the periodic table arises naturally from the total number of electrons that occupy a complete set of s, p, d, and f atomic orbitals, respectively, although for higher values of the quantum number n, particularly when the atom in question bears a positive charge, the energies of certain sub-shells become very similar and so the order in which they are said to be populated by electrons (e.g. Cr = [Ar]4s13d5 and Cr2+ = [Ar]3d4) can only be rationalized somewhat arbitrarily.
"
Atomic_radius,Chemistry,1,"The atomic radius of a chemical element is a measure of the size of its atoms, usually the mean or typical distance from the center of the nucleus to the boundary of the surrounding shells of electrons.  Since the boundary is not a well-defined physical entity, there are various non-equivalent definitions of atomic radius. Three widely used definitions of atomic radius are:  Van der Waals radius, ionic radius, and covalent radius.
 Depending on the definition, the term may apply only to isolated atoms, or also to atoms in condensed matter, covalently bonding in molecules, or in ionized and excited states; and its value may be obtained through experimental measurements, or computed from theoretical models. The value of the radius may depend on the atom's state and context.[1] Electrons do not have definite orbits, or sharply defined ranges. Rather, their positions must be described as probability distributions that taper off gradually as one moves away from the nucleus, without a sharp cutoff; these are referred to as atomic orbitals or electron clouds. Moreover, in condensed matter and molecules, the electron clouds of the atoms usually overlap to some extent, and some of the electrons may roam over a large region encompassing two or more atoms.
 Under most definitions the radii of isolated neutral atoms range between 30 and 300 pm (trillionths of a meter), or between 0.3 and 3 ångströms. Therefore, the radius of an atom is more than 10,000 times the radius of its nucleus (1–10 fm),[2] and less than 1/1000 of the wavelength of visible light (400–700 nm).
 For many purposes, atoms can be modeled as spheres. This is only a crude approximation, but it can provide quantitative explanations and predictions for many phenomena, such as the density of liquids and solids, the diffusion of fluids through molecular sieves, the arrangement of atoms and ions in crystals, and the size and shape of molecules.[citation needed] Atomic radii vary in a predictable and explicable manner across the periodic table. For instance, the radii generally decrease along each period (row) of the table, from the alkali metals to the noble gases; and increase down each group (column).  The radius increases sharply between the noble gas at the end of each period and the alkali metal at the beginning of the next period.  These trends of the atomic radii (and of various other chemical and physical properties of the elements) can be explained by the electron shell theory of the atom; they provided important evidence for the development and confirmation of quantum theory. The atomic radii decrease across the Periodic Table because as the atomic number increases, the number of protons increases across the period, but the extra electrons are only added to the same quantum shell. Therefore, the effective nuclear charge towards the outermost electrons increases, drawing the outermost electrons closer. As a result, the electron cloud contracts and the atomic radius decreases.
"
Atomic_weight,Chemistry,1,"Relative atomic mass (symbol: Ar) or atomic weight is a dimensionless physical quantity defined as the ratio of the average mass of atoms of a chemical element in a given sample to the atomic mass constant. The atomic mass constant (symbol: mu) is defined as being 1/12 of the mass of a carbon-12 atom.[1][2] Since both quantities in the ratio are masses, the resulting value is dimensionless; hence the value is said to be relative.
 For a single given sample, the relative atomic mass of a given element is the weighted arithmetic mean of the masses of the individual atoms (including their isotopes) that are present in the sample. This quantity can vary substantially between samples because the sample's origin (and therefore its radioactive history or diffusion history) may have produced unique combinations of isotopic abundances. For example, due to a different mixture of stable carbon-12 and carbon-13 isotopes, a sample of elemental carbon from volcanic methane will have a different relative atomic mass than one collected from plant or animal tissues.
 The more common, and more specific quantity known as standard atomic weight (Ar, standard) is an application of the relative atomic mass values obtained from multiple different samples. It is sometimes interpreted as the expected range of the relative atomic mass values for the atoms of a given element from all terrestrial sources, with the various sources being taken from Earth.[3] ""Atomic weight"" is often loosely and incorrectly used as a synonym for standard atomic weight (incorrectly because standard atomic weights are not from a single sample). Standard atomic weight is nevertheless the most widely published variant of relative atomic mass.
 Additionally, the continued use of the term ""atomic weight"" (for any element) as opposed to ""relative atomic mass"" has attracted considerable controversy since at least the 1960s, mainly due to the technical difference between weight and mass in physics.[4] Still, both terms are officially sanctioned by the IUPAC. The term ""relative atomic mass"" now seems to be replacing ""atomic weight"" as the preferred term, although the term ""standard atomic weight"" (as opposed to the more correct ""standard relative atomic mass"") continues to be used.
"
Average_atomic_mass,Chemistry,1,"Relative atomic mass (symbol: Ar) or atomic weight is a dimensionless physical quantity defined as the ratio of the average mass of atoms of a chemical element in a given sample to the atomic mass constant. The atomic mass constant (symbol: mu) is defined as being 1/12 of the mass of a carbon-12 atom.[1][2] Since both quantities in the ratio are masses, the resulting value is dimensionless; hence the value is said to be relative.
 For a single given sample, the relative atomic mass of a given element is the weighted arithmetic mean of the masses of the individual atoms (including their isotopes) that are present in the sample. This quantity can vary substantially between samples because the sample's origin (and therefore its radioactive history or diffusion history) may have produced unique combinations of isotopic abundances. For example, due to a different mixture of stable carbon-12 and carbon-13 isotopes, a sample of elemental carbon from volcanic methane will have a different relative atomic mass than one collected from plant or animal tissues.
 The more common, and more specific quantity known as standard atomic weight (Ar, standard) is an application of the relative atomic mass values obtained from multiple different samples. It is sometimes interpreted as the expected range of the relative atomic mass values for the atoms of a given element from all terrestrial sources, with the various sources being taken from Earth.[3] ""Atomic weight"" is often loosely and incorrectly used as a synonym for standard atomic weight (incorrectly because standard atomic weights are not from a single sample). Standard atomic weight is nevertheless the most widely published variant of relative atomic mass.
 Additionally, the continued use of the term ""atomic weight"" (for any element) as opposed to ""relative atomic mass"" has attracted considerable controversy since at least the 1960s, mainly due to the technical difference between weight and mass in physics.[4] Still, both terms are officially sanctioned by the IUPAC. The term ""relative atomic mass"" now seems to be replacing ""atomic weight"" as the preferred term, although the term ""standard atomic weight"" (as opposed to the more correct ""standard relative atomic mass"") continues to be used.
"
Avogadro%27s_law,Chemistry,1,"Avogadro's law (sometimes referred to as Avogadro's hypothesis or Avogadro's principle) is an experimental gas law relating the volume of a gas to the amount of substance of gas present.[1] The law is a specific case of the ideal gas law. A modern statement is:
 Avogadro's law states that ""equal volumes of all gases, at the same temperature and pressure, have the same number of molecules.""[1] For a given mass of an ideal gas, the volume and amount (moles) of the gas are directly proportional if the temperature and pressure are constant.
 The law is named after Amedeo Avogadro who, in 1812,[2][3] hypothesized that two given samples of an ideal gas, of the same volume and at the same temperature and pressure, contain the same number of molecules. As an example, equal volumes of molecular hydrogen and nitrogen contain the same number of molecules when they are at the same temperature and pressure, and observe ideal gas behavior. In practice, real gases show small deviations from the ideal behavior and the law holds only approximately, but is still a useful approximation for scientists.
"
Avogadro%27s_number,Chemistry,1,"
 The Avogadro constant (NA[1] or L[2]) is the proportionality factor that relates the number of constituent particles (usually molecules, atoms or ions) in a sample with the amount of substance in that sample.  Its SI unit is the reciprocal mole, and it is defined as NA = 6.02214076×1023 mol−1.[3][1][4][5][6]  It is named after the Italian scientist Amedeo Avogadro.[7]Although this is called Avogadro's constant (or number), he is not the chemist who determined its value. Stanislao Cannizzarro explained this number four years after Avogadro's death while at the Karlsruhe Congress in 1860.[8] The numeric value of the Avogadro constant expressed in reciprocal mole, a dimensionless number, is called the Avogadro number, sometimes denoted N[9][10] or N0,[11][12] which is thus the number of particles that are contained in one mole, exactly 6.02214076×1023.[4][1] The value of the Avogadro constant was chosen so that the mass of one mole of a chemical compound, in grams, is numerically equal (for all practical purposes) to the average mass of one molecule of the compound, in daltons (universal atomic mass units); one dalton being 1/12 of the mass of one carbon-12 atom, which is approximately the mass of one nucleon (proton or neutron). Twelve grams of carbon contains one mole of carbon atoms.  
 For example, the average mass of one molecule of water is about 18.0153 daltons, and one mole of water (N molecules) is about 18.0153 grams. Thus, the Avogadro constant NA is the proportionality factor that relates the molar mass of a substance to the average mass of one molecule, and the Avogadro number is also the approximate number of nucleons in one gram of ordinary matter.[13] The Avogadro constant also relates the molar volume of a substance to the average volume nominally occupied by one of its particles, when both are expressed in the same units of volume.  For example, since the molar volume of water in ordinary conditions is about 18 mL/mol, the volume occupied by one molecule of water is about 18/6.022×10−23 mL, or about 30 Å3 (cubic angstroms).  For a crystalline substance, it similarly relates its molar volume (in mL/mol), the volume of the repeating unit cell of the crystals (in mL), and the number of molecules in that cell.
 The Avogadro number (or constant) has been defined in many different ways through its long history. Its approximate value was first determined, indirectly, by Josef Loschmidt in 1865.[14] (Avogadro's number is closely related to the Loschmidt constant, and the two concepts are sometimes confused.) It was initially defined by Jean Perrin as the number of atoms in 16 grams of oxygen.[7]  It was later redefined in the 14th conference of the International Bureau of Weights and Measures (BIPM) as the number of atoms in 12 grams of the isotope carbon-12 (12C).[15]  In each case, the mole was defined as the quantity of a substance that contained the same number of atoms as those reference samples.  In particular, when carbon-12 was the reference, one mole of carbon-12 was exactly 12 grams of the element.
 These definitions meant that the value of the Avogadro number depended on the experimentally determined value of the mass (in grams) of one atom of those elements, and therefore it was known only to a limited number of decimal digits.  However, in its 26th Conference, the BIPM adopted a different approach: effective 20 May 2019, it defined the Avogadro number as the exact value N = 6.02214076×1023, and redefined the mole as the amount of a substance under consideration that contains N constituent particles of the substance.  Under the new definition, the mass of one mole of any substance (including hydrogen, carbon-12, and oxygen-16) is N times the average mass of one of its constituent particles—a physical quantity whose precise value has to be determined experimentally for each substance.
"
Azeotrope,Chemistry,1,"
 An azeotrope (/əˈziːəˌtroʊp/)[1] or a constant boiling point mixture is a mixture of two or more liquids whose proportions cannot be altered or changed by simple distillation.[2] This happens because when an azeotrope is boiled, the vapour has the same proportions of constituents as the unboiled mixture. Because their composition is unchanged by distillation, azeotropes are also called (especially in older texts) constant boiling point mixtures.
 Some azeotropic mixtures of pairs of compounds are known,[3] and many azeotropes of three or more compounds are also known.[4] In such a case it is not possible to separate the components by fractional distillation. There are two types of azeotropes: minimum boiling azeotrope and maximum boiling azeotrope. A solution that shows greater positive deviation from Raoult's law forms a minimum boiling azeotrope at a specific composition. For example, an ethanol–water mixture (obtained by fermentation of sugars) on fractional distillation yields a solution containing at most 97.2% by volume of ethanol. Once this composition has been achieved, the liquid and vapour have the same composition, and no further separation occurs.
A solution that shows large negative deviation from Raoult's law forms a maximum boiling azeotrope at a specific composition. Nitric acid and water is an example of this class of azeotrope. This azeotrope has an approximate composition of 68% nitric acid and 32% water by mass, with a boiling point of 393.5 K (120.4 °C).
"
Barometer,Chemistry,1,"
 A barometer is a scientific instrument that is used to measure air pressure in a certain environment. Pressure tendency can forecast short term changes in the weather. Many measurements of air pressure are used within surface weather analysis to help find surface troughs, pressure systems and frontal boundaries.
 Barometers and pressure altimeters (the most basic and common type of altimeter) are essentially the same instrument, but used for different purposes. An altimeter is intended to be used at different levels matching the corresponding atmospheric pressure to the altitude, while a barometer is kept at the same level and measures subtle pressure changes caused by weather and elements of weather.  The average atmospheric pressure on the earth's surface varies between 940 and 1040 hPa (mbar). The average atmospheric pressure at sea level is 1013 hPa (mbar).
"
Base_(chemistry),Chemistry,1,"
 
 In chemistry, there are three definitions in common use of the word base, known as Arrhenius bases, Brønsted bases and Lewis bases. All definitions agree that bases are substances which react with acids as originally proposed by G.-F. Rouelle in the mid-18th century.
 Arrhenius proposed in 1884 that a base is a substance which dissociates in aqueous solution to form hydroxide ions OH−. These ions can react with hydrogen ions (H+ according to Arrhenius) from the dissociation of acids to form water in an acid-base reaction. A base was therefore a metal hydroxide such as NaOH or Ca(OH)2. Such aqueous hydroxide solutions were also described by certain characteristic properties. They are  slippery to the touch, can taste bitter[1] and change the color of pH indicators (e.g., turn red litmus paper blue).
 In water, by altering the autoionization equilibrium, bases yield solutions in which the hydrogen ion activity is lower than it is in pure water, i.e., the water has a pH higher than 7.0 at standard conditions. A soluble base is called an alkali if it contains and releases OH− ions quantitatively. Metal oxides, hydroxides, and especially alkoxides are basic, and conjugate bases of weak acids are weak bases.
 Bases and acids are seen as chemical opposites because the effect of an acid is to increase the hydronium (H3O+) concentration in water, whereas bases reduce this concentration. A reaction between aqueous solutions of an acid and a base is called neutralization, producing a solution of water and a salt in which the salt separates into its component ions. If the aqueous solution is saturated with a given salt solute, any additional such salt precipitates out of the solution.
 In the more general Brønsted–Lowry acid–base theory (1923), a base is a substance that can accept hydrogen cations (H+)—otherwise known as protons. This does include aqueous hydroxides since OH− does react with H+ to form water, so that Arrhenius bases are a subset of Brønsted bases. However there are also other Brønsted bases which accept protons, such as aqueous solutions of ammonia (NH3) or its organic derivatives (amines).[2] These bases do not contain a hydroxide ion but nevertheless react with water, resulting in an increase in the concentration of hydroxide ion.[3] Also, some non-aqueous solvents contain Brønsted bases which react with solvated protons. For example in liquid ammonia, NH2− is the basic ion species which accepts protons from NH4+, the acidic species in this solvent.
 G. N. Lewis realized that water, ammonia and other bases can form a bond with a proton due to the unshared pair of electrons that the bases possess.[3]  In the Lewis theory, a base is an electron pair donor which can share a pair of electrons with an electron acceptor which is described as a Lewis acid.[4] The Lewis theory is more general than the Brønsted model because the Lewis acid is not necessarily a proton, but can be another molecule (or ion) with a vacant low-lying orbital which can accept a pair of electrons. One notable example is boron trifluoride (BF3).
 Some other definitions of both bases and acids have been proposed in the past, but are not commonly used today.
"
Base_anhydride,Chemistry,1,"A base anhydride is an oxide of a chemical element from group 1 or 2 (the alkali metals and alkaline earth metals, respectively). They are obtained by removing water from the corresponding hydroxide base. If water is added to a base anhydride, a corresponding hydroxide salt can be re-formed.
 Base anhydrides are not Brønsted–Lowry bases because they are not proton acceptors. However, they are Lewis bases, because they will share an electron pair with some Lewis acids, most notably acidic oxides.[1] They are potent alkalis and will produce alkali burns on skin, because their affinity for water (that is, their affinity for being slaked) makes them react with body water. For example, quicklime (calcium oxide) reacts with skin to become hydrated lime (calcium hydroxide), which is a strong base, chemically akin to lye.
"
Beaker_(glassware),Chemistry,1,"In laboratory equipment, a beaker is generally a cylindrical container with a flat bottom.[1] Most also have a small spout (or ""beak"") to aid pouring, as shown in the picture. Beakers are available in a wide range of sizes, from one milliliter up to several liters. A beaker is distinguished from a flask by having straight rather than sloping sides. The exception to this definition is a slightly conical-sided beaker called a Philips beaker.  The beaker shape in general drinkware is similar.
 Beakers are commonly made of glass (today usually borosilicate glass[2]), but can also be in metal (such as stainless steel or aluminum) or certain plastics (notably polythene, polypropylene, PTFE). A common use for polypropylene beakers is gamma spectral analysis of liquid and solid samples.
"
Beer%E2%80%93Lambert_law,Chemistry,1,"The Beer–Lambert law, also known as Beer's law, the Lambert–Beer law, or the Beer–Lambert–Bouguer law relates the attenuation of light to the properties of the material through which the light is travelling. The law is commonly applied to chemical analysis measurements and used in understanding attenuation in physical optics, for photons, neutrons, or rarefied gases. In mathematical physics, this law arises as a solution of the BGK equation.
"
Biochemistry,Chemistry,1,"Biochemistry or biological chemistry, is the study of chemical processes within and relating to living organisms.[1]  A sub-discipline of both biology and chemistry, biochemistry may be divided into three fields: structural biology, enzymology and metabolism.  Over the last decades of the 20th century, biochemistry has become successful at explaining living processes through these three disciplines.  Almost all areas of the life sciences are being uncovered and developed through biochemical methodology and research.[2]  Biochemistry focuses on understanding the chemical basis which allows biological molecules to give rise to the processes that occur within living cells and between cells,[3] in turn relating greatly to the understanding of tissues and organs, as well as organism structure and function.[4] Biochemistry is closely related to molecular biology which is the study of the molecular mechanisms of biological phenomena.[5] Much of biochemistry deals with the structures, functions, and interactions of biological macromolecules, such as proteins, nucleic acids, carbohydrates, and lipids.  They provide the structure of cells and perform many of the functions associated with life.[6]  The chemistry of the cell also depends upon the reactions of small molecules and ions.  These can be inorganic (for example, water and metal ions) or organic (for example, the amino acids, which are used to synthesize proteins).[7]  The mechanisms used by cells to harness energy from their environment via chemical reactions are known as metabolism.  The findings of biochemistry are applied primarily in medicine, nutrition and agriculture.  In medicine, biochemists investigate the causes and cures of diseases.[8]  Nutrition studies how to maintain health and wellness and also the effects of nutritional deficiencies.[9]  In agriculture, biochemists investigate soil and fertilizers. Improving crop cultivation, crop storage, and pest control are also goals.
"
Bohr_model,Chemistry,1,"In atomic physics, the Bohr model or Rutherford–Bohr model, presented by Niels Bohr and Ernest Rutherford in 1913, is a system consisting of a small, dense nucleus surrounded by orbiting electrons—similar to the structure of the Solar System, but with attraction provided by electrostatic forces in place of gravity. After the cubical model (1902), the plum pudding model (1904), the Saturnian model (1904), and the Rutherford model (1911) came the Rutherford–Bohr model or just Bohr model for short (1913). The improvement over the 1911 Rutherford model mainly concerned the new quantum physical interpretation.
 The model's key success lay in explaining the Rydberg formula for the spectral emission lines of atomic hydrogen. While the Rydberg formula had been known experimentally, it did not gain a theoretical underpinning until the Bohr model was introduced. Not only did the Bohr model explain the reasons for the structure of the Rydberg formula, it also provided a justification for the fundamental physical constants that make up the formula's empirical results.
 The Bohr model is a relatively primitive model of the hydrogen atom, compared to the valence shell atom model. As a theory, it can be derived as a first-order approximation of the hydrogen atom using the broader and much more accurate quantum mechanics and thus may be considered to be an obsolete scientific theory. However, because of its simplicity, and its correct results for selected systems (see below for application), the Bohr model is still commonly taught to introduce students to quantum mechanics or energy level diagrams before moving on to the more accurate, but more complex, valence shell atom. A related model was originally proposed by Arthur Erich Haas in 1910 but was rejected. The quantum theory of the period between Planck's discovery of the quantum (1900) and the advent of a mature quantum mechanics (1925) is often referred to as the old quantum theory.
"
Boiling,Chemistry,1,"Boiling is the rapid vaporization of a liquid, which occurs when a liquid is heated to its boiling point, the temperature at which the vapour pressure of the liquid is equal to the pressure exerted on the liquid by the surrounding atmosphere. There are two main types of boiling: nucleate boiling where small bubbles of vapour form at discrete points, and critical heat flux boiling where the boiling surface is heated above a certain critical temperature and a film of vapor forms on the surface. Transition boiling is an intermediate, unstable form of boiling with elements of both types. The boiling point of water is 100 °C or 212 °F but is lower with the decreased atmospheric pressure found at higher altitudes.
 Boiling water is used as a method of making it potable by killing microbes and viruses that may be present. The sensitivity of different micro-organisms to heat varies. But if water is held at 100 °C (212 °F) for one minute, most micro-organisms and viruses are inactivated. Ten minutes at a temperature of 70 °C (158 °F) is also sufficient for most bacteria.
 Boiling water is also used in several cooking methods including boiling, steaming and poaching. 
"
Boiling_point,Chemistry,1,"The boiling point of a substance is the temperature at which the vapor pressure of a liquid equals the pressure surrounding the liquid[1][2] and the liquid changes into a vapor.
 The boiling point of a liquid varies depending upon the surrounding environmental pressure.  A liquid in a partial vacuum has a lower boiling point than when that liquid is at atmospheric pressure.  A liquid at high pressure has a higher boiling point than when that liquid is at atmospheric pressure. For example, water boils at 100 °C (212 °F) at sea level, but at 93.4 °C (200.1 °F) at 1,905 metres (6,250 ft) [3] altitude. For a given pressure, different liquids will boil at different temperatures. 
 The normal boiling point (also called the atmospheric boiling point or the atmospheric pressure boiling point) of a liquid is the special case in which the vapor pressure of the liquid equals the defined atmospheric pressure at sea level, one atmosphere.[4][5] At that temperature, the vapor pressure of the liquid becomes sufficient to overcome atmospheric pressure and allow bubbles of vapor to form inside the bulk of the liquid. The standard boiling point has been defined by IUPAC since 1982 as the temperature at which boiling occurs under a pressure of one bar.[6] The heat of vaporization is the energy required to transform a given quantity (a mol, kg, pound, etc.) of a substance from a liquid into a gas at a given pressure (often atmospheric pressure).
 Liquids may change to a vapor at temperatures below their boiling points through the process of evaporation. Evaporation is a surface phenomenon in which molecules located near the liquid's edge, not contained by enough liquid pressure on that side, escape into the surroundings as vapor. On the other hand, boiling is a process in which molecules anywhere in the liquid escape, resulting in the formation of vapor bubbles within the liquid.
"
Boiling-point_elevation,Chemistry,1,"Boiling-point elevation describes the phenomenon that the boiling point of a liquid (a solvent) will be higher when another compound is added, meaning that a solution has a higher boiling point than a pure solvent. This happens whenever a non-volatile solute, such as a salt, is added to a pure solvent, such as water. The boiling point can be measured accurately using an ebullioscope.
"
Chemical_bond,Chemistry,1,"
 A chemical bond is a lasting attraction between atoms, ions or molecules that enables the formation of chemical compounds. The bond may result from the electrostatic force of attraction between oppositely charged ions as in ionic bonds or through the sharing of electrons as in covalent bonds. The strength of chemical bonds varies considerably; there are ""strong bonds"" or ""primary bonds"" such as covalent, ionic and metallic bonds, and ""weak bonds"" or ""secondary bonds"" such as dipole–dipole interactions, the London dispersion force and hydrogen bonding.
 Since opposite charges attract via a simple electromagnetic force, the negatively charged electrons that are orbiting the nucleus and the positively charged protons in the nucleus attract each other. An electron positioned between two nuclei will be attracted to both of them, and the nuclei will be attracted toward electrons in this position. This attraction constitutes the chemical bond. Due to the matter wave nature of electrons and their smaller mass, they must occupy a much larger amount of volume compared with the nuclei, and this volume occupied by the electrons keeps the atomic nuclei in a bond relatively far apart, as compared with the size of the nuclei themselves.
 In general, strong chemical bonding is associated with the sharing or transfer of electrons between the participating atoms. The atoms in molecules, crystals, metals and diatomic gases—indeed most of the physical environment around us—are held together by chemical bonds, which dictate the structure and the bulk properties of matter.
 All bonds can be explained by quantum theory, but, in practice, simplification rules allow chemists to predict the strength, directionality, and polarity of bonds. The octet rule and VSEPR theory are two examples. More sophisticated theories are valence bond theory, which includes orbital hybridization and resonance, and molecular orbital theory which includes linear combination of atomic orbitals and ligand field theory. Electrostatics are used to describe bond polarities and the effects they have on chemical substances.
"
Boyle%27s_law,Chemistry,1,"Boyle's law, also referred to as the Boyle–Mariotte law, or Mariotte's law (especially in France), is an experimental gas law that describes how the pressure of a gas tends to increase as the volume of the container decreases. A modern statement of Boyle's law is:
 The absolute pressure exerted by a given mass of an ideal gas is inversely proportional to the volume it occupies if the temperature and amount of gas remain unchanged within a closed system.[1][2] Mathematically, Boyle's law can be stated as:
 or
 where P is the pressure of the gas, V is the volume of the gas, and k is a constant.
 The equation states that the product of pressure and volume is a constant for a given mass of confined gas and this holds as long as the temperature is constant. For comparing the same substance under two different sets of conditions, the law can be usefully expressed as:
 This equation shows that, as volume increases, the pressure of the gas decreases in proportion. Similarly, as volume decreases, the pressure of the gas increases. The law was named after chemist and physicist Robert Boyle, who published the original law in 1662.[3]"
Bragg%27s_law,Chemistry,1,"In physics, Bragg's law, or Wulff–Bragg's condition, a special case of Laue diffraction, gives the angles for coherent and incoherent scattering from a crystal lattice. When X-rays are incident on an atom, they make the electronic cloud move, as does any electromagnetic wave. The movement of these charges re-radiates waves with the same frequency, blurred slightly due to a variety of effects; this phenomenon is known as Rayleigh scattering (or elastic scattering). The scattered waves can themselves be scattered but this secondary scattering is assumed to be negligible.
 A similar process occurs upon scattering neutron waves from the nuclei or by a coherent spin interaction with an unpaired electron. These re-emitted wave fields interfere with each other either constructively or destructively (overlapping waves either add up together to produce stronger peaks or are subtracted from each other to some degree), producing a diffraction pattern on a detector or film.  The resulting wave interference pattern is the basis of diffraction analysis. This analysis is called Bragg diffraction.
"
Brownian_motion,Chemistry,1,"
 Brownian motion, or pedesis (from Ancient Greek: πήδησις /pɛ̌ːdɛːsis/ ""leaping""), is the random motion of particles suspended in a medium (a liquid or a gas).[2] This pattern of motion typically consists of random fluctuations in a particle's position inside a fluid sub-domain, followed by a relocation to another sub-domain. Each relocation is followed by more fluctuations within the new closed volume. This pattern describes a fluid at thermal equilibrium, defined by a given temperature. Within such a fluid, there exists no preferential direction of flow (as in transport phenomena). More specifically, the fluid's overall linear and angular momenta remain null over time. The kinetic energies of the molecular Brownian motions, together with those of molecular rotations and vibrations, sum up to the caloric component of a fluid's internal energy (the Equipartition theorem).
 This motion is named after the botanist Robert Brown, who first described the phenomenon in 1827, while looking through a microscope at pollen of the plant Clarkia pulchella immersed in water. In 1905, almost eighty years later, theoretical physicist Albert Einstein published a paper where he modeled the motion of the pollen particles as being moved by individual water molecules, making one of his first major scientific contributions.[3] This explanation of Brownian motion served as convincing evidence that atoms and molecules exist and was further verified experimentally by Jean Perrin in 1908. Perrin was awarded the Nobel Prize in Physics in 1926 ""for his work on the discontinuous structure of matter"".[4] The direction of the force of atomic bombardment is constantly changing, and at different times the particle is hit more on one side than another, leading to the seemingly random nature of the motion.
 The many-body interactions that yield the Brownian pattern cannot be solved by a model accounting for every involved molecule. In consequence, only probabilistic models applied to molecular populations can be employed to describe it. Two such models of the statistical mechanics, due to Einstein and Smoluchowski are presented below. Another, pure probabilistic class of models is the class of the stochastic process models. There exist sequences of both simpler and more complicated stochastic processes which converge (in the limit) to Brownian motion (see random walk and Donsker's theorem).[5][6]"
Buffered_solution,Chemistry,1,"A buffer solution (more precisely, pH buffer or hydrogen ion buffer) is an aqueous solution consisting of a mixture of a weak acid and its conjugate base, or vice versa. Its pH changes very little when a small amount of strong acid or base is added to it. Buffer solutions are used as a means of keeping pH at a nearly constant value in a wide variety of chemical applications. In nature, there are many systems that use buffering for pH regulation. For example, the bicarbonate buffering system is used to regulate the pH of blood.
"
Bumping_(chemistry),Chemistry,1,"Bumping is a phenomenon in chemistry where homogenous liquids boiled in a test tube or other container will superheat and, upon nucleation, rapid boiling will expel the liquid from the container. In extreme cases, the container may be broken.[1]"
Bung,Chemistry,1,"A bung, stopper or cork is a cylindrical or conical closure used to seal a container, such as a bottle, tube or barrel. Unlike a lid, which encloses a container from the outside without displacing the inner volume, a bung is partially or wholly inserted inside the container to act as a seal. 
 A bung can be defined as ""a plug or closure used to close an opening in a drum or barrel.  Called a plug when referring to a steel drum closure.""[1] A glass stopper is often called a ""ground glass joint"" (or ""joint taper""), a rubber stopper is sometimes called a ""rubber bung"", and a cork stopper is called simply a ""cork"". Bung stoppers used for wine bottles are referred to as ""corks"", even when made from another material.
 A common every-day example of a bung is the cork of a wine bottle. Bungs are used to seal the bunghole of barrels.  Other bungs, particularly those used in chemical barrels, may be made of metal and be screwed into place via threading.
"
Burette,Chemistry,1,"A burette (also buret) is a graduated glass tube with a tap at one end, for delivering known volumes of a liquid, especially in titrations. It is a long, graduated glass tube, with a stopcock at its lower end and a tapered capillary tube at the stopcock's outlet. The flow of liquid from the tube to the burette tip is controlled by the stopcock valve.  There are two main types of burette; the volumetric burette and the Piston burette or Digital burette.
 A volumetric burette delivers measured volumes of liquid. Piston burettes are similar to syringes, but with a precision bore and a plunger. Piston burettes may be manually operated or may be motorized.[1] A weight burette delivers measured weights of a liquid.[2]"
Calorimeter,Chemistry,1,"A calorimeter is an object used for calorimetry, or the process of measuring the heat of chemical reactions or physical changes as well as heat capacity. Differential scanning calorimeters, isothermal micro calorimeters, titration calorimeters and accelerated rate calorimeters are among the most common types. A simple calorimeter just consists of a thermometer attached to a metal container full of water suspended above a combustion chamber.  It is one of the measurement devices used in the study of thermodynamics, chemistry, and biochemistry.
 To find the enthalpy change per mole of a substance A in a reaction between two substances A and B, the substances are separately added to a calorimeter and the initial and final temperatures (before the reaction has started and after it has finished) are noted. Multiplying the temperature change by the mass and specific heat capacities of the substances gives a value for the energy given off or absorbed during the reaction. Dividing the energy change by how many moles of A were present gives its enthalpy change of reaction.
 Where q is the amount of heat according to the change in temperature measured in joules and Cv is the heat capacity of the calorimeter which is a value associated with each individual apparatus in units of energy per temperature (Joules/Kelvin).  
"
Carbanion,Chemistry,1,"A carbanion is an anion in which carbon is trivalent (forms three bonds) and bears a formal negative charge (in at least one significant resonance form).[1] Formally, a carbanion is the conjugate base of a carbon acid:
 where B stands for the base.  The carbanions formed from deprotonation of alkanes (at an sp3 carbon), alkenes (at an sp2 carbon), arenes (at an sp2 carbon), and alkynes (at an sp carbon) are known as alkyl, alkenyl (vinyl), aryl, and alkynyl (acetylide) anions, respectively.
 Carbanions have a concentration of electron density at the negatively charged carbon, which, in most cases, reacts efficiently with a variety of electrophiles of varying strengths, including carbonyl groups, imines/iminium salts, halogenating reagents (e.g., N-bromosuccinimide and diiodine), and proton donors.  A carbanion is one of several reactive intermediates in organic chemistry. In organic synthesis, organolithium reagents and Grignard reagents are commonly treated and referred to as ""carbanions."" This is a convenient approximation, although these species are generally clusters or complexes containing highly polar, but still covalent bonds metal–carbon bonds (Mδ+–Cδ–) rather than true carbanions.
"
Carbocation,Chemistry,1,"A carbocation (/ˌkɑːrboʊˈkætaɪən/[1]) is an ion with a positively charged carbon atom. Among the simplest examples are the methenium CH+3,  methanium CH+5 and vinyl C2H+3 cations. Occasionally, carbocations that bear more than one positively charged carbon atom are also encountered (e.g., ethylene dication C2H2+4).[2] Until the early 1970s, all carbocations were called carbonium ions.[3] In the present-day definition given by the IUPAC, a carbocation is any even-electron cation with significant partial positive charge on a carbon atom.  They are further classified in two main categories according to the coordination number of the charged carbon: three in the carbenium ions and five in the carbonium ions.  This nomenclature was proposed by G. A. Olah.[4] Carbonium ions, as originally defined by Olah, are characterized by a three-center two-electron delocalized bonding scheme and are essentially synonymous with so-called 'non-classical carbocations', which are carbocations that contain bridging C–C or C–H σ-bonds.  However, others have more narrowly defined the term 'carbonium ion' as formally protonated or alkylated alkanes (i.e., CR5+, where R is hydrogen or alkyl), to the exclusion of non-classical carbocations like the 2-norbornyl cation.[5]"
Catalyst,Chemistry,1,Catalysis (/kəˈtæləsɪs/) is the process of increasing the rate of a chemical reaction by adding a substance known as a catalyst[1][2] (/ˈkætəlɪst/).  Catalysts are not consumed in the catalyzed reaction but can act repeatedly. Often only very small amounts of catalyst are required.[3] The global demand for catalysts in 2010 was estimated at approximately US$29.5 billion.[4]
Cathode,Chemistry,1,"
A cathode is the electrode from which a conventional current leaves a polarized electrical device. This definition can be recalled by using the mnemonic CCD for Cathode Current Departs. A conventional current describes the direction in which positive charges move. Electrons have a negative electrical charge, so the movement of electrons is opposite to that of the conventional current flow. Consequently, the mnemonic cathode current departs also means that electrons flow into the device's cathode from the external circuit.
 The electrode through which conventional current flows the other way, into the device, is termed an anode.
"
Cation,Chemistry,1,"
 An ion (/ˈaɪɒn, -ən/)[1] is an particle,atom or molecule with a net electrical charge. 
 The charge of the electron is considered negative by convention.  The negative charge of an ion is equal and opposite to charged proton(s) considered positive by convention.  The net charge of an ion is non-zero due to its total number of electrons being unequal to its total number of protons. 
 A cation is a positively charged ion, with fewer electrons than protons, while an anion is negatively charged, with more electrons than protons. Because of their opposite electric charges, cations and anions attract each other and readily form ionic compounds.
 Ions consisting of only a single atom are termed atomic or monatomic ions, while two or more atoms form molecular ions or polyatomic ions. In the case of physical ionization in a fluid (gas or liquid), ""ion pairs"" are created by spontaneous molecule collisions, where each generated pair consists of a free electron and a positive ion.[2] Ions are also created by chemical interactions, such as the dissolution of a salt in liquids, or by other means, such as passing a direct current through a conducting solution, dissolving an anode via ionization.
"
Centrifugation,Chemistry,1,"
 Centrifugation is a mechanical process which involves the use of the centrifugal force to separate particles from a solution according to their size, shape, density, medium viscosity and rotor speed.[1] The more dense components of the mixture migrate away from the axis of the centrifuge, while the less dense components of the mixture migrate towards the axis. Chemists and biologists may increase the effective gravitational force of the test tube so that the precipitate (pellet) will travel quickly and fully to the bottom of the tube. The remaining liquid that lies above the precipitate is called a supernatant or supernate.
 There is a correlation between the size and density of a particle and the rate that the particle separates from a heterogeneous mixture, when the only force applied is that of gravity. The larger the size and the larger the density of the particles, the faster they separate from the mixture. By applying a larger effective gravitational force to the mixture, like a centrifuge does, the separation of the particles is accelerated. This is ideal in industrial and lab settings because particles that would naturally separate over a long period of time can be separated in much less time.[2] The rate of centrifugation is specified by the angular velocity usually expressed as revolutions per minute (RPM), or acceleration expressed as g. The conversion factor between RPM and g depends on the radius of the centrifuge rotor. The particles' settling velocity in centrifugation is a function of their size and shape, centrifugal acceleration, the volume fraction of solids present, the density difference between the particle and the liquid, and the viscosity. The most common application is the separation of solid from highly concentrated suspensions, which is used in the treatment of sewage sludges for dewatering where less consistent sediment is produced.[3] The centrifugation method has a wide variety of industrial and laboratorial applications; not only is this process used to separate two miscible substances, but also to analyze the hydrodynamic properties of macromolecules.[4] It is one of the most important and commonly used research methods in biochemistry, cell and molecular biology. In the chemical and food industries, special centrifuges can process a continuous stream of particle-laden liquid. Centrifugation is also the most common method used for uranium enrichment, relying on the slight mass difference between atoms of U-238 and U-235 in uranium hexafluoride gas.[5]"
Centrifuge,Chemistry,1,"A centrifuge is a device that uses centrifugal force to separate various components of a fluid. This is achieved by spinning the fluid at high speed within a container, thereby separating fluids of different densities (e.g. cream from milk) or liquids from solids. It works by causing denser substances and particles to move outward in the radial direction.  At the same time, objects that are less dense are displaced and move to the centre. In a laboratory centrifuge that uses sample tubes, the radial acceleration causes denser particles to settle to the bottom of the tube, while low-density substances rise to the top.[1] A centrifuge can be a very effective filter that separates contaminants from the main body of fluid.
 Industrial scale centrifuges are commonly used in manufacturing and waste processing to sediment suspended solids, or to separate immiscible liquids. An example is the cream separator found in dairies. Very high speed centrifuges and ultracentrifuges able to provide very high accelerations can separate fine particles down to the nano-scale, and molecules of different masses. Large centrifuges are used to simulate high gravity or acceleration environments (for example, high-G training for test pilots).  Medium-sized centrifuges are used in washing machines and at some swimming pools to draw water out of fabrics. Gas centrifuges are used for isotope separation, such as to enrich nuclear fuel for fissile isotopes.
"
Resting_potential,Chemistry,1,"The relatively static membrane potential of quiescent cells is called the resting membrane potential (or resting voltage), as opposed to the specific dynamic electrochemical phenomena called action potential and graded membrane potential.
 Apart from the latter two, which occur in excitable cells (neurons, muscles, and some secretory cells in glands), membrane voltage in the majority of non-excitable cells can also undergo changes in response to environmental or intracellular stimuli. The resting potential exists due to the differences in membrane permeabilities for potassium, sodium, calcium, and chloride ions, which in turn result from functional activity of various ion channels, ion transporters, and exchangers.  Conventionally, resting membrane potential can be defined as a relatively stable, ground value of transmembrane voltage in animal and plant cells. The typical resting membrane potential of a cell arises from the separation of potassium ions from intracellular, relatively immobile anions across the membrane of the cell. Because the membrane permeability for potassium is much higher than that for other ions, and because of the strong chemical gradient for potassium, potassium ions flow from the cytosol into the extracellular space carrying out positive charge, until their movement is balanced by build-up of negative charge on the inner surface of the membrane. Again, because of the high relative permeability for potassium, the resulting membrane potential is almost always close to the potassium reversal potential. But in order for this process to occur, a concentration gradient of potassium ions must first be set up. This work is done by the ion pumps/transporters and/or exchangers and generally is powered by ATP.
 In the case of the resting membrane potential across an animal cell's plasma membrane, potassium (and sodium) gradients are established by the Na+/K+-ATPase (sodium-potassium pump) which transports 2 potassium ions inside and 3 sodium ions outside at the cost of 1 ATP molecule. In other cases, for example, a membrane potential may be established by acidification of the inside of a membranous compartment (such as the proton pump that generates membrane potential across synaptic vesicle membranes).[citation needed]"
Chain_reaction,Chemistry,1,"A chain reaction is a sequence of reactions where a reactive product or by-product causes additional reactions to take place. In a chain reaction, positive feedback leads to a self-amplifying chain of events.
 Chain reactions are one way that systems which are not in thermodynamic equilibrium can release energy or increase entropy in order to reach a state of higher entropy. For example, a system may not be able to reach a lower energy state by releasing energy into the environment, because it is hindered or prevented in some way from taking the path that will result in the energy release. If a reaction results in a small energy release making way for more energy releases in an expanding chain, then the system will typically collapse explosively until much or all of the stored energy has been released. 
 A macroscopic metaphor for chain reactions is thus a snowball causing a larger snowball until finally an avalanche results (""snowball effect""). This is a result of stored gravitational potential energy seeking a path of release over friction. Chemically, the equivalent to a snow avalanche is a spark causing a forest fire. In nuclear physics, a single stray neutron can result in a prompt critical event, which may finally be energetic enough for a nuclear reactor meltdown or (in a bomb) a nuclear explosion.
 Numerous chain reactions can be represented by a mathematical model based on Markov chains.
"
Charge_number,Chemistry,1,"Charge number (z) refers to a quantized value of electric charge, with the quantum of electric charge being the elementary charge, so that the charge number equals the electric charge (q) in coulombs divided by the elementary-charge constant (e), or z = q/e.  The charge numbers for ions (and also subatomic particles) are written in superscript, e.g. Na+ is a sodium ion with charge number positive one (an electric charge of one elementary charge).  Atomic numbers (Z) are a special case of charge numbers, referring to the charge number of an atomic nucleus, as opposed to the net charge of an atom or ion.  All particles of ordinary matter have integer-value charge numbers, with the exception of quarks, which cannot exist in isolation under ordinary circumstances (the strong force keeps them bound into hadrons of integer charge numbers).
"
Charles%27s_law,Chemistry,1,"Charles's law (also known as the law of volumes) is an experimental gas law that describes how gases tend to expand when heated. A modern statement of Charles's law is:
 When the pressure on a sample of a dry gas is held constant, the Kelvin temperature and the volume will be in direct proportion.[1] This relationship of direct proportion can be written as:
 So this means:
 V is the volume of the gas,
 T is the temperature of the gas (measured in kelvins),
 and k is a non-zero constant.
 This law describes how a gas expands as the temperature increases; conversely, a decrease in temperature will lead to a decrease in volume. For comparing the same substance under two different sets of conditions, the law can be written as:
 The equation shows that, as absolute temperature increases, the volume of the gas also increases in proportion.
"
Chelating_agent,Chemistry,1,"Chelation /ˈkiːˌleɪˈʃən/ is a type of bonding of ions and molecules to metal ions. It involves the formation or presence of two or more separate coordinate bonds between a polydentate (multiple bonded) ligand and a single central atom.[1][2]  These ligands are called chelants, chelators, chelating agents, or sequestering agents.  They are usually organic compounds, but this is not a necessity, as in the case of zinc and its use as a maintenance therapy to prevent the absorption of copper in people with Wilson's disease.[3] Chelation is useful in applications such as providing nutritional supplements, in chelation therapy to remove toxic metals from the body, as contrast agents in MRI scanning, in manufacturing using homogeneous catalysts, in chemical water treatment to assist in the removal of metals, and in fertilizers.
"
Chelation,Chemistry,1,"Chelation /ˈkiːˌleɪˈʃən/ is a type of bonding of ions and molecules to metal ions. It involves the formation or presence of two or more separate coordinate bonds between a polydentate (multiple bonded) ligand and a single central atom.[1][2]  These ligands are called chelants, chelators, chelating agents, or sequestering agents.  They are usually organic compounds, but this is not a necessity, as in the case of zinc and its use as a maintenance therapy to prevent the absorption of copper in people with Wilson's disease.[3] Chelation is useful in applications such as providing nutritional supplements, in chelation therapy to remove toxic metals from the body, as contrast agents in MRI scanning, in manufacturing using homogeneous catalysts, in chemical water treatment to assist in the removal of metals, and in fertilizers.
"
Chemical_composition,Chemistry,1,"Chemical composition refers to the identity and relative number of the chemical elements that make up any particular compound.
"
Chemical_decomposition,Chemistry,1,"Chemical decomposition, or chemical breakdown, is the process or effect of simplifying a single chemical entity (normal molecule, reaction intermediate, etc.) into two or more fragments.[1] Chemical decomposition is usually regarded and defined as the exact opposite of chemical synthesis. In short, the chemical reaction in which two or more products are formed from a single reactant is called a decomposition reaction. 
 The details of a decomposition process are not always well defined but some of the process is understood; much energy is needed to break bonds. Since all decomposition reactions break apart the bonds holding it together in order to produce into its simpler basic parts, the reactions would require some form of this energy in varying degrees.  Because of this fundamental rule, it is known that most of these reactions are endothermic although exceptions do exist. 
 The stability of a chemical compound is eventually limited when exposed to extreme environmental conditions such as heat, radiation, humidity, or the acidity of a solvent. Because of this chemical decomposition is often an undesired chemical reaction.  However chemical decomposition is being used in a growing number of ways.  
 For example this method is employed for several analytical techniques, notably mass spectrometry, traditional gravimetric analysis, and thermogravimetric analysis.  Additionally decomposition reactions are used today for a number of other reasons in the production of a wide variety of products. One of these is the explosive breakdown reaction of sodium azide [(NaN3)2] into nitrogen gas (N2) and sodium (Na). It is this process which powers the life-saving airbags present in virtually all of today's automobiles.[2] Decomposition reactions can be generally classed into three categories; thermal, electrolytic, and photolytic decomposition reactions.[3]"
Chemical_formula,Chemistry,1,"
 A chemical formula is a way of  presenting information about the chemical proportions of atoms that constitute a particular chemical compound or molecule, using chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, commas and plus (+) and minus (−) signs. These are limited to a single typographic line of symbols, which may include subscripts and superscripts. A chemical formula is not a chemical name, and it contains no words. Although a chemical formula may imply certain simple chemical structures, it is not the same as a full chemical structural formula. Chemical formulae can fully specify the structure of only the simplest of molecules and chemical substances, and are generally more limited in power than are chemical names and structural formulae.
 The simplest types of chemical formulae are called empirical formulas, which use letters and numbers indicating the numerical proportions of atoms of each type. Molecular formulae indicate the simple numbers of each type of atom in a molecule, with no information on structure. For example, the empirical formula for glucose is CH2O (twice as many hydrogen atoms as carbon and oxygen), while its molecular formula is C6H12O6 (12 hydrogen atoms, six carbon and oxygen atoms).
 Sometimes a chemical formula is complicated by being written as a condensed formula (or condensed molecular formula, occasionally called a ""semi-structural formula""), which conveys additional information about the particular ways in which the atoms are chemically bonded together, either in covalent bonds, ionic bonds, or various combinations of these types. This is possible if the relevant bonding is easy to show in one dimension. An example is the condensed molecular/chemical formula for ethanol, which is CH3-CH2-OH or CH3CH2OH. However, even a condensed chemical formula is necessarily limited in its ability to show complex bonding relationships between atoms, especially atoms that have bonds to four or more different substituents.
 Since a chemical formula must be expressed as a single line of chemical element symbols, it often cannot be as informative as a true structural formula, which is a graphical representation of the spatial relationship between atoms in chemical compounds (see for example the figure for butane structural and chemical formulae, at right). For reasons of structural complexity, a single condensed chemical formula (or semi-structural formula) may correspond to different molecules, known as isomers. For example glucose shares its molecular formula C6H12O6 with a number of other sugars, including fructose, galactose and mannose. Linear equivalent chemical names exist that can and do specify uniquely any complex structural formula (see chemical nomenclature), but such names must use many terms (words), rather than the simple element symbols, numbers, and simple typographical symbols that define a chemical formula.
 Chemical formulae may be used in chemical equations to describe chemical reactions and other chemical transformations, such as the dissolving of ionic compounds into solution. While, as noted, chemical formulae do not have the full power of structural formulae to show chemical relationships between atoms, they are sufficient to keep track of numbers of atoms and numbers of electrical charges in chemical reactions, thus balancing chemical equations so that these equations can be used in chemical problems involving conservation of atoms, and conservation of electric charge.
"
Chemical_law,Chemistry,1,"Chemical laws  are those laws of nature relevant to chemistry. The most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics.
 The laws of stoichiometry, that  is, the gravimetric proportions by which chemical elements participate in chemical reactions, elaborate on the law of conservation of mass.  Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important.
 Dalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers (i.e. 1:2 O:H in water); although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction. Such compounds are known as non-stoichiometric compounds.
 The third stoichiometric law is the law of reciprocal proportions, which provides the basis for establishing equivalent weights for each chemical element. Elemental equivalent weights can then be used to derive atomic weights for each element.
 More modern laws of chemistry define the relationship between energy and transformations. 
"
Chemical_nomenclature,Chemistry,1,"A chemical nomenclature is a set of rules to generate systematic names for chemical compounds. The nomenclature used most frequently worldwide is the one created and developed by the International Union of Pure and Applied Chemistry (IUPAC).
 The IUPAC's rules for naming organic and inorganic compounds are contained in two publications, known as the Blue Book[1][2] and the Red Book,[3] respectively. A third publication, known as the Green Book,[4] describes the recommendations for the use of symbols for physical quantities (in association with the IUPAP), while a fourth, the Gold Book,[5] contains the definitions of many technical terms used in chemistry. Similar compendia exist for biochemistry[6] (the White Book, in association with the IUBMB), analytical chemistry[7] (the Orange Book), macromolecular chemistry[8] (the Purple Book) and clinical chemistry[9] (the Silver Book). These ""color books"" are supplemented by shorter recommendations for specific circumstances that are published periodically in the journal Pure and Applied Chemistry.
"
Chemical_process,Chemistry,1,"In a scientific sense, a chemical process is a method or means of somehow changing one or more chemicals or chemical compounds. Such a chemical process can occur by itself or be caused by an outside force, and involves a chemical reaction of some sort. In an ""engineering"" sense, a chemical process is a method intended to be used in manufacturing or on an industrial scale (see Industrial process) to change the composition of chemical(s) or material(s), usually using technology similar or related to that used in chemical plants or the chemical industry.  
 Neither of these definitions are exact in the sense that one can always tell definitively what is a chemical process and what is not; they are practical definitions.  There is also significant overlap in these two definition variations.   Because of the inexactness of the definition, chemists and other scientists use the term ""chemical process"" only in a general sense or in the engineering sense.  However, in the ""process (engineering)"" sense, the term ""chemical process"" is used extensively.  The rest of the article will cover the engineering type of chemical processes.  
 Although this type of chemical process may sometimes involve only one step, often multiple steps, referred to as unit operations, are involved.  In a plant,   each of the unit operations commonly occur in individual vessels or sections of the plant called units.  Often, one or more chemical reactions are involved, but other ways of changing chemical (or material) composition may be used, such as mixing or separation processes.  The process steps may be sequential in time or sequential in space along a stream of flowing or moving material; see Chemical plant.  For a given amount of a feed (input) material or product (output) material, an expected amount of material can be determined at key steps in the process from empirical data and material balance calculations.  These amounts can be scaled up or down to suit the desired capacity or operation of a particular chemical plant built for such a process.  More than one chemical plant may use the same chemical process, each plant perhaps at differently scaled capacities.  
Chemical processes like distillation and crystallization go back to alchemy in Alexandria, Egypt.
 Such chemical processes can be illustrated generally as block flow diagrams or in more detail as process flow diagrams.  Block flow diagrams show the units as blocks and the streams flowing between them as connecting lines with arrowheads to show direction of flow.   
 In addition to chemical plants for producing chemicals, chemical processes with similar technology and equipment are also used in oil refining and other refineries, natural gas processing, polymer and pharmaceutical manufacturing, food processing, and water and wastewater treatment.
"
Chemical_reaction,Chemistry,1,"
 A chemical reaction is a process that leads to the chemical transformation of one set of chemical substances to another.[1] Classically, chemical reactions encompass changes that only involve the positions of electrons in the forming and breaking of chemical bonds between atoms, with no change to the nuclei (no change to the elements present), and can often be described by a chemical equation. Nuclear chemistry is a sub-discipline of chemistry that involves the chemical reactions of unstable and radioactive elements where both electronic and nuclear changes can occur.
 The substance (or substances) initially involved in a chemical reaction are called reactants or reagents. Chemical reactions are usually characterized by a chemical change, and they yield one or more products, which usually have properties different from the reactants. Reactions often consist of a sequence of individual sub-steps, the so-called elementary reactions, and the information on the precise course of action is part of the reaction mechanism. Chemical reactions are described with chemical equations, which symbolically present the starting materials, end products, and sometimes intermediate products and reaction conditions.
 Chemical reactions happen at a characteristic reaction rate at a given temperature and chemical concentration. Typically, reaction rates increase with increasing temperature because there is more thermal energy available to reach the activation energy necessary for breaking bonds between atoms.
 Reactions may proceed in the forward or reverse direction until they go to completion or reach equilibrium. Reactions that proceed in the forward direction to approach equilibrium are often described as spontaneous, requiring no input of free energy to go forward. Non-spontaneous reactions require input of free energy to go forward (examples include charging a battery by applying an external electrical power source, or photosynthesis driven by absorption of electromagnetic radiation in the form of sunlight).
 Different chemical reactions are used in combinations during chemical synthesis in order to obtain a desired product. In biochemistry, a consecutive series of chemical reactions (where the product of one reaction is the reactant of the next reaction) form metabolic pathways. These reactions are often catalyzed by protein enzymes. Enzymes increase the rates of biochemical reactions, so that metabolic syntheses and decompositions impossible under ordinary conditions can occur at the temperatures and concentrations present within a cell.
 The general concept of a chemical reaction has been extended to reactions between entities smaller than atoms, including nuclear reactions, radioactive decays, and reactions between elementary particles, as described by quantum field theory.
"
Chemical_species,Chemistry,1,"A chemical species is a chemical substance or ensemble composed of chemically identical molecular entities that can explore the same set of molecular energy levels on a characteristic or delineated time scale. These energy levels determine the way the chemical species will interact with others (engaging in chemical bonds, etc.). The species can be atom, molecule, ion, radical, and it has a chemical name and chemical formula. The term is also applied to a set of chemically identical atomic or molecular structural units in a solid array.[1][2] In supramolecular chemistry, chemical species are those supramolecular structures whose interactions and associations are brought about via intermolecular bonding and debonding actions, and function to form the basis of this branch of chemistry.
 For instance:
"
Chemical_substance,Chemistry,1,"A chemical substance is a form of matter having constant chemical composition and characteristic properties.[1][2] Some references add that chemical substance cannot be separated into its constituent elements by physical separation methods, i.e., without breaking chemical bonds.[3] Chemical substances can be simple substances,[4] chemical compounds, or alloys. Chemical elements may or may not be included in the definition, depending on expert viewpoint.[4] Chemical substances are often called 'pure' to set them apart from mixtures. A common example of a chemical substance is pure water; it has the same properties and the same ratio of hydrogen to oxygen whether it is isolated from a river or made in a laboratory. Other chemical substances commonly encountered in pure form are diamond (carbon), gold, table salt (sodium chloride) and refined sugar (sucrose). However, in practice, no substance is entirely pure, and chemical purity is specified according to the intended use of the chemical.
 Chemical substances exist as solids, liquids, gases, or plasma, and may change between these phases of matter with changes in temperature or pressure. Chemical substances may be combined or converted to others by means of chemical reactions.
 Forms of energy, such as light and heat, are not matter, and are thus not ""substances"" in this regard.
"
Chemical_synthesis,Chemistry,1,"In chemistry, chemical synthesis is the artificial execution of useful chemical reactions to obtain one or several products.[1] This occurs by physical and chemical manipulations usually involving one or more reactions. In modern laboratory uses, the process is reproducible and reliable.
 A chemical synthesis involves one or more compounds (known as reagents or reactants) that will undergo a transformation when subjected to certain conditions. Various reaction types can be applied to formulate a desired product. This requires mixing the compounds in a reaction vessel, such as a chemical reactor or a simple round-bottom flask. Many reactions require some form of work-up or purification procedure to isolate the final product.[1] The amount of product produced in a chemical synthesis is known as the reaction yield. Typically, chemical yields are expressed as a mass in grams (in a laboratory setting) or as a percentage of the total theoretical quantity of product that could be produced based on the limiting reagent. A side reaction is an unwanted chemical reaction taking place which reduces the yield of the desired product. The word synthesis was first used by the chemist Hermann Kolbe.[2]"
Chemistry,Chemistry,1,"
 
 Chemistry is the scientific discipline involved with elements and compounds composed of atoms, molecules and ions: their composition, structure, properties, behavior and the changes they undergo during a reaction with other substances.[1][2][3][4] In the scope of its subject, chemistry occupies an intermediate position between physics and biology.[5] It is sometimes called the central science because it provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level.[6] For example, chemistry explains aspects of plant chemistry (botany), the formation of igneous rocks (geology), how atmospheric ozone is formed and how environmental pollutants are degraded (ecology), the properties of the soil on the moon (cosmochemistry), how medications work (pharmacology), and how to collect DNA evidence at a crime scene (forensics).
 Chemistry addresses topics such as how atoms and molecules interact via chemical bonds to form new chemical compounds. There are two types of chemical bonds: 1. Primary Chemical bonds e.g covalent bonds, in which atoms share one or more electron(s); ionic bonds, in which an atom donates one or more electrons to another atom to produce ions (cations and anions); Metallic bonds and 2. Secondary chemical bonds e.g. hydrogen bonds;Van der Waals force bonds, ion-ion interaction, ion-dipole interaction etc.
"
Chirality,Chemistry,1,"Chirality /kaɪˈrælɪtiː/ is a property of asymmetry important in several branches of science. The word chirality is derived from the Greek χειρ (kheir), ""hand,"" a familiar chiral object.
 An object or a system is chiral if it is distinguishable from its mirror image; that is, it cannot be superimposed onto it.  Conversely, a mirror image of an achiral object, such as a sphere, cannot be distinguished from the object. A chiral object and its mirror image are called enantiomorphs (Greek, ""opposite forms"") or, when referring to molecules, enantiomers. A non-chiral object is called achiral (sometimes also amphichiral) and can be superposed on its mirror image.
 The term was first used by Lord Kelvin in 1893 in the second Robert Boyle Lecture at the Oxford University Junior Scientific Club which was published in 1894:
 I call any geometrical figure, or group of points, 'chiral', and say that it has chirality if its image in a plane mirror, ideally realized, cannot be brought to coincide with itself.[1] Human hands are perhaps the most universally recognized example of chirality. The left hand is a non-superimposable mirror image of the right hand; no matter how the two hands are oriented, it is impossible for all the major features of both hands to coincide across all axes.[2] This difference in symmetry becomes obvious if someone attempts to shake the right hand of a person using their left hand, or if a left-handed glove is placed on a right hand. In mathematics, chirality is the property of a figure that is not identical to its mirror image. A molecule is said to be chiral if its all valence is occupied by different atom or group of atom.
"
Chromatography,Chemistry,1,"
 Chromatography is a laboratory technique for the separation of a mixture.
The mixture is dissolved in a fluid (gas, solvent, water, ...) called the mobile phase, which carries it through a system (a column, a capillary tube, a plate, or a sheet) on which is fixed a material called the stationary phase. The different constituents of the mixture have different affinities for the stationary phase. The different molecules stay longer or shorter on the stationary phase, depending on their interactions with its surface sites. So, they travel at different apparent velocities in the mobile fluid, causing them to separate. The separation is based on the differential partitioning between the mobile and the stationary phases. Subtle differences in a compound's partition coefficient result in differential retention on the stationary phase and thus affect the separation.[1] Chromatography may be preparative or analytical. The purpose of preparative chromatography is to separate the components of a mixture for later use, and is thus a form of purification. Analytical chromatography is done normally with smaller amounts of material and is for establishing the presence or measuring the relative proportions of analytes in a mixture. The two are not mutually exclusive.[2]"
Cis%E2%80%93trans_isomerism,Chemistry,1,"Cis–trans isomerism, also known as geometric isomerism or configurational isomerism, is a term used in organic chemistry. The prefixes ""cis"" and ""trans"" are from Latin: ""this side of"" and ""the other side of"", respectively. In the context of chemistry, cis indicates that the functional groups are on the same side of the carbon chain[1] while trans conveys that functional groups are on opposing sides of the carbon chain. Cis-trans isomers are stereoisomers, that is, pairs of molecules which have the same formula but whose functional groups are rotated into a different orientation in three-dimensional space. It is not to be confused with E–Z isomerism, which is an absolute stereochemical description. In general, stereoisomers contain double bonds that do not rotate, or they may contain ring structures, where the rotation of bonds is restricted or prevented.[2] Cis and trans isomers occur both in organic molecules and in inorganic coordination complexes. Cis and trans descriptors are not used for cases of conformational isomerism where the two geometric forms easily interconvert, such as most open-chain single-bonded structures; instead, the terms ""syn"" and ""anti"" are used.
 The term ""geometric isomerism"" is considered by IUPAC to be an obsolete synonym of ""cis–trans isomerism"".[3]"
Closed_system,Chemistry,1,"A closed system is a physical system that does not allow transfer of matter in or out of the system, though, in different contexts, such as physics, chemistry or engineering, the transfer of energy is or is not allowed.
"
Cluster_chemistry,Chemistry,1," In chemistry, an atom cluster (or simply cluster) is an ensemble of bound atoms or molecules that is intermediate in size between a simple molecule and a nanoparticle; that is, up to a few nanometers (nm) in diameter.  The term microcluster may be used for ensembles with up to couple dozen atoms.
 Clusters with a definite number and type of atoms in a specific arrangement are often considered a specific chemical compound and are studied as such.  For example, fullerene is a cluster of 60 carbon atoms arranged as the vertices of a truncated icosahedron, and decaborane is a cluster of 10 boron atoms forming an incomplete icosahedron, surrounded by 14 hydrogen atoms.
 The term is most commonly used for ensembles consisting of several atoms of the same element, or of a few different elements, bonded in a three-dimensional arrangement.  Transition metals and main group elements form especially robust clusters.[1] Indeed, in some contexts, the term may refer specifically to a metal cluster, whose core atoms are metals and contains at least one metallic bond.[2]  In this case, the qualifier polynuclear specifies a cluster with more than one metal atom, and heteronuclear specifies a cluster with at least two different metal elements. Naked metal clusters have only metal atoms, as opposed to clusters with outer shell of other elements.  The latter may be functional groups such as cyanide or methyl, covalently bonded to the core atoms; or many be ligands attached by coordination bonds, such as carbon monoxide, halides, isocyanides, alkenes, and hydrides. 
 However, the terms is also used for ensembles that contain no metals (such as the boranes and carboranes) and whose core atoms are held together by covalent or ionic bonds.  It is also used for ensembles of atoms or molecules held together by Van der Waals or hydrogen bonds, as in water clusters.
 Clusters may play an important role in phase transitions such as precipitation from solutions, condensation and evaporation of liquids and solids, freezing and melting, and adsorbtion to other materials.[citation needed]"
Cohesion_(chemistry),Chemistry,1,"Cohesion (from Latin cohaesiō ""cling"" or ""unity"") or cohesive attraction or cohesive force is the action or property of like molecules sticking together, being mutually attractive. It is an intrinsic property of a substance that is caused by the shape and structure of its molecules, which makes the distribution of  surrounding electrons irregular when molecules get close to one another, creating electrical attraction that can maintain a microscopic structure such as a water drop. In other words, cohesion allows for surface tension, creating a ""solid-like"" state upon which light-weight or low-density materials can be placed.
 Water, for example, is strongly cohesive as each molecule may make four hydrogen bonds to other water molecules in a tetrahedral configuration. This results in a relatively strong Coulomb force between molecules. In simple terms, the polarity (a state in which a molecule is oppositely charged on its poles) of water molecules allows them to be attracted to each other. The polarity is due to the electronegativity of the atom of oxygen: oxygen is more electronegative than the atoms of hydrogen, so the electrons they share through the covalent bonds are more often close to oxygen rather than hydrogen. These are called polar covalent bonds, covalent bonds between atoms that thus become oppositely charged.[1] In the case of a water molecule, the hydrogen atoms carry positive charges while the oxygen atom has a negative charge. This charge polarization within the molecule allows it to align with adjacent molecules through strong intermolecular hydrogen bonding, rendering the bulk liquid cohesive. Van der Waals gases such as methane, however, have weak cohesion due only to van der Waals forces that operate by induced polarity  in non-polar molecules.
 Cohesion, along with adhesion (attraction between unlike molecules), helps explain phenomena such as meniscus, surface tension and capillary action.
 Mercury in a glass flask is a good example of the effects of the ratio between cohesive and adhesive forces. Because of its high cohesion and low adhesion to the glass, mercury does not spread out to cover the bottom of the flask, and if enough is placed in the flask to cover the bottom, it exhibits a strongly convex meniscus, whereas the meniscus of water is concave. Mercury will not wet the glass, unlike water and many other liquids,[2] and if the glass is tipped, it will 'roll' around inside.
"
Colligative_property,Chemistry,1,"In chemistry, colligative properties are those properties of solutions that depend on the ratio of the number of solute particles to the number of solvent molecules in a solution, and not on the nature of the chemical species present.[1] The number ratio can be related to the various units for concentration of a solution, for example, molarity, molality, normality (chemistry), etc. The assumption that solution properties are independent of nature of solute particles is exact only for ideal solutions, and is approximate for dilute real solutions. In other words, colligative properties are a set of solution properties that can be reasonably approximated by the assumption that the solution is ideal. 
 Only properties which result from the dissolution of nonvolatile solute in a volatile liquid solvent are considered.[2] They are essentially solvent properties which are changed by the presence of the solute. The solute particles displace some solvent molecules in the liquid phase and thereby reduce the concentration of solvent, so that the colligative properties are independent of the nature of the solute. The word colligative is derived from the Latin colligatus meaning bound together.[3] This indicates that all colligative properties have a common feature, namely that they are related only to the number of solute molecules relative to the number of solvent molecules and not to the nature of the solute.[4] Colligative properties include: 
 For a given solute-solvent mass ratio, all colligative properties are inversely proportional to solute molar mass.
 Measurement of colligative properties for a dilute solution of a non-ionized solute such as urea or glucose in water or another solvent can lead to determinations of relative molar masses, both for small molecules and for polymers which cannot be studied by other means. Alternatively, measurements for ionized solutes can lead to an estimation of the percentage of dissociation taking place.
 Colligative properties are studied mostly for dilute solutions, whose behavior may be approximated as that of an ideal solution. In fact, all of the properties listed above are colligative only in the dilute limit: at higher concentrations, the freezing point depression, boiling point elevation, vapour pressure elevation or depression, and osmotic pressure are all dependent on the chemical nature of the solvent and the solute.
"
Colloid,Chemistry,1,"
 In chemistry, a colloid is a phase separated mixture in which one substance of microscopically dispersed insoluble or soluble particles is suspended throughout another substance. Sometimes the dispersed substance alone is called the colloid;[1] the term colloidal suspension refers unambiguously to the overall mixture (although a narrower sense of the word suspension is distinguished from colloids by larger particle size). Unlike a solution, whose solute and solvent constitute only one phase, a colloid has a dispersed phase (the suspended particles) and a continuous phase (the medium of suspension) that arise by phase separation. Typically, colloids do not completely settle or take a long time to settle completely into two separated layers.
 The dispersed-phase particles have a diameter between approximately 1 and 1000 nanometers.[2] Such particles are normally easily visible in an optical microscope, although at the smaller size range (r < 250 nm), an ultramicroscope or an electron microscope may be required. Homogeneous mixtures with a dispersed phase in this size range may be called colloidal aerosols, colloidal emulsions, colloidal foams, colloidal dispersions, or hydrosols. The dispersed-phase particles or droplets are affected largely by the surface chemistry present in the colloid.
 Some colloids are translucent because of the Tyndall effect, which is the scattering of light by particles in the colloid. Other colloids may be opaque or have a slight color.  The cytoplasm of living cells is an example of a colloid, containing many types of biomolecular condensate.
 Colloidal suspensions are the subject of interface and colloid science. This field of study was introduced in 1845 by Italian chemist Francesco Selmi[3] and further investigated since 1861 by Scottish scientist Thomas Graham.[4]"
Combustion,Chemistry,1,"Combustion, or burning,[1] is a high-temperature exothermic redox chemical reaction between a fuel (the reductant) and an oxidant, usually atmospheric oxygen, that produces oxidized, often gaseous products, in a mixture termed as smoke.  Combustion does not always result in fire, but when it does, a flame is a characteristic indicator of the reaction. While the activation energy must be overcome to initiate combustion (e.g., using a lit match to light a fire), the heat from a flame may provide enough energy to make the reaction self-sustaining.  Combustion is often a complicated sequence of elementary radical reactions. Solid fuels, such as wood and coal, first undergo endothermic pyrolysis to produce gaseous fuels whose combustion then supplies the heat required to produce more of them. Combustion is often hot enough that incandescent light in the form of either glowing or a flame is produced. A simple example can be seen in the combustion of hydrogen and oxygen into water vapor, a reaction commonly used to fuel rocket engines. This reaction releases 242 kJ/mol of heat and reduces the enthalpy accordingly (at constant temperature and pressure):
 Combustion of an organic fuel in air is always exothermic because the double bond in O2 is much weaker than other double bonds or pairs of single bonds, and therefore the formation of the stronger bonds in the combustion products CO2 and H2O results in the release of energy.[2] The bond energies in the fuel play only a minor role, since they are similar to those in the combustion products; e.g., the sum of the bond energies of CH4 is nearly the same as that of CO2. The heat of combustion is approximately −418 kJ per mole of O2 used up in the combustion reaction, and can be estimated from the elemental composition of the fuel.[2] Uncatalyzed combustion in air requires relatively high temperatures. Complete combustion is stoichiometric concerning the fuel, where there is no remaining fuel, and ideally, no residual oxidant. Thermodynamically, the chemical equilibrium of combustion in air is overwhelmingly on the side of the products. However, complete combustion is almost impossible to achieve, since the chemical equilibrium is not necessarily reached, or may contain unburnt products such as carbon monoxide, hydrogen and even carbon (soot or ash). Thus, the produced smoke is usually toxic and contains unburned or partially oxidized products. Any combustion at high temperatures in atmospheric air, which is 78 percent nitrogen, will also create small amounts of several nitrogen oxides, commonly referred to as NOx, since the combustion of nitrogen is thermodynamically favored at high, but not low temperatures. Since burning is rarely clean, fuel gas cleaning or catalytic converters may be required by law.
 Fires occur naturally, ignited by lightning strikes or by volcanic products. Combustion (fire) was the first controlled chemical reaction discovered by humans, in the form of campfires and bonfires, and continues to be the main method to produce energy for humanity. Usually, the fuel is carbon, hydrocarbons, or more complicated mixtures such as wood that contains partially oxidized hydrocarbons. The thermal energy produced from combustion of either fossil fuels such as coal or oil, or from renewable fuels such as firewood, is harvested for diverse uses such as cooking, production of electricity or industrial or domestic heating. Combustion is also currently the only reaction used to power rockets. Combustion is also used to destroy (incinerate) waste, both nonhazardous and hazardous.
 Oxidants for combustion have high oxidation potential and include atmospheric or pure oxygen, chlorine, fluorine, chlorine trifluoride, nitrous oxide and nitric acid. For instance, hydrogen burns in chlorine to form hydrogen chloride with the liberation of heat and light characteristic of combustion. Although usually not catalyzed, combustion can be catalyzed by platinum or vanadium, as in the contact process.
"
Commission_on_Isotopic_Abundances_and_Atomic_Weights,Chemistry,1,"The Commission on Isotopic Abundances and Atomic Weights (CIAAW) is an international scientific committee of the International Union of Pure and Applied Chemistry (IUPAC) under its Division of Inorganic Chemistry. Since 1899, it is entrusted with periodic critical evaluation of atomic weights of chemical elements and other cognate data, such as the isotopic composition of elements.[1] The biennial CIAAW Standard Atomic Weights are accepted as the authoritative source in science and appear worldwide on the periodic table wall charts.[2] The use of CIAAW Standard Atomic Weights is also required legally, for example, in calculation of calorific value of natural gas (ISO 6976:1995), or in gravimetric preparation of primary reference standards in gas analysis (ISO 6142:2006). In addition, until 2019 the definition of kelvin, the SI unit for thermodynamic temperature, made direct reference to the isotopic composition of oxygen and hydrogen as recommended by CIAAW.[3] The latest CIAAW report was published in February 2016.[4]  After 20 May 2019 a new definition for kelvin came into force based on the Boltzmann constant.
"
Compression_(physics),Chemistry,1,"In mechanics, compression is the application of balanced inward (""pushing"") forces to different points on a material or structure, that is, forces with no net sum or torque directed so as to reduce its size in one or more directions.[1] It is contrasted with tension or traction, the application of balanced outward (""pulling"") forces; and with shearing forces, directed so as to displace layers of the material parallel to each other.  The compressive strength of materials and structures is an important engineering consideration.
 In uniaxial compression, the forces are directed along one direction only, so that they act towards decreasing the object's length along that direction.  The compressive forces may also be applied in multiple directions; for example inwards along the edges of a plate or all over the side surface of a cylinder, so as to reduce its area (biaxial compression), or inwards over the entire surface of a body, so as to reduce its volume.
 Technically, a material is under a state of compression, at some specific point and along a specific direction 



x


{  x}
, if the normal component of the stress vector across a surface with normal direction 



x


{  x}
 is directed opposite to 



x


{  x}
.  If the stress vector itself is opposite to 



x


{  x}
, the material is said to be under normal compression or pure compressive stress along 



x


{  x}
.  In a solid, the amount of compression generally depends on the direction 



x


{  x}
, and the material may be under compression along some directions but under traction along others.  If the stress vector is purely compressive and has the same magnitude for all directions, the material is said to be under isotropic or hydrostatic compression at that point.  This is the only type of static compression that liquids and gases can bear.
 In a mechanical longitudinal wave, or compression wave, the medium is displaced in the wave's direction, resulting in areas of compression and rarefaction.
"
Chemical_compound,Chemistry,1,"
 A chemical compound is a chemical substance composed of many identical molecules (or molecular entities) composed of atoms from more than one element held together by chemical bonds. A molecule consisting of atoms of only one element is therefore not a compound.
 There are four types of compounds, depending on how the constituent atoms are held together:
 A chemical formula specifies the number of atoms of each element in a compound molecule, using the standard abbreviations for the chemical elements and numerical subscripts. For example, a water molecule has formula H2O indicating two hydrogen atoms bonded to one oxygen atom. Many chemical compounds have a unique CAS number identifier assigned by the Chemical Abstracts Service. Globally, more than 350,000 chemical compounds (including mixtures of chemicals) have been registered for production and use.[1] A compound can be converted to a different chemical substance by interaction with a second substance via a chemical reaction. In this process, bonds between atoms may be broken in either or both of the interacting substances, and new bonds formed.
"
Concentration,Chemistry,1,"In chemistry, concentration is the abundance of a constituent divided by the total volume of a mixture. Several types of mathematical description can be distinguished: mass concentration, molar concentration, number concentration, and volume concentration.[1] A concentration can be any kind of chemical mixture, but most frequently solutes and solvents in solutions. The molar (amount) concentration has variants such as normal concentration and osmotic concentration.
"
Condensation,Chemistry,1,"Condensation is the change of the physical state of matter from the gas phase into the liquid phase, and is the reverse of vaporization. The word most often refers to the water cycle.[1] It can also be defined as the change in the state of water vapor to liquid water when in contact with a liquid or solid surface or cloud condensation nuclei within the atmosphere. When the transition happens from the gaseous phase into the solid phase directly, the change is called deposition.
"
Condosity,Chemistry,1,"Condosity is a comparative measurement of electrical conductivity of a solution.
 The condosity of any given solution is defined as the molar concentration of a sodium chloride (NaCl) solution that has the same specific electrical conductance as the solution under test.[1][2][3] By way of example, for a 2 Molar potassium chloride (KCl) solution, the condosity would be expected to be somewhat greater than 2.0. This is because potassium is a better conductor than sodium.
"
Electrical_conductor,Chemistry,1,"In physics and electrical engineering, a conductor is an object or type of material that allows the flow of charge (electrical current) in one or more directions. Materials made of metal are common electrical conductors. Electrical current is generated by the flow of negatively charged electrons, positively charged holes, and positive or negative ions in some cases. 
 In order for current to flow, it is not necessary for one charged particle to travel from the machine producing the current to that consuming it. Instead, the charged particle simply needs to nudge its neighbor a finite amount who will nudge its neighbor and on and on until a particle is nudged into the consumer, thus powering the machine. Essentially what is occurring is a long chain of momentum transfer between mobile charge carriers; the Drude model of conduction describes this process more rigorously. This momentum transfer model makes metal an ideal choice for a conductor; metals, characteristically, possess a delocalized sea of electrons which gives the electrons enough mobility to collide and thus effect a momentum transfer. 
 As discussed above, electrons are the primary mover in metals; however, other devices such as the cationic electrolyte(s) of a battery, or the mobile protons of the proton conductor of a fuel cell rely on positive charge carriers. Insulators are non-conducting materials with few mobile charges that support only insignificant electric currents.
"
Conformation_(chemistry),Chemistry,1,"In chemistry, conformational isomerism is a form of stereoisomerism in which the isomers can be interconverted just by rotations about formally single bonds (refer to figure on single bond rotation). While any two arrangements of atoms in a molecule that differ by rotation about single bonds can be referred to as different conformations, conformations that correspond to local minima on the energy surface are specifically called conformational isomers or conformers.[1] Conformations that correspond to local maxima on the energy surface are the transition states between the local-minimum conformational isomers.  Rotations about single bonds involve overcoming a rotational energy barrier to interconvert one conformer to another. If the energy barrier is low, there is free rotation[2] and a sample of the compound exists as a rapidly equilibrating mixture of multiple conformers; if the energy barrier is high enough then there is restricted rotation, a molecule may exist for a relatively long time period as a stable rotational isomer or rotamer (an isomer arising from hindered single-bond rotation). When the time scale for interconversion is long enough for isolation of individual rotamers (usually arbitrarily defined as a half-life of interconversion of 1000 seconds or longer), the isomers are termed atropisomers (see: atropisomerism).[1][3][4] The ring-flip of substituted cyclohexanes constitutes another common form of conformational isomerism.
 Conformational isomers are thus distinct from the other classes of stereoisomers (i. e. configurational isomers) where interconversion necessarily involves breaking and reforming of chemical bonds.[5] For example, L/D- and R/S- configurations of organic molecules have different handedness and optical activities, and can only be interconverted by breaking one or more bonds connected to the chiral atom and reforming a similar bond in a different direction or spatial orientation. They also differ from geometric (cis/trans) isomers, another class of stereoisomers, which require the π-component of double bonds to break for interconversion. (Although the distinction is not always clear-cut, since certain bonds that are formally single bonds actually have double bond character that becomes apparent only when secondary resonance contributors are considered, like the C–N bonds of amides, for instance.) Due to rapid interconversion, conformers are usually not isolable at room temperature.
 The study of the energetics between different conformations is referred to as conformational analysis.[6] It is useful for understanding the stability of different isomers, for example, by taking into account the spatial orientation and through-space interactions of substituents. In addition, conformational analysis can be used to predict and explain product selectivity, mechanisms, and rates of reactions.[7] Conformational analysis also plays an important role in rational, structure-based drug design.
"
Conjugate_acid,Chemistry,1,"A conjugate acid, within the Brønsted–Lowry acid–base theory, is a  chemical compound formed when an acid donates a proton (H+) to a base—in other words, it is a base with a hydrogen ion added to it, as in the reverse reaction it loses a hydrogen ion. On the other hand, a conjugate base is what is left over after an acid has donated a proton during a chemical reaction. Hence, a conjugate base is a species formed by the removal of a proton from an acid, as in the reverse reaction it is able to gain a hydrogen ion.[1] Because some acids are capable of releasing multiple protons, the conjugate base of an acid may itself be acidic.
 In summary, this can be represented as the following chemical reaction:
 Johannes Nicolaus Brønsted and Martin Lowry introduced the Brønsted–Lowry theory,
which proposed that any compound that can transfer a proton to any other compound is an acid, and the compound that accepts the proton is a base. A proton is a nuclear particle with a unit positive electrical charge; it is represented by the symbol H+ because it constitutes the nucleus of a hydrogen atom,[2] that is, a hydrogen cation.
 A cation can be a conjugate acid, and an anion can be a conjugate base, depending on which substance is involved and which acid–base theory is the viewpoint. The simplest anion which can be a conjugate base is the solvated electron whose conjugate acid is the atomic hydrogen.
"
Conjugate_base,Chemistry,1,"A conjugate acid, within the Brønsted–Lowry acid–base theory, is a  chemical compound formed when an acid donates a proton (H+) to a base—in other words, it is a base with a hydrogen ion added to it, as in the reverse reaction it loses a hydrogen ion. On the other hand, a conjugate base is what is left over after an acid has donated a proton during a chemical reaction. Hence, a conjugate base is a species formed by the removal of a proton from an acid, as in the reverse reaction it is able to gain a hydrogen ion.[1] Because some acids are capable of releasing multiple protons, the conjugate base of an acid may itself be acidic.
 In summary, this can be represented as the following chemical reaction:
 Johannes Nicolaus Brønsted and Martin Lowry introduced the Brønsted–Lowry theory,
which proposed that any compound that can transfer a proton to any other compound is an acid, and the compound that accepts the proton is a base. A proton is a nuclear particle with a unit positive electrical charge; it is represented by the symbol H+ because it constitutes the nucleus of a hydrogen atom,[2] that is, a hydrogen cation.
 A cation can be a conjugate acid, and an anion can be a conjugate base, depending on which substance is involved and which acid–base theory is the viewpoint. The simplest anion which can be a conjugate base is the solvated electron whose conjugate acid is the atomic hydrogen.
"
Conjugated_system,Chemistry,1,"In chemistry, a conjugated system is a system of connected p orbitals with delocalized electrons in a molecule, which in general lowers the overall energy of the molecule and increases stability. It is conventionally represented as having alternating single and multiple bonds. Lone pairs, radicals or carbenium ions may be part of the system, which may be cyclic, acyclic, linear or mixed.  The term ""conjugated"" was coined in 1899 by the German chemist Johannes Thiele.[1] Conjugation is the overlap of one p orbital with another across an adjacent 
σ bond (in transition metals d orbitals can be involved).[2][3] A conjugated system has a region of overlapping p orbitals, bridging the interjacent locations that simple diagrams illustrate as not having a π bond. They allow a delocalization of π electrons across all the adjacent aligned p orbitals.[4]
The π electrons do not belong to a single bond or atom, but rather to a group of atoms.
 The largest conjugated systems are found in graphene, graphite, conductive polymers and carbon nanotubes.
"
Cooling_curve,Chemistry,1,"A cooling curve is a line graph that represents the change of phase of matter, typically from a gas to a solid or a liquid to a solid. The independent variable (X-axis) is time and the dependent variable (Y-axis) is temperature.[1] Below is an example of a cooling curve used in castings.
 
 The initial point of the graph is the starting temperature of the matter, here noted as the ""pouring temperature"". When the phase change occurs, there is a ""thermal arrest""; that is, the temperature stays constant. This is because the matter has more internal energy as a liquid or gas than in the state that it is cooling to. The amount of energy required for a phase change is known as latent heat. The ""cooling rate"" is the slope of the cooling curve at any point. 
"
Coordinate_covalent_bond,Chemistry,1,"A coordinate covalent bond,[1] also known as a dative bond,[2] dipolar bond,[3] or coordinate bond[4] is a kind of two-center, two-electron covalent bond in which the two electrons derive from the same atom.  The bonding of metal ions to ligands involves this kind of interaction.  This type of interaction is central to Lewis theory.
"
Coordination_complex,Chemistry,1,"A coordination complex consists of a central atom or ion, which is usually metallic and is called the coordination centre, and a surrounding array of bound molecules or ions, that are in turn known as ligands or complexing agents.[1][2][3]  Many metal-containing compounds, especially those of transition metals, are coordination complexes.[4] A coordination complex whose centre is a metal atom is called a metal complex of d block element.
"
Corrosion,Chemistry,1,"Corrosion is a natural process that converts a refined metal into a more chemically stable form such as oxide, hydroxide, or sulfide. It is the gradual destruction of materials (usually a metal) by chemical and/or electrochemical reaction with their environment.  Corrosion engineering is the field dedicated to controlling and preventing corrosion.
 In the most common use of the word, this means electrochemical oxidation of metal in reaction with an oxidant such as oxygen or sulfates. Rusting, the formation of iron oxides, is a well-known example of electrochemical corrosion. This type of damage typically produces oxide(s) or salt(s) of the original metal and results in a distinctive orange colouration. Corrosion can also occur in materials other than metals, such as ceramics or polymers, although in this context, the term ""degradation"" is more common. Corrosion degrades the useful properties of materials and structures including strength, appearance and permeability to liquids and gases.
 Many structural alloys corrode merely from exposure to moisture in air, but the process can be strongly affected by exposure to certain substances. Corrosion can be concentrated locally to form a pit or crack, or it can extend across a wide area more or less uniformly corroding the surface. Because corrosion is a diffusion-controlled process, it occurs on exposed surfaces. As a result, methods to reduce the activity of the exposed surface, such as passivation and chromate conversion, can increase a material's corrosion resistance. However, some corrosion mechanisms are less visible and less predictable.
 The chemistry of corrosion is quite complex but it may be considered essentially as an electrochemical phenomenon. During corrosion at a particular spot on the surface of the object made of iron, oxidation takes place and that spot behaves as an anode. The electrons released at this anodic spot move through the metal and go to another spot on the metal and reduce oxygen at that spot in presence of H+  (which is believed to be available from H2CO3 formed due to dissolution of carbon dioxide from air into water in moist air condition of atmosphere. Hydrogen ion in water may also be available due to dissolution of other acidic oxides from the atmosphere). This spot behaves as a cathode.
"
Coulomb,Chemistry,1,"
 The coulomb (symbol: C) is the International System of Units (SI) unit of electric charge. Under the 2019 redefinition of the SI base units, which took effect on 20 May 2019,[2] the coulomb is exactly 1/(1.602176634×10−19) (which is approximately 6.2415090744×1018, or 1.036×10−5 mol) elementary charges. The same number of electrons has the same magnitude but opposite sign of charge, that is, a charge of −1 C.
"
Covalent_bond,Chemistry,1,"A covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are known as shared pairs or bonding pairs, and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is known as covalent bonding.[1] For many molecules, the sharing of electrons allows each atom to attain the equivalent of a full outer shell, corresponding to a stable electronic configuration. In organic chemistry, covalent bonds are much more common than ionic bonds.
 Covalent bonding also includes many kinds of interactions, including σ-bonding, π-bonding, metal-to-metal bonding, agostic interactions, bent bonds, three-center two-electron bonds and three-center four-electron bonds.[2][3] The term covalent bond dates from 1939.[4] The prefix co- means jointly, associated in action, partnered to a lesser degree,  etc.; thus a ""co-valent bond"", in essence, means that the atoms share ""valence"", such as is discussed in valence bond theory.
 In the molecule H2, the hydrogen atoms share the two electrons via covalent bonding.[5] Covalency is greatest between atoms of similar electronegativities. Thus, covalent bonding does not necessarily require that the two atoms be of the same elements, only that they be of comparable electronegativity. Covalent bonding that entails the sharing of electrons over more than two atoms is said to be delocalized.
"
Critical_point_(thermodynamics),Chemistry,1,"In thermodynamics, a critical point (or critical state) is the end point of a phase equilibrium curve. The most prominent example is the liquid–vapor critical point, the end point of the pressure–temperature curve that designates conditions under which a liquid and its vapor can coexist. At higher temperatures, the gas cannot be liquefied by pressure alone. At the critical point, defined by a critical temperature Tc and a critical pressure pc, phase boundaries vanish. Other examples include the liquid–liquid critical points in mixtures.
"
Crystal,Chemistry,1,"
 A crystal or crystalline solid is a solid material whose constituents (such as atoms, molecules, or ions) are arranged in a highly ordered microscopic structure, forming a crystal lattice that extends in all directions.[1][2] In addition, macroscopic single crystals are usually identifiable by their geometrical shape, consisting of flat faces with specific, characteristic orientations. The scientific study of crystals and crystal formation is known as crystallography. The process of crystal formation via mechanisms of crystal growth is called crystallization or solidification.
 The word crystal derives from the Ancient Greek word κρύσταλλος (krustallos), meaning both ""ice"" and ""rock crystal"",[3] from κρύος (kruos), ""icy cold, frost"".[4][5] Examples of large crystals include snowflakes, diamonds, and table salt. Most inorganic solids are not crystals but polycrystals, i.e. many microscopic crystals fused together into a single solid. Examples of polycrystals include most metals, rocks, ceramics, and ice. A third category of solids is amorphous solids, where the atoms have no periodic structure whatsoever. Examples of amorphous solids include glass, wax, and many plastics.
 Despite the name, lead crystal, crystal glass, and related products are not crystals, but rather types of glass, i.e. amorphous solids.
 Crystals are often used in pseudoscientific practices such as crystal therapy, and, along with gemstones, are sometimes associated with spellwork in Wiccan beliefs and related religious movements.[6][7][8]"
Crystallography,Chemistry,1,"Crystallography is the experimental science of determining the arrangement of atoms in crystalline solids (see crystal structure). The word ""crystallography"" is derived from the Greek words crystallon ""cold drop, frozen drop"", with its meaning extending to all solids with some degree of transparency, and graphein ""to write"".  In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming that 2014 would be the International Year of Crystallography.[1] Before the development of X-ray diffraction crystallography (see below), the study of crystals was based on physical measurements of their geometry using a goniometer.[2] This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.
 Crystallographic methods now depend on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons.  Crystallographers often explicitly state the type of beam used, as in the terms X-ray crystallography, neutron diffraction and electron diffraction. These three types of radiation interact with the specimen in different ways.
 Because of these different forms of interaction, the three types of radiation are suitable for different crystallographic studies.
"
Cuvette,Chemistry,1,"A cuvette (French: cuvette = ""little vessel"") is a small tube-like container with straight sides and a circular or square cross section. It is sealed at one end, and made of a clear, transparent material such as plastic, glass, or fused quartz. Cuvettes are designed to hold samples for spectroscopic measurement, where a beam of light is passed through the sample within the cuvette to measure the absorbance, transmittance, fluorescence intensity, fluorescence polarization, or fluorescence lifetime of the sample. This measurement is done with a spectrophotometer.
"
D-block,Chemistry,1,"A block of the periodic table is a set of elements unified by the orbitals their valence electrons or vacancies lie in.[1] The term appears to have been first used by Charles Janet.[2] Each block is named after its characteristic orbital: s-block, p-block, d-block, and f-block.
 The block names (s, p, d, and f) are derived from the spectroscopic notation for the value of an electron's azimuthal quantum number: shape (0), principal (1), diffuse (2), or fundamental (3). Succeeding notations proceed in alphabetical order, as g, h, etc.
"
Dalton%27s_law,Chemistry,1,"Dalton's law (also called Dalton's law of partial pressures) states that in a mixture of non-reacting gases, the total  pressure exerted is equal to the sum of the partial pressures of the individual gases.[1] This empirical law was observed by John Dalton in 1801 and published in 1802.[2] Dalton's law is related to the ideal gas laws.
"
DI_water,Chemistry,1,"Purified water is water that has been mechanically filtered or processed to remove impurities and make it suitable for use. Distilled water has been the most common form of purified water, but, in recent years, water is more frequently purified by other processes including capacitive deionization, reverse osmosis, carbon filtering, microfiltration, ultrafiltration, ultraviolet oxidation, or electrodeionization. Combinations of a number of these processes have come into use to produce ultrapure water of such high purity that its trace contaminants are measured in parts per billion (ppb) or parts per trillion (ppt). 
 Purified water has many uses, largely in the production of medications, in science and engineering laboratories and industries, and is produced in a range of purities. It is also used in the commercial beverage industry as the primary ingredient of any given trademarked bottling formula, in order to maintain product consistency. It can be produced on site for immediate use or purchased in containers. Purified water in colloquial English can also refer to water which has been treated (""rendered potable"") to neutralize, but not necessarily remove contaminants considered harmful to humans or animals.
"
Deliquescence,Chemistry,1,"Hygroscopy is the phenomenon of attracting and holding water molecules via either absorption  or adsorption from the surrounding environment, which is usually at normal or room temperature. If water molecules become suspended among the substance's molecules, adsorbing substances can become physically changed, e.g., changing in volume, boiling point, viscosity or some other physical characteristic or property of the substance.
"
Delocalized_electron,Chemistry,1,"In chemistry, delocalized electrons are electrons in a molecule, ion or solid metal that are not associated with a single atom or a covalent bond.[1]
The term delocalization is general and can have slightly different meanings in different fields. In organic chemistry, this refers to resonance in conjugated systems and aromatic compounds. In solid-state physics, this refers to free electrons that facilitate electrical conduction. In quantum chemistry, this refers to molecular orbital electrons that have extended over several adjacent atoms.
"
Density,Chemistry,1,"
 The density (more precisely, the volumetric mass density; also known as specific mass), of a substance is its mass per unit volume. The symbol most often used for density is ρ (the lower case Greek letter rho), although the Latin letter D can also be used. Mathematically, density is defined as mass divided by volume:[1] where ρ is the density, m is the mass, and V is the volume. In some cases (for instance, in the United States oil and gas industry), density is loosely defined as its weight per unit volume,[2] although this is scientifically inaccurate – this quantity is more specifically called specific weight.
 For a pure substance the density has the same numerical value as its mass concentration.
Different materials usually have different densities, and density may be relevant to buoyancy, purity and packaging. Osmium and iridium are the densest known elements at standard conditions for temperature and pressure.
 To simplify comparisons of density across different systems of units, it is sometimes replaced by the dimensionless quantity ""relative density"" or ""specific gravity"", i.e. the ratio of the density of the material to that of a standard material, usually water. Thus a relative density less than one means that the substance floats in water.
 The density of a material varies with temperature and pressure. This variation is typically small for solids and liquids but much greater for gases. Increasing the pressure on an object decreases the volume of the object and thus increases its density. Increasing the temperature of a substance (with a few exceptions) decreases its density by increasing its volume. In most materials, heating the bottom of a fluid results in convection of the heat from the bottom to the top, due to the decrease in the density of the heated fluid. This causes it to rise relative to more dense unheated material.
 The reciprocal of the density of a substance is occasionally called its specific volume, a term sometimes used in thermodynamics. Density is an intensive property in that increasing the amount of a substance does not increase its density; rather it increases its mass.
"
Denticity,Chemistry,1,"Denticity refers to the number of donor groups in a single ligand that bind to a central atom  in a coordination complex.[1][2]  In many cases, only one atom in the ligand binds to the metal, so the denticity equals one, and the ligand is said to be monodentate (sometimes called unidentate). Ligands with more than one bonded atom are called polydentate or multidentate.  The word denticity is derived from dentis, the Latin word for tooth. The ligand is thought of as biting the metal at one or more linkage points. The denticity of a ligand is described with the Greek letter κ ('kappa').[3]  For example, κ6-EDTA describes an EDTA ligand that coordinates through 6 non-contiguous atoms.
 Denticity is different from hapticity because hapticity refers exclusively to ligands where the coordinating atoms are contiguous.  In these cases the η ('eta') notation is used.[4] Bridging ligands use the μ ('mu') notation.[5][6]"
Dependent_and_independent_variables,Chemistry,1,"Dependent and independent variables are variables in mathematical modeling, statistical modeling and experimental sciences. Dependent variables receive this name because, in an experiment, their values are studied under the supposition or hypothesis that they depend, by some law or rule (e.g., by a mathematical function), on the values of other variables. Independent variables, in turn, are not seen as depending on any other variable in the scope of the experiment in question; thus, even if the existing dependency is invertible (e.g., by finding the inverse function when it exists), the nomenclature is kept if the inverse dependency is not the object of study in the experiment. In this sense, some common independent variables are time, space, density, mass, fluid flow rate,[1][2] and previous values of some observed value of interest (e.g. human population size) to predict future values (the dependent variable).[3]Variables are given a special name that only applies to experimental investigations. The independent variable is the variable the experimenter changes or controls and is assumed to have a direct effect on the dependent variable. Two examples of common independent variables are gender and educational level.
 Of the two, it is always the dependent variable whose variation is being studied, by altering inputs, also known as regressors in a statistical context. In an experiment, any variable that the experimenter manipulates[clarification needed] can be called an independent variable.  Models and experiments test the effects that the independent variables have on the dependent variables. Sometimes, even if their influence is not of direct interest, independent variables may be included for other reasons, such as to account for their potential confounding effect.
"
Deposition_(chemistry),Chemistry,1,"In chemistry, deposition occurs when molecules settle out of a solution.
 Deposition can be viewed as a reverse process to dissolution or particle re-entrainment. It is a phase change from the gaseous state to a solid, without passing through the liquid state, also called re-sublimation. 
"
Dewar_flask,Chemistry,1,"A vacuum flask (also known as a Dewar flask, Dewar bottle or thermos) is an insulating storage vessel that greatly lengthens the time over which its contents remain hotter or cooler than the flask's surroundings. Invented by Sir James Dewar in 1892, the vacuum flask consists of two flasks, placed one within the other and joined at the neck. The gap between the two flasks is partially evacuated of air, creating a near-vacuum which significantly reduces heat transfer by conduction or convection.
 Vacuum flasks are used domestically, to keep beverages hot or cold for extended periods of time, and for many purposes in industry.
"
Diatomic,Chemistry,1,"
 Diatomic molecules are molecules composed of only two atoms, of the same or different chemical elements. The prefix di- is of Greek origin, meaning ""two"". If a diatomic molecule consists of two atoms of the same element, such as hydrogen (H2) or oxygen (O2), then it is said to be homonuclear. Otherwise, if a diatomic molecule consists of two different atoms, such as carbon monoxide (CO) or nitric oxide (NO), the molecule is said to be heteronuclear. The bond in a homonuclear diatomic molecule is non-polar.
 The only chemical elements that form stable homonuclear diatomic molecules at standard temperature and pressure (STP) (or typical laboratory conditions of 1 bar and 25 °C) are the gases hydrogen (H2), nitrogen (N2), oxygen (O2), fluorine (F2), and chlorine (Cl2).[1] The noble gases (helium, neon, argon, krypton, xenon, and radon) are also gases at STP, but they are monatomic. The homonuclear diatomic gases and noble gases together are called ""elemental gases"" or ""molecular gases"", to distinguish them from other gases that are chemical compounds.[2] At slightly elevated temperatures, the halogens bromine (Br2) and iodine (I2) also form diatomic gases.[3] All halogens have been observed as diatomic molecules, except for astatine and tennessine, which are uncertain.
 Other elements form diatomic molecules when evaporated, but these diatomic species repolymerize when cooled. Heating (""cracking"") elemental phosphorus gives diphosphorus, P2. Sulfur vapor is mostly disulfur (S2). Dilithium (Li2) and disodium (Na2)[4] are known in the gas phase. Ditungsten (W2) and dimolybdenum (Mo2) form with sextuple bonds in the gas phase. Dirubidium (Rb2) is diatomic.
"
Diatomic_molecule,Chemistry,1,"
 Diatomic molecules are molecules composed of only two atoms, of the same or different chemical elements. The prefix di- is of Greek origin, meaning ""two"". If a diatomic molecule consists of two atoms of the same element, such as hydrogen (H2) or oxygen (O2), then it is said to be homonuclear. Otherwise, if a diatomic molecule consists of two different atoms, such as carbon monoxide (CO) or nitric oxide (NO), the molecule is said to be heteronuclear. The bond in a homonuclear diatomic molecule is non-polar.
 The only chemical elements that form stable homonuclear diatomic molecules at standard temperature and pressure (STP) (or typical laboratory conditions of 1 bar and 25 °C) are the gases hydrogen (H2), nitrogen (N2), oxygen (O2), fluorine (F2), and chlorine (Cl2).[1] The noble gases (helium, neon, argon, krypton, xenon, and radon) are also gases at STP, but they are monatomic. The homonuclear diatomic gases and noble gases together are called ""elemental gases"" or ""molecular gases"", to distinguish them from other gases that are chemical compounds.[2] At slightly elevated temperatures, the halogens bromine (Br2) and iodine (I2) also form diatomic gases.[3] All halogens have been observed as diatomic molecules, except for astatine and tennessine, which are uncertain.
 Other elements form diatomic molecules when evaporated, but these diatomic species repolymerize when cooled. Heating (""cracking"") elemental phosphorus gives diphosphorus, P2. Sulfur vapor is mostly disulfur (S2). Dilithium (Li2) and disodium (Na2)[4] are known in the gas phase. Ditungsten (W2) and dimolybdenum (Mo2) form with sextuple bonds in the gas phase. Dirubidium (Rb2) is diatomic.
"
Diffusion,Chemistry,1,"Diffusion is the net movement of anything (for example, atom, ions, molecules) from a region of higher concentration to a region of lower concentration. Diffusion is driven by a gradient in concentration.
 The concept of diffusion is widely used in many fields, including physics (particle diffusion), chemistry, biology, sociology, economics, and finance (diffusion of people, ideas, and price values). The central idea of diffusion, however, is common to all of these: an object (for example, atom, idea, etc.) undergoing diffusion spreads out from a point or location at which there is a higher concentration of that object.
 A gradient is the change in the value of a quantity, for example, concentration, pressure, or temperature with the change in another variable, usually distance. A change in concentration over a distance is called a concentration gradient, a change in pressure over a distance is called a pressure gradient, and a change in temperature over a distance is called a temperature gradient.
 The word diffusion derives from the Latin word, diffundere, which means ""to spread  out.""
 A distinguishing feature of diffusion is that it depends on particle random walk, and results in mixing or mass transport without requiring directed bulk motion. Bulk motion, or bulk flow, is the characteristic of advection.[1] The term convection is used to describe the combination of both transport phenomena.
 If a diffusion process can be described by Fick's laws, it's called a normal diffusion (or Fickian diffusion); Otherwise, it's called an anomalous diffusion (or non-Fickian diffusion).
 When talking about the extent of diffusion, two length scales are used in two different scenarios:
"
Dimer_(chemistry),Chemistry,1,"A dimer (/ˈdaɪmər/) (di-, ""two"" + -mer, ""parts"") is an oligomer consisting of two monomers joined by bonds that can be either strong or weak, covalent or intermolecular. The term homodimer is used when the two molecules are identical (e.g. A–A) and heterodimer when they are not (e.g. A–B). The reverse of dimerisation is often called dissociation. When two oppositely charged ions associate into dimers, they are referred to as Bjerrum pairs.[1]"
Dipolar_bond,Chemistry,1,"A coordinate covalent bond,[1] also known as a dative bond,[2] dipolar bond,[3] or coordinate bond[4] is a kind of two-center, two-electron covalent bond in which the two electrons derive from the same atom.  The bonding of metal ions to ligands involves this kind of interaction.  This type of interaction is central to Lewis theory.
"
Dipole,Chemistry,1,"In electromagnetism, there are two kinds of dipoles:
 Dipoles, whether electric or magnetic, can be characterized by their dipole moment, a vector quantity. For the simple electric dipole, the electric dipole moment points from the negative charge towards the positive charge, and has a magnitude equal to the strength of each charge times the separation between the charges. (To be precise: for the definition of the dipole moment, one should always consider the ""dipole limit"", where, for example, the distance of the generating charges should converge to 0 while simultaneously, the charge strength should diverge to infinity in such a way that the product remains a positive constant.)
 For the magnetic (dipole) current loop, the magnetic dipole moment points through the loop (according to the right hand grip rule), with a magnitude equal to the current in the loop times the area of the loop.
 Similar to magnetic current loops, the electron particle and some other fundamental particles have magnetic dipole moments, as an electron generates a magnetic field identical to that generated by a very small current loop. However, an electron's magnetic dipole moment is not due to a current loop, but to an intrinsic property of the electron.[3] The electron may also have an electric dipole moment though such has yet to be observed (see electron electric dipole moment).
 A permanent magnet, such as a bar magnet, owes its magnetism to the intrinsic magnetic dipole moment of the electron. The two ends of a bar magnet are referred to as poles—not to be confused with monopoles, see Classification below)—and may be labeled ""north"" and ""south"". In terms of the Earth's magnetic field, they are respectively ""north-seeking"" and ""south-seeking"" poles: if the magnet were freely suspended in the Earth's magnetic field, the north-seeking pole would point towards the north and the south-seeking pole would point towards the south. The dipole moment of the bar magnet points from its magnetic south to its magnetic north pole. In a magnetic compass, the north pole of a bar magnet points north. However, that means that Earth's geomagnetic north pole is the south pole (south-seeking pole) of its dipole moment and vice versa.
 The only known mechanisms for the creation of magnetic dipoles are by current loops or quantum-mechanical spin since the existence of magnetic monopoles has never been experimentally demonstrated.
 The term comes from the Greek δίς (dis), ""twice""[4] and πόλος (polos), ""axis"".[5][6]"
Bond_dipole_moment,Chemistry,1,"The bond dipole moment[1] uses the idea of electric dipole moment to measure the polarity of a chemical bond within a molecule. It occurs whenever there is a separation of positive and negative charges. The bond dipole μ is given by:
 The bond dipole is modeled as δ+ — δ– with a distance d between the partial charges δ+ and δ–. It is a vector,  parallel to the bond axis, pointing from minus to plus,[2] as is conventional for electric dipole moment vectors.
 Chemists often draw the vector pointing from plus to minus.[3] This vector can be physically interpreted as the movement undergone by electrons when the two atoms are placed a distance d apart and allowed to interact, the electrons will move from their free state positions to be localised more around the more electronegative atom.
 The SI unit for electric dipole moment is the coulomb–meter. This is too large to be practical on the molecular scale.  
Bond dipole moments are commonly measured in debyes, represented by the symbol D, which is obtained by measuring the charge 



δ


{  \delta }
 in units of 10−10 statcoulomb and the distance d in Angstroms.  Based on the conversion factor of
10−10 statcoulomb being 0.208 units of elementary charge, so 1.0 debye results from an electron and a proton separated by 0.208 Å. A useful conversion factor is 1 D = 3.335 64×10−30 C m.[4] For diatomic molecules there is only one (single or multiple) bond so the bond dipole moment is the molecular dipole moment, with typical values in the range of 0 to 11 D.  At one extreme, a symmetrical molecule such as chlorine, Cl2, has zero dipole moment, while near the other extreme, gas phase potassium bromide, KBr, which is highly ionic, has a dipole moment of 10.5 D.[5][page needed] For polyatomic molecules, there is more than one bond. The total molecular dipole moment may be approximated as the vector sum of the individual bond dipole moments. Often bond dipoles are obtained by the reverse process: a known total dipole of a molecule can be decomposed into bond dipoles. This is done to transfer bond dipole moments to molecules that have the same bonds, but for which the total dipole moment is not yet known. The vector sum of the transferred bond dipoles gives an estimate for the total (unknown) dipole of the molecule.
"
Dispersion_(chemistry),Chemistry,1,"A dispersion is a system in which distributed particles of one material are dispersed in a continuous phase of another material.  The two phases may be in the same or different states of matter.
 Dispersions are classified in a number of different ways, including how large the particles are in relation to the particles of the continuous phase, whether or not precipitation occurs, and the presence of Brownian motion.  In general, dispersions of particles sufficiently large for sedimentation are called suspensions, while those of smaller particles are called colloids and solutions.
"
Dissociation_(chemistry),Chemistry,1,"Dissociation in chemistry and biochemistry is a general process in which molecules (or ionic compounds such as salts, or complexes) separate or split into smaller particles such as atoms, ions, or radicals, usually in a reversible manner. For instance, when an acid dissolves in water, a covalent bond between an electronegative atom and a hydrogen atom is broken by heterolytic fission, which gives a proton (H+) and a negative ion. Dissociation is the opposite of association or recombination.
"
Solvation,Chemistry,1,"Solvation describes the interaction of solvent with dissolved molecules. Both ionized and uncharged molecules interact strongly with solvent, and the strength and nature of this interaction influence many properties of the solute, including solubility, reactivity, and color, as well as influencing the properties of the solvent such as the viscosity and density.[1] In the process of solvation, ions are surrounded by a concentric shell of solvent.  Solvation is the process of reorganizing solvent and solute molecules into solvation complexes.  Solvation involves bond formation, hydrogen bonding, and van der Waals forces.  Solvation of a solute by water is called hydration.[2] Solubility of solid compounds depends on a competition between lattice energy and solvation, including entropy effects related to changes in the solvent structure.[3]"
Distillation,Chemistry,1,"
 Distillation is the process of separating the components or substances from a liquid mixture by using selective boiling and condensation. Distillation may result in essentially complete separation (nearly pure components), or it may be a partial separation that increases the concentration of selected components in the mixture. In either case, the process exploits differences in the relative volatility of the mixture's components. In industrial applications, distillation is a unit operation of practically universal importance, but it is a physical separation process, not a chemical reaction.
 Distillation has many applications. For example:
 An installation used for distillation, especially of distilled beverages, is a distillery. The distillation equipment itself is a still.
"
Double_bond,Chemistry,1,"A double bond in chemistry is a covalent bond between two atoms involving four bonding electrons as opposed to two in a single bond. Double bonds occur most commonly between two carbon atoms, for example in alkenes. Many double bonds exist between two different elements: for example, in a carbonyl group between a carbon atom and an oxygen atom. Other common double bonds are found in azo compounds (N=N), imines (C=N) and sulfoxides (S=O). In a skeletal formula, a double bond is drawn as two parallel lines (=) between the two connected atoms; typographically, the equals sign is used for this.[1][2] Double bonds were first introduced in chemical notation by Russian chemist Alexander Butlerov.[citation needed] Double bonds involving carbon are stronger than single bonds and are also shorter. The bond order is two. Double bonds are also electron-rich, which makes them potentially more reactive in the presence of a strong electron acceptor (as in addition reactions of the halogens).
"
Salt_metathesis_reaction,Chemistry,1,"A salt metathesis reaction, sometimes called a double replacement reaction, double displacement reaction,  is a chemical process involving the exchange of bonds between two non-reacting chemical species which results in the creation of products with similar or identical bonding affiliations.[1] This reaction is represented by the general scheme:
 The bond between the reacting species can be either ionic or covalent. Classically, these reactions result in the precipitation of one product.
 In older literature, the term double decomposition is frequently encountered. The term double decomposition is more specifically used when at least one of the substances does not dissolve in the solvent, as the ligand or ion exchange takes place in the solid state of the reactant. For example:
"
Ductility,Chemistry,1,"Ductility is a mechanical property commonly described as a material's amenability to drawing (e.g. into wire).[1] In materials science, ductility is defined by the degree to which a material can sustain plastic deformation under tensile stress before failure.[2][3] Ductility is an important consideration in engineering and manufacturing, defining a material's suitability for certain manufacturing operations (such as cold working) and its capacity to absorb mechanical overload.[4] Materials that are generally described as ductile include gold and copper.[5] Malleability, a similar mechanical property, is characterized by a material's ability to deform plastically without failure under compressive stress.[6][7] Historically, materials were considered malleable if they were amenable to forming by hammering or rolling.[1] Lead is an example of a material which is, relatively, malleable but not ductile.[5][8]"
Electric_charge,Chemistry,1,"Electric charge is the physical property of matter that causes it to experience a force when placed in an electromagnetic field. There are two types of electric charge: positive and negative (commonly carried by protons and electrons respectively). Like charges repel each other and unlike charges attract each other. An object with an absence of net charge is referred to as neutral. Early knowledge of how charged substances interact is now called classical electrodynamics, and is still accurate for problems that do not require consideration of quantum effects.
 Electric charge is a conserved property; the net charge of an isolated system, the amount of positive charge minus the amount of negative charge, cannot change. Electric charge is carried by subatomic particles. In ordinary matter, negative charge is carried by electrons, and positive charge is carried by the protons in the nuclei of atoms. If there are more electrons than protons in a piece of matter, it will have a negative charge, if there are fewer it will have a positive charge, and if there are equal numbers it will be neutral. Charge is quantized; it comes in integer multiples of individual small units called the elementary charge, e, about 1.602×10−19 coulombs,[1] which is the smallest charge which can exist freely (particles called quarks have smaller charges, multiples of 1/3e, but they are only found in combination, and always combine to form particles with integer charge). The proton has a charge of +e, and the electron has a charge of −e.
 Electric charges produce electric fields.[2] A moving charge also produces a magnetic field.[3] The interaction of electric charges with an electromagnetic field (combination of  electric and magnetic fields) is the source of the electromagnetic (or Lorentz) force,[4] which is one of the four fundamental forces in physics. The study of photon-mediated interactions among charged particles is called quantum electrodynamics.[5] The SI derived unit of electric charge is the coulomb (C) named after  French physicist Charles-Augustin de Coulomb. In electrical engineering, it is also common to use the ampere hour (Ah); in physics and chemistry, it is common to use the elementary charge (e as a unit). Chemistry also uses the Faraday constant as the charge on a mole of electrons. The lowercase symbol q often denotes charge.
"
Electrolyte,Chemistry,1,"An electrolyte is a substance that produces an electrically conducting solution when dissolved in a polar solvent, such as water. The dissolved electrolyte separates into cations and anions, which disperse uniformly through the solvent. Electrically, such a solution is neutral. If an electric potential is applied to such a solution, the cations of the solution are drawn to the electrode that has an abundance of electrons, while the anions are drawn to the electrode that has a deficit of electrons. The movement of anions and cations in opposite directions within the solution amounts to a current. This includes most soluble salts, acids, and bases. Some gases, such as hydrogen chloride, under conditions of high temperature or low pressure can also function as electrolytes. Electrolyte solutions can also result from the dissolution of some biological (e.g., DNA, polypeptides) and synthetic polymers (e.g., polystyrene sulfonate), termed ""polyelectrolytes"", which contain charged functional groups. A substance that dissociates into ions in solution acquires the capacity to conduct electricity. Sodium, potassium, chloride, calcium, magnesium, and phosphate are examples of electrolytes.
 In medicine, electrolyte replacement is needed when a person has prolonged vomiting or diarrhea, and as a response to strenuous athletic activity. Commercial electrolyte solutions are available, particularly for sick children (such as oral rehydration solution, Suero Oral, or Pedialyte) and athletes (sports drinks). Electrolyte monitoring is important in the treatment of anorexia and bulimia.
"
Electrochemical_cell,Chemistry,1,"
 An electrochemical cell is a device capable of either generating electrical energy from chemical reactions or using electrical energy to cause chemical reactions. The electrochemical cells which generate an electric current are called voltaic cells or galvanic cells and those that generate chemical reactions, via electrolysis for example, are called electrolytic cells.[1][2][better source needed] A common example of a galvanic cell is a standard 1.5 volt[3][better source needed] cell meant for consumer use.  A battery consists of one or more cells, connected in parallel, series or series-and-parallel pattern.
"
Electromagnetic_radiation,Chemistry,1,"
 In physics, electromagnetic radiation (EM radiation or EMR) refers to the waves (or their quanta, photons) of the electromagnetic field, propagating (radiating) through space, carrying electromagnetic radiant energy.[1] It includes radio waves, microwaves, infrared, (visible) light, ultraviolet, X-rays, and gamma rays.[2] Classically, electromagnetic radiation consists of electromagnetic waves, which are synchronized oscillations of electric and magnetic fields. In a vacuum, electromagnetic waves travel at the speed of light, commonly denoted c. In homogeneous, isotropic media, the oscillations of the two fields are perpendicular to each other and perpendicular to the direction of energy and wave propagation, forming a transverse wave. The wavefront of electromagnetic waves emitted from a point source (such as a light bulb) is a sphere. The position of an electromagnetic wave within the electromagnetic spectrum can be characterized by either its frequency of oscillation or its wavelength.  Electromagnetic waves of different frequency are called by different names since they have different sources and effects on matter.  In order of increasing frequency and decreasing wavelength these are: radio waves, microwaves, infrared radiation, visible light, ultraviolet radiation, X-rays and gamma rays.[3] Electromagnetic waves are emitted by electrically charged particles undergoing acceleration,[4][5] and these waves can subsequently interact with other charged particles, exerting force on them.  EM waves carry energy, momentum and angular momentum away from their source particle and can impart those quantities to matter with which they interact.  Electromagnetic radiation is associated with those EM waves that are free to propagate themselves (""radiate"") without the continuing influence of the moving charges that produced them, because they have achieved sufficient distance from those charges. Thus, EMR is sometimes referred to as the far field. In this language, the near field refers to EM fields near the charges and current that directly produced them, specifically electromagnetic induction and electrostatic induction phenomena.
 In quantum mechanics, an alternate way of viewing EMR is that it consists of photons, uncharged elementary particles with zero rest mass which are the quanta of the electromagnetic force, responsible for all electromagnetic interactions.[6] Quantum electrodynamics is the theory of how EMR interacts with matter on an atomic level.[7]  Quantum effects provide additional sources of EMR, such as the transition of electrons to lower energy levels in an atom and black-body radiation.[8] The energy of an individual photon is quantized and is greater for photons of higher frequency. This relationship is given by Planck's equation E = hf, where E is the energy per photon, f is the frequency of the photon, and h is Planck's constant. A single gamma ray photon, for example, might carry ~100,000 times the energy of a single photon of visible light.
 The effects of EMR upon chemical compounds and biological organisms depend both upon the radiation's power and its frequency. EMR of visible or lower frequencies (i.e., visible light, infrared, microwaves, and radio waves) is called non-ionizing radiation, because its photons do not individually have enough energy to ionize atoms or molecules or break chemical bonds. The effects of these radiations on chemical systems and living tissue are caused primarily by heating effects from the combined energy transfer of many photons. In contrast, high frequency ultraviolet, X-rays and gamma rays are called ionizing radiation, since individual photons of such high frequency have enough energy to ionize molecules or break chemical bonds. These radiations have the ability to cause chemical reactions and damage living cells beyond that resulting from simple heating, and can be a health hazard.
"
Electromagnetic_spectrum,Chemistry,1,"
 The electromagnetic spectrum is the range of frequencies (the spectrum) of electromagnetic radiation and their respective wavelengths and photon energies.
 The electromagnetic spectrum covers electromagnetic waves with frequencies ranging from below one hertz to above 1025 hertz, corresponding to wavelengths from thousands of kilometers down to a fraction of the size of an atomic nucleus.  This frequency range is divided into separate bands, and the electromagnetic waves within each frequency band are called by different names; beginning at the low frequency (long wavelength) end of the spectrum these are: radio waves, microwaves, infrared, visible light, ultraviolet, X-rays, and gamma rays at the high-frequency (short wavelength) end.  The electromagnetic waves in each of these bands have different characteristics, such as how they are produced, how they interact with matter, and their practical applications.  The limit for long wavelengths is the size of the universe itself, while it is thought that the short wavelength limit is in the vicinity of the Planck length.[4]  Gamma rays, X-rays, and high ultraviolet are classified as ionizing radiation as their photons have enough energy to ionize atoms, causing chemical reactions.
 In most of the frequency bands above, a technique called spectroscopy can be used to physically separate waves of different frequencies, producing a spectrum showing the constituent frequencies.  Spectroscopy is used to study the interactions of electromagnetic waves with matter.[5]  Other technological uses are described under electromagnetic radiation.
"
Electromagnetism,Chemistry,1,"
 Electromagnetism is a branch of physics involving the study of the electromagnetic force, a type of physical interaction that occurs between electrically charged particles. The electromagnetic force is carried by electromagnetic fields composed of electric fields and magnetic fields, and it is responsible for electromagnetic radiation such as light.  It is one of the four fundamental interactions (commonly called forces) in nature, together with the strong interaction, the weak interaction, and gravitation.[1] At high energy the weak force and electromagnetic force are unified as a single electroweak force.
 Electromagnetic phenomena are defined in terms of the electromagnetic force, sometimes called the Lorentz force, which includes both electricity and magnetism as different manifestations of the same phenomenon. The electromagnetic force plays a major role in determining the internal properties of most objects encountered in daily life.  The electromagnetic attraction between atomic nuclei and their orbital electrons holds atoms together.  Electromagnetic forces are responsible for the chemical bonds between atoms which create molecules, and intermolecular forces.   The electromagnetic force governs all chemical processes, which arise from interactions between the electrons of neighboring atoms.
 There are numerous mathematical descriptions of the electromagnetic field.  In classical electrodynamics, electric fields are described as electric potential and electric current. In Faraday's law, magnetic fields are associated with electromagnetic induction and magnetism, and Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents.
 The theoretical implications of electromagnetism, particularly the establishment of the speed of light based on properties of the ""medium"" of propagation (permeability and permittivity), led to the development of special relativity by Albert Einstein in 1905.
"
Electromotive_force,Chemistry,1,"In electromagnetism and electronics, electromotive force (emf, denoted 





E




{  {\mathcal {E}}}
 and measured in volts),[1] is the electrical action produced by a non-electrical source.[2] Devices (known as transducers) provide an emf[3] by converting other forms of energy into electrical energy,[3] such as batteries (which convert chemical energy) or generators (which convert mechanical energy).[2]  Sometimes an analogy to water pressure is used to describe electromotive force.[4]  (The word ""force"" in this case is not used to mean forces of interaction between bodies).
 In electromagnetic induction, emf can be defined around a closed loop of conductor as the electromagnetic work that would be done on an  electric charge (an electron in this instance) if it travels once around the loop.[5] For a time-varying magnetic flux linking a loop, the electric potential's scalar field is not defined due to a circulating electric vector field, but an emf nevertheless does work that can be measured as a virtual electric potential around the loop.[6] In the case of a two-terminal device (such as an electrochemical cell) which is modeled as a Thévenin's equivalent circuit, the equivalent emf can be measured as the open-circuit potential difference, or voltage, between the two terminals. This potential difference can drive an electric current if an external circuit is attached to the terminals, in which case the device becomes the voltage source of that circuit.
"
Electron,Chemistry,1,"The electron  is a subatomic particle, symbol e− or β−, whose electric charge is negative one elementary charge.[9] Electrons belong to the first generation of the lepton particle family,[10] and are generally thought to be elementary particles because they have no known components or substructure.[1]  The electron has a mass that is approximately 1/1836 that of the proton.[11] Quantum mechanical properties of the electron include an intrinsic angular momentum (spin) of a half-integer value, expressed in units of the reduced Planck constant, ħ. Being fermions, no two electrons can occupy the same quantum state, in accordance with the Pauli exclusion principle.[10] Like all elementary particles, electrons exhibit properties of both particles and waves: they can collide with other particles and can be diffracted like light. The wave properties of electrons are easier to observe with experiments than those of other particles like neutrons and protons because electrons have a lower mass and hence a longer de Broglie wavelength for a given energy.
 Electrons play an essential role in numerous physical phenomena, such as electricity, magnetism, chemistry and thermal conductivity, and they also participate in gravitational, electromagnetic and weak interactions.[12] Since an electron has charge, it has a surrounding electric field, and if that electron is moving relative to an observer, said observer will observe it to generate a magnetic field. Electromagnetic fields produced from other sources will affect the motion of an electron according to the Lorentz force law. Electrons radiate or absorb energy in the form of photons when they are accelerated. Laboratory instruments are capable of trapping individual electrons as well as electron plasma by the use of electromagnetic fields. Special telescopes can detect electron plasma in outer space. Electrons are involved in many applications such as electronics, welding, cathode ray tubes, electron microscopes, radiation therapy, lasers, gaseous ionization detectors and particle accelerators.
 Interactions involving electrons with other subatomic particles are of interest in fields such as chemistry and nuclear physics. The Coulomb force interaction between the positive protons within atomic nuclei and the negative electrons without, allows the composition of the two known as atoms. Ionization or differences in the proportions of negative electrons versus positive nuclei changes the binding energy of an atomic system. The exchange or sharing of the electrons between two or more atoms is the main cause of chemical bonding.[13] In 1838, British natural philosopher Richard Laming first hypothesized the concept of an indivisible quantity of electric charge to explain the chemical properties of atoms.[3] Irish physicist George Johnstone Stoney named this charge 'electron' in 1891, and J. J. Thomson and his team of British physicists identified it as a particle in 1897.[5]  Electrons can also participate in nuclear reactions, such as nucleosynthesis in stars, where they are known as beta particles. Electrons can be created through beta decay of radioactive isotopes and in high-energy collisions, for instance when cosmic rays enter the atmosphere.  The antiparticle of the electron is called the positron; it is identical to the electron except that it carries electrical charge  of the opposite sign. When an electron collides with a positron, both particles can be annihilated, producing gamma ray photons.
"
Electron_configuration,Chemistry,1,"In atomic physics and quantum chemistry, the electron configuration is the distribution of electrons of an atom or molecule (or other physical structure) in atomic or molecular orbitals.[1] For example, the electron configuration of the neon atom is 1s2 2s2 2p6, using the notation explained below.
 Electronic configurations describe each electron as moving independently in an orbital, in an average field created by all other orbitals. Mathematically, configurations are described by Slater determinants or configuration state functions.
 According to the laws of quantum mechanics, for systems with only one electron, a level of energy is associated with each electron configuration and in certain conditions, electrons are able to move from one configuration to another by the emission or absorption of a quantum of energy, in the form of a photon.
 Knowledge of the electron configuration of different atoms is useful in understanding the structure of the periodic table of elements. This is also useful for describing the chemical bonds that hold atoms together. In bulk materials, this same idea helps explain the peculiar properties of lasers and semiconductors.
"
Electron_deficiency,Chemistry,1,"Electron deficiency is a term describing atoms or molecules having fewer than the number of electrons required for maximum stability. For each atom in a molecule, main group atoms having less than 8 electrons or transition metal atoms having less than 18 electrons are described as electron-deficient. For a whole molecule, molecules which have an incompletely filled set of bonding molecular orbitals are considered to be electron-deficient. Thus, CH3 and BH3 are electron-deficient, while methane (CH4) and diborane (B2H6) are not. Not surprisingly, electron-deficient molecules are typically strongly electron-attracting (electrophilic).[1] As the most extreme form of electron deficiency one can consider the metallic bond.
"
Electron_pair,Chemistry,1,"In chemistry, an electron pair or a Lewis pair consists of two electrons that occupy the same molecular orbital but have opposite spins. The electron pair concept was introduced in a 1916 paper of Gilbert N. Lewis.[1] Because electrons are fermions, the Pauli exclusion principle forbids these particles from having exactly the same quantum numbers. Therefore, the only way to occupy the same orbital, i.e. have the same orbital quantum numbers, is to differ in the spin quantum number. This limits the number of electrons in the same orbital to exactly two.
 The pairing of spins is often energetically favorable and electron pairs therefore play a very large role in chemistry. They can form a chemical bond between two atoms, or they can occur as a lone pair of valence electrons.  They also fill the core levels of an atom.
 Because the spins are paired, the magnetic moment of the electrons cancels and the contribution of the pair to the magnetic properties will in general be a diamagnetic one.
 Although a strong tendency to pair off electrons can be observed in chemistry, it is also possible that electrons occur as unpaired electrons.
 In the case of metallic bonding the magnetic moments also compensate to a large extent, but the bonding is more communal so that individual pairs of electrons cannot be distinguished and it is better to consider the electrons as a collective 'ocean'.
 A very special case of electron pair formation occurs in superconductivity: the formation of Cooper pairs.
"
Electron_shell,Chemistry,1,"

In chemistry and atomic physics, an electron shell may be thought of as an orbit followed by electrons around an atom's nucleus. The closest shell to the nucleus is called the ""1 shell"" (also called the ""K shell""), followed by the ""2 shell"" (or ""L shell""), then the ""3 shell"" (or ""M shell""), and so on farther and farther from the nucleus. The shells correspond to the principal quantum numbers (n = 1, 2, 3, 4 ...) or are labeled alphabetically with the letters used in X-ray notation (K, L, M, …).
 Each shell can contain only a fixed number of electrons: The first shell can hold up to two electrons, the second shell can hold up to eight (2 + 6) electrons, the third shell can hold up to 18 (2 + 6 + 10) and so on. The general formula is that the nth shell can in principle hold up to 2(n2) electrons.[1] For an explanation of why electrons exist in these shells see electron configuration.[2] Each shell consists of one or more subshells, and each subshell consists of one or more atomic orbitals.
"
Electronegativity,Chemistry,1,"Electronegativity, symbol χ, measures the tendency of an atom to attract a shared pair of electrons (or electron density).[1] An atom's electronegativity is affected by both its atomic number and the distance at which its valence electrons reside from the charged nucleus. The higher the associated electronegativity, the more an atom or a substituent group attracts electrons.
 On the most basic level, electronegativity is determined by factors like the nuclear charge (the more protons an atom has, the more ""pull"" it will have on electrons) and the number and location of other electrons in the atomic shells (the more electrons an atom has, the farther from the nucleus the valence electrons will be, and as a result the less positive charge they will experience—both because of their increased distance from the nucleus, and because the other electrons in the lower energy core orbitals will act to shield the valence electrons from the positively charged nucleus).
 The opposite of electronegativity is electropositivity: a measure of an element's ability to donate electrons.
 The term ""electronegativity"" was introduced by Jöns Jacob Berzelius in 1811,[2]
though the concept was known before that and was studied by many chemists including Avogadro.[2]
In spite of its long history, an accurate scale of electronegativity was not developed until 1932, when Linus Pauling proposed an electronegativity scale which depends on bond energies, as a development of valence bond theory.[3] It has been shown to correlate with a number of other chemical properties. Electronegativity cannot be directly measured and must be calculated from other atomic or molecular properties. Several methods of calculation have been proposed, and although there may be small differences in the numerical values of the electronegativity, all methods show the same periodic trends between elements.
 The most commonly used method of calculation is that originally proposed by Linus Pauling. This gives a dimensionless quantity, commonly referred to as the Pauling scale (χr), on a relative scale running from 0.79 to 3.98 (hydrogen = 2.20). When other methods of calculation are used, it is conventional (although not obligatory) to quote the results on a scale that covers the same range of numerical values: this is known as an electronegativity in Pauling units.
 As it is usually calculated, electronegativity is not a property of an atom alone, but rather a property of an atom in a molecule.[4] Properties of a free atom include ionization energy and electron affinity. It is to be expected that the electronegativity of an element will vary with its chemical environment,[5] but it is usually considered to be a transferable property, that is to say that similar values will be valid in a variety of situations.
 Caesium is the least electronegative element (0.79); fluorine is the most (3.98). Francium and caesium were originally both assigned 0.7; caesium's value was later refined to 0.79, but no experimental data allows a similar refinement for francium. However, francium's ionization energy is known to be slightly higher than caesium's, in accordance with the relativistic stabilization of the 7s orbital, and this in turn implies that francium is in fact more electronegative than caesium.
"
Electrophile,Chemistry,1,"In 
chemistry, an electrophile is a chemical species that forms bonds with nucleophiles by accepting an electron pair.[1] Because electrophiles accept electrons, they are Lewis acids.[2] Most electrophiles are positively charged, have an atom that carries a partial positive charge, or have an atom that does not have an octet of electrons. 
 Electrophiles mainly interact with nucleophiles through addition and substitution reactions. Frequently seen electrophiles in organic syntheses include cations such as H+ and NO+, polarized neutral molecules such as HCl, alkyl halides, acyl halides, and carbonyl compounds, polarizable neutral molecules such as Cl2 and Br2, oxidizing agents such as organic peracids, chemical species that do not satisfy the octet rule such as carbenes and radicals, and some Lewis acids such as BH3  and DIBAL.
"
Electrosynthesis,Chemistry,1,"
Electrosynthesis in chemistry is the synthesis of chemical compounds in an electrochemical cell.[1][2][3][4]  Compared to ordinary redox reaction, electrosynthesis sometimes offers improved selectivity and yields. Electrosynthesis is actively studied as a science and also has industrial applications. Electro-oxidation has potential for wastewater treatment as well.
"
Chemical_element,Chemistry,1,"
 In chemistry, an element is a pure substance which cannot be broken down by chemical means, consisting of atoms which have identical numbers of protons in their atomic nuclei. The number of protons in the nucleus is the defining property of an element, and is referred to as the atomic number (represented by the symbol Z).[1] Chemical elements constitute all of the baryonic matter of the universe.
 In total, 118 elements have been identified. The first 94 occur naturally on Earth, and the remaining 24 are synthetic elements produced in nuclear reactions. Save for unstable radioactive elements (radionuclides) which decay quickly, nearly all of the elements are available industrially in varying amounts.
 When different elements are combined, they may produce a chemical reaction and form into compounds due to chemical bonds holding the constituent atoms together. Only a minority of elements are found uncombined as relatively pure native element minerals. Nearly all other naturally-occurring elements appear as compounds or mixtures; for example, atmospheric air is primarily a mixture of the elements nitrogen, oxygen, and argon.
 The history of the discovery and use of the elements began with primitive human societies that discovered native minerals like carbon, sulfur, copper and gold (though the concept of a chemical element was not yet understood). Attempts to classify materials such as these resulted in the concepts of classical elements, alchemy, and various similar theories throughout human history.
 Much of the modern understanding of elements is attributed to Dmitri Mendeleev, a Russian chemist who published the first recognizable periodic table in 1869. The properties of the chemical elements are summarized in this table, which organizes them by increasing atomic number into rows (""periods"") in which the columns (""groups"") share recurring (""periodic"") physical and chemical properties. The use of the periodic table allows chemists to derive relationships between various elements and predict the behavior of theoretical but undiscovered new ones; the discovery and synthesis of further new elements is an ongoing area of scientific study.
"
Elementary_reaction,Chemistry,1,"An elementary reaction is a chemical reaction in which one or more chemical species react directly to form products in a single reaction step and with a single transition state. In practice, a reaction is assumed to be elementary if no reaction intermediates have been detected or need to be postulated to describe the reaction on a molecular scale.[1] An apparently elementary reaction may be in fact a stepwise reaction, i.e. a complicated sequence of chemical reactions, with reaction intermediates of variable lifetimes.
 In a unimolecular elementary reaction, a molecule A dissociates or isomerises to form the products(s)
 At constant temperature, the rate of such a reaction is proportional to the concentration of the species A
 In a bimolecular elementary reaction, two atoms, molecules, ions or radicals, A and B, react together to form the product(s)
 The rate of such a reaction, at constant temperature, is proportional to the product of the concentrations of the species A and B
 The rate expression for an elementary bimolecular reaction is sometimes referred to as the Law of Mass Action as it was first proposed by Guldberg and Waage in 1864. An example of this type of reaction is a cycloaddition reaction.
This rate expression can be derived from first principles by using collision theory for ideal gases. For the case of dilute fluids equivalent results have been obtained from simple probabilistic arguments.[2] According to collision theory the probability of three chemical species reacting simultaneously with each other in a termolecular elementary reaction is negligible. Hence such termolecular reactions are commonly referred as non-elementary reactions and can be broken down into a more fundamental set of bimolecular reactions,[3][4] in agreement with the law of mass action. It is not always possible to derive overall reaction schemes, but solutions based on rate equations are often possible in terms of steady-state or Michaelis-Menten approximations.
"
Enantiomer,Chemistry,1,"In chemistry, an enantiomer (/ɪˈnæntiəmər, ɛ-, -tioʊ-/[1] ə-NAN-tee-ə-mər; from Greek  ἐνάντιος (enántios) 'opposite', and  μέρος (méros) 'part') (also named optical isomer,[2] antipode,[3] or optical antipode[4]) is one of two stereoisomers that are mirror images of each other that are non-superposable (not identical), much as one's left and right hands are mirror images of each other that cannot appear identical simply by reorientation.[5] A single chiral atom or similar structural feature in a compound causes that compound to have two possible structures which are non-superposable, each a mirror image of the other. Each member of the pair is termed an enantiomorph (enantio = opposite; morph = form);[6] the structural property is termed enantiomerism. The presence of multiple chiral features in a given compound increases the number of geometric forms possible, though there may still be some perfect-mirror-image pairs.
 A sample of a chemical is considered enantiopure (also termed enantiomerically pure) when it has, within the limits of detection, molecules of only one chirality.[7] When present in a symmetric environment, enantiomers have identical chemical and physical properties except for their ability to rotate plane-polarized light (+/−) by equal amounts but in opposite directions (although the polarized light can be considered an asymmetric medium). Such compounds are therefore described as optically active, with specific terms for each enantiomer based on the direction: a dextrorotatory compound rotates light a clockwise (+) direction whereas a levorotatory compound rotates light in a counter-clockwise (–) direction. A mixture of equal number of both enantiomers is called a racemic mixture or a racemate.[8] In a racemic mixture, the amount of positive rotation is exactly counteracted by the equal amount of negative rotation, so the net rotation is zero (the mixture is not optically active). For all intents and purposes, pairs of enantiomers have the same Gibbs free energy. However, theoretical physics predicts that due to parity violation of the weak nuclear force (the only force in nature that can ""tell left from right""), there is actually a minute difference in energy between enantiomers (on the order of 10−12 eV or 10−10 kJ/mol or less) due to the weak neutral current mechanism. This difference in energy is far smaller than energy changes caused by even a trivial change in molecular conformation and far too small to measure by current technology, and is therefore chemically inconsequential.[9][10][11] Enantiomer members often have different chemical reactions with other enantiomer substances. Since many biological molecules are enantiomers, there is sometimes a marked difference in the effects of two enantiomers on biological organisms. In drugs, for example, often only one of a drug's enantiomers is responsible for the desired physiological effects, while the other enantiomer is less active, inactive, or sometimes even productive of adverse effects. Owing to this discovery, drugs composed of only one enantiomer (""enantiopure"") can be developed to make the drug work better and sometimes eliminate some side effects. An example is eszopiclone (Lunesta), which is just a single enantiomer of an older racemic drug called zopiclone. One enantiomer is responsible for all the desired effects, while the other enantiomer seems to be inactive, so the dose of eszopiclone is half that of zopiclone.
 In chemical synthesis of enantiomeric substances, non-enantiomeric precursors inevitably produce racemic mixtures. In the absence of an effective enantiomeric environment (precursor, chiral catalyst, or kinetic resolution), separation of a racemic mixture into its enantiomeric components is impossible, although certain racemic mixtures spontaneously crystallize in the form of a racemic conglomerate, in which crystals of the enantiomers are physically segregated and may be separated mechanically (e.g., the enantiomers of tartaric acid, whose crystallized enantiomers were separated with tweezers by Pasteur). However, most racemates will crystallize in crystals containing both enantiomers in a 1:1 ratio, arranged in a regular lattice.
"
Enantiomorph,Chemistry,1,"In geometry, a figure is chiral (and said to have chirality) if it is not identical to its mirror image, or, more precisely, if it cannot be mapped to its mirror image by rotations and translations alone. An object that is not chiral is said to be achiral.
 A chiral object and its mirror image are said to be enantiomorphs. The word chirality is derived from the Greek χείρ (cheir), the hand, the most familiar chiral object; the word enantiomorph stems from the Greek ἐναντίος (enantios) 'opposite' + μορφή (morphe) 'form'.
"
Endothermic_process,Chemistry,1,"An endothermic process is any process with an increase in the enthalpy H (or internal energy U) of the system.[1] In such a process, a closed system usually absorbs thermal energy from its surroundings, which is heat transfer into the system. It may be a chemical process, such as dissolving ammonium nitrate in water, or a physical process, such as the melting of ice cubes. 
 The term was coined by Marcellin Berthelot from the Greek roots endo-, derived from the word ""endon"" (ἔνδον) meaning ""within"", and the root ""therm"" (θερμ-), meaning ""hot"" or ""warm"" in the sense that a reaction depends on absorbing heat if it is to proceed. The opposite of an endothermic process is an exothermic process, one that releases or ""gives out"" energy, usually in the form of heat and sometimes as electrical energy.[2] Thus in each term (endothermic and exothermic) the prefix refers to where heat (or electrical energy) goes as the process occurs.
"
Energy,Chemistry,1,"
 
 In physics, energy is the quantitative property that must be transferred to an object in order to perform work on, or to heat, the object.[note 1]  Energy is a conserved quantity; the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The SI unit of energy is the joule, which is the energy transferred to an object by the work of moving it a distance of 1 metre against a force of 1 newton.
 Common forms of energy include the kinetic energy of a moving object, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), the elastic energy stored by stretching solid objects, the chemical energy released when a fuel burns, the radiant energy carried by light, and the thermal energy due to an object's temperature.
 Mass and energy are closely related. Due to mass–energy equivalence, any object that has mass when stationary (called rest mass) also has an equivalent amount of energy whose form is called rest energy, and any additional energy (of any form) acquired by the object above that rest energy will increase the object's total mass just as it increases its total energy. For example, after heating an object, its increase in energy could be measured as a small increase in mass, with a sensitive enough scale.
 Living organisms require energy to stay alive, such as the energy humans get from food.  Human civilization requires energy to function, which it gets from  energy resources such as fossil fuels, nuclear fuel, or renewable energy. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.
"
Amount_of_substance,Chemistry,1,"In chemistry, the amount of substance in a given sample of matter is defined as the number of discrete atomic-scale particles in it divided by the Avogadro constant NA. In a truly atomistic view, the amount of substance is simply the number of particles that constitute the substance.[1][2][3] The particles or entities may be molecules, atoms, ions, electrons, or other, depending on the context. The value of the Avogadro constant NA has been defined as 6.02214076×1023 mol−1. In the truly atomistic view, 1 mol = 6.02214076×1023 particles (the Avogadro number) [4] and therefore the conversion constant is simply NA = 1.[3] The amount of substance is sometimes referred to as the chemical amount.
 The mole (symbol: mol) is a unit of amount of substance in the International System of Units, defined (since 2019) by fixing the Avogadro constant at the given value.  Historically, the mole was defined as the amount of substance in 12 grams of the carbon-12 isotope. As a consequence, the mass of one mole of a chemical compound, in grams, is numerically equal (for all practical purposes) to the mass of one molecule of the compound, in daltons, and the molar mass of an isotope in grams per mole is equal to the mass number. For example, a molecule of water has a mass of about 18.015  daltons on average, whereas a mole of water (which contains 6.02214076×1023 water molecules) has a total mass of about 18.015 grams.
 In chemistry, because of the law of multiple proportions, it is often much more convenient to work with amounts of substances (that is, number of moles or of molecules) than with masses (grams) or volumes (liters).  For example, the chemical fact ""1 molecule of oxygen (O2) will react with 2 molecules of hydrogen (H2) to make 2 molecules of water (H2O)"" can also be stated as ""1 mole of O2 will react with 2 moles of H2 to form 2 moles of water"". The same chemical fact, expressed in terms of masses, would be ""32 g  (1 mole) of oxygen  will react with approximately 4.0304 g (2 moles of H2) hydrogen to make approximately 36.0304 g (2 moles) of water"" (and the numbers would depend on the isotopic composition of the reagents). In terms of volume, the numbers would depend on the pressure and temperature of the reagents and products.  For the same reasons, the concentrations of reagents and products in solution are often specified in moles per liter, rather than grams per liter.
 The amount of substance is also a convenient concept in thermodynamics.  For example, the pressure of a certain quantity of a noble gas in a recipient of a given volume, at a given temperature, is directly related to the number of molecules in the gas (through the ideal gas law), not to its mass.
 This technical sense of the term ""amount of substance"" should not be confused with the general sense of ""amount"" in the English language. The latter may refer to other measurements such as mass or volume,[5] rather than the number of particles. There are proposals to replace ""amount of substance"" with more easily-distinguishable terms, such as enplethy[6]  and stoichiometric amount.[5] The IUPAC recommends that ""amount of substance"" should be used instead of ""number of moles"", just as the quantity mass should not be called ""number of kilograms"".[7]"
Enthalpy,Chemistry,1,"Enthalpy /ˈɛnθəlpi/ (listen) is a property of a thermodynamic system, defined as the sum of the system's internal energy and the product of its pressure and volume.[1][2] 
It is a convenient state function preferred in many measurements in chemical, biological, and physical systems at a constant pressure. The pressure-volume term expresses the work required to establish the system's physical dimensions, i.e. to make room for it by displacing its surroundings.[3][4] As a state function, enthalpy depends only on the final configuration of internal energy, pressure, and volume, and not on the path taken to achieve it.
 The unit of measurement for enthalpy in the International System of Units (SI) is the joule. Other historical conventional units still in use include the British thermal unit (BTU) and the calorie.
 The total enthalpy of a system cannot be measured directly, because the internal energy contains components that are unknown, not easily accessible, or are not of interest in thermodynamics. In practice, a change in enthalpy (ΔH) is the preferred expression for measurements at constant pressure, because it simplifies the description of energy transfer. When matter transfer into or out of the system is also prevented, the enthalpy change equals the energy exchanged with the environment by heat. For calibration of enthalpy changes a specific and convenient reference point is established. Enthalpies for chemical substances at constant pressure usually refer to standard state: most commonly 1 bar (100 kPa) pressure. Standard state does not strictly specify a temperature, but expressions for enthalpy generally reference the standard heat of formation at 25 °C (298 K). For endothermic processes, the change ΔH is a positive value, and is negative in exothermic (heat-releasing) processes.
 The enthalpy of an ideal gas is independent of its pressure, and depends only on its temperature, which correlates to its internal energy. Real gases at common temperatures and pressures often closely approximate this behavior, which simplifies practical thermodynamic design and analysis.
"
Enthalpy_of_fusion,Chemistry,1,"The enthalpy of fusion of a substance, also known as (latent) heat of fusion is the change in its enthalpy resulting from providing energy, typically heat, to a specific quantity of the substance to change its state from a solid to a liquid, at constant pressure. For example, when melting 1 kg of ice (at 0 °C under a wide range of pressures), 333.55 kJ of energy is absorbed with no temperature change. The heat of solidification (when a substance changes from liquid to solid) is equal and opposite.
 This energy includes the contribution required to make room for any associated change in volume by displacing its environment against ambient pressure. The temperature at which the phase transition occurs is the melting point or the freezing point, according to context. By convention, the pressure is assumed to be 1 atm (101.325 kPa) unless otherwise specified.
"
Entropy,Chemistry,1,"
 Entropy is an extensive property of a thermodynamic system. It quantifies the number Ω of microscopic configurations (known as microstates) that are consistent with the macroscopic quantities that characterize the system (such as its volume, pressure and temperature).[1] Under the assumption that each microstate of a large ensemble is equally probable, the entropy 



S


{  S}
 is the natural logarithm of the number of microstates, multiplied by the Boltzmann constant kB.
 The second law of thermodynamics states that the entropy of an isolated system never decreases over time. Isolated systems spontaneously evolve towards thermodynamic equilibrium, the state with maximum entropy. Non-isolated systems, like organisms, may lose entropy, provided their environment's entropy increases by at least that amount so that the total entropy either increases or remains constant. Therefore, the entropy in a specific system can decrease as long as the total entropy of the Universe does not. Entropy is a function of the state of the system, so the change in entropy of a system is determined by its initial and final states. In the idealization that a process is reversible, the entropy does not change, while irreversible processes always increase the total entropy.
 Because it is determined by the number of random microstates, entropy is related to the amount of additional information needed to specify the exact physical state of a system, given its macroscopic specification. For this reason, it is often said that entropy is an expression of the disorder, or randomness of a system, or of the lack of information about it.[2] The concept of entropy plays a central role in information theory.
"
Environmental_chemistry,Chemistry,1,"Environmental chemistry is the scientific study of the chemical and biochemical phenomena that occur in natural places. It should not be confused with green chemistry, which seeks to reduce potential pollution at its source. It can be defined as the study of the sources, reactions, transport, effects, and fates of chemical species in the air, soil, and water environments; and the effect of human activity and biological activity on these. Environmental chemistry is an interdisciplinary science that includes atmospheric, aquatic and soil chemistry, as well as heavily relying on analytical chemistry and being related to environmental and other areas of science.
 Environmental chemistry involves first understanding how the uncontaminated environment works, which chemicals in what concentrations are present naturally, and with what effects. Without this it would be impossible to accurately study the effects humans have on the environment through the release of chemicals.
 Environmental chemists draw on a range of concepts from chemistry and various environmental sciences to assist in their study of what is happening to a chemical species in the environment. Important general concepts from chemistry include understanding chemical reactions and equations, solutions, units, sampling, and analytical techniques.[1]"
Enzyme,Chemistry,1,"
 Enzymes /ˈɛnzaɪmz/ are proteins that act as biological catalysts (biocatalysts). Catalysts accelerate chemical reactions. The molecules upon which enzymes may act are called substrates, and the enzyme converts the substrates into different molecules known as products. Almost all metabolic processes in the cell need enzyme catalysis in order to occur at rates fast enough to sustain life.[1]:8.1 Metabolic pathways depend upon enzymes to catalyze individual steps. The study of enzymes is called enzymology and a new field of pseudoenzyme analysis has recently grown up, recognising that during evolution, some enzymes have lost the ability to carry out biological catalysis, which is often reflected in their amino acid sequences and unusual 'pseudocatalytic' properties.[2][3] Enzymes are known to catalyze more than 5,000 biochemical reaction types.[4] Other biocatalysts are catalytic RNA molecules, called ribozymes. Enzymes' specificity comes from their unique three-dimensional structures.
 Like all catalysts, enzymes increase the reaction rate by lowering its activation energy. Some enzymes can make their conversion of substrate to product occur many millions of times faster. An extreme example is orotidine 5'-phosphate decarboxylase, which allows a reaction that would otherwise take millions of years to occur in milliseconds.[5][6] Chemically, enzymes are like any catalyst and are not consumed in chemical reactions, nor do they alter the equilibrium of a reaction. Enzymes differ from most other catalysts by being much more specific. Enzyme activity can be affected by other molecules: inhibitors are molecules that decrease enzyme activity, and activators are molecules that increase activity. Many therapeutic drugs and poisons are enzyme inhibitors. An enzyme's activity decreases markedly outside its optimal temperature and pH, and many enzymes are (permanently) denatured when exposed to excessive heat, losing their structure and catalytic properties.
 Some enzymes are used commercially, for example, in the synthesis of antibiotics. Some household products use enzymes to speed up chemical reactions: enzymes in biological washing powders break down protein, starch or fat stains on clothes, and enzymes in meat tenderizer break down proteins into smaller molecules, making the meat easier to chew.
"
Empirical_formula,Chemistry,1,"In chemistry, the empirical formula of a chemical compound is the simplest positive integer ratio of atoms present in a compound.[1] A simple example of this concept is that the empirical formula of sulfur monoxide, or SO, would simply be SO, as is the empirical formula of disulfur dioxide, S2O2. Thus, sulfur monoxide and disulfur dioxide, both compounds of sulphur and oxygen, have the same empirical formula. However, their molecular formulas, which express the number of atoms in each molecule of a chemical compound, are not the same. 
 An empirical formula makes no mention of the arrangement or number of atoms. It is standard for many ionic compounds, like calcium chloride (CaCl2), and for macromolecules, such as silicon dioxide (SiO2).
 The molecular formula, on the other hand, shows the number of each type of atom in a molecule. The structural formula shows the arrangement of the molecule. It is also possible for different types of compounds to have equal empirical formulas.
 Samples are analyzed in specific elemental analysis tests to determine what percent of a particular element the sample is composed of.
"
List_of_types_of_equilibrium,Chemistry,1,"This is a list of various types of equilibrium, the condition of a system in which all competing influences are balanced.
"
Erlenmeyer_flask,Chemistry,1,"An Erlenmeyer flask, also known as a conical flask (British English)[1] or a titration flask, is a type of laboratory flask which features a flat bottom, a conical body, and a cylindrical neck. It is named after the German chemist Emil Erlenmeyer (1825–1909), who created it in 1860.[2] Erlenmeyer flasks have wide bases, with sides that taper upward to a short vertical neck. They may be graduated, and often spots of ground glass or enamel are used where they can be labeled with a pencil. It differs from the beaker in its tapered body and narrow neck.[3] Depending on the application, they may be constructed from glass or plastic,[4] in a wide range of volumes.[5] The mouth of the Erlenmeyer flask may have a beaded lip that can be stopped or covered. Alternatively, the neck may be fitted with ground glass or other connector for use with more specialized stoppers or attachment to other apparatus. A Büchner flask is a common design modification used for filtration under vacuum.
"
Exothermic_process,Chemistry,1,"In thermodynamics, the term exothermic process (exo- : ""outside"") describes a process or reaction that releases energy from the system to its surroundings, usually in the form of heat, but also in a form of light (e.g. a spark, flame, or flash), electricity (e.g. a battery), or sound (e.g. explosion heard when burning hydrogen). Its etymology stems from the Greek prefix έξω (exō, which means ""outwards"") and the Greek word θερμικός (thermikόs, which means ""thermal"").[1] The term exothermic was first coined by Marcellin Berthelot. 
 The opposite of an exothermic process is an endothermic process, one that absorbs energy usually in the form of heat.  The concept is frequently applied in the physical sciences to chemical reactions where chemical bond energy is converted to thermal energy (heat).
"
Intensive_and_extensive_properties,Chemistry,1,"Physical properties of materials and systems can often be categorized as being either intensive or extensive, according to how the property changes when the size (or extent) of the system changes. According to IUPAC, an intensive quantity is one whose magnitude is independent of the size of the system[1] whereas an extensive quantity is one whose magnitude is additive for subsystems.[2] This reflects the corresponding mathematical ideas of mean and measure, respectively.
 An intensive property is a bulk property, meaning that it is a local physical property of a system that does not depend on the system size or the amount of material in the system. Examples of intensive properties include temperature, T; refractive index, n; density, ρ; and hardness of an object, η.
 By contrast, extensive properties such as the mass, volume and entropy of systems are additive for subsystems because they increase and decrease as they grow larger and smaller, respectively.[3] These two categories are not exhaustive since some physical properties are neither exclusively intensive nor extensive.[4]  For example, the electrical impedance of two subsystems is additive when — and only when — they are combined in series; whilst if they are combined in parallel, the resulting impedance is less than that of either subsystem.
 The terms intensive and extensive quantities were introduced by American physicist and chemist Richard C. Tolman in 1917.[5]"
Extraction_(chemistry),Chemistry,1,"Extraction in chemistry is a separation process consisting in the separation of a substance from a matrix. Common examples include liquid-liquid extraction, and solid phase extraction. The distribution of a solute between two phases is an equilibrium condition described by partition theory. This is based on exactly how the analyte moves from the initial solvent into the extracting solvent. The term washing may also be used to refer to an extraction in which impurities are extracted from the solvent containing the desired compound.
"
Extrinsic_property,Chemistry,1,"In science and engineering, an intrinsic property is a property of a specified subject that exists itself or within the subject. An extrinsic property is not essential or inherent to the subject that is being characterized. For example, mass is an intrinsic property of any physical object, whereas weight is an extrinsic property that depends on the strength of the gravitational field in which the object is placed.
"
F-block,Chemistry,1,"A block of the periodic table is a set of elements unified by the orbitals their valence electrons or vacancies lie in.[1] The term appears to have been first used by Charles Janet.[2] Each block is named after its characteristic orbital: s-block, p-block, d-block, and f-block.
 The block names (s, p, d, and f) are derived from the spectroscopic notation for the value of an electron's azimuthal quantum number: shape (0), principal (1), diffuse (2), or fundamental (3). Succeeding notations proceed in alphabetical order, as g, h, etc.
"
Freezing,Chemistry,1,"Freezing is a phase transition where a liquid turns into a solid when its temperature is lowered below its freezing point. In accordance with the internationally established definition, freezing means the solidification phase change of a liquid or the liquid content of a substance, usually due to cooling.[1][2] Although some authors differentiate solidification from freezing as a process where a liquid turns into a solid by increasing the pressure, the two terms are used interchangeably.
 For most substances, the melting and freezing points are the same temperature; however, certain substances possess differing solid–liquid transition temperatures. For example, agar displays a hysteresis in its melting point and freezing point. It melts at 85°C (185°F) and solidifies from 32°C to 40°C (89.6°F to 104°F).[3]"
Faraday_constant,Chemistry,1,"The Faraday constant, denoted by the symbol F and sometimes stylized as ℱ, is named after Michael Faraday. In chemistry and physics, this constant represents the magnitude of electric charge per mole of electrons.[1] It has the currently accepted value
 Since 1 mol electrons = 6.02214076×1023 electrons (Avogadro's number),[3] the Faraday constant is equal to the elementary charge e, the magnitude of the charge of an electron multiplied by 1 mole:[4] One common use of the Faraday constant is in electrolysis calculations. One can divide the amount of charge in coulombs by the Faraday constant in order to find the chemical amount (in moles) of the element that has been oxidized.
 The value of F was first determined by weighing the amount of silver deposited in an electrochemical reaction in which a measured current was passed for a measured time, and using Faraday's law of electrolysis.[5]"
Faraday%27s_laws_of_electrolysis,Chemistry,1,Faraday's laws of electrolysis are quantitative relationships based on the electrochemical research published by Michael Faraday in 1833.[1][2] [3]
Fick%27s_laws_of_diffusion,Chemistry,1,"
 Fick's laws of diffusion describe diffusion and were derived by Adolf Fick in 1855.[1] They can be used to solve for the diffusion coefficient, D. Fick's first law can be used to derive his second law which in turn is identical to the diffusion equation.
 A diffusion process that obeys Fick's laws is called normal or Fickian diffusion; otherwise, it is called anomalous diffusion or non-Fickian diffusion.
"
Filtration,Chemistry,1,"
 Filtration is a physical, biological or chemical operation that separates solid matter and fluid from a mixture with a filter medium that has a complex structure through which only the fluid can pass. Solid particles that cannot pass through the filter medium are described as oversize and the fluid that passes through is called the filtrate.[1] Oversize particles may form a filter cake on top of the filter and may also block the filter lattice, preventing the fluid phase from crossing the filter, known as blinding. The size of the largest particles that can successfully pass through a filter is called the effective pore size of that filter. The separation of solid and fluid is imperfect; solids will be contaminated with some fluid and filtrate will contain fine particles (depending on the pore size, filter thickness and biological activity). Filtration occurs both in nature and in engineered systems; there are biological, geological, and industrial forms.[2]"
Rate_equation#First_order_reactions,Chemistry,1,"The rate law or rate equation for a chemical reaction is an equation that links the initial or forward reaction rate with the concentrations or pressures of the reactants and constant parameters (normally rate coefficients and partial reaction orders).[1] For many reactions, the initial rate is given by a power law such as
 where [A] and [B] express the concentration of the species A and B, usually in moles per liter (molarity, M). The exponents x and y are the partial orders of reaction for A and B and the overall reaction order is the sum of the exponents. These are often positive integers, but they may also be zero, fractional, or negative. The constant k is the reaction rate constant or rate coefficient of the reaction. Its value may depend on conditions such as temperature, ionic strength, surface area of an adsorbent, or light irradiation. If the reaction goes to completion, the rate equation for the reaction rate 



v

=

k
[

A


]

x


[

B


]

y




{  v\;=\;k[\mathrm {A} ]^{x}[\mathrm {B} ]^{y}}
 applies throughout the course of the reaction.
 Elementary (single-step) reactions and reaction steps have reaction orders equal to the stoichiometric coefficients for each reactant. The overall reaction order, i.e. the sum of stoichiometric coefficients of reactants, is always equal to the molecularity of the elementary reaction. However, complex (multi-step) reactions may or may not have reaction orders equal to their stoichiometric coefficients. This implies that the order and the rate equation of a given reaction cannot be reliably deduced from the stoichiometry and must be determined experimentally, since an unknown reaction mechanism could be either elementary or complex. When the experimental rate equation has been determined, it is often of use for deduction of the reaction mechanism.
 The rate equation of a reaction with an assumed multi-step mechanism can often be derived theoretically using quasi-steady state assumptions from the underlying elementary reactions, and compared with the experimental rate equation as a test of the assumed mechanism. The equation may involve a fractional order, and may depend on the concentration of an intermediate species.
 A reaction can also have an undefined reaction order with respect to a reactant if the rate is not simply proportional to some power of the concentration of that reactant; for example, one cannot talk about reaction order in the rate equation for a bimolecular reaction between adsorbed molecules:
"
Laboratory_flask,Chemistry,1,"Laboratory flasks are vessels or containers that fall into the category of laboratory equipment known as glassware.  In laboratory and other scientific settings, they are usually referred to simply as flasks.  Flasks come in a number of shapes and a wide range of sizes, but a common distinguishing aspect in their shapes is a wider vessel ""body"" and one (or sometimes more) narrower tubular sections at the top called necks which have an opening at the top.  Laboratory flask sizes are specified by the volume they can hold, typically in metric units such as milliliters (mL or ml) or liters (L or l).  Laboratory flasks have traditionally been made of glass, but can also be made of plastic.
 At the opening(s) at top of the neck of some glass flasks such as round-bottom flasks, retorts, or sometimes volumetric flasks, there are outer (or female) tapered (conical) ground glass joints. Some flasks, especially volumetric flasks, come with a laboratory rubber stopper, bung, or cap for capping the opening at the top of the neck. Such stoppers can be made of glass or plastic. Glass stoppers typically have a matching tapered inner (or male) ground glass joint surface, but often only of stopper quality.  Flasks which do not come with such stoppers or caps included may be capped with a rubber bung or cork stopper.
 Flasks can be used for making solutions or for holding, containing, collecting, or sometimes volumetrically measuring chemicals, samples, solutions, etc. for chemical reactions or other processes such as mixing, heating, cooling, dissolving, precipitation, boiling (as in distillation), or analysis.
"
Formal_charge,Chemistry,1,"In chemistry, a formal charge (FC) is the charge assigned to an atom in a molecule, assuming that electrons in all chemical bonds are shared equally between atoms, regardless of relative electronegativity.[1] When determining the best Lewis structure (or predominant resonance structure) for a molecule, the structure is chosen such that the formal charge on each of the atoms is as close to zero as possible.
 The formal charge of any atom in a molecule can be calculated by the following equation:
 where V is the number of valence electrons of the neutral atom in isolation (in its ground state); N is the number of non-bonding valence electrons on this atom in the molecule; and B is the total number of electrons shared in bonds with other atoms in the molecule.
"
Fractional_distillation,Chemistry,1,"Fractional distillation  is the separation of a mixture into its component parts, or fractions. Chemical compounds are separated by heating them to a temperature at which one or more fractions of the mixture will vaporize. It uses distillation to fractionate. Generally the component parts have boiling points that differ by less than 25 °C (45 °F) from each other under a pressure of one atmosphere. If the difference in boiling points is greater than 25 °C, a simple distillation is typically used.
"
Radical_(chemistry),Chemistry,1,"In chemistry, a radical is an atom, molecule, or ion that has an unpaired valence electron.[1][2]
With some exceptions, these unpaired electrons make radicals highly chemically reactive.  Many radicals spontaneously dimerize.  Most organic radicals have short lifetimes.
 A notable example of a radical is the hydroxyl radical (HO•), a molecule that has one unpaired electron on the oxygen atom. Two other examples are triplet oxygen and triplet carbene (:CH2) which have two unpaired electrons.
 Radicals may be generated in a number of ways, but typical methods involve redox reactions. Ionizing radiation, heat, electrical discharges, and electrolysis are known to produce radicals. Radicals are intermediates in many chemical reactions, more so than is apparent from the balanced equations.
 Radicals are important in combustion, atmospheric chemistry, polymerization, plasma chemistry, biochemistry, and many other chemical processes. A majority of natural products are generated by radical-generating enzymes.  In living organisms, the radicals superoxide and nitric oxide and their reaction products regulate many processes, such as control of vascular tone and thus blood pressure. They also play a key role in the intermediary metabolism of various biological compounds. Such radicals can even be messengers in a process dubbed redox signaling.  A radical may be trapped within a solvent cage or be otherwise bound.
"
Freezing-point_depression,Chemistry,1,"Freezing-point depression is the decrease of the freezing point of a solvent on the addition of a non-volatile solute. Examples include salt in water, alcohol in water, or the mixing of two solids such as impurities into a finely powdered drug. In all cases, the substance added/present in smaller amounts is considered the solute, while the original substance present in larger quantity is thought of as the solvent. The resulting liquid solution or solid-solid mixture has a lower freezing point than the pure solvent or solid because the chemical potential of the solvent in the mixture is lower than that of the pure solvent, the difference between the two being proportional to the natural logarithm of the mole fraction. In a similar manner, the chemical potential of the vapor above the solution is lower than that above a pure solvent, which results in boiling-point elevation. Freezing-point depression is what causes sea water, (a mixture of salt and other compounds in water), to remain liquid at temperatures below 0 °C (32 °F), the freezing point of pure water.
"
Freezing_point,Chemistry,1,"The melting point (or, rarely, liquefaction point) of a substance is the temperature at which it changes state from solid to liquid. At the melting point the solid and liquid phase exist in equilibrium. The melting point of a substance depends on pressure and is usually specified at a standard pressure such as 1 atmosphere or 100 kPa.
 When considered as the temperature of the reverse change from liquid to solid, it is referred to as the freezing point or crystallization point. Because of the ability of some substances to supercool, the freezing point is not considered as a characteristic property of a substance. When the ""characteristic freezing point"" of a substance is determined, in fact the actual methodology is almost always ""the principle of observing the disappearance rather than the formation of ice, that is, the melting point.[1]"
Frequency,Chemistry,1,"Frequency is the number of occurrences of a repeating event per unit of time.[1] It is also referred to as temporal frequency, which emphasizes the contrast to spatial frequency and angular frequency. Frequency is measured in units of hertz (Hz) which is equal to one occurrence of a repeating event per second.  The period is the duration of time of one cycle in a repeating event, so the period is the reciprocal of the frequency.[2]  For example: if a newborn baby's heart beats at a frequency of 120 times a minute (2 hertz), its period, T, — the time interval between beats—is half a second (60 seconds divided by 120 beats). Frequency is an important parameter used in science and engineering to specify the rate of oscillatory and vibratory phenomena, such as mechanical vibrations, audio signals (sound), radio waves, and light.
"
Functional_group,Chemistry,1,"In organic chemistry, functional groups are specific substituents or moieties within molecules that may be responsible for the characteristic chemical reactions of those molecules. The same functional group will undergo the same or similar chemical reaction(s) regardless of the size of the molecule it is a part of.[1][2] This allows for systematic prediction of chemical reactions and behavior of chemical compounds and design of chemical syntheses. Furthermore, the reactivity of a functional group can be modified by other functional groups nearby. In organic synthesis, functional group interconversion is one of the basic types of transformations.
 Functional groups are groups of one or more atoms of distinctive chemical properties no matter what they are attached to. The atoms of functional groups are linked to each other and to the rest of the molecule by covalent bonds. For repeating units of polymers, functional groups attach to their nonpolar core of carbon atoms and thus add chemical character to carbon chains. Functional groups can also be charged, e.g. in carboxylate salts (–COO−), which turns the molecule into a polyatomic ion or a complex ion. Functional groups binding to a central atom in a coordination complex are called ligands. Complexation and solvation are also caused by specific interactions of functional groups. In the common rule of thumb ""like dissolves like"", it is the shared or mutually well-interacting functional groups which give rise to solubility. For example, sugar dissolves in water because both share the hydroxyl functional group (–OH) and hydroxyls interact strongly with each other. Plus, when functional groups are more electronegative than atoms they attach to, the functional groups will become polar, and the otherwise nonpolar molecules containing these functional groups become polar and so become soluble in some aqueous environment.
 Combining the names of functional groups with the names of the parent alkanes generates what is termed a systematic nomenclature for naming organic compounds. In traditional nomenclature, the first carbon atom after the carbon that attaches to the functional group is called the alpha carbon; the second, beta carbon, the third, gamma carbon, etc. If there is another functional group at a carbon, it may be named with the Greek letter, e.g., the gamma-amine in gamma-aminobutyric acid is on the third carbon of the carbon chain attached to the carboxylic acid group. IUPAC conventions call for numeric labeling of the position, e.g. 4-aminobutanoic acid. In traditional names various qualifiers are used to label isomers, for example, isopropanol (IUPAC name: propan-2-ol) is an isomer of n-propanol (propan-1-ol). The term moiety has some overlap with the term ""functional group"". However, a moiety is an entire ""half"" of a molecule, which can be not only a single functional group, but also a larger unit consisting of multiple functional groups. For example, an ""aryl moiety"" may be any group containing an aromatic ring, regardless of how many functional groups the said aryl has.
"
Galvanic_cell,Chemistry,1,"A galvanic cell or voltaic cell, named after Luigi Galvani or Alessandro Volta, respectively, is an electrochemical cell that derives electrical energy from spontaneous redox reactions taking place within the cell. It generally consists of two different metals immersed in electrolytes, or of individual half-cells with different metals and their ions in solution connected by a salt bridge or separated by a porous membrane.
 Volta was the inventor of the voltaic pile, the first electrical battery. In common usage, the word ""battery"" has come to include a single galvanic cell, but a battery properly consists of multiple cells.[1]"
Gas,Chemistry,1,"
 Gas is one of the four fundamental states of matter (the others being solid, liquid, and plasma). A pure gas may be made up of individual atoms (e.g. a noble gas like neon), elemental molecules made from one type of atom (e.g. oxygen), or compound molecules made from a variety of atoms (e.g. carbon dioxide). A gas mixture, such as air, contains a variety of pure gases. What distinguishes a gas from liquids and solids is the vast separation of the individual gas particles. This separation usually makes a colorless gas invisible to the human observer. The interaction of gas particles in the presence of  electric and  gravitational fields are considered[by whom?] negligible, as indicated by the constant velocity vectors in the image.
 The gaseous state of matter occurs between the liquid and plasma states,[1] the latter of which provides the upper temperature boundary for gases. Bounding the lower end of the temperature scale lie degenerative quantum gases[2] which are gaining increasing attention.[3]
High-density atomic gases super-cooled to very low temperatures are classified by their statistical behavior as either Bose gases or Fermi gases. For a comprehensive listing of these exotic states of matter see list of states of matter.
"
Gas_chromatography,Chemistry,1,"Gas chromatography (GC) is a common type of chromatography used in analytical chemistry for separating and analyzing compounds that can be vaporized without decomposition. Typical uses of GC include testing the purity of a particular substance, or separating the different components of a mixture (the relative amounts of such components can also be determined). In some situations, GC may help in identifying a compound. In preparative chromatography, GC can be used to prepare pure compounds from a mixture.[1][2] In gas chromatography, the mobile phase (or ""moving phase"") is a carrier gas, usually an inert gas such as helium or an unreactive gas such as nitrogen. Helium remains the most commonly used carrier gas in about 90% of instruments although hydrogen is preferred for improved separations.[3] The stationary phase is a microscopic layer of liquid or polymer on an inert solid support, inside a piece of glass or metal tubing called a column (an homage to the fractionating column used in distillation). The instrument used to perform gas chromatography is called a gas chromatograph (or ""aerograph"", ""gas separator"").
 The gaseous compounds being analyzed interact with the walls of the column, which is coated with a stationary phase. This causes each compound to elute at a different time, known as the retention time of the compound. The comparison of retention times is what gives GC its analytical usefulness.
 Gas chromatography is in principle similar to column chromatography (as well as other forms of chromatography, such as HPLC, TLC), but has several notable differences. First, the process of separating the compounds in a mixture is carried out between a liquid stationary phase and a gas mobile phase, whereas in column chromatography the stationary phase is a solid and the mobile phase is a liquid. (Hence the full name of the procedure is ""Gas–liquid chromatography"", referring to the mobile and stationary phases, respectively.) Second, the column through which the gas phase passes is located in an oven where the temperature of the gas can be controlled, whereas column chromatography (typically) has no such temperature control. Finally, the concentration of a compound in the gas phase is solely a function of the vapor pressure of the gas.[1] Gas chromatography is also sometimes known as vapor-phase chromatography (VPC), or gas–liquid partition chromatography (GLPC). These alternative names, as well as their respective abbreviations, are frequently used in scientific literature. Strictly speaking, GLPC is the most correct terminology, and is thus preferred by many authors.[1]"
Gay-Lussac%27s_law,Chemistry,1,"Gay-Lussac's law (more correctly referred to as Amontons's law) states that the pressure of a given mass of gas varies directly with the absolute temperature of the gas, when the volume is kept constant.[1] Mathematically, it can be written as: 





P
T


=
k


{  {\frac {P}{T}}=k}
.
 Gay-Lussac is incorrectly recognized for the Pressure Law which established that the pressure of an enclosed gas is directly proportional to its temperature and which he was the first to formulate (c. 1809).[2] He is also sometimes credited[3][4][5] with being the first to publish convincing evidence that shows the relationship between the pressure and temperature of a fixed mass of gas kept at a constant volume.[4] These laws are also known variously as the Pressure Law or Amontons's law and Dalton's law respectively.[3][4][5][6]"
Geochemistry,Chemistry,1,"Geochemistry is the science that uses the tools and principles of chemistry to explain the mechanisms behind major geological systems such as the Earth's crust and its oceans.[1]:1 The realm of geochemistry extends beyond the Earth, encompassing the entire Solar System,[2] and has made important contributions to the understanding of a number of processes including mantle convection, the formation of planets and the origins of granite and basalt.[1]:1 It is an integrated field of chemistry and geology/geography. 
"
Gibbs_energy,Chemistry,1,"In thermodynamics, the Gibbs free energy is a thermodynamic potential that can be used to calculate the maximum reversible work that may be performed by a thermodynamic system at a constant temperature and pressure.  The Gibbs free energy (



Δ
G
=
Δ
H
−
T
Δ
S


{  \Delta G=\Delta H-T\Delta S}
, measured in joules in SI) is the maximum amount of non-expansion work that can be extracted from a thermodynamically closed system (can exchange heat and work with its surroundings, but not matter). This maximum can be attained only in a completely reversible process. When a system transforms reversibly from an initial state to a final state, the decrease in Gibbs free energy equals the work done by the system to its surroundings, minus the work of the pressure forces.[1] The Gibbs energy (symbol 



G


{  G}
) is also the thermodynamic potential that is minimized when a system reaches chemical equilibrium at constant pressure and temperature. Its derivative with respect to the reaction coordinate of the system vanishes at the equilibrium point. As such, a reduction in 



G


{  G}
 is necessary for a reaction to be spontaneous at constant pressure and temperature.
 The Gibbs free energy was originally called available energy, was developed in the 1870s by the American scientist Josiah Willard Gibbs. In 1873, Gibbs described this ""available energy"" as[2]:400 the greatest amount of mechanical work which can be obtained from a given quantity of a certain substance in a given initial state, without increasing its total volume or allowing heat to pass to or from external bodies, except such as at the close of the processes are left in their initial condition. The initial state of the body, according to Gibbs, is supposed to be such that ""the body can be made to pass from it to states of dissipated energy by reversible processes"".  In his 1876 magnum opus On the Equilibrium of Heterogeneous Substances, a graphical analysis of multi-phase chemical systems, he engaged his thoughts on chemical-free energy in full.
 If the reactants and products are all in their thermodynamic standard states, then the defining equation is written as 



Δ

G

∘


=
Δ

H

∘


−
T
Δ

S

∘




{  \Delta G^{\circ }=\Delta H^{\circ }-T\Delta S^{\circ }}
.
"
Glass,Chemistry,1,"
 Glass is a non-crystalline, often transparent amorphous solid, that has widespread practical, technological, and decorative use in, for example, window panes, tableware, and optics. Glass is most often formed by rapid cooling (quenching) of the molten form; some glasses such as volcanic glass are naturally occurring. The most familiar, and historically the oldest, types of manufactured glass are ""silicate glasses"" based on the chemical compound silica (silicon dioxide, or quartz), the primary constituent of sand. Soda-lime glass, containing around 70% silica, accounts for around 90% of manufactured glass. The term glass, in popular usage, is often used to refer only to this type of material, although silica-free glasses often have desirable properties for applications in modern communications technology. Some objects, such as drinking glasses and eyeglasses, are so commonly made of silicate-based glass that they are simply called by the name of the material.
 Although brittle, silicate glass is extremely durable and many examples of glass fragments exist from early glass-making cultures. Archaeological evidence suggests glass-making dates back to at least 3,600 BC in Mesopotamia, Egypt, or Syria. The earliest known glass objects were beads, perhaps created accidentally during metalworking or the production of faience. Due to its ease of formability into any shape, glass has been traditionally used for vessels, such as bowls, vases, bottles, jars and drinking glasses. In its most solid forms, it has also been used for paperweights and marbles. Glass can be coloured by adding metal salts or painted and printed with vitreous enamels, leading to its use in stained glass windows and other glass art objects.
 The refractive, reflective and transmission properties of glass make glass suitable for manufacturing optical lenses, prisms, and optoelectronics materials. Extruded glass fibres have application as optical fibres in communications networks, thermal insulating material when matted as glass wool so as to trap air, or in glass-fibre reinforced plastic (fibreglass).
"
Gram-atom,Chemistry,1,"In chemistry, the molar mass of a chemical compound is defined as the mass of a sample of that compound divided by the amount of substance in that sample, measured in moles.[1] The molar mass is a bulk, not molecular, property of a substance. The molar mass is an average of many instances of the compound, which often vary in mass due to the presence of isotopes. Most commonly, the molar mass is computed from the standard atomic weights and is thus a terrestrial average and a function of the relative abundance of the isotopes of the constituent atoms on Earth. The molar mass is appropriate for converting between the mass of a substance and the amount of a substance for bulk quantities.
 The molecular weight is very commonly used as a synonym of molar mass, particularly for molecular compounds; however, the most authoritative sources define it differently (see molecular mass).
 The formula weight is a synonym of molar mass that is frequently used for non-molecular compounds, such as ionic salts.
 The molar mass is an intensive property of the substance, that does not depend on the size of the sample. In the International System of Units (SI), the base unit of molar mass is kg/mol. However, for historical reasons, molar masses are almost always expressed in g/mol.
 The mole was defined in such as way that the molar mass of a compound, in g/mol, is numerically equal (for all practical purposes) to the average mass of one molecule, in daltons. Thus, for example, the average mass of a molecule of water is about 18.0153 daltons, and the molar mass of water is about 18.0153 g/mol.
 For chemical elements without isolated molecules, such as carbon and metals, the molar mass is computed dividing by the number of moles of atoms instead. Thus, for example, the molar mass of iron is about 55.845 g/mol.
 Between 1971 and 2019, SI defined the ""amount of substance"" as a separate dimension of measurement, and the mole was defined as the amount of substance that has as many constituent particles as there are atoms in 12 grams of carbon-12. In that period, the molar mass of carbon-12 was thus exactly 12 g/mol, by definition. Since 2019, a mole of any substance has been redefined in the SI as the amount of that substance containing an exactly defined number of particles, N = 6.02214076×1023. Therefore, the molar mass of a compound now is simply the mass of this number of molecules of the compound.
"
Grignard_reaction,Chemistry,1,"
 The Grignard reaction (pronounced /ɡriɲar/) is an organometallic chemical reaction in which alkyl, allyl, vinyl, or aryl-magnesium halides (Grignard reagent) is added to a carbonyl group in an aldehyde or ketone.[1][2]  This reaction is important for the formation of carbon–carbon bonds.[3][4] The reaction of an organic halide with magnesium is not a Grignard reaction, but provides a Grignard reagent.[5] Grignard reactions and reagents were discovered by and are named after the French chemist François Auguste Victor Grignard (University of Nancy, France), who published it in 1900 and was awarded the 1912 Nobel Prize in Chemistry for this work.[6]"
Ground_glass_joint,Chemistry,1,"Ground glass joints are used in laboratories to quickly and easily fit leak-tight apparatus together from commonly available parts. For example, a round bottom flask, Liebig condenser, and oil bubbler with ground glass joints may be rapidly fitted together to reflux a reaction mixture. This is a large improvement compared with older methods of custom-made glassware, which was time-consuming and expensive, or the use of less chemical resistant and heat resistant corks or rubber bungs and glass tubes as joints, which took time to prepare as well.
 To connect the hollow inner spaces of the glassware components, ground glass joints are hollow on the inside and open at the ends, except for stoppers.
"
Group_(periodic_table),Chemistry,1,"
 In chemistry, a group (also known as a family[1]) is a column of elements in the periodic table of the chemical elements. There are 18 numbered groups in the periodic table; the f-block columns (between groups 3 and 4) are not numbered. The elements in a group have similar physical or chemical characteristics of the outermost electron shells of their atoms (i.e., the same core charge), because most chemical properties are dominated by the orbital location of the outermost electron.  
 There are three systems of group numbering for the groups; the same number may be assigned to different groups depending on the system being used. The modern numbering system of ""group 1"" to ""group 18"" has been recommended by the International Union of Pure and Applied Chemistry (IUPAC) since about 1990. It replaces two older incompatible naming schemes, used by the Chemical Abstract Service (CAS, more popular in the US), and by IUPAC before 1990 (more popular in Europe). The system of eighteen groups is generally accepted by the chemistry community, but some dissent exists about membership of several elements. Disagreements mostly involve elements number 1 and 2 (hydrogen and helium), as well as inner transition metals.
 Groups may also be identified using their topmost element, or have a specific name. For example, group 16 is also described as the ""oxygen group"" and as the ""chalcogens"". An exception is the ""iron group"", which usually refers to ""group 8"", but in chemistry may also mean iron, cobalt, and nickel, or some other set of elements with similar chemical properties. In astrophysics and nuclear physics, it usually refers to iron, cobalt, nickel, chromium, and manganese.
"
Halogen,Chemistry,1,"The halogens (/ˈhælədʒən, ˈheɪ-, -loʊ-, -ˌdʒɛn/[1][2][3]) are a group in the periodic table consisting of five chemically related elements: fluorine (F), chlorine (Cl), bromine (Br), iodine (I), and astatine (At). The artificially created element 117, tennessine (Ts), may also be a halogen. In the modern IUPAC nomenclature, this group is known as group 17.
 Legend
 The name ""halogen"" means ""salt-producing"". When halogens react with metals, they produce a wide range of salts, including calcium fluoride, sodium chloride (common table salt), silver bromide and potassium iodide.
 The group of halogens is the only periodic table group that contains elements in three of the main states of matter at standard temperature and pressure. All of the halogens form acids when bonded to hydrogen. Most halogens are typically produced from minerals or salts. The middle halogens—chlorine, bromine, and iodine—are often used as disinfectants. Organobromides are the most important class of flame retardants, while elemental halogens are dangerous and can be lethally toxic.
"
Hadron,Chemistry,1,"In particle physics, a hadron /ˈhædrɒn/ (listen) (Greek: ἁδρός, hadrós; ""stout, thick"") is a subatomic composite particle made of two or more quarks held together by the strong force in a similar way as molecules are held together by the electromagnetic force. Most of the mass of ordinary matter comes from two hadrons: the proton and the neutron.
 Hadrons are categorized into two families: baryons, made of an odd number of quarks – usually three quarks – and mesons, made of an even number of quarks—usually one quark and one antiquark.[1] Protons and neutrons (which make the majority of the mass of an atom) are examples of baryons; pions are an example of a meson. ""Exotic"" hadrons, containing more than three valence quarks, have been discovered in recent years. A tetraquark state (an exotic meson), named the Z(4430)−, was discovered in 2007 by the Belle Collaboration[2] and confirmed as a resonance in 2014 by the LHCb collaboration.[3] Two pentaquark states (exotic baryons), named P+c(4380) and P+c(4450), were discovered in 2015 by the LHCb collaboration.[4] There are several more exotic hadron candidates, and other colour-singlet quark combinations that may also exist.
 Almost all ""free"" hadrons and antihadrons (meaning, in isolation and not bound within an atomic nucleus) are believed to be unstable and eventually decay (break down) into other particles. The only known exception relates to free protons, which are possibly stable, or at least, take immense amounts of time to decay (order of 1034+ years). Free neutrons are unstable and decay with a half-life of about 611 seconds. Their respective antiparticles are expected to follow the same pattern, but they are difficult to capture and study, because they immediately annihilate on contact with ordinary matter. ""Bound"" protons and neutrons, contained within an atomic nucleus, are generally considered stable. Experimentally, hadron physics is studied by colliding protons or nuclei of heavy elements such as lead or gold, and detecting the debris in the produced particle showers. In the natural environment, mesons such as pions are produced by the collisions of cosmic rays with the atmosphere.
"
Heat,Chemistry,1,"
 In thermodynamics, heat is energy in transfer to or from a thermodynamic system, by mechanisms other than thermodynamic work or transfer of matter.[1][2][3][4][5][6][7] The various mechanisms of energy transfer that define heat are stated in the next section of this article.
 Like thermodynamic  work, heat transfer is a process involving more than one system, not a property of any one system. In thermodynamics, energy transferred as heat contributes to change in the system's cardinal energy variable of state, for example its internal energy, or for example its enthalpy. This is to be distinguished from the ordinary language conception of heat as a property of an isolated system.
 The quantity of energy transferred as heat in a process is the amount of transferred energy excluding any thermodynamic work that was done and any energy contained in matter transferred. For the precise definition of heat, it is necessary that it occur by a path that does not include transfer of matter.[8] Though not immediately by the definition, but in special kinds of process, quantity of energy transferred as heat can be measured by its effect on the states of interacting bodies. For example, respectively in special circumstances, heat transfer can be measured by the amount of ice melted, or by change in temperature of a body in the surroundings of the system.[9] Such methods are called calorimetry.
 The conventional symbol used to represent the amount of heat transferred in a thermodynamic process is Q. As an amount of energy (being transferred),  the SI unit of heat is the joule (J). 
"
Enthalpy_of_fusion,Chemistry,1,"The enthalpy of fusion of a substance, also known as (latent) heat of fusion is the change in its enthalpy resulting from providing energy, typically heat, to a specific quantity of the substance to change its state from a solid to a liquid, at constant pressure. For example, when melting 1 kg of ice (at 0 °C under a wide range of pressures), 333.55 kJ of energy is absorbed with no temperature change. The heat of solidification (when a substance changes from liquid to solid) is equal and opposite.
 This energy includes the contribution required to make room for any associated change in volume by displacing its environment against ambient pressure. The temperature at which the phase transition occurs is the melting point or the freezing point, according to context. By convention, the pressure is assumed to be 1 atm (101.325 kPa) unless otherwise specified.
"
Henry%27s_law,Chemistry,1,"In physical chemistry, Henry's law is a gas law that states that the amount of dissolved gas in a liquid is proportional to its partial pressure above the liquid. The proportionality factor is called Henry's law constant. It was formulated by the English chemist William Henry, who studied the topic in the early 19th century. In his publication about the quantity of gases absorbed by water,[1] he described the results of his experiments:
 … water takes up, of gas condensed by one, two, or more additional atmospheres, a quantity which, ordinarily compressed, would be equal to twice, thrice, &c. the volume absorbed under the common pressure of the atmosphere. An example where Henry's law is at play is in the depth-dependent dissolution of oxygen and nitrogen in the blood of underwater divers that changes during decompression, leading to decompression sickness. An everyday example is given by one's experience with carbonated soft drinks, which contain dissolved carbon dioxide. Before opening, the gas above the drink in its container is almost pure carbon dioxide, at a pressure higher than atmospheric pressure. After the bottle is opened, this gas escapes, moving the partial pressure of carbon dioxide above the liquid to be much lower, resulting in degassing as the dissolved carbon dioxide comes out of solution.
"
Hess%27_law_of_constant_heat_summation,Chemistry,1,"Hess' law of constant heat summation, also known as Hess' law (or Hess's law), is a relationship in physical chemistry named after Germain Hess, a Switzerland-born Russian chemist and physician who published it in 1840. The law states that the total enthalpy change during the complete course of a chemical reaction is the same whether the reaction is made in one step or in several steps.[1][2] Hess' law is now understood as an expression of the principle of conservation of energy, also expressed in the first law of thermodynamics, and the fact that the enthalpy of a chemical process is independent of the path taken from the initial to the final state (i.e. enthalpy is a state function). Reaction enthalpy changes can be determined by calorimetry for many reactions. The values are usually stated for processes with the same initial and final temperatures and pressures, although the conditions can vary during the reaction. Hess' law can be used to determine the overall energy required for a chemical reaction, when it can be divided into synthetic steps that are individually easier to characterize. This affords the compilation of standard enthalpies of formation, that may be used as a basis to design complex syntheses.
"
Hund%27s_rules,Chemistry,1,"In atomic physics, Hund's rules refers to a set of rules that German physicist Friedrich Hund formulated around 1927, which are used to determine the term symbol that corresponds to the ground state of a multi-electron atom. The first rule is especially important in chemistry, where it is often referred to simply as Hund's Rule. 
 The three rules are:[1][2][3] These rules specify in a simple way how usual energy interactions determine which term includes the ground state. The rules assume that the repulsion between the outer electrons is much greater than the spin–orbit interaction, which is in turn stronger than any other remaining interactions.  This is referred to as the LS coupling regime.
 Full shells and subshells do not contribute to the quantum numbers for total S, the total spin angular momentum and for L, the total orbital angular momentum. It can be shown that for full orbitals and suborbitals both the residual electrostatic energy (repulsion between electrons) and the spin–orbit interaction can only shift all the energy levels together.  Thus when determining the ordering of energy levels in general only the outer valence electrons must be considered.
"
Hydrate,Chemistry,1,"In chemistry, a hydrate is a substance that contains water or its constituent elements. The chemical state of the water varies widely between different classes of hydrates, some of which were so labeled before their chemical structure was understood.
"
Hydration_reaction,Chemistry,1,"In chemistry, a hydration reaction is a chemical reaction in which a substance combines with water. In organic chemistry, water is added to an unsaturated substrate, which is usually an alkene or an alkyne. This type of reaction is employed industrially to produce ethanol, isopropanol, and butan-2-ol.[1]"
Hydrogen,Chemistry,1,"
 
 Hydrogen is the chemical element with the symbol H and atomic number 1. With a standard atomic weight of 1.008, hydrogen is the lightest element in the periodic table. Hydrogen is the most abundant chemical substance in the universe, constituting roughly 75% of all baryonic mass.[7][note 1] Non-remnant stars are mainly composed of hydrogen in the plasma state. The most common isotope of hydrogen, termed protium (name rarely used, symbol 1H), has one proton and no neutrons.
 The universal emergence of atomic hydrogen first occurred during the recombination epoch (Big Bang). At standard temperature and pressure, hydrogen is a colorless, odorless, tasteless, non-toxic, nonmetallic, highly combustible diatomic gas with the molecular formula H2. Since hydrogen readily forms covalent compounds with most nonmetallic elements, most of the hydrogen on Earth exists in molecular forms such as water or organic compounds. Hydrogen plays a particularly important role in acid–base reactions because most acid-base reactions involve the exchange of protons between soluble molecules. In ionic compounds, hydrogen can take the form of a negative charge (i.e., anion) when it is known as a hydride, or as a positively charged (i.e., cation) species denoted by the symbol H+. The hydrogen cation is written as though composed of a bare proton, but in reality, hydrogen cations in ionic compounds are always more complex. As the only neutral atom for which the Schrödinger equation can be solved analytically,[8] study of the energetics and bonding of the hydrogen atom has played a key role in the development of quantum mechanics.
 Hydrogen gas was first artificially produced in the early 16th century by the reaction of acids on metals. In 1766–81, Henry Cavendish was the first to recognize that hydrogen gas was a discrete substance,[9] and that it produces water when burned, the property for which it was later named: in Greek, hydrogen means ""water-former"".
 Industrial production is mainly from steam reforming natural gas, and less often from more energy-intensive methods such as the electrolysis of water.[10] Most hydrogen is used near the site of its production, the two largest uses being fossil fuel processing (e.g., hydrocracking) and ammonia production, mostly for the fertilizer market. Hydrogen is problematic in metallurgy because it can embrittle many metals,[11] complicating the design of pipelines and storage tanks.[12]"
Hydrogen_bond,Chemistry,1,"A hydrogen bond (often informally abbreviated H-bond) is a primarily electrostatic force of attraction between a hydrogen (H) atom which is covalently bound to a more electronegative atom or group, particularly the second-row elements nitrogen (N), oxygen (O), or fluorine (F)—the hydrogen bond donor (Dn)—and another electronegative atom bearing a lone pair of electrons—the hydrogen bond acceptor (Ac). 
Such an interacting system is generally denoted Dn–H···Ac, where the solid line denotes a polar covalent bond, and the dotted or dashed line indicates the hydrogen bond.  The use of three centered dots for the hydrogen bond is specifically recommended by the IUPAC.[4] While hydrogen bonding has both covalent and electrostatic contributions, and the degrees to which they contribute are currently debated, the present evidence strongly implies that the primary contribution is covalent.[5] Hydrogen bonds can be intermolecular (occurring between separate molecules) or intramolecular (occurring among parts of the same molecule).[6][7][8][9] Depending on the nature of the donor and acceptor atoms which constitute the bond, their geometry, and environment, the energy of a hydrogen bond can vary between 1 and 40 kcal/mol.[10] This makes them somewhat stronger than a van der Waals interaction, and weaker than fully covalent or ionic bonds. This type of bond can occur in inorganic molecules such as water and in organic molecules like DNA and proteins.
 The hydrogen bond is responsible for many of the anomalous physical and chemical properties of compounds of N, O, and F.  In particular, intermolecular hydrogen bonding is responsible for the high boiling point of water (100 °C) compared to the other group 16 hydrides that have much weaker hydrogen bonds.[11] Intramolecular hydrogen bonding is partly responsible for the secondary and tertiary structures of proteins and nucleic acids. It also plays an important role in the structure of polymers, both synthetic and natural.
 Weaker hydrogen bonds[12] are known for hydrogen atoms bound to elements such as sulfur (S) or chlorine (Cl); even carbon (C) can serve as a donor, particularly when the carbon or one of its neighbors is electronegative (e.g., in chloroform, aldehydes and terminal acetylenes).[13][14]    Gradually, it was recognized that there are many examples of weaker hydrogen bonding involving donor other than N, O, or F and/or acceptor Ac with electronegativity approaching that of hydrogen (rather than being much more electronegative).  Though these ""non-traditional"" hydrogen bonding interactions are often quite weak (~1 kcal/mol), they are also ubiquitous and are increasingly recognized as important control elements in receptor-ligand interactions in medicinal chemistry or intra-/intermolecular interactions in materials sciences.   The definition of hydrogen bonding has gradually broadened over time to include these weaker attractive interactions.  In 2011, an IUPAC Task Group recommended a modern evidence-based definition of hydrogen bonding, which was published in the IUPAC journal Pure and Applied Chemistry. This definition specifies:
 The hydrogen bond is an attractive interaction between a hydrogen atom from a molecule or a molecular fragment X–H in which X is more electronegative than H, and an atom or a group of atoms in the same or a different molecule, in which there is evidence of bond formation.[15] As part of a more detailed list of criteria, the IUPAC publication acknowledges that the attractive interaction can arise from some combination of electrostatics (multipole-multipole and multipole-induced multipole interactions), covalency (charge transfer by orbital overlap), and dispersion (London forces), and states that the relative importance of each will vary depending on the system.  However, a footnote to the criterion recommends the exclusion of interactions in which dispersion is the primary contributor, specifically giving Ar---CH4 and CH4---CH4 as examples of such interactions to be excluded from the definition.[4] Nevertheless, most introductory textbooks still restrict the definition of hydrogen bond to the ""classical"" type of hydrogen bond characterized in the opening paragraph.
"
Hydrogenation,Chemistry,1,"Hydrogenation is a chemical reaction between molecular hydrogen (H2) and another compound or element, usually in the presence of a catalyst such as nickel, palladium or platinum. The process is commonly employed to reduce or saturate organic compounds. Hydrogenation typically constitutes the addition of pairs of hydrogen atoms to a molecule, often an alkene. Catalysts are required for the reaction to be usable; non-catalytic hydrogenation takes place only at very high temperatures. Hydrogenation reduces double and triple bonds in hydrocarbons.[1]"
Hydrolysis,Chemistry,1,"Hydrolysis (/haɪˈdrɒlɪsɪs/; from Ancient Greek  hydro- 'water', and  lysis 'to unbind') is any chemical reaction in which a molecule of water ruptures one or more chemical bonds. The term is used broadly for substitution, elimination, and solvation reactions in which water is the nucleophile.[1] Biological hydrolysis is the cleavage of biomolecules where a water molecule is consumed to effect the separation of a larger molecule into component parts. When a carbohydrate is broken into its component sugar molecules by hydrolysis (e.g., sucrose being broken down into glucose and fructose), this is recognized as saccharification.[2] Hydrolysis reactions can be the reverse of a condensation reaction in which two molecules join together into a larger one and eject a water molecule. Thus hydrolysis adds water to break down, whereas condensation builds up by removing water and any other solvents.[3]"
Hygroscopy,Chemistry,1,"Hygroscopy is the phenomenon of attracting and holding water molecules via either absorption  or adsorption from the surrounding environment, which is usually at normal or room temperature. If water molecules become suspended among the substance's molecules, adsorbing substances can become physically changed, e.g., changing in volume, boiling point, viscosity or some other physical characteristic or property of the substance.
"
Ideal_gas,Chemistry,1,"An ideal gas is a theoretical gas composed of many randomly moving point particles that are not subject to interparticle interactions.[1] The ideal gas concept is useful because it obeys the ideal gas law, a simplified equation of state, and is amenable to analysis under statistical mechanics. The requirement of zero interaction can often be relaxed if for example the interaction is perfectly elastic or regarded as point like collisions.
 Under various conditions of temperature and pressure, many real gases behave qualitatively like an ideal gas where the gas molecules (or atoms for monatomic gas) play the role of the ideal particles. Many gases such as nitrogen, oxygen, hydrogen, noble gases, some heavier gases like carbon dioxide and mixtures such as air, can be treated like ideal gases within reasonable tolerances[2] over a considerable parameter range around standard temperature and pressure. Generally, a gas behaves more like an ideal gas at higher temperature and lower pressure,[2] as the potential energy due to intermolecular forces becomes less significant compared with the particles' kinetic energy, and the size of the molecules becomes less significant compared to the empty space between them. One mole of an ideal gas has a capacity of 22.710947(13) litres[3] at standard temperature and pressure (a temperature of 273.15 K and an absolute pressure of exactly 105 Pa) as defined by IUPAC since 1982.[note 1] The ideal gas model tends to fail at lower temperatures or higher pressures, when intermolecular forces and molecular size becomes important.  It also fails for most heavy gases, such as many refrigerants,[2] and for gases with strong intermolecular forces, notably water vapor. At high pressures, the volume of a real gas is often considerably larger than that of an ideal gas. At low temperatures, the pressure of a real gas is often considerably less than that of an ideal gas. At some point of low temperature and high pressure, real gases undergo a phase transition, such as to a liquid or a solid. The model of an ideal gas, however, does not describe or allow phase transitions.  These must be modeled by more complex equations of state. The deviation from the ideal gas behavior can be described by a dimensionless quantity, the compressibility factor, Z.
 The ideal gas model has been explored in both the Newtonian dynamics (as in ""kinetic theory"") and in quantum mechanics (as a ""gas in a box""). The ideal gas model has also been used to model the behavior of electrons in a metal (in the Drude model and the free electron model), and it is one of the most important models in statistical mechanics.
 If the pressure of an ideal gas is reduced in a throttling process the temperature of the gas does not change. (If the pressure of a real gas is reduced in a throttling process, its temperature either falls or rises, depending on whether its Joule–Thomson coefficient is positive or negative.)
"
Gas_constant,Chemistry,1,"The gas constant (also known as the molar gas constant, universal gas constant, or ideal gas constant) is denoted by the symbol R or R. It is equivalent to the Boltzmann constant, but expressed in units of energy per temperature increment per mole, i.e. the pressure–volume product, rather than energy per temperature increment per particle. The constant is also a combination of the constants from Boyle's law, Charles's law, Avogadro's law, and Gay-Lussac's law.  It is a physical constant that is featured in many fundamental equations in the physical  sciences, such as the ideal gas law, the Arrhenius equation, and the Nernst equation.
 Physically, the gas constant is the constant of proportionality that relates the energy scale in physics to the temperature scale, when a mole of particles at the stated temperature is being considered. Thus, the value of the gas constant ultimately derives from historical decisions and accidents in the setting of the energy and temperature scales, plus similar historical setting of the value of the molar scale used for the counting of particles. The last factor is not a consideration in the value of the Boltzmann constant, which does a similar job of equating linear energy and temperature scales.
 The gas constant R is defined as the Avogadro constant NA multiplied by the Boltzmann constant (kB or k):
 Since the 2019 redefinition of SI base units, which came into effect on 20 May 2019, both NA and k are defined with exact numerical values when expressed in SI units.[2] As a consequence, the value of the gas constant is also exactly defined, at precisely 8.31446261815324 J⋅K−1⋅mol−1.
 Some have suggested that it might be appropriate to name the symbol R the Regnault constant in honour of the French chemist Henri Victor Regnault, whose accurate experimental data were used to calculate the early value of the constant; however, the origin of the letter R to represent the constant is elusive.[3][4] The gas constant occurs in the ideal gas law, as follows:
 where P is the absolute pressure (SI unit pascals), V is the volume of gas (SI unit cubic metres), n is the amount of gas (SI unit moles), m is the mass (SI unit kilograms) contained in V,  and T is the thermodynamic temperature (SI unit kelvins). Rspecific is the mass-specific gas constant. The gas constant is expressed in the same physical units as molar entropy and molar heat capacity.
"
Ideal_gas_law,Chemistry,1,"The ideal gas law, also called the general gas equation, is the equation of state of a hypothetical ideal gas. It is a good approximation of the behavior of many gases under many conditions, although it has several limitations. It was first stated by Benoît Paul Émile Clapeyron in 1834 as a combination of the empirical Boyle's law, Charles's law, Avogadro's law, and Gay-Lussac's law.[1] The ideal gas law is often written in an empirical form:
 where 



P


{  P}
, 



V


{  V}
 and 



T


{  T}
 are the pressure, volume and temperature; 



n


{  n}
 is the amount of substance; and 



R


{  R}
 is the ideal gas constant. It is the same for all gases.
It can also be derived from the microscopic kinetic theory, as was achieved (apparently independently) by August Krönig in 1856[2] and Rudolf Clausius in 1857.[3] Note that this law makes no comment as to whether a gas heats or cools during compression or expansion.  An ideal gas may not change temperature, but most gases like air are not ideal and follow the Joule–Thomson effect.[dubious  – discuss]"
Ideal_solution,Chemistry,1,"In chemistry, an ideal solution or ideal mixture is a solution in which the gas phase exhibits thermodynamic properties analogous to those of a mixture of ideal gases.[1] The enthalpy of mixing is zero[2] as is the volume change on mixing by definition; the closer to zero the enthalpy of mixing is, the more ""ideal"" the behaviour of the solution becomes. The vapor pressure of the solution obeys either Raoult's law or Henry's law (or both),[3] and the activity coefficient of each component (which measures deviation from ideality) is equal to one.[4] The concept of an ideal solution is fundamental to chemical thermodynamics and its applications, such as the use of colligative properties.
"
Dependent_and_independent_variables,Chemistry,1,"Dependent and independent variables are variables in mathematical modeling, statistical modeling and experimental sciences. Dependent variables receive this name because, in an experiment, their values are studied under the supposition or hypothesis that they depend, by some law or rule (e.g., by a mathematical function), on the values of other variables. Independent variables, in turn, are not seen as depending on any other variable in the scope of the experiment in question; thus, even if the existing dependency is invertible (e.g., by finding the inverse function when it exists), the nomenclature is kept if the inverse dependency is not the object of study in the experiment. In this sense, some common independent variables are time, space, density, mass, fluid flow rate,[1][2] and previous values of some observed value of interest (e.g. human population size) to predict future values (the dependent variable).[3]Variables are given a special name that only applies to experimental investigations. The independent variable is the variable the experimenter changes or controls and is assumed to have a direct effect on the dependent variable. Two examples of common independent variables are gender and educational level.
 Of the two, it is always the dependent variable whose variation is being studied, by altering inputs, also known as regressors in a statistical context. In an experiment, any variable that the experimenter manipulates[clarification needed] can be called an independent variable.  Models and experiments test the effects that the independent variables have on the dependent variables. Sometimes, even if their influence is not of direct interest, independent variables may be included for other reasons, such as to account for their potential confounding effect.
"
PH_indicator,Chemistry,1,"
 A pH indicator is a halochromic chemical compound added in small amounts to a solution so the pH (acidity or basicity) of the solution can be determined visually. Hence, a pH indicator is a chemical detector for hydronium ions (H3O+) or hydrogen ions (H+) in the Arrhenius model. Normally, the indicator causes the color of the solution to change depending on the pH. Indicators can also show change in other physical properties; for example, olfactory indicators show change in their odor. The pH value of a neutral solution is 7.0 at 25°C (standard laboratory conditions). Solutions with a pH value below 7.0 are considered acidic and solutions with pH value above 7.0 are basic (alkaline). As most naturally occurring organic compounds are weak protolytes, carboxylic acids and amines, pH indicators find many applications in biology and analytical chemistry. Moreover, pH indicators form one of the three main types of indicator compounds used in chemical analysis. For the quantitative analysis of metal cations, the use of complexometric indicators is preferred,[1][2] whereas the third compound class, the redox indicators, are used in titrations involving a redox reaction as the basis of the analysis.
"
Induced_radioactivity,Chemistry,1,"Induced radioactivity, also called artificial radioactivity or man-made radioactivity, is the process of using radiation to make a previously stable material radioactive.[1] The husband and wife team of Irène Joliot-Curie and Frédéric Joliot-Curie  discovered induced radioactivity in 1934, and they shared the 1935 Nobel Prize in Chemistry for this discovery.[2] Irène Curie began her research with her parents, Marie Curie and Pierre Curie, studying the natural radioactivity found in radioactive isotopes. Irene branched off from the Curies to study turning stable isotopes into radioactive isotopes by bombarding the stable material with alpha particles (denoted α). The Joliot-Curies showed that when lighter elements, such as boron and aluminium, were bombarded with α-particles, the lighter elements continued to emit radiation even after the α−source was removed. They showed that this radiation consisted of particles carrying one unit positive charge with mass equal to that of an electron, now known as a positron.
 Neutron activation is the main form of induced radioactivity. It occurs when an atomic nucleus captures one or more free neutrons. This new, heavier isotope may be either stable or unstable (radioactive), depending on the chemical element involved. 
Because neutrons disintegrate within minutes outside of an atomic nucleus, free neutrons can be obtained only from nuclear decay, nuclear reaction, and high-energy interaction, such as cosmic radiation or particle accelerator emissions. Neutrons that have been slowed through a neutron moderator (thermal neutrons) are more likely to be captured by nuclei than fast neutrons.
 A less common form of induced radioactivity results from removing a neutron by photodisintegration. In this reaction, a high energy photon (a gamma ray) strikes a nucleus with an energy greater than the binding energy of the nucleus, which releases a neutron. This reaction has a minimum cutoff of 2 MeV (for deuterium) and around 10 MeV for most heavy nuclei.[3]  Many radionuclides do not produce gamma rays with energy high enough to induce this reaction. 
The isotopes used in food irradiation (cobalt-60, caesium-137) both have energy peaks below this cutoff and thus cannot induce radioactivity in the food.[4] The conditions inside certain types of nuclear reactors with high neutron flux can induce radioactivity. The components in those reactors may become highly radioactive from the radiation to which they are exposed. Induced radioactivity increases the amount of nuclear waste that must eventually be disposed, but it is not referred to as radioactive contamination unless it is uncontrolled.
 Further research originally done by Irene and Frederic Joliot-Curie has led to modern techniques to treat various types of cancers.[5]"
Chemically_inert,Chemistry,1,"In chemistry, the term chemically inert is used to describe a substance that is not chemically reactive. From a thermodynamic perspective, a substance is inert, or nonlabile, if it is thermodynamically unstable (positive standard Gibbs free energy of formation) yet decomposes at a slow, or negligible rate.[1] Most Group 8 or 18 elements that appear in the last column of the periodic table (Helium, Neon, Argon, Krypton, Xenon and Radon) are classified as inert (or unreactive). These elements are stable in their naturally occurring form (gaseous form) and they are called inert gases.[2]"
Inorganic_compound,Chemistry,1,"An inorganic compound is typically a chemical compound that lacks carbon–hydrogen bonds, that is, a compound that is not an organic compound. However, the distinction is not clearly defined and agreed upon, and authorities have differing views on the subject.[1][2][3] The study of inorganic compounds is known as inorganic chemistry.
 Inorganic compounds comprise most of the Earth's crust, although the compositions of the deep mantle remain active areas of investigation.[4] Some simple compounds that contain carbon are often considered inorganic. Examples include carbon monoxide, carbon dioxide, carbonates, carbides, cyanides, cyanates, and thiocyanates. Many of these are normal parts of mostly organic systems, including organisms; describing a chemical as inorganic does not necessarily mean that it does not occur within living things.
"
Inorganic_chemistry,Chemistry,1,"Inorganic chemistry deals with synthesis and behavior of inorganic and organometallic compounds. This field covers all chemical compounds except the myriad of organic compounds (carbon-based compounds, usually containing C-H bonds), which are the subjects of organic chemistry. The distinction between the two disciplines is far from absolute, as there is much overlap in the subdiscipline of organometallic chemistry. It has applications in every aspect of the chemical industry, including catalysis, materials science, pigments, surfactants, coatings, medications, fuels, and agriculture.[1]"
Insulator_(electrical),Chemistry,1,"An electrical insulator is a material in which the electron does not flow freely or the atom of the insulator have tightly bound electrons whose internal electric charges do not flow freely; very little electric current will flow through it under the influence of an electric field. This contrasts with other materials, semiconductors and conductors, which conduct electric current more easily. The property that distinguishes an insulator is its resistivity; insulators have higher resistivity than semiconductors or conductors. The most common examples are non-metals.
 A perfect insulator does not exist because even insulators contain small numbers of mobile charges (charge carriers) which can carry current. In addition, all insulators become electrically conductive when a sufficiently large voltage is applied that the electric field tears electrons away from the atoms. This is known as the breakdown voltage of an insulator. Some materials such as glass, paper and Teflon, which have high resistivity, are very good electrical insulators. A much larger class of materials, even though they may have lower bulk resistivity, are still good enough to prevent significant current from flowing at normally used voltages, and thus are employed as insulation for electrical wiring and cables. Examples include rubber-like polymers and most plastics which can be thermoset or thermoplastic in nature.
 Insulators are used in electrical equipment to support and separate electrical conductors without allowing current through themselves. An insulating material used in bulk to wrap electrical cables or other equipment is called insulation. The term insulator is also used more specifically to refer to insulating supports used to attach electric power distribution or transmission lines to utility poles and transmission towers. They support the weight of the suspended wires without allowing the current to flow through the tower to ground.
"
Intensive_and_extensive_properties,Chemistry,1,"Physical properties of materials and systems can often be categorized as being either intensive or extensive, according to how the property changes when the size (or extent) of the system changes. According to IUPAC, an intensive quantity is one whose magnitude is independent of the size of the system[1] whereas an extensive quantity is one whose magnitude is additive for subsystems.[2] This reflects the corresponding mathematical ideas of mean and measure, respectively.
 An intensive property is a bulk property, meaning that it is a local physical property of a system that does not depend on the system size or the amount of material in the system. Examples of intensive properties include temperature, T; refractive index, n; density, ρ; and hardness of an object, η.
 By contrast, extensive properties such as the mass, volume and entropy of systems are additive for subsystems because they increase and decrease as they grow larger and smaller, respectively.[3] These two categories are not exhaustive since some physical properties are neither exclusively intensive nor extensive.[4]  For example, the electrical impedance of two subsystems is additive when — and only when — they are combined in series; whilst if they are combined in parallel, the resulting impedance is less than that of either subsystem.
 The terms intensive and extensive quantities were introduced by American physicist and chemist Richard C. Tolman in 1917.[5]"
Intermolecular_force,Chemistry,1,"Intermolecular forces (IMF) are the forces which mediate interaction between atoms, including forces of attraction or repulsion which act between atoms and other types of neighboring particles, e.g. atoms or ions. Intermolecular forces are weak relative to intramolecular forces – the forces which hold a molecule together. For example, the covalent bond, involving sharing electron pairs between atoms, is much stronger than the forces present between neighboring molecules. Both sets of forces are essential parts of force fields frequently used in molecular mechanics.
 The investigation of intermolecular forces starts from macroscopic observations which indicate the existence and action of forces at a molecular level. These observations include non-ideal-gas thermodynamic behavior reflected by virial coefficients, vapor pressure, viscosity, superficial tension, and absorption data.
 The first reference to the nature of microscopic forces is found in Alexis Clairaut's work Theorie de la Figure de la Terre.[1] Other scientists who have contributed to the investigation of microscopic forces include: Laplace, Gauss, Maxwell and Boltzmann.
 Attractive intermolecular forces are categorized into the following types:
 Information on intermolecular forces is obtained by macroscopic measurements of properties like viscosity, pressure, volume, temperature (PVT) data. The link to microscopic aspects is given by virial coefficients and Lennard-Jones potentials.
"
International_System_of_Units,Chemistry,1,"
 The  International System of Units (SI, abbreviated from the French Système international (d'unités)) is the modern form of the metric system. It is the only system of measurement with an official status in nearly every country in the world. It comprises a coherent system of units of measurement starting with seven base units, which are the second (the unit of time with the symbol s), metre (length, m), kilogram (mass, kg), ampere (electric current, A), kelvin (thermodynamic temperature, K), mole (amount of substance, mol), and candela (luminous intensity, cd). The system allows for an unlimited number of additional units, called derived units, which can always be represented as products of powers of the base units.[Note 1] Twenty-two derived units have been provided with special names and symbols.[Note 2] The seven base units and the 22 derived units with special names and symbols may be used in combination to express other derived units,[Note 3] which are adopted to facilitate measurement of diverse quantities. The SI system also provides twenty prefixes to the unit names and unit symbols that may be used when specifying power-of-ten (i.e. decimal) multiples and sub-multiples of SI units. The SI is intended to be an evolving system; units and prefixes are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves.
 Since 2019, the magnitudes of all SI units have been defined by declaring exact numerical values for seven defining constants when expressed in terms of their SI units. These defining constants are the speed of light in vacuum, c, the hyperfine transition frequency of caesium ΔνCs, the Planck constant h, the elementary charge e, the Boltzmann constant k, the Avogadro constant NA, and the luminous efficacy Kcd. The nature of the defining constants ranges from fundamental constants of nature such as c to the purely technical constant Kcd. Prior to 2019, h, e, k, and NA were not defined a priori but were rather very precisely measured quantities. In 2019, their values were fixed by definition to their best estimates at the time, ensuring continuity with previous definitions of the base units. One consequence of the redefinition of the SI is that the distinction between the base units and derived units is in principle not needed, since any unit can be constructed directly from the seven defining constants.[2]:129 The current way of defining the SI system is a result of a decades-long move towards increasingly abstract and idealised formulation in which the realisations of the units are separated conceptually from the definitions. A consequence is that as science and technologies develop, new and superior realisations may be introduced without the need to redefine the unit.  One problem with artefacts is that they can be lost, damaged, or changed; another is that they introduce uncertainties that cannot be reduced by advancements in science and technology. The last artefact used by the SI was the International Prototype of the Kilogram, a cylinder of platinum-iridium.
 The original motivation for the development of the SI was the diversity of units that had sprung up within the centimetre–gram–second (CGS) systems (specifically the inconsistency between the systems of electrostatic units and electromagnetic units) and the lack of coordination between the various disciplines that used them. The General Conference on Weights and Measures (French: Conférence générale des poids et mesures – CGPM), which was established by the Metre Convention of 1875, brought together many international organisations to establish the definitions and standards of a new system and to standardise the rules for writing and presenting measurements. The system was published in 1960 as a result of an initiative that began in 1948,so is based on the metre–kilogram–second system of units (MKS) rather than any variant of the CGS.
"
International_Union_of_Pure_and_Applied_Chemistry,Chemistry,1,"
 The International Union of Pure and Applied Chemistry (IUPAC /ˈaɪjuːpæk, ˈjuː-/) is an international federation of National Adhering Organizations that represents chemists in individual countries. It is a member of the International Science Council (ISC).[2] IUPAC is registered in Zürich, Switzerland, and the administrative office, known as the ""IUPAC Secretariat"", is in Research Triangle Park, North Carolina, United States. This administrative office is headed by IUPAC's executive director,[3] currently (2020) Lynn Soby.[4] IUPAC was established in 1919 as the successor of the International Congress of Applied Chemistry for the advancement of chemistry. Its members, the National Adhering Organizations, can be national chemistry societies, national academies of sciences, or other bodies representing chemists. There are fifty-four National Adhering Organizations and three Associate National Adhering Organizations.[2] IUPAC's Inter-divisional Committee on Nomenclature and Symbols (IUPAC nomenclature) is the recognized world authority in developing standards for the naming of the chemical elements and compounds. Since its creation, IUPAC has been run by many different committees with different responsibilities.[5] These committees run different projects which include standardizing nomenclature,[6] finding ways to bring chemistry to the world,[7] and publishing works.[8][9][10] IUPAC is best known for its works standardizing nomenclature in chemistry, but IUPAC has publications in many science fields including chemistry, biology and physics.[11] Some important work IUPAC has done in these fields includes standardizing nucleotide base sequence code names; publishing books for environmental scientists, chemists, and physicists; and improving education in science.[11][12] IUPAC is also known for standardizing the atomic weights of the elements through one of its oldest standing committees, the Commission on Isotopic Abundances and Atomic Weights (CIAAW).
"
Intramolecular_force,Chemistry,1,"An intramolecular force is any force that binds together the atoms making up a molecule or compound, not to be confused with intermolecular forces, which are the forces present between molecules.[1] The subtle difference in the name comes from the Latin roots of English with inter meaning between or among and intra meaning inside.[2] Chemical bonds are considered to be intramolecular forces, for example. These forces are often stronger than intermolecular forces, which are present between atoms or molecules that are not bonded.
"
Intrinsic_property,Chemistry,1,"In science and engineering, an intrinsic property is a property of a specified subject that exists itself or within the subject. An extrinsic property is not essential or inherent to the subject that is being characterized. For example, mass is an intrinsic property of any physical object, whereas weight is an extrinsic property that depends on the strength of the gravitational field in which the object is placed.
"
Ion,Chemistry,1,"
 An ion (/ˈaɪɒn, -ən/)[1] is an particle,atom or molecule with a net electrical charge. 
 The charge of the electron is considered negative by convention.  The negative charge of an ion is equal and opposite to charged proton(s) considered positive by convention.  The net charge of an ion is non-zero due to its total number of electrons being unequal to its total number of protons. 
 A cation is a positively charged ion, with fewer electrons than protons, while an anion is negatively charged, with more electrons than protons. Because of their opposite electric charges, cations and anions attract each other and readily form ionic compounds.
 Ions consisting of only a single atom are termed atomic or monatomic ions, while two or more atoms form molecular ions or polyatomic ions. In the case of physical ionization in a fluid (gas or liquid), ""ion pairs"" are created by spontaneous molecule collisions, where each generated pair consists of a free electron and a positive ion.[2] Ions are also created by chemical interactions, such as the dissolution of a salt in liquids, or by other means, such as passing a direct current through a conducting solution, dissolving an anode via ionization.
"
Ionic_bond,Chemistry,1,"
 Ionic bonding is a type of chemical bonding that involves the electrostatic attraction between oppositely charged ions[citation needed], or between two atoms with sharply different electronegativities,[1] and is the primary interaction occurring in ionic compounds. It is one of the main types of bonding along with covalent bonding and metallic bonding. Ions are atoms (or groups of atoms) with an electrostatic charge. Atoms that gain electrons make negatively charged ions (called anions). Atoms that lose electrons make positively charged ions (called cations). This transfer of electrons is known as electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complex nature, e.g. molecular ions like NH+4 or SO2−4. In simpler words, an ionic bond results from the transfer of electrons from a metal to a non-metal in order to obtain a full valence shell for both atoms.
 It is important to recognize that clean ionic bonding — in which one atom or molecule completely transfers an electron to another — cannot exist: all ionic compounds have some degree of covalent bonding, or electron sharing. Thus, the term ""ionic bonding"" is given when the ionic character is greater than the covalent character – that is, a bond in which a large electronegativity difference exists between the two atoms, causing the bonding to be more polar (ionic) than in covalent bonding where electrons are shared more equally. Bonds with partially ionic and partially covalent character are called polar covalent bonds. 
 Ionic compounds conduct electricity when molten or in solution, typically not when solid. Ionic compounds generally have a high melting point, depending on the charge of the ions they consist of. The higher the charges the stronger the cohesive forces and the higher the melting point. They also tend to be soluble in water; the stronger the cohesive forces, the lower the solubility.[2]"
Ionization,Chemistry,1,"Ionization or ionisation is the process by which an atom or a molecule acquires a negative or positive charge by gaining or losing electrons, often in conjunction with other chemical changes. The resulting electrically charged atom or molecule is called an ion. Ionization can result from the loss of an electron after collisions with subatomic particles, collisions with other atoms, molecules and ions, or through the interaction with electromagnetic radiation. Heterolytic bond cleavage and heterolytic substitution reactions can result in the formation of ion pairs. Ionization can occur through radioactive decay by the internal conversion process, in which an excited nucleus transfers its energy to one of the inner-shell electrons causing it to be ejected.
"
Isoelectronicity,Chemistry,1,"Isoelectronicity  is an effect observed when two or more molecules have the same structure (positions and connectivities among atoms) and the same electron configurations, but differ by what specific elements are at certain locations in the structure.
 For example, CO, NO+, and N2  are isoelectronic, whilst CH3COCH3 and CH3N=NCH3 are not.[1] This definition is sometimes termed valence isoelectronicity.  Definitions can sometimes be not as strict, sometimes requiring identity of the total electron count and with it the entire electron configuration.[2]  More usually, definitions are broader, and may extend to allowing different numbers of atoms in the species being compared.[3] The importance of the concept lies in identifying significantly related species, as pairs or series.  Isoelectronic species can be expected to show useful consistency and predictability in their properties, so identifying a compound as isoelectronic with one already characterised offers clues to possible properties and reactions  (Differences in properties such as electronegativity of the atoms in isolelectronic species can affect reactivity.)
 In quantum mechanics, hydrogen-like atoms are ions with only one electron such as Li3+. These ions would be described as being isoelectronic with hydrogen.
"
Isomerization,Chemistry,1,"In chemistry isomerization or isomerisation is the process in which a molecule, ion or molecular fragment is transformed into an isomer with a different chemical structure.[1] Enolization is an example of isomerization, as is tautomerization.[2] When the isomerization occurs intramolecularly it may be called a rearrangement reaction.
 When the activation energy for the isomerization reaction is sufficiently small, both isomers will exist in a temperature-dependent equilibrium with each other. Many values of the standard free energy difference, 



Δ

G

∘




{  \Delta G^{\circ }}
, have been calculated, with good agreement between observed and calculated data.[3]"
Isomer,Chemistry,1,"
 In chemistry, isomers are molecules or polyatomic ions with identical molecular formulas — that is, same number of atoms of each element — but distinct arrangements of atoms in space.[1] Isomerism is existence or possibility of isomers.
 Isomers do not necessarily share similar chemical or physical properties. Two main forms of isomerism are structural or constitutional isomerism, in which bonds between the atoms differ; and stereoisomerism or spatial isomerism, in which the bonds are the same but the relative positions of the atoms differ.
 Isomeric relationships form a hierarchy. Two chemicals might be the same constitutional isomer, but upon deeper analysis be stereoisomers of each other. Two molecules that are the same stereoisomer as each other might be in different conformational forms or be different isotopologues. The depth of analysis depends on the field of study or the chemical and physical properties of interest.
 The English word ""isomer"" (/ˈaɪsəmər/) is a back-formation from ""isomeric"",[2] which was borrowed through German isomerisch[3] from Swedish isomerisk;  which in turn was coined from Greek ἰσόμερoς  isómeros, with roots isos = ""equal"",  méros = ""part"".[4]"
Isotope,Chemistry,1,"
 Isotopes are variants of a particular chemical element which differ in neutron number, and consequently in nucleon number. All isotopes of a given element have the same number of protons but different numbers of neutrons in each atom.[1] The term isotope is formed from the Greek roots isos (ἴσος ""equal"") and topos (τόπος ""place""), meaning ""the same place""; thus, the meaning behind the name is that different isotopes of a single element occupy the same position on the periodic table.[2] It was coined by Scottish doctor and writer Margaret Todd in 1913 in a suggestion to chemist Frederick Soddy.
 The number of protons within the atom's nucleus is called atomic number and is equal to the number of electrons in the neutral (non-ionized) atom. Each atomic number identifies a specific element, but not the isotope; an atom of a given element may have a wide range in its number of neutrons. The number of nucleons (both protons and neutrons) in the nucleus is the atom's mass number, and each isotope of a given element has a different mass number.
 For example, carbon-12, carbon-13, and carbon-14 are three isotopes of the element carbon with mass numbers 12, 13, and 14, respectively. The atomic number of carbon is 6, which means that every carbon atom has 6 protons, so that the neutron numbers of these isotopes are 6, 7, and 8 respectively.
"
Joule,Chemistry,1,"The joule (/dʒaʊl, dʒuːl/ jowl, jool;[1][2][3] symbol: J) is a derived unit of energy in the International System of Units.[4] It is equal to the energy transferred to (or work done on) an object when a force of one newton acts on that object in the direction of the force's motion through a distance of one metre (1 newton metre or N⋅m). It is also the energy dissipated as heat when an electric current of one ampere passes through a resistance of one ohm for one second. It is named after the English physicist James Prescott Joule (1818–1889).[5][6][7]"
Kelvin,Chemistry,1,"
 The kelvin is the base unit of temperature in the International System of Units (SI), having the unit symbol K. It is named after the Belfast-born Glasgow University engineer and physicist William Thomson, 1st Baron Kelvin (1824–1907).
 The kelvin is now defined by fixing the numerical value of the Boltzmann constant k to 1.380 649×10−23 J⋅K−1. This unit is equal to kg⋅m2⋅s−2⋅K−1, where the kilogram, metre and second are defined in terms of the Planck constant, the speed of light, and the duration of the caesium-133 ground-state hyperfine transition respectively.[1] Thus, this definition depends only on universal constants, and not on any physical artifacts as practiced previously, such as the International Prototype of the Kilogram, whose mass diverged over time from the original value.
 One kelvin is equal to a change in the thermodynamic temperature T that results in a change of thermal energy kT by 1.380 649×10−23 J.[2] The Kelvin scale fulfills Thomson's requirements as an absolute thermodynamic temperature scale. It uses absolute zero as its null point.
 Unlike the degree Fahrenheit and degree Celsius, the kelvin is not referred to or written as a degree. The kelvin is the primary unit of temperature measurement in the physical sciences, but is often used in conjunction with the degree Celsius, which has the same magnitude.
"
Ketone,Chemistry,1,"
 In chemistry, a ketone /ˈkiːtoʊn/ is a functional group with the structure R2C=O, where R can be a variety of carbon-containing substituents. Ketones contain a carbonyl group (a carbon-oxygen double bond). The simplest ketone is acetone (R = R' = methyl), with the formula CH3C(O)CH3. Many ketones are of great importance in industry and in biology. Examples include many sugars (ketoses), many steroids (e.g., testosterone), and the solvent acetone.[1][page needed]"
Chemical_kinetics,Chemistry,1,"
 Chemical kinetics, also known as reaction kinetics, is the branch of physical chemistry that is concerned with understanding the rates of chemical reactions. It is to be contrasted with thermodynamics, which deals with the direction in which a process occurs but in itself tells nothing about its rate. Chemical kinetics includes investigations of how experimental conditions influence the speed of a chemical reaction and yield information about the reaction's mechanism and transition states, as well as the construction of mathematical models that also can describe the characteristics of a chemical reaction.
"
Kinetic_energy,Chemistry,1,"
 In physics, the kinetic energy (KE) of an object is the energy that it possesses due to its motion.[1]
It is defined as the work needed to accelerate a body of a given mass from rest to its stated velocity. Having gained this energy during its acceleration, the body maintains this kinetic energy unless its speed changes. The same amount of work is done by the body when decelerating from its current speed to a state of rest.
 In classical mechanics, the kinetic energy of a non-rotating object of mass m traveling at a speed v is 










1
2


m

v

2









{  {\begin{smallmatrix}{\frac {1}{2}}mv^{2}\end{smallmatrix}}}
. In relativistic mechanics, this is a good approximation only when v is much less than the speed of light.
 The standard unit of kinetic energy is the joule, while the imperial unit of kinetic energy is the foot-pound.
"
Lability,Chemistry,1,"Lability refers to something that is constantly undergoing change or is likely to undergo change.
"
Lanthanides,Chemistry,1,"The lanthanide (/ˈlænθənaɪd/) or lanthanoid (/ˈlænθənɔɪd/) series of chemical elements[1] comprises the 15 metallic chemical elements with atomic numbers 57–71, from lanthanum through lutetium.[2][3][4] These elements, along with the chemically similar elements scandium and yttrium, are often collectively known as the rare earth elements.
 The informal chemical symbol Ln is used in general discussions of lanthanide chemistry to refer to any lanthanide. All but one of the lanthanides are f-block elements, corresponding to the filling of the 4f electron shell; depending on the source, either lanthanum or lutetium is considered a d-block element, but is included due to its chemical similarities with the other 14.[5] All lanthanide elements form trivalent cations, Ln3+, whose chemistry is largely determined by the ionic radius, which decreases steadily from lanthanum to lutetium.
 They are called lanthanides because the elements in the series are chemically similar to lanthanum. Both lanthanum and lutetium have been labeled as group 3 elements, because they have a single valence electron in the 5d shell. However, both elements are often included in discussions of the chemistry of lanthanide elements. Lanthanum is the more often omitted of the two, because its placement as a group 3 element is somewhat more common in texts and for semantic reasons: since ""lanthanide"" means ""like lanthanum"", it has been argued that lanthanum cannot logically be a lanthanide, but IUPAC acknowledges its inclusion based on common usage.[6] In presentations of the periodic table, the lanthanides and the actinides are customarily shown as two additional rows below the main body of the table,[2] with placeholders or else a selected single element of each series (either lanthanum and actinium, or lutetium and lawrencium) shown in a single cell of the main table, between barium and hafnium, and radium and rutherfordium, respectively. This convention is entirely a matter of aesthetics and formatting practicality; a rarely used wide-formatted periodic table inserts the lanthanide and actinide series in their proper places, as parts of the table's sixth and seventh rows (periods).
 The 1985 International Union of Pure and Applied Chemistry ""Red Book"" (p. 45) recommends that ""lanthanoid"" is used rather than ""lanthanide"". The ending ""-ide"" normally indicates a negative ion. However, owing to wide current use, ""lanthanide"" is still allowed.
 Primordial  From decay  Synthetic Border shows natural occurrence of the element
"
Crystal_structure,Chemistry,1,"In crystallography, crystal structure is a description of the ordered arrangement of atoms, ions or molecules in a crystalline material.[3] Ordered structures occur from the intrinsic nature of the constituent particles to form symmetric patterns that repeat along the principal directions of three-dimensional space in matter.
 The smallest group of particles in the material that constitutes this repeating pattern is the unit cell of the structure. The unit cell completely reflects the symmetry and structure of the entire crystal, which is built up by repetitive translation of the unit cell along its principal axes. The translation vectors define the nodes of the Bravais lattice.
 The lengths of the principal axes, or edges, of the unit cell and the angles between them are the lattice constants, also called lattice parameters or cell parameters. The symmetry properties of the crystal are described by the concept of space groups.[3] All possible symmetric arrangements of particles in three-dimensional space may be described by the 230 space groups.
 The crystal structure and symmetry play a critical role in determining many physical properties, such as cleavage, electronic band structure, and optical transparency.
"
Lattice_energy,Chemistry,1,"The lattice energy of a crystalline solid is a measure of the energy released when ions are combined to make a compound.  It is a measure of the cohesive forces that bind ions.  Lattice energy is relevant to many practical properties including solubility, hardness, and volatility.  The lattice energy is usually deduced from the Born–Haber cycle.[1]"
Law_of_conservation_of_energy,Chemistry,1,"
 In physics and chemistry, the law of conservation of energy states that the total energy of an isolated system remains constant; it is said to be conserved over time.[1] This law, first proposed and tested by Émilie du Châtelet, means that energy can neither be created nor destroyed; rather, it can only be transformed or transferred from one form to another. For instance, chemical energy is converted to kinetic energy when a stick of dynamite explodes. If one adds up all forms of energy that were released in the explosion, such as the kinetic energy and potential energy of the pieces, as well as heat and sound, one will get the exact decrease of chemical energy in the combustion of the dynamite.  Classically, conservation of energy was distinct from conservation of mass; however, special relativity showed that mass is related to energy and vice versa by E = mc2, and science now takes the view that mass–energy as a whole is conserved. Theoretically, this implies that any object with mass can itself be converted to pure energy, and vice versa, though this is believed to be possible only under the most extreme of physical conditions, such as likely existed in the universe very shortly after the Big Bang or when black holes emit Hawking radiation.
 Conservation of energy can be rigorously proven by Noether's theorem as a consequence of continuous time translation symmetry; that is, from the fact that the laws of physics do not change over time.
 A consequence of the law of conservation of energy is that a perpetual motion machine of the first kind cannot exist, that is to say, no system without an external energy supply can deliver an unlimited amount of energy to its surroundings.[2] For systems which do not have time translation symmetry, it may not be possible to define conservation of energy. Examples include curved spacetimes in general relativity[3] or time crystals in condensed matter physics.[4][5][6][7]"
Law_of_conservation_of_mass,Chemistry,1,"In physics and chemistry, the law of conservation of mass or principle of mass conservation states that for any system closed to all transfers of matter and energy, the mass of the system must remain constant over time, as the system's mass cannot change, so quantity can neither be added nor be removed. Therefore, the quantity of mass is conserved over time.
 The law implies that mass can neither be created nor destroyed, although it may be rearranged in space, or the entities associated with it may be changed in form. For example, in chemical reactions, the mass of the chemical components before the reaction is equal to the mass of the components after the reaction. Thus, during any chemical reaction and low-energy thermodynamic processes in an isolated system, the total mass of the reactants, or starting materials, must be equal to the mass of the products.
 The concept of mass conservation is widely used in many fields such as chemistry, mechanics, and fluid dynamics. Historically, mass conservation was demonstrated in chemical reactions independently by Mikhail Lomonosov and later rediscovered by Antoine Lavoisier in the late 18th century. The formulation of this law was of crucial importance in the progress from alchemy to the modern natural science of chemistry.
 The conservation of mass only holds approximately and is considered part of a series of assumptions coming from classical mechanics. The law has to be modified to comply with the laws of quantum mechanics and special relativity under the principle of mass-energy equivalence, which states that energy and mass form one conserved quantity. For very energetic systems the conservation of mass-only is shown not to hold, as is the case in nuclear reactions and particle-antiparticle annihilation in particle physics.
 Mass is also not generally conserved in open systems. Such is the case when various forms of energy and matter are allowed into, or out of, the system. However, unless radioactivity or nuclear reactions are involved, the amount of energy escaping (or entering) such systems as heat, mechanical work, or electromagnetic radiation is usually too small to be measured as a decrease (or increase) in the mass of the system.
 For systems where large gravitational fields are involved, general relativity has to be taken into account, where mass-energy conservation becomes a more complex concept, subject to different definitions, and neither mass nor energy is as strictly and simply conserved as is the case in special relativity.
"
Law_of_multiple_proportions,Chemistry,1,"
 In chemistry, the law of multiple proportions states that if two elements form more than one compound between them, then the ratios of the masses of the second element which combine with a fixed mass of the first element will always be ratios of small whole numbers.[1] This law is sometimes called Dalton's Law, named after John Dalton, the chemist who first expressed it.
 For example, Dalton knew that the element carbon forms two oxides by combining with oxygen in different proportions. A fixed mass of carbon, say 100 grams, may react with 133 grams of oxygen to produce one oxide, or with 266 grams of oxygen to produce the other. The ratio of the masses of oxygen that can react with 100 grams of carbon is 266:133 = 2:1, a ratio of small whole numbers.[2] Dalton interpreted this result in his atomic theory by proposing (correctly in this case) that the two oxides have one and two oxygen atoms respectively for each carbon atom. In modern notation the first is CO (carbon monoxide) and the second is CO2 (carbon dioxide).
 John Dalton first expressed this observation in 1804.[3] A few years previously, the French chemist Joseph Proust had proposed the law of definite proportions, which expressed that the elements combined to form compounds in certain well-defined proportions, rather than mixing in just any proportion; and Antoine Lavoisier proved the law of conservation of mass, which helped out Dalton. Careful study of the actual numerical values of these proportions led Dalton to propose his law of multiple proportions. This was an important step toward the atomic theory that he would propose later that year, and it laid the basis for chemical formulas for compounds.
 Another example of the law can be seen by comparing ethane (C2H6) with propane (C3H8). The weight of hydrogen which combines with 1 g carbon is 0.252 g in ethane and 0.224 g in propane. The ratio of those weights is 1.125, which can be expressed as the ratio of two small numbers 9:8.
"
Laws_of_thermodynamics,Chemistry,1,"The four fundamental laws of thermodynamics express empirical facts and define physical quantities, such as temperature, heat, thermodynamic work, and entropy, that characterize thermodynamic processes and thermodynamic systems in thermodynamic equilibrium. They describe the relationships between these quantities, and form a basis for precluding the possibility of certain phenomena, such as perpetual motion. In addition to their use in thermodynamics, the laws have interdisciplinary applications in physics and chemistry.
 Traditionally, thermodynamics has stated three fundamental laws: the first law, the second law, and the third law.[1][2][3] A more fundamental statement was later labelled the 'zeroth law'. The law of conservation of mass is also an equally fundamental concept in the theory of thermodynamics, but it is not generally included as a law of thermodynamics.
 The zeroth law of thermodynamics defines thermal equilibrium and forms a basis for the definition of temperature. It says that if two systems are each in thermal equilibrium with a third system, then they are in thermal equilibrium with each other.
 The first law of thermodynamics says that when energy passes into or out of a system (as work, heat, or matter), the system's internal energy changes in accord with the law of conservation of energy. Equivalently, perpetual motion machines of the first kind (machines that produce work with no energy input) are impossible.
 The second law of thermodynamics can be expressed in two main ways. In terms of possible processes, Rudolf Clausius stated that heat does not spontaneously pass from a colder body to a warmer body. Equivalently, perpetual motion machines of the second kind (machines that spontaneously convert thermal energy into mechanical work) are impossible. In terms of entropy, in a natural thermodynamic process, the sum of the entropies of interacting thermodynamic systems increases.
 The third law of thermodynamics states that a system's entropy approaches a constant value as the temperature approaches absolute zero.  With the exception of non-crystalline solids (glasses) the entropy of a system at absolute zero is typically close to zero.[2]"
Leveling_effect,Chemistry,1,"Leveling effect or solvent leveling refers to the effect of solvent on the properties of acids and bases.  The strength of a strong acid is limited (""leveled"") by the basicity of the solvent. Similarly the strength of a strong base is leveled by the acidity of the solvent.  When a strong acid is dissolved in water, it reacts with it to form hydronium ion (H3O+).[2] An example of this would be the following reaction, where ""HA"" is the strong acid:
 Any acid that is stronger than H3O+ reacts with H2O to form H3O+.  Therefore, no acid stronger than H3O+ exists in H2O. For example, aqueous perchloric acid (HClO4), aqueous hydrochloric acid (HCl) and aqueous nitric acid (HNO3) are all completely ionized, and are all equally strong acids.[3] Similarly, when ammonia is the solvent, the strongest acid is ammonium (NH4+), thus HCl and a super acid exert the same acidifying effect.
 The same argument applies to bases. In water, OH− is the strongest base.  Thus, even though sodium amide (NaNH2) is an exceptional base (pKa of NH3 ~ 33), in water it is only as good as sodium hydroxide.  On the other hand, NaNH2 is a far more basic reagent in ammonia than is NaOH.
 The pH range allowed by a particular solvent is called the acid-base discrimination window.[1]"
Lewis_acid,Chemistry,1,"A Lewis acid is a chemical species that contains an empty orbital which is capable of accepting an electron pair from a Lewis base to form a Lewis adduct. A Lewis base, then, is any species that has a filled orbital containing an electron pair which is not involved in bonding but may form a dative bond with a Lewis acid to form a Lewis adduct. For example, NH3 is a Lewis base, because it can donate its lone pair of electrons. Trimethylborane (Me3B) is a Lewis acid as it is capable of accepting a lone pair. In a Lewis adduct, the Lewis acid and base share an electron pair furnished by the Lewis base, forming a dative bond.[1] In the context of a specific chemical reaction between NH3 and Me3B, the lone pair from NH3 will form a dative bond with the empty orbital of Me3B to form an adduct NH3•BMe3. The terminology refers to the contributions of Gilbert N. Lewis.[2] The terms nucleophile and electrophile are more or less interchangeable with Lewis base and Lewis acid, respectively. However, these terms, especially their abstract noun forms nucleophilicity and electrophilicity, emphasize the kinetic aspect of reactivity, while the Lewis basicity and Lewis acidity emphasize the thermodynamic aspect of Lewis adduct formation.[3]"
Lewis_base,Chemistry,1,"A Lewis acid is a chemical species that contains an empty orbital which is capable of accepting an electron pair from a Lewis base to form a Lewis adduct. A Lewis base, then, is any species that has a filled orbital containing an electron pair which is not involved in bonding but may form a dative bond with a Lewis acid to form a Lewis adduct. For example, NH3 is a Lewis base, because it can donate its lone pair of electrons. Trimethylborane (Me3B) is a Lewis acid as it is capable of accepting a lone pair. In a Lewis adduct, the Lewis acid and base share an electron pair furnished by the Lewis base, forming a dative bond.[1] In the context of a specific chemical reaction between NH3 and Me3B, the lone pair from NH3 will form a dative bond with the empty orbital of Me3B to form an adduct NH3•BMe3. The terminology refers to the contributions of Gilbert N. Lewis.[2] The terms nucleophile and electrophile are more or less interchangeable with Lewis base and Lewis acid, respectively. However, these terms, especially their abstract noun forms nucleophilicity and electrophilicity, emphasize the kinetic aspect of reactivity, while the Lewis basicity and Lewis acidity emphasize the thermodynamic aspect of Lewis adduct formation.[3]"
Lewis_structure,Chemistry,1,"Lewis structures, also known as Lewis dot diagrams, Lewis dot formulas, Lewis dot structures, electron dot structures, or Lewis electron dot structures (LEDS), are diagrams that show the bonding between atoms of a molecule and the lone pairs of electrons that may exist in the molecule.[1][2][3] A Lewis structure can be drawn for any covalently bonded molecule, as well as coordination compounds. The Lewis structure was named after Gilbert N. Lewis, who introduced it in his 1916 article The Atom and the Molecule.[4] Lewis structures extend the concept of the electron dot diagram by adding lines between atoms to represent shared pairs in a chemical bond.
 Lewis structures show each atom and its position in the structure of the molecule using its chemical symbol. Lines are drawn between atoms that are bonded to one another (pairs of dots can be used instead of lines). Excess electrons that form lone pairs are represented as pairs of dots, and are placed next to the atoms.
 Although main group elements of the second period and beyond usually react by gaining, losing, or sharing electrons until they have achieved a valence shell electron configuration with a full octet of (8) electrons, hydrogen (H) can only form bonds which share just two electrons.
"
Ligand,Chemistry,1,"
 In coordination chemistry, a ligand[a] is an ion or molecule (functional group) that binds to a central metal atom to form a coordination complex. The bonding with the metal generally involves formal donation of one or more of the ligand's electron pairs. The nature of metal–ligand bonding can range from covalent to ionic. Furthermore, the metal–ligand bond order can range from one to three. Ligands are viewed as Lewis bases, although rare cases are known to involve Lewis acidic ""ligands"".[1][2] Metals and metalloids are bound to ligands in virtually all circumstances, although gaseous ""naked"" metal ions can be generated in a high vacuum. Ligands in a complex dictate the reactivity of the central atom, including ligand substitution rates, the reactivity of the ligands themselves, and redox. Ligand selection is a critical consideration in many practical areas, including bioinorganic and medicinal chemistry, homogeneous catalysis, and environmental chemistry.
 Ligands are classified in many ways, including: charge, size (bulk), the identity of the coordinating atom(s), and the number of electrons donated to the metal (denticity or hapticity). The size of a ligand is indicated by its cone angle.
"
Light,Chemistry,1,"
 Light or visible light is electromagnetic radiation within the portion of the electromagnetic spectrum that can be perceived by the human eye.[1] Visible light is usually defined as having wavelengths in the range of 400–700 nanometers (nm), or 4.00 × 10−7 to 7.00 × 10−7 m, between the infrared (with longer wavelengths) and the ultraviolet (with shorter wavelengths).[2][3] This wavelength means a frequency range of roughly 430–750 terahertz (THz).
 The main source of light on Earth is the Sun. Sunlight provides the energy that green plants use to create sugars mostly in the form of starches, which release energy into the living things that digest them. This process of photosynthesis provides virtually all the energy used by living things. Historically, another important source of light for humans has been fire, from ancient campfires to modern kerosene lamps. With the development of electric lights and power systems, electric lighting has effectively replaced firelight. Some species of animals generate their own light, a process called bioluminescence. For example, fireflies use light to locate mates, and vampire squids use it to hide themselves from prey.
 The primary properties of visible light are intensity, propagation direction, frequency or wavelength spectrum, and polarization, while its speed in a vacuum, 299,792,458 meters per second, is one of the fundamental constants of nature. Visible light, as with all types of electromagnetic radiation (EMR), is experimentally found to always move at this speed in a vacuum.[4] In physics, the term light sometimes refers to electromagnetic radiation of any wavelength, whether visible or not.[5][6] In this sense, gamma rays, X-rays, microwaves and radio waves are also light. Like all types of EM radiation, visible light propagates as waves. However, the energy imparted by the waves is absorbed at single locations the way particles are absorbed. The absorbed energy of the EM waves is called a photon, and represents the quanta of light. When a wave of light is transformed and absorbed as a photon, the energy of the wave instantly collapses to a single location, and this location is where the photon ""arrives."" This is what is called the wave function collapse. This dual wave-like and particle-like nature of light is known as the wave–particle duality. The study of light, known as optics, is an important research area in modern physics.
"
Liquefaction,Chemistry,1,"In materials science, liquefaction[1] is a process that generates a liquid from a solid or a gas[2] or that generates a non-liquid phase which behaves in accordance with fluid dynamics.[3] 
It occurs both naturally and artificially. As an example of the latter, a ""major commercial application of liquefaction is the liquefaction of air to allow separation of the constituents, such as oxygen, nitrogen, and the noble gases.""[4] Another is the conversion of solid coal into a liquid form usable as a substitute for liquid fuels.[5]"
Liquid,Chemistry,1,"A liquid is a nearly incompressible fluid that conforms to the shape of its container but retains a (nearly) constant volume independent of pressure. As such, it is one of the four fundamental states of matter (the others being solid, gas, and plasma), and is the only state with a definite volume but no fixed shape. A liquid is made up of tiny vibrating particles of matter, such as atoms, held together by intermolecular bonds. Like a gas, a liquid is able to flow and take the shape of a container. Most liquids resist compression, although others can be compressed. Unlike a gas, a liquid does not disperse to fill every space of a container, and maintains a fairly constant density. A distinctive property of the liquid state is surface tension, leading to wetting phenomena. Water is, by far, the most common liquid on Earth.
 The density of a liquid is usually close to that of a solid, and much higher than in a gas. Therefore, liquid and solid are both termed condensed matter. On the other hand, as liquids and gases share the ability to flow, they are both called fluids. Although liquid water is abundant on Earth, this state of matter is actually the least common in the known universe, because liquids require a relatively narrow temperature/pressure range to exist. Most known matter in the universe is in gaseous form (with traces of detectable solid matter) as interstellar clouds or in plasma from within stars.
"
Locant,Chemistry,1,"In the nomenclature of organic chemistry, a locant is a figure to indicate the position of a functional group within a molecule.[1]"
London_dispersion_forces,Chemistry,1,"London dispersion forces (LDF, also known as dispersion forces, London forces, instantaneous dipole–induced dipole forces, or loosely as van der Waals forces) are a type of force acting between atoms and molecules.[1] They are part of the van der Waals forces. The LDF is named after the German physicist Fritz London.
"
Magnetic_quantum_number,Chemistry,1,"The magnetic quantum number (symbol ml) is one of four quantum numbers in atomic physics. The set is: principal quantum number, azimuthal quantum number, magnetic quantum number, and spin quantum number. Together, they describe the unique quantum state of an electron. The magnetic quantum number distinguishes the orbitals available within a subshell, and is used to calculate the azimuthal component of the orientation of orbital in space.  Electrons in a particular subshell (such as s, p, d, or f) are defined by values of ℓ (0, 1, 2, or 3).  The value of ml can range from -ℓ to +ℓ, including zero.  Thus the s, p, d, and f subshells contain 1, 3, 5, and 7 orbitals each, with values of m within the ranges 0, ±1, ±2, ±3 respectively.  Each of these orbitals can accommodate up to two electrons (with opposite spins), forming the basis of the periodic table.
"
Malleability,Chemistry,1,"Ductility is a mechanical property commonly described as a material's amenability to drawing (e.g. into wire).[1] In materials science, ductility is defined by the degree to which a material can sustain plastic deformation under tensile stress before failure.[2][3] Ductility is an important consideration in engineering and manufacturing, defining a material's suitability for certain manufacturing operations (such as cold working) and its capacity to absorb mechanical overload.[4] Materials that are generally described as ductile include gold and copper.[5] Malleability, a similar mechanical property, is characterized by a material's ability to deform plastically without failure under compressive stress.[6][7] Historically, materials were considered malleable if they were amenable to forming by hammering or rolling.[1] Lead is an example of a material which is, relatively, malleable but not ductile.[5][8]"
Manometer,Chemistry,1,"Pressure measurement is the analysis of an applied force by a fluid (liquid or gas) on a surface. Pressure is typically measured in units of force per unit of surface area. Many techniques have been developed for the measurement of pressure and vacuum. Instruments used to measure and display pressure in an integral unit are called pressure meters or pressure gauges or vacuum gauges. A manometer is a good example, as it uses the surface area and weight of a column of liquid to both measure and indicate pressure. Likewise the widely used Bourdon gauge is a mechanical device, which both measures and indicates and is probably the best known type of gauge.
 A vacuum gauge is a pressure gauge used to measure pressures lower than the ambient atmospheric pressure, which is set as the zero point, in negative values (e.g.: −15 psig or −760 mmHg equals total vacuum). Most gauges measure pressure relative to atmospheric pressure as the zero point, so this form of reading is simply referred to as ""gauge pressure"". However, anything greater than total vacuum is technically a form of pressure. For very accurate readings, especially at very low pressures, a gauge that uses total vacuum as the zero point may be used, giving pressure readings in an absolute scale.
 Other methods of pressure measurement involve sensors that can transmit the pressure reading to a remote indicator or control system (telemetry).
"
Mass,Chemistry,1,"
 Mass is both a property of a physical body and a measure of its resistance to acceleration (a change in its state of motion) when a net force is applied.[1] An object's mass also determines the strength of its gravitational attraction to other bodies.
 The basic SI unit of mass is the kilogram (kg). In physics, mass is not the same as weight, even though mass is often determined by measuring the object's weight using a spring scale, rather than balance scale comparing it directly with known masses. An object on the Moon would weigh less than it does on Earth because of the lower gravity, but it would still have the same mass. This is because weight is a force, while mass is the property that (along with gravity) determines the strength of this force.
"
Mass_concentration_(chemistry),Chemistry,1,"In chemistry, the mass concentration ρi (or γi) is defined as the mass of a constituent mi divided by the volume of the mixture V.[1] For a pure chemical the mass concentration equals its density (mass divided by volume); thus the mass concentration of a component in a mixture can be called the density of a component in a mixture. This explains the usage of ρ (the lower case Greek letter rho), the symbol most often used for density.
"
Mass_fraction_(chemistry),Chemistry,1,"In chemistry, the mass fraction of a substance within a mixture is the ratio  




w

i




{  w_{i}}
 (alternatively denoted 




Y

i




{  Y_{i}}
) of the mass 




m

i




{  m_{i}}
 of that substance to the total mass 




m

tot




{  m_{\text{tot}}}
 of the mixture.[1] Expressed as a formula, the mass fraction is:
 Because the individual masses of the ingredients of a mixture sum to 




m

tot




{  m_{\text{tot}}}
, their mass fractions sum to unity:
 Mass fraction can also be expressed, with a denominator of 100, as percentage by mass (in commercial contexts often called percentage by weight, abbreviated wt%; see mass versus weight). It is one way of expressing the composition of a mixture in a dimensionless size; mole fraction (percentage by moles, mol%) and volume fraction (percentage by volume, vol%) are others.
 When the prevalences of interest are those of individual chemical elements, rather than of compounds or other substances, the term mass fraction can also refer to the ratio of the mass of an element to the total mass of a sample. In these contexts an alternative term is mass percent composition.  The mass fraction of an element in a compound can be calculated from the compound's empirical formula[2] or its chemical formula.[3]"
Mass_number,Chemistry,1,"The mass number (symbol A, from the German word Atomgewicht [atomic weight]),[1] also called  atomic mass number or nucleon number, is the total number of protons and neutrons (together known as nucleons) in an atomic nucleus. It is approximately equal to the atomic (also known as isotopic) mass of the atom expressed in atomic mass units. Since protons and neutrons are both baryons, the mass number A is identical with the baryon number B as of the nucleus as of the whole atom or ion. The mass number is different for each different isotope of a chemical element. Hence, the difference between the mass number and the atomic number Z gives the number of neutrons (N) in a given nucleus: N = A − Z.[2] The mass number is written either after the element name or as a superscript to the left of an element's symbol. For example, the most common isotope of carbon is carbon-12, or 12C, which has 6 protons and 6 neutrons. The full isotope symbol would also have the atomic number (Z) as a subscript to the left of the element symbol directly below the mass number: 126C.[3]"
Mass_spectrometry,Chemistry,1,"Mass spectrometry (MS) is an analytical technique that measures the mass-to-charge ratio of  ions. The results are typically presented as a mass spectrum, a plot of intensity as a function of the mass-to-charge ratio. Mass spectrometry is used in many different fields and is applied to pure samples as well as complex mixtures.
 A mass spectrum is a plot of the ion signal as a function of the mass-to-charge ratio. These spectra are used to determine the elemental or isotopic signature of a sample, the masses of particles and of molecules, and to elucidate the chemical identity or structure of molecules and other chemical compounds.
 In a typical MS procedure, a sample, which may be solid, liquid, or gaseous, is ionized, for example by bombarding it with electrons. This may cause some of the sample's molecules to break into charged fragments or simply become charged without fragmenting. These ions are then separated according to their mass-to-charge ratio, for example by accelerating them and subjecting them to an electric or magnetic field: ions of the same mass-to-charge ratio will undergo the same amount of deflection.[1] The ions are detected by a mechanism capable of detecting charged particles, such as an electron multiplier. Results are displayed as spectra of the signal intensity of detected ions as a function of the mass-to-charge ratio. The atoms or molecules in the sample can be identified by correlating known masses (e.g. an entire molecule) to the identified masses or through a characteristic fragmentation pattern.
"
Matter,Chemistry,1,"
 In classical physics and general chemistry, matter is any substance that has mass and takes up space by having volume.[1]:21 All everyday objects that can be touched are ultimately composed of atoms, which are made up of interacting subatomic particles, and in everyday as well as scientific usage, ""matter"" generally includes atoms and anything made up of them, and any particles (or combination of particles) that act as if they have both rest mass and volume. However it does not include massless particles such as photons, or other energy phenomena or waves such as light.[1]:21[2] Matter exists in various states (also known as phases). These include classical everyday phases such as solid, liquid, and gas – for example water exists as ice, liquid water, and gaseous steam – but other states are possible, including plasma, Bose–Einstein condensates, fermionic condensates, and quark–gluon plasma.[3] Usually atoms can be imagined as a nucleus of protons and neutrons, and a surrounding ""cloud"" of orbiting electrons which ""take up space"".[4][5] However this is only somewhat correct, because subatomic particles and their properties are governed by their quantum nature, which means they do not act as everyday objects appear to act – they can act like waves as well as particles and they do not have well-defined sizes or positions. In the Standard Model of particle physics, matter is not a fundamental concept because the elementary constituents of atoms are quantum entities which do not have an inherent ""size"" or ""volume"" in any everyday sense of the word. Due to the exclusion principle and other fundamental interactions, some ""point particles"" known as fermions (quarks, leptons), and many composites and atoms, are effectively forced to keep a distance from other particles under everyday conditions; this creates the property of matter which appears to us as matter taking up space.
 For much of the history of the natural sciences people have contemplated the exact nature of matter. The idea that matter was built of discrete building blocks, the so-called particulate theory of matter, independently appeared in ancient Greece and ancient India among Buddhists, Hindus and Jains in 1st-millennium BC.[6] Ancient philosophers who proposed the particulate theory of matter include Kanada (c. 6th–century BC or after),[7] Leucippus (~490 BC) and Democritus (~470–380 BC).[8]"
Metal,Chemistry,1,"A metal (from Greek μέταλλον métallon, ""mine, quarry, metal"") is a material that, when freshly prepared, polished, or fractured, shows a lustrous appearance, and conducts electricity and heat relatively well. Metals are typically malleable (they can be hammered into thin sheets) or ductile (can be drawn into wires). A metal may be a chemical element such as iron; an alloy such as stainless steel; or a molecular compound such as polymeric sulfur nitride.
 In physics, a metal is generally regarded as any substance capable of conducting electricity at a temperature of absolute zero.[1] Many elements and compounds that are not normally classified as metals become metallic under high pressures. For example, the nonmetal iodine gradually becomes a metal at a pressure of between 40 and 170 thousand times atmospheric pressure. Equally, some materials regarded as metals can become nonmetals. Sodium, for example, becomes a nonmetal at pressure of just under two million times atmospheric pressure.
 In chemistry, two elements that would otherwise qualify (in physics) as brittle metals—arsenic and antimony—are commonly instead recognised as metalloids due to their chemistry (predominately non-metallic for arsenic, and balanced between metallicity and nonmetallicity for antimony). Around 95 of the 118 elements in the periodic table are metals (or are likely to be such). The number is inexact as the boundaries between metals, nonmetals, and metalloids fluctuate slightly due to a lack of universally accepted definitions of the categories involved.
 In astrophysics the term ""metal"" is cast more widely to refer to all chemical elements in a star that are heavier than the lightest two, hydrogen and helium, and not just traditional metals. In this sense the first four ""metals"" collecting in stellar cores through nucleosynthesis are carbon, nitrogen, oxygen, and neon, all of which are strictly non-metals in chemistry.  A star fuses lighter atoms, mostly hydrogen and helium, into heavier atoms over its lifetime. Used in that sense, the metallicity of an astronomical object is the proportion of its matter made up of the heavier chemical elements.[2] Metals, as chemical elements, comprise 25% of the Earth's crust and are present in many aspects of modern life. The strength and resilience of some metals has led to their frequent use in, for example, high-rise building and bridge construction, as well as most vehicles, many home appliances, tools, pipes, and railroad tracks. Precious metals were historically used as coinage, but in the modern era, coinage metals have extended to at least 23 of the chemical elements.[3] The history of refined metals is thought to begin with the use of copper about 11,000 years ago. Gold, silver, iron (as meteoric iron), lead, and brass were likewise in use before the first known appearance of bronze in the 5th millennium BCE. Subsequent developments include the production of early forms of steel; the discovery of sodium—the first light metal—in 1809; the rise of modern alloy steels; and, since the end of World War II, the development of more sophisticated alloys.
"
Melting,Chemistry,1,"Melting, or fusion, is a physical process that results in the phase transition of a substance from a solid to a liquid. This occurs when the internal energy of the solid increases, typically by the application of heat or pressure, which increases the substance's temperature to the melting point. At the melting point, the ordering of ions or molecules in the solid breaks down to a less ordered state, and the solid melts to become a liquid.
 Substances in the molten state generally have reduced viscosity as  the temperature increases. An exception to this principle is the element sulfur, whose viscosity increases in the range of 160 °C to 180 °C due to polymerization.[1] Some organic compounds melt through mesophases, states of partial order between solid and liquid.
"
Melting_point,Chemistry,1,"The melting point (or, rarely, liquefaction point) of a substance is the temperature at which it changes state from solid to liquid. At the melting point the solid and liquid phase exist in equilibrium. The melting point of a substance depends on pressure and is usually specified at a standard pressure such as 1 atmosphere or 100 kPa.
 When considered as the temperature of the reverse change from liquid to solid, it is referred to as the freezing point or crystallization point. Because of the ability of some substances to supercool, the freezing point is not considered as a characteristic property of a substance. When the ""characteristic freezing point"" of a substance is determined, in fact the actual methodology is almost always ""the principle of observing the disappearance rather than the formation of ice, that is, the melting point.[1]"
Metalloid,Chemistry,1,"Recognition status, as metalloids, of some elements in the p-block of the periodic table. Percentages are median appearance frequencies in the lists of metalloids.[n 1] The staircase-shaped line is a typical example of the arbitrary metal–nonmetal dividing line found on some periodic tables.
 A metalloid is a type of chemical element which has a preponderance of properties in between, or that are a mixture of, those of metals and nonmetals. There is no standard definition of a metalloid and no complete agreement on which elements are metalloids. Despite the lack of specificity, the term remains in use in the literature of chemistry.
 The six commonly recognised metalloids are boron, silicon, germanium, arsenic, antimony, and tellurium. Five elements are less frequently so classified: carbon, aluminium, selenium, polonium, and astatine. On a standard periodic table, all eleven elements are in a diagonal region of the p-block extending from boron at the upper left to astatine at lower right. Some periodic tables include a dividing line between metals and nonmetals, and the metalloids may be found close to this line.
 Typical metalloids have a metallic appearance, but they are brittle and only fair conductors of electricity. Chemically, they behave mostly as nonmetals. They can form alloys with metals. Most of their other physical properties and chemical properties are intermediate in nature. Metalloids are usually too brittle to have any structural uses. They and their compounds are used in alloys, biological agents, catalysts, flame retardants, glasses, optical storage and optoelectronics, pyrotechnics, semiconductors, and electronics.
 The electrical properties of silicon and germanium enabled the establishment of the semiconductor industry in the 1950s and the development of solid-state electronics from the early 1960s.[1] The term metalloid originally referred to nonmetals. Its more recent meaning, as a category of elements with intermediate or hybrid properties, became widespread in 1940–1960. Metalloids are sometimes called semimetals, a practice that has been discouraged,[2] as the term semimetal has a different meaning in physics than in chemistry. In physics, it refers to a specific kind of electronic band structure of a substance. In this context, only arsenic and antimony are semimetals, and commonly recognised as metalloids.
"
Methylene_blue,Chemistry,1,"Methylene blue, also known as methylthioninium chloride, is a medication and dye.[4] As a medication, it is mainly used to treat methemoglobinemia.[4][2] Specifically, it is used to treat methemoglobin levels that are greater than 30% or in which there are symptoms despite oxygen therapy.[2] It has previously been used for cyanide poisoning and urinary tract infections, but this use is no longer recommended.[4] It is typically given by injection into a vein.[4] Common side effects include headache, vomiting, confusion, shortness of breath, and high blood pressure.[4] Other side effects include serotonin syndrome, red blood cell breakdown, and allergic reactions.[4] Use often turns the urine, sweat, and stool blue to green in color.[2] While use during pregnancy may harm the baby, not using it in methemoglobinemia is likely more dangerous.[4][2] Methylene blue is a thiazine dye.[4] It works by converting the ferric iron in hemoglobin to ferrous iron.[2] Methylene blue was first prepared in 1876, by Heinrich Caro.[5] It is on the World Health Organization's List of Essential Medicines.[6] In the United States, a 50 mg vial costs about US$191.70.[7] In the United Kingdom, a 50 mg vial costs the NHS about £39.38.[2]"
Microcentrifuge,Chemistry,1,"A laboratory centrifuge is a piece of laboratory equipment, driven by a motor, which spins liquid samples at high speed. 
There are various types of centrifuges, depending on the size and the sample capacity.[1] Like all other centrifuges, laboratory centrifuges work by the sedimentation principle, where the centripetal acceleration is used to separate substances of greater and lesser density.
"
Mineral,Chemistry,1,"
 In geology and mineralogy, a mineral or mineral species is, broadly speaking, a solid chemical compound with a fairly well-defined chemical composition and a specific crystal structure, that occurs naturally in pure form.[1].[2] The geological definition of mineral normally excludes compounds that occur only in living beings. However some minerals are often biogenic (such as calcite) or are organic compounds in the sense of chemistry (such as mellite). Moreover, living beings often synthesize inorganic minerals (such as hydroxylapatite) that also occur in rocks.
 The concept of mineral is distinct from rock, any bulk solid geologic material that is relatively homogeneous at a large enough scale.  A rock may consist of one type of mineral, or may be an aggregate of two or more different types of minerals, spacially segregated into distinct phases.[3] Some natural solid substances without a definite crystalline structure, such as opal or obsidian, are more properly called mineraloids.[4]  If a chemical compound occurs naturally with different crystal structures, each structure is considered a different mineral species.  Thus, for example, quartz and stishovite are two different minerals consisting of the same compound, silicon dioxide.
 The International Mineralogical Association (IMA) is the generally recognized standard body for the definition and nomenclature of mineral species. As of March 2020[update], the IMA recognizes 5,562 official mineral species[5] out of more than 5,750 proposed or traditional ones.[6] The chemical composition of a named mineral species may vary somewhat by the inclusion of small amounts of impurities.  Specific varieties of a species sometimes have conventional or official names of their own.[7] For example, amethyst is a purple variety of the mineral species quartz.  Some mineral species can have variable proportions of two or more chemical elements that occupy equivalent positions in the mineral's structure; for example, the formula of mackinawite is given as (Fe,Ni)9S8, meaning FexNi9-xS8, where x is a variable number between 0 and 9.  Sometimes a mineral with variable composition is split into separate species, more or less arbitrarily, forming a mineral group; that is the case of the silicates CaxMgyFe2-x-ySiO4, the olivine group.
 Besides the essential chemical composition and crystal structure, the description of a mineral species usually includes its common physical properties such as  habit, hardness, lustre, diaphaneity, colour, streak, tenacity, cleavage, fracture, parting, specific gravity, magnetism, fluorescence, radioactivity, as well as its taste or smell and its reaction to acid.
 Minerals are classified by key chemical constituents; the two dominant systems are the Dana classification and the Strunz classification. Silicate minerals comprise approximately 90% of the Earth's crust.[8] Other important mineral groups include the native elements, sulfides, oxides, halides, carbonates, sulfates, and phosphates.
"
Miscibility,Chemistry,1,"Miscibility /mɪsɪˈbɪlɪti/ is the property of two substances to mix in all proportions (that is, to fully dissolve in each other at any concentration), forming a homogeneous solution. The term is most often applied to liquids but also applies to solids and gases. For example, water and ethanol are miscible because they mix in all proportions.[1] By contrast, substances are said to be immiscible if there are certain proportions in which the mixture does not form a solution. For one example, oil is not soluble in water, so these two solvents are immiscible. As another example, butanone (methyl ethyl ketone) is significantly soluble in water, but these two solvents are also immiscible because they are not soluble in all proportions.[2]"
Mixture,Chemistry,1,"
In chemistry, a mixture is a material made up of two or more different substances which are physically combined.[1] A mixture is the physical combination of two or more substances in which the identities are retained and are mixed in the form of solutions, suspensions and colloids.[2][3] Mixtures are one product of mechanically blending or mixing chemical substances such as elements and compounds, without chemical bonding or other chemical change, so that each ingredient substance retains its own chemical properties and makeup.[4] Despite the fact that there are no chemical changes to its constituents, the physical properties of a mixture, such as its melting point, may differ from those of the components. Some mixtures can be separated into their components by using physical (mechanical or thermal) means. Azeotropes are one kind of mixture that usually poses considerable difficulties regarding the separation processes required to obtain their constituents (physical or chemical processes or, even a blend of them).[5][6][7]"
Moiety_(chemistry),Chemistry,1,"In organic chemistry, a moiety (/ˈmɔɪəti/) is a part of a molecule[1][2] which is typically given a name as it can be found within other kinds of molecules as well. 
 The term moiety should be reserved to describe the larger characteristic parts of molecules and not used to describe smaller functional groups,[1][2] which are made up of atoms that participate in similar chemical reactions in most molecules that contain them.[3] In some instances moieties may be composed of yet smaller moieties and functional groups. 
 Moieties that constitute branches extending from the backbone of a hydrocarbon molecule, which can often be broken off and substituted with others, are called substituents or side chains.
"
Molality,Chemistry,1,"Molality is a measure of number of moles of solute present in 1 kg of solvent. This contrasts with the definition of molarity which is based on a specified volume of solution.
 A commonly used unit for molality in chemistry is mol/kg. A solution of concentration 1 mol/kg is also sometimes denoted as 1 molal.
"
Molar_attenuation_coefficient,Chemistry,1,"The molar attenuation coefficient is a measurement of how strongly a chemical species attenuates light at a given wavelength. It is an intrinsic property of the species. The SI unit of molar attenuation coefficient is the square metre per mole (m2/mol), but in practice, quantities are usually expressed in terms of M−1⋅cm−1 or L⋅mol−1⋅cm−1 (the latter two units are both equal to 0.1 m2/mol). In older literature, the cm2/mol is sometimes used; 1 M−1⋅cm−1 equals 1000 cm2/mol. The molar attenuation coefficient is also known as the molar extinction coefficient and molar absorptivity, but the use of these alternative terms has been discouraged by the IUPAC.[1][2]"
Molar_concentration,Chemistry,1,"Molar concentration (also called molarity, amount concentration or substance concentration) is a measure of the concentration of a chemical species, in particular of a solute in a solution, in terms of amount of substance per unit volume of solution. In chemistry, the most commonly used unit for molarity is the number of moles per liter, having the unit symbol mol/L or mol⋅dm−3 in SI unit. A solution with a concentration of 1 mol/L is said to be 1 molar, commonly designated as 1 M.  To avoid confusion with SI prefix mega, which has the same abbreviation, small caps ᴍ or italicized M are also used in journals and textbooks.[1]"
Molar_fraction,Chemistry,1,"In chemistry, the mole fraction or molar fraction (xi) is defined as unit of the amount of a constituent (expressed in moles), ni divided by the total amount of all constituents in a mixture (also expressed in moles), ntot:.[1]This expression is given below:-
 The sum of all the mole fractions is equal to 1:
 The same concept expressed with a denominator of 100 is the mole percent, molar percentage or molar proportion (mol%).
 The mole fraction is also called the amount fraction.[1] It is identical to the number fraction, which is defined as the number of molecules of a constituent Ni divided by the total number of all molecules Ntot. The mole fraction is sometimes denoted by the lowercase Greek letter χ (chi) instead of a Roman x.[2][3] For mixtures of gases, IUPAC recommends the letter y.[1] The National Institute of Standards and Technology of the United States prefers the term amount-of-substance fraction over mole fraction because it does not contain the name of the unit mole.[4] Whereas mole fraction is a ratio of moles to moles, molar concentration is a quotient of moles to volume.
 The mole fraction is one way of expressing the composition of a mixture with a dimensionless quantity; mass fraction (percentage by weight, wt%) and volume fraction (percentage by volume, vol%) are others.
"
Molar_mass,Chemistry,1,"In chemistry, the molar mass of a chemical compound is defined as the mass of a sample of that compound divided by the amount of substance in that sample, measured in moles.[1] The molar mass is a bulk, not molecular, property of a substance. The molar mass is an average of many instances of the compound, which often vary in mass due to the presence of isotopes. Most commonly, the molar mass is computed from the standard atomic weights and is thus a terrestrial average and a function of the relative abundance of the isotopes of the constituent atoms on Earth. The molar mass is appropriate for converting between the mass of a substance and the amount of a substance for bulk quantities.
 The molecular weight is very commonly used as a synonym of molar mass, particularly for molecular compounds; however, the most authoritative sources define it differently (see molecular mass).
 The formula weight is a synonym of molar mass that is frequently used for non-molecular compounds, such as ionic salts.
 The molar mass is an intensive property of the substance, that does not depend on the size of the sample. In the International System of Units (SI), the base unit of molar mass is kg/mol. However, for historical reasons, molar masses are almost always expressed in g/mol.
 The mole was defined in such as way that the molar mass of a compound, in g/mol, is numerically equal (for all practical purposes) to the average mass of one molecule, in daltons. Thus, for example, the average mass of a molecule of water is about 18.0153 daltons, and the molar mass of water is about 18.0153 g/mol.
 For chemical elements without isolated molecules, such as carbon and metals, the molar mass is computed dividing by the number of moles of atoms instead. Thus, for example, the molar mass of iron is about 55.845 g/mol.
 Between 1971 and 2019, SI defined the ""amount of substance"" as a separate dimension of measurement, and the mole was defined as the amount of substance that has as many constituent particles as there are atoms in 12 grams of carbon-12. In that period, the molar mass of carbon-12 was thus exactly 12 g/mol, by definition. Since 2019, a mole of any substance has been redefined in the SI as the amount of that substance containing an exactly defined number of particles, N = 6.02214076×1023. Therefore, the molar mass of a compound now is simply the mass of this number of molecules of the compound.
"
Mole_(unit),Chemistry,1,"The mole (symbol: mol) is the unit of measurement for amount of substance in the International System of Units (SI). A mole of a substance[1] or a mole of particles[2] is defined as containing exactly 6.02214076×1023 particles, which may be atoms, molecules, ions, or electrons.[1] In short, 1 mol contains 6.02214076×1023 of the specified particles.[3][2] The current definition was adopted in November 2018 as one of the seven SI base units,[1] revising the previous definition that specified one mole as the amount of substance in 12 grams of carbon-12 (12C), an isotope of carbon.
 The number 6.02214076×1023 (the Avogadro number) was chosen so that the mass of one mole of a chemical compound in grams is numerically equal, for most practical purposes, to the average mass of one molecule of the compound in daltons. Thus, for example, one mole of water contains 6.02214076×1023 molecules, whose total mass is about 18.015 grams – and the mean mass of one molecule of water is about 18.015 daltons.
 The mole is widely used in chemistry as a convenient way to express amounts of reactants and products of chemical reactions. For example, the chemical equation 2H2 + O2 → 2H2O can be interpreted to mean that for each 2 mol dihydrogen (H2) and 1 mol dioxygen (O2) that react, 2 mol of water (H2O) form. The mole may also be used to measure the amount of atoms, ions, electrons, or other entities.[2] The concentration of a solution is commonly expressed by its molarity, defined as the amount of dissolved substance in mole(s) per unit volume of solution, for which the unit typically used is moles per litre (mol/l), commonly abbreviated M.
 The term gram-molecule (g mol) was formerly used for ""mole of molecules"",[4] and gram-atom (g atom) for ""mole of atoms"". For example, 1 mole of MgBr2 is 1 gram-molecule of MgBr2 but 3 gram-atoms of MgBr2.[5][6]"
Molecular_formula,Chemistry,1,"
 A chemical formula is a way of  presenting information about the chemical proportions of atoms that constitute a particular chemical compound or molecule, using chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, commas and plus (+) and minus (−) signs. These are limited to a single typographic line of symbols, which may include subscripts and superscripts. A chemical formula is not a chemical name, and it contains no words. Although a chemical formula may imply certain simple chemical structures, it is not the same as a full chemical structural formula. Chemical formulae can fully specify the structure of only the simplest of molecules and chemical substances, and are generally more limited in power than are chemical names and structural formulae.
 The simplest types of chemical formulae are called empirical formulas, which use letters and numbers indicating the numerical proportions of atoms of each type. Molecular formulae indicate the simple numbers of each type of atom in a molecule, with no information on structure. For example, the empirical formula for glucose is CH2O (twice as many hydrogen atoms as carbon and oxygen), while its molecular formula is C6H12O6 (12 hydrogen atoms, six carbon and oxygen atoms).
 Sometimes a chemical formula is complicated by being written as a condensed formula (or condensed molecular formula, occasionally called a ""semi-structural formula""), which conveys additional information about the particular ways in which the atoms are chemically bonded together, either in covalent bonds, ionic bonds, or various combinations of these types. This is possible if the relevant bonding is easy to show in one dimension. An example is the condensed molecular/chemical formula for ethanol, which is CH3-CH2-OH or CH3CH2OH. However, even a condensed chemical formula is necessarily limited in its ability to show complex bonding relationships between atoms, especially atoms that have bonds to four or more different substituents.
 Since a chemical formula must be expressed as a single line of chemical element symbols, it often cannot be as informative as a true structural formula, which is a graphical representation of the spatial relationship between atoms in chemical compounds (see for example the figure for butane structural and chemical formulae, at right). For reasons of structural complexity, a single condensed chemical formula (or semi-structural formula) may correspond to different molecules, known as isomers. For example glucose shares its molecular formula C6H12O6 with a number of other sugars, including fructose, galactose and mannose. Linear equivalent chemical names exist that can and do specify uniquely any complex structural formula (see chemical nomenclature), but such names must use many terms (words), rather than the simple element symbols, numbers, and simple typographical symbols that define a chemical formula.
 Chemical formulae may be used in chemical equations to describe chemical reactions and other chemical transformations, such as the dissolving of ionic compounds into solution. While, as noted, chemical formulae do not have the full power of structural formulae to show chemical relationships between atoms, they are sufficient to keep track of numbers of atoms and numbers of electrical charges in chemical reactions, thus balancing chemical equations so that these equations can be used in chemical problems involving conservation of atoms, and conservation of electric charge.
"
Molecular_orbital,Chemistry,1,"In chemistry, a molecular orbital is a mathematical function describing the location and wave-like behavior of an electron in a molecule. This function can be used to calculate chemical and physical properties such as the probability of finding an electron in any specific region. The term orbital was introduced by Robert S. Mulliken in 1932 as an abbreviation for one-electron orbital wave function.[1] At an elementary level, it is used to describe the region of space in which the function has a significant amplitude.  In an isolated atom, the orbital electrons' location is determined by functions called atomic orbitals.  When multiple atoms combine chemically into a molecule, the electrons' locations are determined by the molecule as a whole, so the atomic orbitals combine to form molecular orbitals.  The electrons from the constituent atoms occupy the molecular orbitals.   Mathematically, molecular orbitals are an approximate solution to the Schrodinger equation for the electrons in the field of the molecule's atomic nuclei.  They are usually constructed by combining atomic orbitals or  hybrid orbitals from each atom of the molecule, or other molecular orbitals from groups of atoms. They can be quantitatively calculated using the Hartree–Fock or self-consistent field (SCF) methods.
 Molecular orbitals are of three types: bonding orbitals which have an energy lower than the energy of the atomic orbitals which formed them, and thus promote the chemical bonds which hold the molecule together; antibonding orbitals which have an energy higher than the energy of their constituent atomic orbitals, and so oppose the bonding of the molecule, and nonbonding orbitals which have the same energy as their constituent atomic orbitals and thus have no effect on the bonding of the molecule.
"
Molecular_orbital_diagram,Chemistry,1,"
 A molecular orbital diagram, or MO diagram, is a qualitative descriptive tool explaining chemical bonding in molecules in terms of molecular orbital theory in general and the linear combination of atomic orbitals (LCAO) method in particular.[1][2][3]   A fundamental principle of these theories is that as atoms bond to form molecules, a certain number of atomic orbitals combine to form the same number of molecular orbitals, although the electrons involved may be redistributed among the orbitals.  This tool is very well suited for simple diatomic molecules such as dihydrogen, dioxygen, and carbon monoxide but becomes more complex when discussing even comparatively simple polyatomic molecules, such as methane.  MO diagrams can explain why some molecules exist and others do not. They can also predict bond strength, as well as the electronic transitions that can take place.
"
Molecule,Chemistry,1,"
 A molecule is an electrically neutral group of two or more atoms held together by chemical bonds.[4][5][6][7][8] Molecules are distinguished from ions by their lack of electrical charge. 
 In quantum physics, organic chemistry, and biochemistry, the distinction from ions is dropped and molecule is often used when referring to polyatomic ions.
 In the kinetic theory of gases, the term molecule is often used for any gaseous particle regardless of its composition. This violates the definition that a molecule contain two or more atoms, since the noble gases are individual atoms.[9] A molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with two atoms in the oxygen molecule (O2); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (two hydrogen atoms and one oxygen atom; H2O). 
 Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are typically not considered single molecules.[10] Molecules as components of matter are common.  They also make up most of the oceans and atmosphere. Most organic substances are molecules.  The substances of life are molecules, e.g. proteins, the amino acids they are made of, the nucleic acids (DNA & RNA), sugars, carbohydrates, fats, and vitamins. The nutrient minerals ordinarily are not molecules, e.g. iron sulfate.
 However, the majority of familiar solid substances on Earth are not made of molecules. These include all of the minerals that make up the substance of the Earth, soil, dirt, sand, clay, pebbles, rocks, boulders, bedrock, the molten interior, and the core of the Earth. All of these contain many chemical bonds, but are not made of identifiable molecules. 
 No typical molecule can be defined for salts nor for covalent crystals, although these are often composed of repeating unit cells that extend either in a plane, e.g. graphene; or three-dimensionally e.g. diamond, quartz, sodium chloride. The theme of repeated unit-cellular-structure also holds for most metals which are condensed phases with metallic bonding. Thus solid metals are not made of molecules. 
 In glasses, which are solids that exist in a vitreous disordered state, the atoms are held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating unit-cellular-structure that characterizes salts, covalent crystals, and metals.
"
Monatomic,Chemistry,1,"In physics and chemistry, ""monatomic"" is a combination of the words ""mono"" and ""atomic"", and means ""single atom"". It is usually applied to gases: a monatomic gas is one in which atoms are not bound to each other. Examples at standard conditions include the noble gases argon, krypton, and xenon, though all chemical elements will be monatomic in the gas phase at sufficiently high temperatures. The thermodynamic behavior of a monatomic gas is extremely simple when compared to polyatomic gases because it is free of any rotational or vibrational energy.[1]"
Natural_abundance,Chemistry,1,"In physics, natural abundance (NA) refers to the abundance of isotopes of a chemical element as naturally found on a planet. The relative atomic mass (a weighted average, weighted by mole-fraction abundance figures) of these isotopes is the atomic weight listed for the element in the periodic table. The abundance of an isotope varies from planet to planet, and even from place to place on the Earth, but remains relatively constant in time (on a short-term scale).
 As an example, uranium has three naturally occurring isotopes: 238U, 235U and 234U. Their respective natural mole-fraction abundances are 99.2739–99.2752%, 0.7198–0.7202%, and 0.0050–0.0059%.[1] For example, if 100,000 uranium atoms were analyzed, one would expect to find approximately 99,274 238U atoms, approximately 720 235U atoms, and very few (most likely 5 or 6) 234U atoms. This is because 238U is much more stable than 235U or 234U, as the half-life of each isotope reveals: 4.468 × 109 years for 238U compared with 7.038 × 108 years for 235U and 245,500 years for 234U.
 Exactly because the different uranium isotopes have different half-lives, when the Earth was younger, the isotopic composition of uranium was different. As an example, 1.7×109 years ago the NA of 235U was 3.1% compared with today's 0.7%, and for that reason a natural nuclear fission reactor was able to form, something that cannot happen today.
 However, the natural abundance of a given isotope is also affected by the probability of its creation in nucleosynthesis (as in the case of samarium; radioactive 147Sm and 148Sm are much more abundant than stable 144Sm) and by production of a given isotope as a daughter of natural radioactive isotopes (as in the case of radiogenic isotopes of lead).
"
Neutron,Chemistry,1,"The neutron is a subatomic particle, symbol n or n0, which has a neutral (not positive or negative) charge and a mass slightly greater than that of a proton. Protons and neutrons constitute the nuclei of atoms. Since protons and neutrons behave similarly within the nucleus, and each has a mass of approximately one atomic mass unit, they are both referred to as nucleons.[6] Their properties and interactions are described by nuclear physics.
 The chemical properties of an atom are mostly determined by the configuration of electrons that orbit the atom's heavy nucleus. The electron configuration is determined by the charge of the nucleus, set by the number of protons, or atomic number. Neutrons do not affect the electron configuration, but the sum of atomic number and the number of neutrons, or neutron number, is the mass of the nucleus.
 Atoms of a chemical element that differ only in neutron number are called isotopes. For example, carbon, with atomic number 6, has an abundant isotope carbon-12 with 6 neutrons and a rare isotope carbon-13 with 7 neutrons.  Some elements occur in nature with only one stable isotope, such as fluorine. Other elements occur with many stable isotopes, such as tin with ten stable isotopes.
 The properties of an atomic nucleus are dependent on both atomic and neutron numbers. With their positive charge, the protons within the nucleus are repelled by the long-range electromagnetic force, but the much stronger, but short-range, nuclear force binds the nucleons closely together. Neutrons are required for the stability of nuclei, with the exception of the single-proton hydrogen nucleus. Neutrons are produced copiously in nuclear fission and fusion. They are a primary contributor to the nucleosynthesis of chemical elements within stars through fission, fusion, and neutron capture processes.
 The neutron is essential to the production of nuclear power. In the decade after the neutron was discovered by James Chadwick in 1932,[7] neutrons were used to induce many different types of nuclear transmutations. With the discovery of nuclear fission in 1938,[8] it was quickly realized that, if a fission event produced neutrons, each of these neutrons might cause further fission events, in a cascade known as a nuclear chain reaction.[9] These events and findings led to the first self-sustaining nuclear reactor (Chicago Pile-1, 1942) and the first nuclear weapon (Trinity, 1945).
 Free neutrons, while not directly ionizing atoms, cause ionizing radiation. As such they can be a biological hazard, depending upon dose.[9] A small natural ""neutron background"" flux of free neutrons exists on Earth, caused by cosmic ray showers, and by the natural radioactivity of spontaneously fissionable elements in the Earth's crust.[10] Dedicated neutron sources like neutron generators, research reactors and spallation sources produce free neutrons for use in irradiation and in neutron scattering experiments.
"
Nitrogen,Chemistry,1,"
 
 Nitrogen is the chemical element with the symbol N and atomic number 7. It was first discovered and isolated by Scottish physician Daniel Rutherford in 1772. Although Carl Wilhelm Scheele and Henry Cavendish had independently done so at about the same time, Rutherford is generally accorded the credit because his work was published first. The name nitrogène was suggested by French chemist Jean-Antoine-Claude Chaptal in 1790 when it was found that nitrogen was present in nitric acid and nitrates. Antoine Lavoisier suggested instead the name azote, from the Greek ἀζωτικός ""no life"", as it is an asphyxiant gas; this name is instead used in many languages, such as French, Italian, Russian, Romanian and Turkish, and appears in the English names of some nitrogen compounds such as hydrazine, azides and azo compounds.
 Nitrogen is the lightest member of group 15 of the periodic table, often called the pnictogens. It is a common element in the universe, estimated at about seventh in total abundance in the Milky Way and the Solar System. At standard temperature and pressure, two atoms of the element bind to form dinitrogen, a colourless and odorless diatomic gas with the formula N2. Dinitrogen forms about 78% of Earth's atmosphere, making it the most abundant uncombined element. Nitrogen occurs in all organisms, primarily in amino acids (and thus proteins), in the nucleic acids (DNA and RNA) and in the energy transfer molecule adenosine triphosphate. The human body contains about 3% nitrogen by mass, the fourth most abundant element in the body after oxygen, carbon, and hydrogen. The nitrogen cycle describes movement of the element from the air, into the biosphere and organic compounds, then back into the atmosphere.
 Many industrially important compounds, such as ammonia, nitric acid, organic nitrates (propellants and explosives), and cyanides, contain nitrogen. The extremely strong triple bond in elemental nitrogen (N≡N), the second strongest bond in any diatomic molecule after carbon monoxide (CO),[3] dominates nitrogen chemistry. This causes difficulty for both organisms and industry in converting N2 into useful compounds, but at the same time means that burning, exploding, or decomposing nitrogen compounds to form nitrogen gas releases large amounts of often useful energy. Synthetically produced ammonia and nitrates are key industrial fertilisers, and fertiliser nitrates are key pollutants in the eutrophication of water systems.
 Apart from its use in fertilisers and energy-stores, nitrogen is a constituent of organic compounds as diverse as Kevlar used in high-strength fabric and cyanoacrylate used in superglue. Nitrogen is a constituent of every major pharmacological drug class, including antibiotics. Many drugs are mimics or prodrugs of natural nitrogen-containing signal molecules: for example, the organic nitrates nitroglycerin and nitroprusside control blood pressure by metabolizing into nitric oxide. Many notable nitrogen-containing drugs, such as the natural caffeine and morphine or the synthetic amphetamines, act on receptors of animal neurotransmitters.
"
Nucleon,Chemistry,1,"In chemistry and physics, a nucleon is either a proton or a neutron, considered in its role as a component of an atomic nucleus. The number of nucleons in a nucleus defines an isotope's mass number (nucleon number).
 Until the 1960s, nucleons were thought to be elementary particles, not made up of smaller parts. Now they are known to be composite particles, made of three quarks bound together by the so-called strong interaction. The interaction between two or more nucleons is called internucleon interaction or nuclear force, which is also ultimately caused by the strong interaction. (Before the discovery of quarks, the term ""strong interaction"" referred to just internucleon interactions.)
 Nucleons sit at the boundary where particle physics and nuclear physics overlap. Particle physics, particularly quantum chromodynamics, provides the fundamental equations that explain the properties of quarks and of the strong interaction. These equations explain quantitatively how quarks can bind together into protons and neutrons (and all the other hadrons). However, when multiple nucleons are assembled into an atomic nucleus (nuclide), these fundamental equations become too difficult to solve directly (see lattice QCD). Instead, nuclides are studied within nuclear physics, which studies nucleons and their interactions by approximations and models, such as the nuclear shell model. These models can successfully explain nuclide properties, as for example, whether or not a particular nuclide undergoes radioactive decay.
 The proton and neutron are in a scheme of categories being at once fermions, hadrons and baryons. The proton carries a positive net charge and the neutron carries a zero net charge; the proton's mass is only about 0.13% less than the neutron's. Thus, they can be viewed as two states of the same nucleon, and together form an isospin doublet (I = ​1⁄2). In isospin space, neutrons can be transformed into protons via SU(2) symmetries, and vice versa. These nucleons are acted upon equally by the strong interaction, which is invariant under rotation in isospin space. According to the Noether theorem, isospin is conserved with respect to the strong interaction.[1]:129–130"
Nucleophile,Chemistry,1,"A nucleophile is a chemical species that donates an electron pair to form a chemical bond in relation to a reaction. All molecules or ions with a free pair of electrons or at least one pi bond can act as nucleophiles. Because nucleophiles donate electrons, they are by definition Lewis bases. 
Nucleophilic describes the affinity of a nucleophile for positively charged atomic nuclei. Nucleophilicity, sometimes referred to as  nucleophile strength, refers to a substance's nucleophilic character and is often used to compare the affinity of atoms. Neutral nucleophilic reactions with solvents such as alcohols and water are named solvolysis. Nucleophiles may take part in nucleophilic substitution, whereby a nucleophile becomes attracted to a full or partial positive charge.
"
Atomic_nucleus,Chemistry,1,"
 The atomic nucleus is the small, dense region consisting of protons and neutrons at the center of an atom, discovered in 1911 by Ernest Rutherford based on the 1909 Geiger–Marsden gold foil experiment.  After the discovery of the neutron in 1932, models for a nucleus composed of protons and neutrons were quickly developed by Dmitri Ivanenko[1] and Werner Heisenberg.[2][3][4][5][6]  An atom is composed of a positively-charged nucleus, with a cloud of negatively-charged electrons surrounding it, bound together by electrostatic force.  Almost all of the mass of an atom is located in the nucleus, with a very small contribution from the electron cloud.  Protons and neutrons are bound together to form a nucleus by the nuclear force.
 The diameter of the nucleus is in the range of 1.7566 fm (1.7566×10−15 m) for hydrogen (the diameter of a single proton) to about 11.7142 fm for uranium.[7] These dimensions are much smaller than the diameter of the atom itself (nucleus + electron cloud), by a factor of about 26,634 (uranium atomic radius is about 156 pm (156×10−12 m))[8] to about 60,250 (hydrogen atomic radius is about 52.92 pm).[a] The branch of physics concerned with the study and understanding of the atomic nucleus, including its composition and the forces which bind it together, is called nuclear physics.
"
Noble_gas,Chemistry,1,"The noble gases (historically also the inert gases; sometimes referred to as aerogens[1]) make up a  class of chemical elements with similar properties; under standard conditions, they are all odorless, colorless, monatomic gases with very low chemical reactivity. The six naturally occurring noble gases are helium (He), neon (Ne), argon (Ar), krypton (Kr), xenon (Xe), and the radioactive radon (Rn). Oganesson (Og) is variously predicted to be a noble gas as well or to break the trend due to relativistic effects; its chemistry has not yet been investigated.
 Legend
 For the first six periods of the periodic table, the noble gases are exactly the members of group 18. Noble gases are typically highly unreactive except when under particular extreme conditions. The inertness of noble gases makes them very suitable in applications where reactions are not wanted. For example, argon is used in incandescent lamps to prevent the hot tungsten filament from oxidizing; also, helium is used in breathing gas by deep-sea divers to prevent oxygen, nitrogen and carbon dioxide (hypercapnia) toxicity.
 The properties of the noble gases can be well explained by modern theories of atomic structure: their outer shell of valence electrons is considered to be ""full"", giving them little tendency to participate in chemical reactions, and it has been possible to prepare only a few hundred noble gas compounds. The melting and boiling points for a given noble gas are close together, differing by less than 10 °C (18 °F); that is, they are liquids over only a small temperature range.
 Neon, argon, krypton, and xenon are obtained from air in an air separation unit using the methods of liquefaction of gases and fractional distillation. Helium is sourced from natural gas fields that have high concentrations of helium in the natural gas, using cryogenic gas separation techniques, and radon is usually isolated from the radioactive decay of dissolved radium, thorium, or uranium compounds. Noble gases have several important applications in industries such as lighting, welding, and space exploration. A helium-oxygen breathing gas is often used by deep-sea divers at depths of seawater over 55 m (180 ft). After the risks caused by the flammability of hydrogen became apparent in the Hindenburg disaster, it was replaced with helium in blimps and balloons.
"
Non-metal,Chemistry,1,"
 In chemistry, a nonmetal (or non-metal) is a chemical element that mostly lacks the characteristics of a metal. Physically, a nonmetal tends to have a relatively low melting point, boiling point, and density.  A nonmetal is typically brittle when solid and usually has poor thermal conductivity and electrical conductivity. Chemically, nonmetals tend to have relatively high ionization energy, electron affinity, and electronegativity. They gain or share electrons when they react with other elements and chemical compounds. Seventeen elements are generally classified as nonmetals: most are gases (hydrogen, helium, nitrogen, oxygen, fluorine, neon, chlorine, argon, krypton, xenon and radon); one is a liquid (bromine); and a few are solids (carbon, phosphorus, sulfur, selenium, and iodine). Metalloids such as boron, silicon, and germanium are sometimes counted as nonmetals.
 The nonmetals are divided into two categories reflecting their relative propensity to form chemical compounds: reactive nonmetals and noble gases. The reactive nonmetals vary in their nonmetallic character. The less electronegative of them, such as carbon and sulfur, mostly have weak to moderately strong nonmetallic properties and tend to form covalent compounds with metals. The more electronegative of the reactive nonmetals, such as oxygen and fluorine, are characterised by stronger nonmetallic properties and a tendency to form predominantly ionic compounds with metals. The noble gases are distinguished by their great reluctance to form compounds with other elements.
 The distinction between categories is not absolute. Boundary overlaps, including with the metalloids, occur as outlying elements in each category show or begin to show less-distinct, hybrid-like, or atypical properties.
 Although five times more elements are metals than nonmetals, two of the nonmetals—hydrogen and helium—make up over 99 percent of the observable universe.[1] Another nonmetal, oxygen, makes up almost half of the Earth's crust, oceans, and atmosphere.[2] Living organisms are composed almost entirely of nonmetals: hydrogen, oxygen, carbon, and nitrogen.[3] Nonmetals form many more compounds than metals.[4]"
Normality_(chemistry),Chemistry,1,"Normality is a measure of concentration equal to the gram equivalent weight per litre of solution. Gram equivalent weight is the measure of the reactive capacity of a molecule. The solute's role in the reaction determines the solution's normality. Normality is also known as the equivalent concentration of a solution. In chemistry, the equivalent concentration or normality of a solution is defined as the molar concentration ci divided by an equivalence factor feq:
"
Atomic_nucleus,Chemistry,1,"
 The atomic nucleus is the small, dense region consisting of protons and neutrons at the center of an atom, discovered in 1911 by Ernest Rutherford based on the 1909 Geiger–Marsden gold foil experiment.  After the discovery of the neutron in 1932, models for a nucleus composed of protons and neutrons were quickly developed by Dmitri Ivanenko[1] and Werner Heisenberg.[2][3][4][5][6]  An atom is composed of a positively-charged nucleus, with a cloud of negatively-charged electrons surrounding it, bound together by electrostatic force.  Almost all of the mass of an atom is located in the nucleus, with a very small contribution from the electron cloud.  Protons and neutrons are bound together to form a nucleus by the nuclear force.
 The diameter of the nucleus is in the range of 1.7566 fm (1.7566×10−15 m) for hydrogen (the diameter of a single proton) to about 11.7142 fm for uranium.[7] These dimensions are much smaller than the diameter of the atom itself (nucleus + electron cloud), by a factor of about 26,634 (uranium atomic radius is about 156 pm (156×10−12 m))[8] to about 60,250 (hydrogen atomic radius is about 52.92 pm).[a] The branch of physics concerned with the study and understanding of the atomic nucleus, including its composition and the forces which bind it together, is called nuclear physics.
"
Nuclear_chemistry,Chemistry,1,"Nuclear chemistry is the sub-field of chemistry dealing with radioactivity, nuclear processes, and transformations in the nuclei of atoms, such as nuclear transmutation and nuclear properties.
 It is the chemistry of radioactive elements such as the actinides, radium and radon together with the chemistry associated with equipment (such as nuclear reactors) which are designed to perform nuclear processes. This includes the corrosion of surfaces and the behavior under conditions of both normal and abnormal operation (such as during an accident). An important area is the behavior of objects and materials after being placed into a nuclear waste storage or disposal site.
 It includes the study of the chemical effects resulting from the absorption of radiation within living animals, plants, and other materials. The radiation chemistry controls much of radiation biology as radiation has an effect on living things at the molecular scale, to explain it another way the radiation alters the biochemicals within an organism, the alteration of the bio-molecules then changes the chemistry which occurs within the organism, this change in chemistry then can lead to a biological outcome. As a result, nuclear chemistry greatly assists the understanding of medical treatments (such as cancer radiotherapy) and has enabled these treatments to improve.
 It includes the study of the production and use of radioactive sources for a range of processes. These include radiotherapy in medical applications; the use of radioactive tracers within industry, science and the environment; and the use of radiation to modify materials such as polymers.[1] It also includes the study and use of nuclear processes in non-radioactive areas of human activity. For instance, nuclear magnetic resonance (NMR) spectroscopy is commonly used in synthetic organic chemistry and physical chemistry and for structural analysis in macro-molecular chemistry.
 Nuclear  chemistry  concerned with the study of nucleus, changes occurring in the nucleus, properties of the particles present in the nucleus and the emission or absorption  of radiation from the nucleus
"
Nuclear_magnetic_resonance_spectroscopy,Chemistry,1,"Nuclear magnetic resonance spectroscopy, most commonly known as NMR spectroscopy or magnetic resonance spectroscopy (MRS), is a spectroscopic technique to observe local magnetic fields around atomic nuclei. The sample is placed in a magnetic field and the NMR signal is produced by excitation of the nuclei sample with radio waves into nuclear magnetic resonance, which is detected with sensitive radio receivers. The intramolecular magnetic field around an atom in a molecule changes the resonance frequency, thus giving access to details of the electronic structure of a molecule and its individual functional groups. As the fields are unique or highly characteristic to individual compounds, in modern organic chemistry practice, NMR spectroscopy is the definitive method to identify monomolecular organic compounds. Similarly, biochemists use NMR to identify proteins and other complex molecules. Besides identification, NMR spectroscopy provides detailed information about the structure, dynamics, reaction state, and chemical environment of molecules. The most common types of NMR are proton and carbon-13 NMR spectroscopy, but it is applicable to any kind of sample that contains nuclei possessing spin.
 NMR spectra are unique, well-resolved, analytically tractable and often highly predictable for small molecules. Different functional groups are obviously distinguishable, and identical functional groups with differing neighboring substituents still give distinguishable signals. NMR has largely replaced traditional wet chemistry tests such as color reagents or typical chromatography for identification. A disadvantage is that a relatively large amount, 2–50 mg, of a purified substance is required, although it may be recovered through a workup. Preferably, the sample should be dissolved in a solvent, because NMR analysis of solids requires a dedicated magic angle spinning machine and may not give equally well-resolved spectra. The timescale of NMR is relatively long, and thus it is not suitable for observing fast phenomena, producing only an averaged spectrum. Although large amounts of impurities do show on an NMR spectrum, better methods exist for detecting impurities, as NMR is inherently not very sensitive - though at higher frequencies, sensitivity is higher.
 Correlation spectroscopy is a development of ordinary NMR. In two-dimensional NMR, the emission is centered around a single frequency, and correlated resonances are observed. This allows identifying the neighboring substituents of the observed functional group, allowing unambiguous identification of the resonances. There are also more complex 3D and 4D methods and a variety of methods designed to suppress or amplify particular types of resonances. In nuclear Overhauser effect (NOE) spectroscopy, the relaxation of the resonances is observed. As NOE depends on the proximity of the nuclei, quantifying the NOE for each nucleus allows for construction of a three-dimensional model of the molecule.
 NMR spectrometers are relatively expensive; universities usually have them, but they are less common in private companies. Between 2000 and 2015, an NMR spectrometer cost around 500,000 - 5 million USD.[2][3] Modern NMR spectrometers have a very strong, large and expensive liquid helium-cooled superconducting magnet, because resolution directly depends on magnetic field strength. Less expensive machines using permanent magnets and lower resolution are also available, which still give sufficient performance for certain applications such as reaction monitoring and quick checking of samples. There are even benchtop nuclear magnetic resonance spectrometers. NMR can be observed in magnetic fields less than a millitesla. Low-resolution NMR produces broader peaks which can easily overlap one another causing issues in resolving complex structures. The use of higher strength magnetic fields result in clear resolution of the peaks and is the standard in industry.[4]"
Nuclear_transmutation,Chemistry,1,"Nuclear transmutation is the conversion of one chemical element or an isotope into another chemical element.[1] Because any element (or isotope of one) is defined by its number of protons (and neutrons) in its atoms, i.e. in the atomic nucleus, nuclear transmutation occurs in any process where the number of protons or neutrons in the nucleus is changed.
 A transmutation can be achieved either by nuclear reactions (in which an outside particle reacts with a nucleus) or by radioactive decay, where no outside cause is needed.
 Natural transmutation by stellar nucleosynthesis in the past created most of the heavier chemical elements in the known existing universe, and continues to take place to this day, creating the vast majority of the most common elements in the universe, including helium, oxygen and carbon. Most stars carry out transmutation through fusion reactions involving hydrogen and helium, while much larger stars are also capable of fusing heavier elements up to iron late in their evolution.
 Elements heavier than iron, such as gold or lead, are created through elemental transmutations that can only naturally occur in supernovae.  As stars begin to fuse heavier elements, substantially less energy is released from each fusion reaction. Reactions that produce elements heavier than iron are endothermic  and unable to generate the energy required to maintain stable fusion inside the star.
 One type of natural transmutation observable in the present occurs when certain radioactive elements present in nature spontaneously decay by a process that causes transmutation, such as alpha or beta decay. An example is the natural decay of potassium-40 to argon-40, which forms most of the argon in the air. Also on Earth, natural transmutations from the different mechanisms of natural nuclear reactions occur, due to cosmic ray bombardment of elements (for example, to form carbon-14), and also occasionally from natural neutron bombardment (for example, see natural nuclear fission reactor).
 Artificial transmutation may occur in machinery that has enough energy to cause changes in the nuclear structure of the elements. Such machines include particle accelerators and tokamak reactors. Conventional fission power reactors also cause artificial transmutation, not from the power of the machine, but by exposing elements to neutrons produced by fission from an artificially produced nuclear chain reaction. For instance, when a uranium atom is bombarded with slow neutrons, fission takes place. This releases, on average, 3 neutrons and a large amount of energy. The released neutrons then cause fission of other uranium atoms, until all of the available uranium is exhausted. This is called a chain reaction.
 Artificial nuclear transmutation has been considered as a possible mechanism for reducing the volume and hazard of radioactive waste.[2]"
Nuclide,Chemistry,1,"A nuclide (or nucleide, from nucleus, also known as nuclear species) is an atom characterized by its number of protons, Z, its number of neutrons, N, and its nuclear energy state.[1] The word nuclide was coined by Truman P. Kohman in 1947.[2][3] Kohman defined nuclide as a ""species of atom characterized by the constitution of its nucleus"" containing a certain number of neutrons and protons. The term thus originally focused on the nucleus.
"
Number_density,Chemistry,1,"The number density (symbol: n or ρN) is an intensive quantity used to describe the degree of concentration of countable objects (particles, molecules, phonons, cells, galaxies, etc.) in physical space: three-dimensional volumetric number density, two-dimensional areal number density, or one-dimensional linear number density. Population density is an example of areal number density. The term number concentration (symbol: lowercase n, or C, to avoid confusion with amount of substance indicated by uppercase N) is sometimes used in chemistry for the same quantity, particularly when comparing with other concentrations.
"
Octet_rule,Chemistry,1,"The octet rule is a chemical rule of thumb that reflects the observation that main group elements tend to bond in such a way that each atom has eight electrons in its valence shell, giving it the same electronic configuration as a noble gas. The rule is especially applicable to carbon, nitrogen, oxygen, and the halogens, but also to metals such as sodium or magnesium.
 The valence electrons can be counted using a Lewis electron dot diagram as shown at the right for carbon dioxide. The electrons shared by the two atoms in a covalent bond are counted twice, once for each atom. In carbon dioxide each oxygen shares four electrons with the central carbon, two (shown in red) from the oxygen itself and two (shown in black) from the carbon. All four of these electrons are counted in both the carbon octet and the oxygen octet, so that both atoms are considered to obey the octet rule.
"
Olefin,Chemistry,1,"
 In chemistry, an alkene is a hydrocarbon that contains a carbon–carbon double bond.[1] The term is often used as synonym of olefin, that is, any hydrocarbon containing one or more double bonds.[2] However, the IUPAC recommends using the name ""alkene"" only for acyclic hydrocarbons with just one double bond; alkadiene, alkatriene, etc., or polyene for acyclic hydrocarbons with two or more double bonds; cycloalkene, cycloalkadiene, etc. for cyclic ones; and ""olefin"" for the general class — cyclic or acyclic, with one or more double bonds.[3][4][5] Acyclic alkenes, with only one double bond and no other functional groups (also known as mono-enes) form a homologous series of hydrocarbons with the general formula CnH2n with n being 2 or more (which is two hydrogens less than the corresponding alkane). When n is four or more, there are multiple isomers with this formula, distinguished by the position and conformation of the double bond.
 Alkenes are generally colorless apolar compounds, somewhat similar to alkanes but more reactive. The first few members of the series are gases or liquids at room temperature.  The simplest alkene, ethylene (C2H4) (or ""ethene"" in the IUPAC nomenclature) is the organic compound produced on the largest scale industrially.[6] Aromatic compounds are often drawn as cyclic alkenes, but their structure and properties are sufficiently distinct that they are not classified as alkenes or olefins.[4]  Hydrocarbons with two overlapping double bonds (C=C=C) are called allenes after the simplest such compound, and those with three or more overlapping bonds (C=C=C=C, C=C=C=C=C, etc.) are called cumulenes.  Some authors do not consider allenes and cumulenes to be ""alkenes"".
"
Optical_activity,Chemistry,1,"Optical rotation, also known as polarization rotation or circular birefringence, is the rotation of the orientation of the plane of polarization about the optical axis of linearly polarized light as it travels through certain materials. Circular birefringence and circular dichroism are the manifestations of optical activity. Optical activity occurs only in chiral materials, those lacking microscopic mirror symmetry. Unlike other sources of birefringence which alter a beam's state of polarization, optical activity can be observed in fluids. This can include gases or solutions of chiral molecules such as sugars, molecules with helical secondary structure such as some proteins, and also chiral liquid crystals.  It can also be observed in chiral solids such as certain crystals with a rotation between adjacent crystal planes (such as quartz) or metamaterials. 
 The rotation of the plane of polarization may be either clockwise, to the right (dextrorotary — d-rotary,represented by (+), or to the left (levorotary — l-rotary ,represented by (-) depending on which stereoisomer is present (or dominant). For instance, sucrose and camphor are d-rotary whereas cholesterol is l-rotary. For a given substance, the angle by which the polarization of light of a specified wavelength is rotated is proportional to the path length through the material and (for a solution) proportional to its concentration. 
 Optical activity is measured using a polarized source and polarimeter. This is a tool particularly used in the sugar industry to measure the sugar concentration of syrup, and generally in chemistry to measure the concentration or enantiomeric ratio of chiral molecules in solution. Modulation of a liquid crystal's optical activity, viewed between two sheet polarizers, is the principle of operation of liquid-crystal displays (used in most modern televisions and computer monitors).
"
Orbital_hybridisation,Chemistry,1,"
 In chemistry, orbital hybridisation (or hybridization) is the concept of mixing atomic orbitals into new hybrid orbitals (with different energies, shapes, etc., than the component atomic orbitals) suitable for the pairing of electrons to form chemical bonds in valence bond theory. Hybrid orbitals are useful in the explanation of molecular geometry and atomic bonding properties and are symmetrically disposed in space.
"
Order_of_reaction,Chemistry,1,"The rate law or rate equation for a chemical reaction is an equation that links the initial or forward reaction rate with the concentrations or pressures of the reactants and constant parameters (normally rate coefficients and partial reaction orders).[1] For many reactions, the initial rate is given by a power law such as
 where [A] and [B] express the concentration of the species A and B, usually in moles per liter (molarity, M). The exponents x and y are the partial orders of reaction for A and B and the overall reaction order is the sum of the exponents. These are often positive integers, but they may also be zero, fractional, or negative. The constant k is the reaction rate constant or rate coefficient of the reaction. Its value may depend on conditions such as temperature, ionic strength, surface area of an adsorbent, or light irradiation. If the reaction goes to completion, the rate equation for the reaction rate 



v

=

k
[

A


]

x


[

B


]

y




{  v\;=\;k[\mathrm {A} ]^{x}[\mathrm {B} ]^{y}}
 applies throughout the course of the reaction.
 Elementary (single-step) reactions and reaction steps have reaction orders equal to the stoichiometric coefficients for each reactant. The overall reaction order, i.e. the sum of stoichiometric coefficients of reactants, is always equal to the molecularity of the elementary reaction. However, complex (multi-step) reactions may or may not have reaction orders equal to their stoichiometric coefficients. This implies that the order and the rate equation of a given reaction cannot be reliably deduced from the stoichiometry and must be determined experimentally, since an unknown reaction mechanism could be either elementary or complex. When the experimental rate equation has been determined, it is often of use for deduction of the reaction mechanism.
 The rate equation of a reaction with an assumed multi-step mechanism can often be derived theoretically using quasi-steady state assumptions from the underlying elementary reactions, and compared with the experimental rate equation as a test of the assumed mechanism. The equation may involve a fractional order, and may depend on the concentration of an intermediate species.
 A reaction can also have an undefined reaction order with respect to a reactant if the rate is not simply proportional to some power of the concentration of that reactant; for example, one cannot talk about reaction order in the rate equation for a bimolecular reaction between adsorbed molecules:
"
Organic_acid,Chemistry,1,"An organic acid is an organic compound with acidic properties. The most common organic acids are the carboxylic acids, whose acidity is associated with their carboxyl group –COOH. Sulfonic acids, containing the group –SO2OH, are relatively stronger acids. Alcohols, with –OH, can act as acids but they are usually very weak. The relative stability of the conjugate base of the acid determines its acidity. Other groups can [also] confer acidity, usually weakly: the thiol group –SH, the enol group, and the phenol group. In biological systems, organic compounds containing these groups are generally referred to as organic acids.
 A few common examples include:
"
Organic_base,Chemistry,1,"An organic base is an organic compound which acts as a base. Organic bases are usually, but not always, proton acceptors. They usually contain nitrogen atoms, which can easily be protonated. For example, amines or nitrogen-containing heterocyclic compounds have a lone pair of electrons on the nitrogen atom and can thus act as proton acceptors.[1] Examples include:
"
Organic_compound,Chemistry,1,"In chemistry,  organic compounds are generally any chemical compounds that contain carbon-hydrogen bonds.  Due to carbon's ability to catenate (form chains with other carbon atoms), millions of organic compounds are known. The study of the properties, reactions, and syntheses of organic compounds comprises the discipline known as organic chemistry.  For historical reasons, a few classes of carbon-containing compounds (e.g., carbonate anion salts and cyanide salts), along with a handful of other exceptions (e.g., carbon dioxide), are not classified as organic compounds and are considered inorganic.  Other than those just named, little consensus exists among chemists on precisely which carbon-containing compounds are excluded, making any rigorous definition of an organic compound elusive.[1] Although organic compounds make up only a small percentage of the Earth's crust, they are of central importance because all known life is based on organic compounds. Living things incorporate inorganic carbon compounds into organic compounds through a network of processes (the carbon cycle) that begins with the conversion of carbon dioxide and a hydrogen source like water into simple sugars and other organic molecules by autotrophic organisms using light (photosynthesis) or other sources of energy.  Most synthetically produced organic compounds are ultimately derived from petrochemicals consisting mainly of hydrocarbons, which are themselves formed from the high pressure and temperature degradation of organic matter underground over geological timescales.[2]  This ultimate derivation notwithstanding, organic compounds are no longer defined as compounds originating in living things, as they were historically.
 In chemical nomenclature, an organyl group, frequently represented by the letter R, refers to any monovalent substituent whose open valence is on a carbon atom.[3]"
Organic_chemistry,Chemistry,1,"Organic chemistry is a branch of chemistry that studies the structure, properties and reactions of organic compounds, which contain carbon in covalent bonding.[1] Study of structure determines their chemical composition and formula. Study of properties includes physical and chemical properties, and evaluation of chemical reactivity to understand their behavior. The study of organic reactions includes the chemical synthesis of natural products, drugs, and polymers, and study of individual organic molecules in the laboratory and via theoretical (in silico) study.
 The range of chemicals studied in organic chemistry includes hydrocarbons (compounds containing only carbon and hydrogen) as well as compounds based on carbon, but also containing other elements,[1][2][3] especially oxygen, nitrogen, sulfur, phosphorus (included in many biochemicals) and the halogens. Organometallic chemistry is the study of compounds containing carbon–metal bonds.
 In addition, contemporary research focuses on organic chemistry involving other organometallics including the lanthanides, but especially the transition metals zinc, copper, palladium, nickel, cobalt, titanium and chromium.
 Organic compounds form the basis of all earthly life and constitute the majority of known chemicals.  The bonding patterns of carbon, with its valence of four—formal single, double, and triple bonds, plus structures with delocalized electrons—make the array of organic compounds structurally diverse, and their range of applications enormous. They form the basis of, or are constituents of, many commercial products including pharmaceuticals; petrochemicals and agrichemicals, and products made from them including lubricants, solvents; plastics; fuels and explosives. The study of organic chemistry overlaps organometallic chemistry and  biochemistry, but also with medicinal chemistry, polymer chemistry, and materials science.[1]"
Organic_redox_reaction,Chemistry,1,"Organic reductions or organic oxidations or organic redox reactions are redox reactions that take place with organic compounds. In organic chemistry oxidations and reductions are different from ordinary redox reactions because many reactions carry the name but do not actually involve electron transfer in the electrochemical sense of the word.[1] Instead the relevant criterion for organic oxidation is gain of oxygen and/or loss of hydrogen, respectively.[2] Simple functional groups can be arranged in order of increasing oxidation state. The oxidation numbers are only an approximation:[1] When methane is oxidized to carbon dioxide its oxidation number changes from −4 to +4. Classical reductions include alkene reduction to alkanes and classical oxidations include oxidation of alcohols to aldehydes. In oxidations electrons are removed and the electron density of a molecule is reduced. In reductions electron density increases when electrons are added to the molecule. This terminology is always centered on the organic compound.  For example, it is usual to refer to the reduction of a ketone by lithium aluminium hydride, but not to the oxidation of lithium aluminium hydride by a ketone. Many oxidations involve removal of hydrogen atoms from the organic molecule, and the reverse, reduction adds hydrogens to an organic molecule.
 Many reactions classified as reductions also appear in other classes. For instance conversion of the ketone to an alcohol by lithium aluminium hydride can be considered a reduction but the hydride is also a good nucleophile in nucleophilic substitution. Many redox reactions in organic chemistry have coupling reaction reaction mechanism involving free radical intermediates. True organic redox chemistry can be found in electrochemical organic synthesis or electrosynthesis. Examples of organic reactions that can take place in an electrochemical cell are the Kolbe electrolysis.[3] In disproportionation reactions the reactant is both oxidised and reduced in the same chemical reaction forming two separate compounds.
 Asymmetric catalytic reductions and asymmetric catalytic oxidations are important in asymmetric synthesis.
"
Osmotic_pressure,Chemistry,1,"Osmotic pressure is the minimum pressure which needs to be applied to a solution to prevent the inward flow of its pure solvent across a semipermeable membrane.[1]
It is also defined as the measure of the tendency of a solution to take in pure solvent by osmosis. Potential osmotic pressure is the maximum osmotic pressure that could develop in a solution if it were separated from its pure solvent by a semipermeable membrane. 
 Osmosis occurs when two solutions containing different concentrations of solute are separated by a selectively permeable membrane. Solvent molecules pass preferentially through the membrane from the low-concentration solution to the solution with higher solute concentration. The transfer of solvent molecules will continue until equilibrium is attained.[1][2]"
Oxidation,Chemistry,1,"Redox (reduction–oxidation, pronunciation: /ˈrɛdɒks/ redoks or /ˈriːdɒks/ reedoks[1]) is a type of chemical reaction in which the oxidation states of atoms are changed. Redox reactions are characterized by the actual or formal transfer of electrons between chemical species, most often with one species (the reducing agent) undergoing oxidation (losing electrons) while another species (the oxidizing agent) undergoes reduction (gains electrons).[2] The chemical species from which the electron is removed is said to have been oxidized, while the chemical species to which the electron is added is said to have been reduced. In other words:
 Many reactions in organic chemistry are redox reactions due to changes in oxidation states but without distinct electron transfer. For example, during the combustion of wood with molecular oxygen, the oxidation state of carbon atoms in the wood increases and that of oxygen atoms decreases as carbon dioxide and water are formed. The oxygen atoms undergo reduction, formally gaining electrons, while the carbon atoms undergo oxidation, losing electrons. Thus oxygen is the oxidizing agent and carbon is the reducing agent in this reaction.[3] Although oxidation reactions are commonly associated with the formation of oxides from oxygen molecules, oxygen is not necessarily included in such reactions, as other chemical species can serve the same function.[3] Redox reactions can occur relatively slowly, as in the formation of rust, or much more rapidly, as in the case of burning fuel. There are simple redox processes, such as the oxidation of carbon to yield carbon dioxide (CO2) or the reduction of carbon by hydrogen to yield methane (CH4), and more complex processes such as the oxidation of glucose (C6H12O6) in the human body. Analysis of bond energies and ionization energies in water allow calculation of the redox potentials.[4][5]"
Oxidation_state,Chemistry,1,"The oxidation state, sometimes referred to as oxidation number, describes the degree of oxidation (loss of electrons) of an atom in a chemical compound. Conceptually, the oxidation state, which may be positive, negative or zero,  is the hypothetical charge that an atom would have if all bonds to atoms of different elements were 100% ionic, with no covalent component. This is never exactly true for real bonds.
 The term oxidation was first used by Antoine Lavoisier to signify reaction of a substance with oxygen. Much later, it was realized that the substance, upon being oxidized, loses electrons, and the meaning was extended to include other reactions in which electrons are lost, regardless of whether oxygen was involved.
 Oxidation states are typically represented by integers which may be positive, zero, or negative. In some cases, the average oxidation state of an element is a fraction, such as 8/3 for iron in magnetite Fe3O4 (see below) . The highest known oxidation state is reported to be +9 in the tetroxoiridium(IX) cation (IrO+4).[1] It is predicted that even a +10 oxidation state may be achievable by platinum in the tetroxoplatinum(X) cation (PtO2+4).[2] The lowest oxidation state is −5, as for boron in Al3BC.[3] The increase in oxidation state of an atom, through a chemical reaction, is known as an oxidation; a decrease in oxidation state is known as a reduction. Such reactions involve the formal transfer of electrons: a net gain in electrons being a reduction, and a net loss of electrons being an oxidation. For pure elements, the oxidation state is zero.
 The oxidation state of an atom does not represent the ""real"" charge on that atom, or any other actual atomic property. This is particularly true of high oxidation states, where the ionization energy required to produce a multiply positive ion is far greater than the energies available in chemical reactions. Additionally, oxidation states of atoms in a given compound may vary depending on the choice of electronegativity scale used in their calculation. Thus, the oxidation state of an atom in a compound is purely a formalism. It is nevertheless important in understanding the nomenclature conventions of inorganic compounds. Also, a number of observations pertaining to chemical reactions may be explained at a basic level in terms of oxidation states.
 In inorganic nomenclature, the oxidation state is represented by a Roman numeral placed after the element name inside a parenthesis or as a superscript after the element symbol.
"
Oxidizing_agent,Chemistry,1,"In chemistry, an oxidizing agent (oxidant, oxidizer), or oxidising agent (oxidiser) is a substance that has the ability to oxidize other substances — in other words to accept their electrons. Common oxidizing agents are oxygen, hydrogen peroxide and the halogens.
 In one sense, an oxidizing agent is a chemical species that undergoes a chemical reaction in which it gains one or more electrons. In that sense, it is one component in an oxidation–reduction (redox) reaction. In the second sense, an oxidizing agent is a chemical species that transfers electronegative atoms, usually oxygen, to a substrate. Combustion, many explosives, and organic redox reactions involve atom-transfer reactions.
"
Oxoacid,Chemistry,1,"An oxyacid, oxoacid, or ternary acid is an acid that contains oxygen. Specifically, it is a compound that contains hydrogen, oxygen, and at least one other element, with at least one hydrogen atom bond to oxygen that can dissociate to produce the H+ cation and the anion of the acid.[1]"
Oxygen,Chemistry,1,"
 
 Oxygen is the chemical element with the symbol O and atomic number 8. It is a member of the chalcogen group in the periodic table, a highly reactive nonmetal, and an oxidizing agent that readily forms oxides with most elements as well as with other compounds. After hydrogen and helium, oxygen is the third-most abundant element in the universe by mass. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula O2. Diatomic oxygen gas constitutes 20.95% of the Earth's atmosphere. Oxygen makes up almost half of the Earth's crust in the form of oxides.[2] Dioxygen provides the energy released in combustion[3] and aerobic cellular respiration,[4] and many major classes of organic molecules in living organisms contain oxygen atoms, such as proteins, nucleic acids, carbohydrates, and fats, as do the major constituent inorganic compounds of animal shells, teeth, and bone. Most of the mass of living organisms is oxygen as a component of water, the major constituent of lifeforms. Oxygen is continuously replenished in Earth's atmosphere by photosynthesis, which uses the energy of sunlight to produce oxygen from water and carbon dioxide. Oxygen is too chemically reactive to remain a free element in air without being continuously replenished by the photosynthetic action of living organisms. Another form (allotrope) of oxygen, ozone (O3), strongly absorbs ultraviolet UVB radiation and the high-altitude ozone layer helps protect the biosphere from ultraviolet radiation. However, ozone present at the surface is a byproduct of smog and thus a pollutant.
 Oxygen was isolated by Michael Sendivogius before 1604, but it is commonly believed that the element was discovered independently by Carl Wilhelm Scheele, in Uppsala, in 1773 or earlier, and Joseph Priestley in Wiltshire, in 1774. Priority is often given for Priestley because his work was published first. Priestley, however, called oxygen ""dephlogisticated air"", and did not recognize it as a chemical element. The name oxygen was coined in 1777 by Antoine Lavoisier, who first recognized oxygen as a chemical element and correctly characterized the role it plays in combustion.
 Common uses of oxygen include production of steel, plastics and textiles, brazing, welding and cutting of steels and other metals, rocket propellant, oxygen therapy, and life support systems in aircraft, submarines, spaceflight and diving.
"
P-block,Chemistry,1,"A block of the periodic table is a set of elements unified by the orbitals their valence electrons or vacancies lie in.[1] The term appears to have been first used by Charles Janet.[2] Each block is named after its characteristic orbital: s-block, p-block, d-block, and f-block.
 The block names (s, p, d, and f) are derived from the spectroscopic notation for the value of an electron's azimuthal quantum number: shape (0), principal (1), diffuse (2), or fundamental (3). Succeeding notations proceed in alphabetical order, as g, h, etc.
"
Paraffin_(disambiguation),Chemistry,1,"Paraffin may refer to:
"
Partial_pressure,Chemistry,1,"In a mixture of gases, each constituent gas has a partial pressure which is the notional pressure of that constituent gas if it alone occupied the entire volume of the original mixture at the same temperature.[1] The total pressure of an ideal gas mixture is the sum of the partial pressures of the gases in the mixture (Dalton's Law).
 The partial pressure of a gas is a measure of thermodynamic activity of the gas's molecules.  Gases dissolve, diffuse, and react according to their partial pressures, and not according to their concentrations in gas mixtures or liquids. This general property of gases is also true in chemical reactions of gases in biology. For example, the necessary amount of oxygen for human respiration, and the amount that is toxic, is set by the partial pressure of oxygen alone. This is true across a very wide range of different concentrations of oxygen present in various inhaled breathing gases or dissolved in blood.[clarification needed][2] The partial pressures of oxygen and carbon dioxide are important parameters in tests of arterial blood gases, but can also be measured in, for example, cerebrospinal fluid.
"
Pascal_(unit),Chemistry,1,"
 The pascal (symbol: Pa) is the SI derived unit of pressure used to quantify internal pressure, stress, Young's modulus and ultimate tensile strength. The unit, named after Blaise Pascal, is defined as one newton per square metre[1] and is equivalent to 10 barye (Ba) in the CGS system. The unit of measurement called standard atmosphere (atm) is defined as 101325 Pa.[2] Common multiple units of the pascal are the hectopascal (1 hPa = 100 Pa), which is equal to one millibar, and the kilopascal (1 kPa = 1000 Pa), which is equal to one centibar. Meteorological forecasts typically report atmospheric pressure in hectopascals per the recommendation of the World Meteorological Organization. Forecasts in the United States typically use millibars,[3][4] in Canada these reports are given in kilopascals.[5]"
Passivation_(chemistry),Chemistry,1,"
 Passivation, in physical chemistry and engineering, refers to a material becoming ""passive,"" that is, less affected or corroded by the environment of future use. Passivation involves creation of an outer layer of shield material that is applied as a microcoating, created by chemical reaction with the base material, or allowed to build from spontaneous oxidation in the air. As a technique, passivation is the use of a light coat of a protective material, such as metal oxide, to create a shell against corrosion.[1] Passivation can occur only in certain conditions, and is used in microelectronics to enhance silicon.[2] The technique of passivation strengthens and preserves the appearance of metallics. In electrochemical treatment of water, passivation reduces the effectiveness of the treatment by increasing the circuit resistance, and active measures are typically used to overcome this effect, the most common being polarity reversal, which results in limited rejection of the fouling layer. Other proprietary systems to avoid electrode passivation, several discussed below, are the subject of ongoing research and development. 
 When exposed to air, many metals naturally form a hard, relatively inert surface, as in the tarnish of silver. In the case of other metals, such as iron, a somewhat rough porous coating is formed from loosely adherent corrosion products. In this case, a substantial amount of metal is removed, which is either deposited or dissolved in the environment. Corrosion coating reduces the rate of corrosion by varying degrees, depending on the kind of base metal and its environment, and is notably slower in room-temperature air for aluminium, chromium, zinc, titanium, and silicon (a metalloid).  The shell of corrosion product inhibits deeper corrosion, and operates as one form of passivation. The inert surface layer, termed the ""native oxide layer"", is usually an oxide or a nitride, with a thickness of a monolayer of 0.1-0.3 nm (1-3 Å) for a noble metal such as platinum, about 1.5 nm (15 Å) for silicon, and nearer to 5 nm (50 Å) for aluminium after several years.[3][4][5] Surface passivation refers to a common semiconductor device fabrication process critical to modern electronics. It is the process by which a semiconductor surface is rendered inert, and does not change semiconductor properties as a result of interaction with air or other materials in contact with the surface or edge of the crystal. This is typically achieved using a form of thermal oxidation. In a silicon semiconductor, this process allows electricity to reliably penetrate to the conducting silicon below the surface, and to overcome the surface states that prevent electricity from reaching the semiconducting layer.[6][7] Surface passivation by thermal oxidation is one of the key features of silicon technology, and is dominant in microelectronics. The surface passivation process was developed by Mohamed M. Atalla at Bell Labs in the late 1950s.[6] It is commonly used to manufacture MOSFETs (metal-oxide-semiconductor field-effect transistors) and silicon integrated circuit chips (with the planar process), and is critical to the semiconductor industry.[6][7] Surface passivation is also critical to solar cell and carbon quantum dot technologies.
"
PH,Chemistry,1,"
 In chemistry, pH (/piːˈeɪtʃ/, denoting 'potential of hydrogen' or 'power of hydrogen'[1]) is a scale used to specify the acidity or basicity of an aqueous solution. Acidic solutions (solutions with higher concentrations of  H+ ions) are measured to have lower pH values than basic or alkaline solutions.
 The pH scale is logarithmic and inversely indicates the concentration of hydrogen ions in the solution. This is because the formula used to calculate pH approximates the negative of the base 10 logarithm of the molar concentration[a] of hydrogen ions in the solution. More precisely, pH is the negative of the base 10 logarithm of the activity of the H+ ion.[2] At 25 °C, solutions with a pH less than 7 are acidic, and solutions with a pH greater than 7 are basic. Solutions with a pH of 7 at this temperature are neutral (e.g. pure water).     The neutral value of the pH depends on the temperature, being lower than 7 if the temperature increases. The pH value can be less than 0 for very strong acids, or greater than 14 for very strong bases.[3] The pH scale is traceable to a set of standard solutions whose pH is established by international agreement.[4] Primary pH standard values are determined using a concentration cell with transference, by measuring the potential difference between a hydrogen electrode and a standard electrode such as the silver chloride electrode. The pH of aqueous solutions can be measured with a glass electrode and a pH meter, or a color-changing indicator. Measurements of pH are important in chemistry, agronomy, medicine, water treatment, and many other applications.
"
Phase_(matter),Chemistry,1,"In the physical sciences, a phase is a region of space (a thermodynamic system), throughout which all physical properties of a material are essentially uniform.[1][2]:86[3]:3 Examples of physical properties include density, index of refraction, magnetization and chemical composition. A simple description is that a phase is a region of material that is chemically uniform, physically distinct, and (often) mechanically separable. In a system consisting of ice and water in a glass jar, the ice cubes are one phase, the water is a second phase, and the humid air is a third phase over the ice and water. The glass of the jar is another separate phase. (See state of matter § Glass)
 The term phase is sometimes used as a synonym for state of matter, but there can be several immiscible phases of the same state of matter. Also, the term phase is sometimes used to refer to a set of equilibrium states demarcated in terms of state variables such as pressure and temperature by a phase boundary on a phase diagram. Because phase boundaries relate to changes in the organization of matter, such as a change from liquid to solid or a more subtle change from one crystal structure to another, this latter usage is similar to the use of ""phase"" as a synonym for state of matter. However, the state of matter and phase diagram usages are not commensurate with the formal definition given above and the intended meaning must be determined in part from the context in which the term is used.
"
Phase_transition,Chemistry,1,"
 In chemistry, thermodynamics, and many other related fields, phase transitions (or phase changes) are the physical processes of transition between the basic states of matter: solid, liquid, and gas, as well as plasma in rare cases. 
 A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change, often discontinuously, as a result of the change of external conditions, such as temperature, pressure, or others. For example, a liquid may become gas upon heating to the boiling point, resulting in an abrupt change in volume. The measurement of the external conditions at which the transformation occurs is termed the phase transition. Phase transitions commonly occur in nature and are used today in many technologies.
"
Phi_bond,Chemistry,1,"In chemistry, phi bonds (φ bonds) are covalent chemical bonds, where six lobes of one involved atomic orbital overlap six lobes of the other involved atomic orbital. This overlap leads to the formation of a bonding molecular orbital with three nodal planes which contain the internuclear axis and go through both atoms.
 The Greek letter φ in their name refers to f orbitals, since the orbital symmetry of the φ bond is the same as that of the usual (6-lobed) type of f orbital when seen down the bond axis.
 There was one possible candidate known in 2005  of a molecule with phi bonding (a U−U bond, in the molecule U2).[1] However, later studies that accounted for spin orbit interactions found that the bonding was only of fourth order.[2] [3][4] As of 2020, no molecules are known to certainly have phi bonds.
"
Physical_chemistry,Chemistry,1,"Physical chemistry is the study of macroscopic, and particulate phenomena in chemical systems in terms of the principles, practices, and concepts of physics such as motion, energy, force, time, thermodynamics, quantum chemistry, statistical mechanics, analytical dynamics and chemical equilibrium.
 Physical chemistry, in contrast to chemical physics, is predominantly (but not always) a macroscopic or supra-molecular science, as the majority of the principles on which it was founded relate to the bulk rather than the molecular/atomic structure alone (for example, chemical equilibrium and colloids).
 Some of the relationships that physical chemistry strives to resolve include the effects of:
"
Pi_bond,Chemistry,1,"Pi bonds (π bonds) are covalent chemical bonds where two lobes of an orbital on one atom overlap two lobes of an orbital on another atom and this overlap occurs laterally. Each of these atomic orbitals has zero electron density at a shared nodal plane, passing through the two bonded nuclei. The same plane is also a nodal plane for the molecular orbital of the pi bond. Pi Bonds can form in double and triple bonds but do not form in single bonds in most cases.
 The Greek letter π in their name refers to p orbitals, since the orbital symmetry of the pi bond is the same as that of the p orbital when seen down the bond axis. One common form of this sort of bonding involves p orbitals themselves, though d orbitals also engage in pi bonding. This latter mode forms part of the basis for metal-metal multiple bonding.
 Pi bonds are usually weaker than sigma bonds. The C-C double bond, composed of one sigma and one pi bond,[1] has a bond energy less than twice that of a C-C single bond, indicating that the stability added by the pi bond is less than the stability of a sigma bond. From the perspective of quantum mechanics, this bond's weakness is explained by significantly less overlap between the component p-orbitals due to their parallel orientation. This is contrasted by sigma bonds which form bonding orbitals directly between the nuclei of the bonding atoms, resulting in greater overlap and a strong sigma bond.
 Pi bonds result from overlap of atomic orbitals that are in contact through two areas of overlap. Pi bonds are more diffuse bonds than the sigma bonds. Electrons in pi bonds are sometimes referred to as pi electrons. Molecular fragments joined by a pi bond cannot rotate about that bond without breaking the pi bond, because rotation involves destroying the parallel orientation of the constituent p orbitals.
 For homonuclear diatomic molecules, bonding π molecular orbitals have only the one nodal plane passing through the bonded atoms, and no nodal planes between the bonded atoms. The corresponding antibonding, or π* (""pi-star"") molecular orbital, is defined by the presence of an additional nodal plane between these two bonded atoms.
"
Pipette,Chemistry,1,"A pipette (sometimes spelled pipet)  is a laboratory tool commonly used in chemistry, biology and medicine to transport a measured volume of liquid, often as a media dispenser. Pipettes come in several designs for various purposes with differing levels of accuracy and precision, from single piece glass pipettes to more complex adjustable or electronic pipettes. Many pipette types work by creating a partial vacuum above the liquid-holding chamber and selectively releasing this vacuum to draw up and dispense liquid. Measurement accuracy varies greatly depending on the instrument.
"
Plasma_(physics),Chemistry,1,"
 
 Plasma (from Ancient Greek  πλάσμα​ 'moldable substance'[1]) is one of the four fundamental states of matter, and was first described by chemist Irving Langmuir[2] in the 1920s.[3] It consists of a gas of ions – atoms which have some of their orbital electrons removed – and free electrons.  Plasma can be artificially generated by heating a neutral gas or subjecting it to a strong electromagnetic field to the point where an ionized gaseous substance becomes increasingly electrically conductive. The resulting charged ions and electrons become influenced by long-range electromagnetic fields, making the plasma dynamics more sensitive to these fields than a neutral gas.[4] Plasma and ionized gases have properties and behaviours unlike those of the other states, and the transition between them is mostly a matter of nomenclature[2] and subject to interpretation.[5] Based on the temperature and density of the environment that contains a plasma, partially ionized or fully ionized forms of plasma may be produced. Neon signs and lightning are examples of partially ionized plasmas.[6] The Earth's ionosphere is a plasma and the magnetosphere contains plasma in the Earth's surrounding space environment. The interior of the Sun is an example of fully ionized plasma,[7] along with the solar corona[8] and stars.[9] Positive charges in ions are achieved by stripping away electrons orbiting the atomic nuclei, where the total number of electrons removed is related to either increasing temperature or the local density of other ionized matter. This also can be accompanied by the dissociation of molecular bonds,[10] though this process is distinctly different from chemical processes of ion interactions in liquids or the behaviour of shared ions in metals. The response of plasma to electromagnetic fields is used in many modern technological devices, such as plasma televisions or plasma etching.[11] Plasma may be the most abundant form of ordinary matter in the universe,[12] although this hypothesis is currently tentative based on the existence and unknown properties of dark matter. Plasma is mostly associated with stars, extending to the rarefied intracluster medium and possibly the intergalactic regions.[13]"
Period_(periodic_table),Chemistry,1,"A period in the periodic table is a row of chemical elements. All elements in a row have the same number of electron shells. Each next element in a period has one more proton and is less metallic than its predecessor. Arranged this way, groups of elements in the same column have similar chemical and physical properties, reflecting the periodic law. For example, the halogens lie in the second-last column (group 17) and share similar properties, such as high reactivity and the tendency to gain one electron to arrive at a noble-gas electronic configuration. As of 2020[update], a total of 118 elements have been discovered and confirmed.
 Modern quantum mechanics explains these periodic trends in properties in terms of electron shells. As atomic number increases, shells fill with electrons in approximately the order shown in the ordering rule diagram. The filling of each shell corresponds to a row in the table.
 In the s-block and p-block of the periodic table, elements within the same period generally do not exhibit trends and similarities in properties (vertical trends down groups are more significant). However, in the d-block, trends across periods become significant, and in the f-block elements show a high degree of similarity across periods.
"
Periodic_table_of_the_elements,Chemistry,1,"
 The periodic table, also known as the periodic table of elements, arranges the chemical elements such as  hydrogen, silicon, iron, and uranium according to their recurring properties. The number of each element corresponds to the number of protons in its nucleus (which is the same as the number of electrons orbiting that nucleus). The modern periodic table provides a useful framework for analyzing chemical reactions, and is widely used in chemistry, physics and other sciences.
 
 The seven rows of the table, called periods, generally have metals on the left and nonmetals on the right. The columns, called groups, contain elements with similar chemical behaviours. Six groups have accepted names as well as assigned numbers: for example, group 17 elements are the halogens; and group 18 are the noble gases. Also displayed are four simple rectangular areas or blocks associated with the filling of different atomic orbitals. The organization of the periodic table can be used to derive relationships between the properties of the various elements, and to predict chemical properties and behaviours of undiscovered or newly synthesized elements.
 Russian chemist Dmitri Mendeleev published a periodic table in 1869, which he developed mainly to illustrate recurring trends among the properties of the then-known elements. He was the first to predict some properties of unidentified elements that were expected to fill gaps within the table. Most of his forecasts proved to be correct, culminating with the discovery of gallium and germanium in 1875 and 1886 respectively, which corroborated his predictions.[1] Mendeleev's idea has been slowly expanded and refined with the discovery or synthesis of further new elements and the development of new theoretical models to explain chemical behaviour.
 The table here shows a widely used layout. Other forms (discussed below) show different structures in detail. Some questions remain as to the placement and categorisation of specific elements, future extensions and limits of the table, and whether there is an optimal form of table.
"
Chemical_polarity,Chemistry,1," In chemistry, polarity is a separation of electric charge leading to a molecule or its chemical groups having an electric dipole moment, with a negatively charged end and a positively charged end.
 Polar molecules must contain polar bonds due to a difference in electronegativity between the bonded atoms. A polar molecule with two or more polar bonds must have a geometry which is asymmetric in at least one direction, so that the bond dipoles do not cancel each other.
 Polar molecules interact through dipole–dipole intermolecular forces and hydrogen bonds. Polarity underlies a number of physical properties including surface tension, solubility, and melting and boiling points.
"
Potential_energy,Chemistry,1,"
 
 In physics, potential energy is the energy held by an object because of its position relative to other objects, stresses within itself, its electric charge, or other factors.[1][2] U            =  ½ · k · x2(elastic)U            = ½ · C · V2 (electric)U            = -m · B (magnetic)
 Common types of potential energy include the gravitational potential energy of an object that depends on its mass and its distance from the center of mass of another object, the elastic potential energy of an extended spring, and the electric potential energy of an electric charge in an electric field. The unit for energy in the International System of Units (SI) is the joule, which has the symbol J.
 The term potential energy was introduced by the 19th-century Scottish engineer and physicist William Rankine,[3][4] although it has links to Greek philosopher Aristotle's concept of potentiality.
Potential energy is associated with forces that act on a body in a way that the total work done by these forces on the body depends only on the initial and final positions of the body in space. These forces, that are called conservative forces, can be represented at every point in space by vectors expressed as gradients of a certain scalar function called potential.
 Since the work of potential forces acting on a body that moves from a start to an end position is determined only by these two positions, and does not depend on the trajectory of the body, there is a function known as potential that can be evaluated at the two positions to determine this work.
"
Pressure,Chemistry,1,"Pressure (symbol: p or P) is the force applied perpendicular to the surface of an object per unit area over which that force is distributed.:445[1] Gauge pressure (also spelled gage pressure)[a] is the pressure relative to the ambient pressure.
 Various units are used to express pressure. Some of these derive from a unit of force divided by a unit of area; the SI unit of pressure, the pascal (Pa), for example, is one newton per square metre (N/m2); similarly, the pound-force per square inch (psi) is the traditional unit of pressure in the imperial and U.S. customary systems. Pressure may also be expressed in terms of standard atmospheric pressure; the atmosphere (atm) is equal to this pressure, and the torr is defined as ​1⁄760 of this. Manometric units such as the centimetre of water, millimetre of mercury, and inch of mercury are used to express pressures in terms of the height of column of a particular fluid in a manometer.
"
Photon,Chemistry,1,"The photon is a type of elementary particle. It is the quantum of the electromagnetic field including electromagnetic radiation such as light and radio waves, and the force carrier for the electromagnetic force. Photons are massless,[a] so they always move at the speed of light in vacuum, 299792458 m/s.
 Like all elementary particles, photons are currently best explained by quantum mechanics and exhibit wave–particle duality, their behavior featuring properties of both waves and particles.[2] The modern photon concept originated during the first two decades of the 20th century with the work of Albert Einstein, who built upon the research of Max Planck. While trying to explain how matter and electromagnetic radiation could be in thermal equilibrium with one another, Planck proposed that the energy stored within a material object should be regarded as composed of an integer number of discrete, equal-sized parts. To explain the photoelectric effect, Einstein introduced the idea that light itself is made of discrete units of energy. In 1926, Gilbert N. Lewis popularized the term photon for these energy units.[3][4][5] Subsequently, many other experiments validated Einstein's approach.[6][7][8] In the Standard Model of particle physics, photons and other elementary particles are described as a necessary consequence of physical laws having a certain symmetry at every point in spacetime. The intrinsic properties of particles, such as charge, mass, and spin, are determined by this gauge symmetry.  The photon concept has led to momentous advances in experimental and theoretical physics, including lasers, Bose–Einstein condensation, quantum field theory, and the probabilistic interpretation of quantum mechanics. It has been applied to photochemistry, high-resolution microscopy, and measurements of molecular distances. Recently, photons have been studied as elements of quantum computers, and for applications in optical imaging and optical communication such as quantum cryptography.
"
Polyatomic,Chemistry,1,"A molecular ion is a covalently bonded set of two or more atoms, or of a metal complex, that can be considered to behave as a single unit and that has a net charge that is not zero. Unlike a molecule, which has a net charge of zero, this chemical species is an ion. (The prefix poly- carries  the meaning ""many"" in Greek, but even ions of two atoms are commonly described as polyatomic.)
 In older literature, a polyatomic ion may instead be referred to as a radical (or less commonly, as a radical group). (In contemporary usage, the term radical refers to various free radicals, which are species that have an unpaired electron and need not be charged.)
 A simple example of a polyatomic ion is the hydroxide ion, which consists of one oxygen atom and one hydrogen atom, jointly carrying a net charge of −1; its chemical formula is OH−. In contrast, an ammonium ion consists of one nitrogen atom and ‘’four’’ hydrogen atoms, with a charge of +1; its chemical formula is NH+4.
 Polyatomic ions often are useful in the context of acid-base chemistry and in the formation of salts. 
 Often, a polyatomic ion can be considered as the conjugate acid or base of a neutral molecule. For example, the conjugate base of sulfuric acid (H2SO4) is the polyatomic hydrogen sulfate anion (HSO−4). The removal of another hydrogen ion produces the sulfate anion (SO2−4).
"
Polyatomic_ion,Chemistry,1,"A molecular ion is a covalently bonded set of two or more atoms, or of a metal complex, that can be considered to behave as a single unit and that has a net charge that is not zero. Unlike a molecule, which has a net charge of zero, this chemical species is an ion. (The prefix poly- carries  the meaning ""many"" in Greek, but even ions of two atoms are commonly described as polyatomic.)
 In older literature, a polyatomic ion may instead be referred to as a radical (or less commonly, as a radical group). (In contemporary usage, the term radical refers to various free radicals, which are species that have an unpaired electron and need not be charged.)
 A simple example of a polyatomic ion is the hydroxide ion, which consists of one oxygen atom and one hydrogen atom, jointly carrying a net charge of −1; its chemical formula is OH−. In contrast, an ammonium ion consists of one nitrogen atom and ‘’four’’ hydrogen atoms, with a charge of +1; its chemical formula is NH+4.
 Polyatomic ions often are useful in the context of acid-base chemistry and in the formation of salts. 
 Often, a polyatomic ion can be considered as the conjugate acid or base of a neutral molecule. For example, the conjugate base of sulfuric acid (H2SO4) is the polyatomic hydrogen sulfate anion (HSO−4). The removal of another hydrogen ion produces the sulfate anion (SO2−4).
"
Protective_group,Chemistry,1,"
 A protecting group or protective group is introduced into a molecule by chemical modification of a functional group to obtain chemoselectivity in a subsequent chemical reaction. It plays an important role in multistep organic synthesis.[1] In many preparations of delicate organic compounds, some specific parts of their molecules cannot survive the required reagents or chemical environments. Then, these parts, or groups, must be protected. For example, lithium aluminium hydride is a highly reactive but useful reagent capable of reducing esters to alcohols. It will always react with carbonyl groups, and this cannot be discouraged by any means. When a reduction of an ester is required in the presence of a carbonyl, the attack of the hydride on the carbonyl has to be prevented. For example, the carbonyl is converted into an acetal, which does not react with hydrides. The acetal is then called a protecting group for the carbonyl. After the step involving the hydride is complete, the acetal is removed (by reacting it with an aqueous acid), giving back the original carbonyl. This step is called deprotection.
 Protecting groups are more commonly used in small-scale laboratory work and initial development than in industrial production processes because their use adds additional steps and material costs to the process. However, the availability of a cheap chiral building block can overcome these additional costs (e.g. shikimic acid for oseltamivir).
"
Proton,Chemistry,1,"A proton is a subatomic particle, symbol p or p+, with a positive electric charge of +1e elementary charge and a mass slightly less than that of a neutron. Protons and neutrons, each with masses of approximately one atomic mass unit, are collectively referred to as ""nucleons"" (particles present in atomic nuclei).
 938.27208816(29) MeV/c2[2] 1.52103220230(46)×10−3 μB[2] One or more protons are present in the nucleus of every atom; they are a necessary part of the nucleus. The number of protons in the nucleus is the defining property of an element, and is referred to as the atomic number (represented by the symbol Z). Since each element has a unique number of protons, each element has its own unique atomic number.
 The word proton is Greek for ""first"", and this name was given to the hydrogen nucleus by Ernest Rutherford in 1920. In previous years, Rutherford had discovered that the hydrogen nucleus (known to be the lightest nucleus) could be extracted from the nuclei of nitrogen by atomic collisions.[3] Protons were therefore a candidate to be a fundamental particle, and hence a building block of nitrogen and all other heavier atomic nuclei.
 Although protons were originally considered fundamental or elementary particles, in the modern Standard Model of particle physics, protons are classified as hadrons, like neutrons, the other nucleon. Protons are composite particles composed of three valence quarks: two up quarks of charge +2/3e and one down quark of charge −1/3e. The rest masses of quarks contribute only about 1% of a proton's mass.[4] The remainder of a proton's mass is due to quantum chromodynamics binding energy, which includes the kinetic energy of the quarks and the energy of the gluon fields that bind the quarks together.  Because protons are not fundamental particles, they possess a measurable size; the root mean square charge radius of a proton is about 0.84–0.87 fm (or 0.84×10−15 to 0.87×10−15 m).[5][6] In 2019, two different studies, using different techniques, have found the radius of the proton to be 0.833 fm, with an uncertainty of ±0.010 fm.[7][8] At sufficiently low temperatures, free protons will bind to electrons. However, the character of such bound protons does not change, and they remain protons. A fast proton moving through matter will slow by interactions with electrons and nuclei, until it is captured by the electron cloud of an atom. The result is a protonated atom, which is a chemical compound of hydrogen. In vacuum, when free electrons are present, a sufficiently slow proton may pick up a single free electron, becoming a neutral hydrogen atom, which is chemically a free radical. Such ""free hydrogen atoms"" tend to react chemically with many other types of atoms at sufficiently low energies. When free hydrogen atoms react with each other, they form neutral hydrogen molecules (H2), which are the most common molecular component of molecular clouds in interstellar space.
"
Protonation,Chemistry,1,"In chemistry, protonation (or hydronation) is the addition of a proton (or hydron, or hydrogen cation), (H+) to an atom, molecule, or ion, forming the conjugate acid.[1] (The complementary process, when a proton is removed from a Brønsted–Lowry acid, is deprotonation.) Some examples include
 Protonation is a fundamental chemical reaction and is a step in many stoichiometric and catalytic processes. Some ions and molecules can undergo more than one protonation and are labeled polybasic, which is true of many biological macromolecules. Protonation and deprotonation (removal of a proton) occur in most acid–base reactions; they are the core of most acid–base reaction theories. A Brønsted–Lowry acid is defined as a chemical substance that protonates another substance. Upon protonating a substrate, the mass and the charge of the species each increase by one unit, making it an essential step in certain analytical procedures such as electrospray mass spectrometry. Protonating or deprotonating a molecule or ion can change many other chemical properties, not just the charge and mass, for example solubility, hydrophilicity, reduction potential, and optical properties can change.
"
Pyrolysis,Chemistry,1,"Pyrolysis is the thermal decomposition of materials at elevated temperatures in an inert atmosphere.[1] It involves a change of chemical composition. The word is coined from the Greek-derived elements pyro ""fire"" and lysis ""separating"".
 Pyrolysis is most commonly used in the treatment of organic materials. It is one of the processes involved in charring wood.[2]  In general, pyrolysis of organic substances produces volatile products and leaves a solid residue enriched in carbon, char. Extreme pyrolysis, which leaves mostly carbon as the residue, is called carbonization. Pyrolysis is considered as the first step in the processes of gasification or combustion.[3][4] The process is used heavily in the chemical industry, for example, to produce ethylene, many forms of carbon, and other chemicals from petroleum, coal, and even wood, to produce coke from coal.  Used also in the conversion of natural gas (primarily methane) into non-polluting hydrogen gas and non-polluting solid carbon char, initiating production in industrial volume.[5] Aspirational applications of pyrolysis would convert biomass into syngas and biochar, waste plastics back into usable oil, or waste into safely disposable substances.
"
Quantum_mechanics,Chemistry,1,"
 Quantum mechanics is a fundamental theory in physics that provides a description of the physical properties of nature at the scale of atoms and subatomic particles.[2] It is the foundation of all quantum physics including quantum chemistry, quantum field theory, quantum technology, and quantum information science.
 Classical physics, the description of physics that existed before the theory of relativity and quantum mechanics, describes many aspects of nature at an ordinary (macroscopic) scale, while quantum mechanics explains the aspects of nature at small (atomic and subatomic) scales, for which classical mechanics is insufficient. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale.[3] Quantum mechanics differs from classical physics in that energy, momentum, angular momentum, and other quantities of a bound system are restricted to discrete values (quantization), objects have characteristics of both particles and waves (wave-particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle).[note 1] Quantum mechanics arose gradually, from theories to explain observations which could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper which explained the photoelectric effect. Early quantum theory was profoundly re-conceived in the mid-1920s by Niels Bohr, Erwin Schrödinger, Werner Heisenberg, Max Born and others. The original interpretation of quantum mechanics is the Copenhagen interpretation, developed by Niels Bohr and Werner Heisenberg in Copenhagen during the 1920s. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of energy, momentum, and other physical properties of a particle.
"
Quark,Chemistry,1,"
 A quark (/kwɔːrk, kwɑːrk/) is a type of elementary particle and a fundamental constituent of matter. Quarks combine to form composite particles called hadrons, the most stable of which are protons and neutrons, the components of atomic nuclei.[1] All commonly observable matter is composed of up quarks, down quarks and electrons. Due to a phenomenon known as color confinement, quarks are never found in isolation; they can be found only within hadrons, which include baryons (such as protons and neutrons) and mesons, or in quark–gluon plasmas.[2][3][nb 1] For this reason, much of what is known about quarks has been drawn from observations of hadrons.
 Quarks have various intrinsic properties, including electric charge, mass, color charge, and spin. They are the only elementary particles in the Standard Model of particle physics to experience all four fundamental interactions, also known as fundamental forces (electromagnetism, gravitation, strong interaction, and weak interaction), as well as the only known particles whose electric charges are not integer multiples of the elementary charge.
 There are six types, known as flavors, of quarks: up, down, charm, strange, top, and bottom.[4] Up and down quarks have the lowest masses of all quarks. The heavier quarks rapidly change into up and down quarks through a process of particle decay: the transformation from a higher mass state to a lower mass state. Because of this, up and down quarks are generally stable and the most common in the universe, whereas strange, charm, bottom, and top quarks can only be produced in high energy collisions (such as those involving cosmic rays and in particle accelerators). For every quark flavor there is a corresponding type of antiparticle, known as an antiquark, that differs from the quark only in that some of its properties (such as the electric charge) have equal magnitude but opposite sign.
 The quark model was independently proposed by physicists Murray Gell-Mann and George Zweig in 1964.[5] Quarks were introduced as parts of an ordering scheme for hadrons, and there was little evidence for their physical existence until deep inelastic scattering experiments at the Stanford Linear Accelerator Center in 1968.[6][7] Accelerator experiments have provided evidence for all six flavors. The top quark, first observed at Fermilab in 1995, was the last to be discovered.[5]"
Quantum,Chemistry,1,"In physics, a quantum (plural quanta) is the minimum amount of any physical entity (physical property) involved in an interaction. The fundamental notion that a physical property can be ""quantized"" is referred to as ""the hypothesis of quantization"".[1] This means that the magnitude of the physical property can take on only discrete values consisting of integer multiples of one quantum.
 For example, a photon is a single quantum of light (or of any other form of electromagnetic radiation). Similarly, the energy of an electron bound within an atom is quantized and can exist only in certain discrete values. (Atoms and matter in general are stable because electrons can exist only at discrete energy levels within an atom.) Quantization is one of the foundations of the much broader physics of quantum mechanics. Quantization of energy and its influence on how energy and matter interact (quantum electrodynamics) is part of the fundamental framework for understanding and describing nature.
"
Racemate,Chemistry,1,"
In chemistry, a racemic mixture, or racemate (/reɪˈsiːmeɪt, rə-, ˈræsɪmeɪt/),[1] is one that has equal amounts of left- and right-handed enantiomers of a chiral molecule. The first known racemic mixture was racemic acid, which Louis Pasteur found to be a mixture of the two enantiomeric isomers of tartaric acid. A sample with only a single enantiomer is an enantiomerically pure or enantiopure compound.[2]"
Radiation,Chemistry,1,"
 In physics, radiation is the emission or transmission of energy in the form of waves or particles through space or through a material medium.[1][2] This includes:
 Radiation is often categorized as either ionizing or non-ionizing depending on the energy of the radiated particles. Ionizing radiation carries more than 10 eV, which is enough to ionize atoms and molecules and break chemical bonds. This is an important distinction due to the large difference in harmfulness to living organisms. A common source of ionizing radiation is radioactive materials that emit α, β, or γ radiation, consisting of helium nuclei, electrons or positrons, and photons, respectively. Other sources include X-rays from medical radiography examinations and muons, mesons, positrons, neutrons and other particles that constitute the secondary cosmic rays that are produced after primary cosmic rays interact with Earth's atmosphere.
 Gamma rays, X-rays and the higher energy range of ultraviolet light constitute the ionizing part of the electromagnetic spectrum. The word ""ionize"" refers to the breaking of one or more electrons away from an atom, an action that requires the relatively high energies that these electromagnetic waves supply. Further down the spectrum, the non-ionizing lower energies of the lower ultraviolet spectrum cannot ionize atoms, but can disrupt the inter-atomic bonds which form molecules, thereby breaking down molecules rather than atoms; a good example of this is sunburn caused by long-wavelength solar ultraviolet. The waves of longer wavelength than UV in visible light, infrared and microwave frequencies cannot break bonds but can cause vibrations in the bonds which are sensed as heat. Radio wavelengths and below generally are not regarded as harmful to biological systems. These are not sharp delineations of the energies; there is some overlap in the effects of specific frequencies.[3] The word radiation arises from the phenomenon of waves radiating (i.e., traveling outward in all directions) from a source. This aspect leads to a system of measurements and physical units that are applicable to all types of radiation. Because such radiation expands as it passes through space, and as its energy is conserved (in vacuum), the intensity of all types of radiation from a point source follows an inverse-square law in relation to the distance from its source. Like any ideal law, the inverse-square law approximates a measured radiation intensity to the extent that the source approximates a geometric point.
"
Radical_(chemistry),Chemistry,1,"In chemistry, a radical is an atom, molecule, or ion that has an unpaired valence electron.[1][2]
With some exceptions, these unpaired electrons make radicals highly chemically reactive.  Many radicals spontaneously dimerize.  Most organic radicals have short lifetimes.
 A notable example of a radical is the hydroxyl radical (HO•), a molecule that has one unpaired electron on the oxygen atom. Two other examples are triplet oxygen and triplet carbene (:CH2) which have two unpaired electrons.
 Radicals may be generated in a number of ways, but typical methods involve redox reactions. Ionizing radiation, heat, electrical discharges, and electrolysis are known to produce radicals. Radicals are intermediates in many chemical reactions, more so than is apparent from the balanced equations.
 Radicals are important in combustion, atmospheric chemistry, polymerization, plasma chemistry, biochemistry, and many other chemical processes. A majority of natural products are generated by radical-generating enzymes.  In living organisms, the radicals superoxide and nitric oxide and their reaction products regulate many processes, such as control of vascular tone and thus blood pressure. They also play a key role in the intermediary metabolism of various biological compounds. Such radicals can even be messengers in a process dubbed redox signaling.  A radical may be trapped within a solvent cage or be otherwise bound.
"
Radioactive_decay,Chemistry,1,"
 Radioactive decay (also known as nuclear decay, radioactivity, radioactive disintegration or nuclear disintegration) is the process by which an unstable atomic nucleus loses energy by radiation. A material containing unstable nuclei is considered radioactive. Three of the most common types of decay are alpha decay, beta decay, and gamma decay, all of which involve emitting one or more particles or photons. The weak force is the mechanism that is responsible for beta decay.[1] Radioactive decay is a stochastic (i.e. random) process at the level of single atoms. According to quantum theory, it is impossible to predict when a particular atom will decay, regardless of how long the atom has existed.[2][3][4]  However, for a significant number of identical atoms, the overall decay rate can be expressed as a decay constant or as half-life. The half-lives of radioactive atoms have a huge range; from nearly instantaneous to far longer than the age of the universe.
 The decaying nucleus is called the parent radionuclide (or parent radioisotope[note 1]), and the process produces at least one daughter nuclide. Except for gamma decay or internal conversion from a nuclear excited state, the decay is a nuclear transmutation resulting in a daughter containing a different number of protons or neutrons (or both). When the number of protons changes, an atom of a different chemical element is created.
 By contrast, there are radioactive decay processes that do not result in a nuclear transmutation. The energy of an excited nucleus may be emitted as a gamma ray in a process called gamma decay, or that energy may be lost when the nucleus interacts with an orbital electron causing its ejection from the atom, in a process called internal conversion. Another type of radioactive decay results in products that vary, appearing as two or more ""fragments"" of the original nucleus with a range of possible masses. This decay, called spontaneous fission, happens when a large unstable nucleus spontaneously splits into two (or occasionally three) smaller daughter nuclei, and generally leads to the emission of gamma rays, neutrons, or other particles from those products.
In contrast, decay products from a nucleus with spin may be distributed non-isotropically with respect to that spin direction. Either because of an external influence such as an electromagnetic field, or because the nucleus was produced in a dynamic process that constrained the direction of its spin, the anisotropy may be detectable. Such a parent process could be a previous decay, or a nuclear reaction.[5][6][7][note 2] For a summary table showing the number of stable and radioactive nuclides in each category, see radionuclide. There are 28 naturally occurring chemical elements on Earth that are radioactive, consisting of 34 radionuclides (6 elements have 2 different radionuclides) that date before the time of formation of the Solar System. These 34 are known as primordial nuclides. Well-known examples are uranium and thorium, but also included are naturally occurring long-lived radioisotopes, such as potassium-40.
 Another 50 or so shorter-lived radionuclides, such as radium-226 and radon-222, found on Earth, are the products of decay chains that began with the primordial nuclides, or are the product of ongoing cosmogenic processes, such as the production of carbon-14 from nitrogen-14 in the atmosphere by cosmic rays. Radionuclides may also be produced artificially in particle accelerators or nuclear reactors, resulting in 650 of these with half-lives of over an hour, and several thousand more with even shorter half-lives. (See List of nuclides for a list of these sorted by half-life.)
"
Raoult%27s_law,Chemistry,1,"Raoult's law (/ˈrɑːuːlz/ law) is a law of physical chemistry, with implications in thermodynamics.  Established by French chemist François-Marie Raoult in 1887,[1] it states that the partial pressure of each component of an ideal mixture of liquids is equal to the vapour pressure of the pure component multiplied by its mole fraction in the mixture. In consequence, the relative lowering of vapour pressure of a dilute solution of nonvolatile solute is equal to the mole fraction of solute in the solution.
 Mathematically, Raoult's law for a single component in an ideal solution is stated as 
 where 




p

i




{  p_{i}}
 is the partial pressure of the component 



i


{  i}
 in the gaseous mixture (above the solution), 




p

i


⋆




{  p_{i}^{\star }}
 is the equilibrium vapor pressure of the pure component 



i


{  i}
, and 




x

i




{  x_{i}}
 is the mole fraction of the component 



i


{  i}
 in the mixture (in the solution).[2] Where two volatile liquids A and B are mixed with each other to form a solution, the vapour phase consists of both components of the solution. Once the components in the solution have reached equilibrium, the total vapor pressure of the solution can be determined by combining Raoult's law with Dalton's law of partial pressures to give
 If a non-volatile solute (zero vapor pressure, does not evaporate) is dissolved into a solvent to form an ideal solution, the vapor pressure of the final solution will be lower than that of the solvent. The decrease in vapor pressure is directly proportional to the mole fraction of solute in an ideal solution;
 "
Rare-earth_metal,Chemistry,1,"
 A rare-earth element or rare-earth metal,[2] is one of a set of seventeen chemical elements, specifically the fifteen lanthanides, as well as scandium and yttrium.[3] Scandium and yttrium are considered rare-earth elements because they tend to occur in the same ore deposits as the lanthanides and exhibit similar chemical properties, but have different electronic and magnetic properties.[4] [5]
They are often found in minerals with thorium (Th), and less commonly uranium (U).
 Despite their name, rare-earth elements are relatively plentiful in Earth's crust, with cerium being the 25th most abundant element at 68 parts per million, more abundant than copper. All isotopes of promethium are radioactive, and it does not occur naturally in the earth's crust; however a trace amount is generated by decay of uranium 238. Because of their geochemical properties, rare-earth elements are typically dispersed and not often found concentrated in rare-earth minerals; as a result economically exploitable ore deposits are less common.[6] The first rare-earth mineral discovered (1787) was gadolinite, a mineral composed of cerium, yttrium, iron, silicon, and other elements. This mineral was extracted from a mine in the village of Ytterby in Sweden; four of the rare-earth elements bear names derived from this single location.
"
Rate_equation,Chemistry,1,"The rate law or rate equation for a chemical reaction is an equation that links the initial or forward reaction rate with the concentrations or pressures of the reactants and constant parameters (normally rate coefficients and partial reaction orders).[1] For many reactions, the initial rate is given by a power law such as
 where [A] and [B] express the concentration of the species A and B, usually in moles per liter (molarity, M). The exponents x and y are the partial orders of reaction for A and B and the overall reaction order is the sum of the exponents. These are often positive integers, but they may also be zero, fractional, or negative. The constant k is the reaction rate constant or rate coefficient of the reaction. Its value may depend on conditions such as temperature, ionic strength, surface area of an adsorbent, or light irradiation. If the reaction goes to completion, the rate equation for the reaction rate 



v

=

k
[

A


]

x


[

B


]

y




{  v\;=\;k[\mathrm {A} ]^{x}[\mathrm {B} ]^{y}}
 applies throughout the course of the reaction.
 Elementary (single-step) reactions and reaction steps have reaction orders equal to the stoichiometric coefficients for each reactant. The overall reaction order, i.e. the sum of stoichiometric coefficients of reactants, is always equal to the molecularity of the elementary reaction. However, complex (multi-step) reactions may or may not have reaction orders equal to their stoichiometric coefficients. This implies that the order and the rate equation of a given reaction cannot be reliably deduced from the stoichiometry and must be determined experimentally, since an unknown reaction mechanism could be either elementary or complex. When the experimental rate equation has been determined, it is often of use for deduction of the reaction mechanism.
 The rate equation of a reaction with an assumed multi-step mechanism can often be derived theoretically using quasi-steady state assumptions from the underlying elementary reactions, and compared with the experimental rate equation as a test of the assumed mechanism. The equation may involve a fractional order, and may depend on the concentration of an intermediate species.
 A reaction can also have an undefined reaction order with respect to a reactant if the rate is not simply proportional to some power of the concentration of that reactant; for example, one cannot talk about reaction order in the rate equation for a bimolecular reaction between adsorbed molecules:
"
Reactant,Chemistry,1,"A reagent /riˈeɪdʒənt/ is a substance or compound added to a system to cause a chemical reaction, or added to test if a reaction occurs.[1] The terms reactant and reagent are often used interchangeably—however, a reactant is more specifically a substance consumed in the course of a chemical reaction.[1] Solvents, though involved in the reaction, are usually not called reactants. Similarly, catalysts are not consumed by the reaction, so they are not reactants. In biochemistry, especially in connection with enzyme-catalyzed reactions, the reactants are commonly called substrates.
"
Reaction_mechanism,Chemistry,1,"In chemistry, a reaction mechanism is the step by step sequence of elementary reactions by which overall chemical change occurs.[1] A chemical mechanism is a theoretical conjecture that tries to describe in detail what takes place at each stage of an overall chemical reaction. The detailed steps of a reaction are not observable in most cases. The conjectured mechanism is chosen because it is thermodynamically  feasible, and has experimental support in isolated intermediates (see next section) or other quantitative and qualitative characteristics of the reaction. It also describes each reactive intermediate, activated complex, and transition state, and which bonds are broken (and in what order), and which bonds are formed (and in what order). A complete mechanism must also explain the reason for the reactants and catalyst used, the stereochemistry observed in reactants and products, all products formed and the amount of each. 
 The electron or arrow pushing method is often used in illustrating a reaction mechanism; for example, see the illustration of the mechanism for benzoin condensation in the following examples section.
 A reaction mechanism must also account for the order in which molecules react. Often what appears to be a single-step conversion is in fact a multistep reaction.
"
Reaction_rate,Chemistry,1,"The reaction rate or rate of reaction is the speed at which a chemical reaction takes place. Reaction rate is defined as the speed at which reactants are converted into products. Reaction rates can vary dramatically. For example, the oxidative rusting of iron under Earth's atmosphere is a slow reaction that can take many years, but the combustion of cellulose in a fire is a reaction that takes place in fractions of a second. For most reactions, the rate decreases as the reaction proceeds. 
 Chemical kinetics is the part of physical chemistry that studies reaction rates. The concepts of chemical kinetics are applied in many disciplines, such as chemical engineering,[1][2][3][4] enzymology and environmental engineering.[5][6][7]"
Reaction_rate_constant,Chemistry,1,"In chemical kinetics a reaction rate constant or reaction rate coefficient, k, quantifies the rate and direction of a chemical reaction.[1] For a reaction between reactants A and B to form product C
 the reaction rate is often found to have the form:
 Here k(T) is the reaction rate constant that depends on temperature, and [A] and [B] are the molar concentrations of substances A and B in moles per unit volume of solution, assuming the reaction is taking place throughout the volume of the solution. (For a reaction taking place at a boundary one would use instead moles of A or B per unit area.)
 The exponents m and n are called partial orders of reaction and are not generally equal to the stoichiometric coefficients a and b. Instead they depend on the reaction mechanism and can be determined experimentally.
"
Reactive_intermediate,Chemistry,1,"In chemistry, a reactive intermediate or an intermediate is a short-lived, high-energy, highly reactive molecule. When generated in a chemical reaction, it will quickly convert into a more stable molecule. Only in exceptional cases can these compounds be isolated and stored, e.g. low temperatures, matrix isolation. When their existence is indicated, reactive intermediates can help explain how a chemical reaction takes place.[1][2][3][4] Most chemical reactions take more than one elementary step to complete, and a reactive intermediate is a high-energy, yet stable, product that exists only in one of the intermediate steps. The series of steps together make a reaction mechanism.  A reactive intermediate differs from a reactant or product or a simple reaction intermediate only in that it cannot usually be isolated but is sometimes observable only through fast spectroscopic methods. It is stable in the sense that an elementary reaction forms the reactive intermediate and the elementary reaction in the next step is needed to destroy it.
 When a reactive intermediate is not observable, its existence must be inferred through experimentation. This usually involves changing reaction conditions such as temperature or concentration and applying the techniques of chemical kinetics, chemical thermodynamics, or spectroscopy. Reactive intermediates based on carbon are radicals, carbenes, carbocations, carbanions, arynes, and carbynes.
"
Reactivity_(chemistry),Chemistry,1,"In chemistry, reactivity is the impetus for which a chemical substance undergoes a chemical reaction, either by itself or with other materials, with an overall release of energy.
 Reactivity refers to:
 The chemical reactivity of a single substance (reactant) covers its behavior in which it: 
 The chemical reactivity of a substance can refer to the variety of circumstances (conditions that include temperature, pressure, presence of catalysts) in which it reacts, in combination with the:
 The term reactivity is related to the concepts of chemical stability and chemical compatibility.
"
Reactivity_series,Chemistry,1,"In chemistry, a reactivity series (or activity series) is an empirical, calculated, and structurally analytical progression[1] of a series of metals, arranged by their ""reactivity"" from highest to lowest.[2][3][4] It is used to summarize information about the reactions of metals with acids and water, single displacement reactions and the extraction of metals from their ores.
"
Reagent,Chemistry,1,"A reagent /riˈeɪdʒənt/ is a substance or compound added to a system to cause a chemical reaction, or added to test if a reaction occurs.[1] The terms reactant and reagent are often used interchangeably—however, a reactant is more specifically a substance consumed in the course of a chemical reaction.[1] Solvents, though involved in the reaction, are usually not called reactants. Similarly, catalysts are not consumed by the reaction, so they are not reactants. In biochemistry, especially in connection with enzyme-catalyzed reactions, the reactants are commonly called substrates.
"
Redox,Chemistry,1,"Redox (reduction–oxidation, pronunciation: /ˈrɛdɒks/ redoks or /ˈriːdɒks/ reedoks[1]) is a type of chemical reaction in which the oxidation states of atoms are changed. Redox reactions are characterized by the actual or formal transfer of electrons between chemical species, most often with one species (the reducing agent) undergoing oxidation (losing electrons) while another species (the oxidizing agent) undergoes reduction (gains electrons).[2] The chemical species from which the electron is removed is said to have been oxidized, while the chemical species to which the electron is added is said to have been reduced. In other words:
 Many reactions in organic chemistry are redox reactions due to changes in oxidation states but without distinct electron transfer. For example, during the combustion of wood with molecular oxygen, the oxidation state of carbon atoms in the wood increases and that of oxygen atoms decreases as carbon dioxide and water are formed. The oxygen atoms undergo reduction, formally gaining electrons, while the carbon atoms undergo oxidation, losing electrons. Thus oxygen is the oxidizing agent and carbon is the reducing agent in this reaction.[3] Although oxidation reactions are commonly associated with the formation of oxides from oxygen molecules, oxygen is not necessarily included in such reactions, as other chemical species can serve the same function.[3] Redox reactions can occur relatively slowly, as in the formation of rust, or much more rapidly, as in the case of burning fuel. There are simple redox processes, such as the oxidation of carbon to yield carbon dioxide (CO2) or the reduction of carbon by hydrogen to yield methane (CH4), and more complex processes such as the oxidation of glucose (C6H12O6) in the human body. Analysis of bond energies and ionization energies in water allow calculation of the redox potentials.[4][5]"
Reducing_agent,Chemistry,1,"A reducing agent (also called a reductant or reducer) is an element or compound that loses (or ""donates"") an electron to an electron recipient (oxidizing agent) in a redox chemical reaction.
 A reducing agent is thus oxidized when it loses electrons in the redox reaction. Reducing agents ""reduce"" (or, are ""oxidized"" by) oxidizing agents. Oxidizers ""oxidize"" (that is, are reduced by) reducers.
 Historically, reduction referred to the removal of oxygen from a compound, hence the name 'reduction'. The modern sense of donating electrons is a generalisation of this idea, acknowledging that other components can play a similar chemical role to oxygen.
 In their pre-reaction states, reducers have extra electrons (that is, they are by themselves reduced) and oxidizers lack electrons (that is, they are by themselves oxidized). A reducing agent typically is in one of its lower possible oxidation states and is known as the electron donor.  Examples of reducing agents include the earth metals, formic acid, oxalic acid, and sulfite compounds.
 For example, consider the overall reaction for aerobic cellular respiration:
 The oxygen (O2) is being reduced, so it is the oxidizing agent. The glucose (C6H12O6) is being oxidized, so it is the reducing agent.
 In organic chemistry, reduction usually refers to the addition of hydrogen to a molecule, though the aforementioned definition still applies. For example, benzene is reduced to cyclohexane in the presence of a platinum catalyst:
"
Reduction_potential,Chemistry,1,"
 Redox potential (also known as oxidation / reduction potential, 'ORP', pe, E0', or 




E

h




{  E_{h}}
) is a measure of the tendency of a chemical species to acquire electrons from or lose electrons to an electrode and thereby be reduced or oxidised respectively. Redox potential is measured in volts (V), or millivolts (mV).  Each species has its own intrinsic redox potential; for example, the more positive the reduction potential (reduction potential is more often used due to general formalism in electrochemistry), the greater the species' affinity for electrons and tendency to be reduced. ORP can reflect the antimicrobial potential of the water.[1]"
Resonance_(chemistry),Chemistry,1,"In chemistry, resonance is a way of describing bonding in certain molecules or ions by the combination of several contributing structures (or forms,[1] also variously known as resonance structures or canonical structures) into a resonance hybrid (or hybrid structure) in valence bond theory. It has particular value for describing delocalized electrons within certain molecules or polyatomic ions where the bonding cannot be expressed by one single Lewis structure.
"
Retort,Chemistry,1,"In a chemistry laboratory, a retort is a device used for distillation or dry distillation of substances. It consists of a spherical vessel with a long downward-pointing neck. The liquid to be distilled is placed in the vessel and heated. The neck acts as a condenser, allowing the vapors to condense and flow along the neck to a collection vessel placed underneath.[1] In the chemical industry, a retort is an airtight vessel in which substances are heated for a chemical reaction producing gaseous products to be collected in a collection vessel or for further processing.  Such industrial-scale retorts are used in shale oil extraction, the production of charcoal and in the recovery of mercury in gold mining processes and hazardous waste.  A process of heating oil shale to produce shale oil, oil shale gas, and spent shale is commonly called retorting. Airtight vessels to apply pressure as well as heat are called autoclaves.
 In the food industry, pressure cookers are often referred to as retorts, meaning ""canning retorts"", for sterilization under high temperature (116–130 °C).
"
Round-bottom_flask,Chemistry,1,"Round-bottom flasks (also called round-bottomed flasks or RB flasks) are types of flasks having spherical bottoms used as laboratory glassware, mostly for chemical or biochemical work.[1] They are typically made of glass for chemical inertness; and in modern days, they are usually made of heat-resistant borosilicate glass. There is at least one tubular section known as the neck with an opening at the tip. Two or three-necked flasks are common as well. Round bottom flasks come in many sizes, from 5 mL to 20 L, with the sizes usually inscribed on the glass. In pilot plants even larger flasks are encountered.  
 The ends of the necks are usually conical (female) ground glass joints. These are standardized, and can accept any similarly-sized tapered (male) fittings. 24/20 is common for 250 mL or larger flasks, while smaller sizes such as 14/20 or 19/22 are used for smaller flasks.
 Because of the round bottom, cork rings are needed to keep the round bottom flasks upright.  When in use, round-bottom flasks are commonly held at the neck by clamps on a stand.
 A round-bottom flask is featured prominently on the logo of the OPCW, the implementing body for the Chemical Weapons Convention.[2]"
Rust,Chemistry,1,"Rust is an iron oxide, a usually reddish brown oxide formed by the reaction of iron and oxygen in the presence of water or air moisture. Rust consists of hydrated iron(III) oxides Fe2O3·nH2O and iron(III) oxide-hydroxide (FeO(OH), Fe(OH)3). It is typically associated with the corrosion of refined iron.
 Given sufficient time, any iron mass, in the presence of water and oxygen, could eventually convert entirely to rust. Surface rust is commonly flaky and friable, and it provides no protection to the underlying iron, unlike the formation of patina on copper surfaces. Rusting is the common term for corrosion of iron and its alloys, such as steel. Many other metals undergo similar corrosion, but the resulting oxides are not commonly called rust.[1] Several forms of rust are distinguishable both visually and by spectroscopy, and form under different circumstances.[2] Other forms of rust include the result of reactions between iron and chloride in an environment deprived of oxygen. Rebar used in underwater concrete pillars, which generates green rust, is an example. Although rusting is generally a negative aspect of iron, a particular form of rusting, known as stable rust, causes the object to have a thin coating of rust over the top, and if kept in low relative humidity, makes the ""stable"" layer protective to the iron below, but not to the extent of other oxides, such as aluminium.[3]"
S-block,Chemistry,1,"A block of the periodic table is a set of elements unified by the orbitals their valence electrons or vacancies lie in.[1] The term appears to have been first used by Charles Janet.[2] Each block is named after its characteristic orbital: s-block, p-block, d-block, and f-block.
 The block names (s, p, d, and f) are derived from the spectroscopic notation for the value of an electron's azimuthal quantum number: shape (0), principal (1), diffuse (2), or fundamental (3). Succeeding notations proceed in alphabetical order, as g, h, etc.
"
Salt_(chemistry),Chemistry,1,"In chemistry, a salt is a chemical compound consisting of an ionic assembly of cations and anions.[1] Salts are composed of related numbers of cations (positively charged ions) and anions (negatively charged ions) so that the product is electrically neutral (without a net charge). These component ions can be inorganic, such as chloride (Cl−), or organic, such as acetate (CH3CO−2); and can be monatomic, such as fluoride (F−) or polyatomic, such as sulfate (SO2−4).
"
Salt_bridge,Chemistry,1,"
 A salt bridge or ion bridge, in electrochemistry, is a laboratory device used to connect the oxidation and reduction half-cells of a galvanic cell (voltaic cell), a type of electrochemical cell. It maintains electrical neutrality within the internal circuit. If no salt bridge were present, the solution in one half cell would accumulate negative charge and the solution in the other half cell would accumulate positive charge as the reaction proceeded, quickly preventing further reaction, and hence production of electricity.[1] Salt bridges usually come in two types: glass tube and filter paper.
"
Saline_(medicine),Chemistry,1,"
 Saline, also known as saline solution, is a mixture of sodium chloride in water and has a number of uses in medicine.[1] Applied to the affected area it is used to clean wounds, help remove contact lenses, and help with dry eyes.[2] By injection into a vein it is used to treat dehydration such as from gastroenteritis and diabetic ketoacidosis.[2] [1] Large amounts may result in fluid overload, swelling, acidosis, and high blood sodium.[1][2] In those with long-standing low blood sodium, excessive use may result in osmotic demyelination syndrome.[2] Saline is in the crystalloid family of medications.[3] It is most commonly used as a sterile 9 g of salt per litre (0.9%) solution, known as normal saline.[1] Higher and lower concentrations may also occasionally be used.[4][5] Saline has a pH of 5.5 (mainly due to dissolved carbon dioxide) making it acidic.[6] The medical use of saline began around 1831.[7] It is on the World Health Organization's List of Essential Medicines.[8] In 2017, sodium was the 225th most commonly prescribed medication in the United States, with more than two million prescriptions. [9][10]"
Schr%C3%B6dinger_equation,Chemistry,1,"
 The Schrödinger equation is a linear partial differential equation that describes the wave function or state function of a quantum-mechanical system.[1]:1–2 It is a key result in quantum mechanics, and its discovery was a significant landmark in the development of the subject. The equation is named after Erwin Schrödinger, who postulated the equation in 1925, and published it in 1926, forming the basis for the work that resulted in his Nobel Prize in Physics in 1933.[2][3] In classical mechanics, Newton's second law (F = ma)[note 1] is used to make a mathematical prediction as to what path a given physical system will take over time following a set of known initial conditions. Solving this equation gives the position and the momentum of the physical system as a function of the external force 




F



{  \mathbf {F} }
 on the system. Those two parameters are sufficient to describe its state at each time instant. In quantum mechanics, the analogue of Newton's law is Schrödinger's equation.
 The concept of a wave function is a fundamental postulate of quantum mechanics; the wave function defines the state of the system at each spatial position and time. Using these postulates, Schrödinger's equation can be derived from the fact that the time-evolution operator must be unitary, and must therefore be generated by the exponential of a self-adjoint operator, which is the quantum Hamiltonian. This derivation is explained below.
 In the Copenhagen interpretation of quantum mechanics, the wave function is the most complete description that can be given of a physical system. Solutions to Schrödinger's equation describe not only molecular, atomic, and subatomic systems, but also macroscopic systems, possibly even the whole universe.[4]:292ff The Schrödinger equation is not the only way to study quantum mechanical systems and make predictions. The other formulations of quantum mechanics include matrix mechanics, introduced by Werner Heisenberg, and the path integral formulation, developed chiefly by Richard Feynman. Paul Dirac incorporated matrix mechanics and the Schrödinger equation into a single formulation.
"
Rate_equation#Second_order_reactions,Chemistry,1,"The rate law or rate equation for a chemical reaction is an equation that links the initial or forward reaction rate with the concentrations or pressures of the reactants and constant parameters (normally rate coefficients and partial reaction orders).[1] For many reactions, the initial rate is given by a power law such as
 where [A] and [B] express the concentration of the species A and B, usually in moles per liter (molarity, M). The exponents x and y are the partial orders of reaction for A and B and the overall reaction order is the sum of the exponents. These are often positive integers, but they may also be zero, fractional, or negative. The constant k is the reaction rate constant or rate coefficient of the reaction. Its value may depend on conditions such as temperature, ionic strength, surface area of an adsorbent, or light irradiation. If the reaction goes to completion, the rate equation for the reaction rate 



v

=

k
[

A


]

x


[

B


]

y




{  v\;=\;k[\mathrm {A} ]^{x}[\mathrm {B} ]^{y}}
 applies throughout the course of the reaction.
 Elementary (single-step) reactions and reaction steps have reaction orders equal to the stoichiometric coefficients for each reactant. The overall reaction order, i.e. the sum of stoichiometric coefficients of reactants, is always equal to the molecularity of the elementary reaction. However, complex (multi-step) reactions may or may not have reaction orders equal to their stoichiometric coefficients. This implies that the order and the rate equation of a given reaction cannot be reliably deduced from the stoichiometry and must be determined experimentally, since an unknown reaction mechanism could be either elementary or complex. When the experimental rate equation has been determined, it is often of use for deduction of the reaction mechanism.
 The rate equation of a reaction with an assumed multi-step mechanism can often be derived theoretically using quasi-steady state assumptions from the underlying elementary reactions, and compared with the experimental rate equation as a test of the assumed mechanism. The equation may involve a fractional order, and may depend on the concentration of an intermediate species.
 A reaction can also have an undefined reaction order with respect to a reactant if the rate is not simply proportional to some power of the concentration of that reactant; for example, one cannot talk about reaction order in the rate equation for a bimolecular reaction between adsorbed molecules:
"
Semiconductor,Chemistry,1,"A semiconductor material has an electrical conductivity value falling between that of a conductor, such as metallic copper, and an insulator, such as glass. Its resistivity falls as its temperature rises; metals are the opposite. Its conducting properties may be altered in useful ways by introducing impurities (""doping"") into the crystal structure. When two differently-doped regions exist in the same crystal, a semiconductor junction is created. The behavior of charge carriers, which include electrons, ions and electron holes, at these junctions is the basis of diodes, transistors and all modern electronics. Some examples of semiconductors are silicon, germanium, gallium arsenide, and elements near the so-called ""metalloid staircase"" on the periodic table. After silicon, gallium arsenide is the second most common semiconductor and is used in laser diodes, solar cells, microwave-frequency integrated circuits and others. Silicon is a critical element for fabricating most electronic circuits.
 Semiconductor devices can display a range of useful properties, such as passing current more easily in one direction than the other, showing variable resistance, and sensitivity to light or heat. Because the electrical properties of a semiconductor material can be modified by doping, or by the application of electrical fields or light, devices made from semiconductors can be used for amplification, switching, and energy conversion.
 The conductivity of silicon is increased by adding a small amount (of the order of 1 in 108) of pentavalent (antimony, phosphorus, or arsenic) or trivalent (boron, gallium, indium) atoms. This process is known as doping and resulting semiconductors are known as doped or extrinsic semiconductors.  Apart from doping, the conductivity of a semiconductor can equally be improved by increasing its temperature. This is contrary to the behaviour of a metal in which conductivity decreases with increase in temperature.
 The modern understanding of the properties of a semiconductor relies on quantum physics to explain the movement of charge carriers in a crystal lattice.[1]  Doping greatly increases the number of charge carriers within the crystal. When a doped semiconductor contains mostly free holes it is called ""p-type"", and when it contains mostly free electrons it is known as ""n-type"". The semiconductor materials used in electronic devices are doped under precise conditions to control the concentration and regions of p- and n-type dopants. A single semiconductor crystal can have many p- and n-type regions; the p–n junctions between these regions are responsible for the useful electronic behavior. Using a hot-point probe, one can determine quickly whether a semiconductor sample is p- or n-type.[2] Some of the properties of semiconductor materials were observed throughout the mid 19th and first decades of the 20th century. The first practical application of semiconductors in electronics was the 1904 development of the cat's-whisker detector, a primitive semiconductor diode used in early radio receivers. Developments in quantum physics in turn led to the development of the transistor in 1947,[3] the integrated circuit in 1958, and the MOSFET (metal–oxide–semiconductor field-effect transistor) in 1959.
"
Serial_dilution,Chemistry,1,"A serial dilution is the stepwise dilution of a substance in solution. Usually the dilution factor at each step is constant, resulting in a geometric progression of the concentration in a logarithmic fashion. A ten-fold serial dilution could be 1 M, 0.1 M, 0.01 M, 0.001 M ... Serial dilutions are used to accurately create highly diluted solutions as well as solutions for experiments resulting in concentration curves with a logarithmic scale. A tenfold dilution for each step is called a logarithmic dilution or log-dilution, a 3.16-fold (100.5-fold) dilution is called a half-logarithmic dilution or half-log dilution, and a 1.78-fold (100.25-fold) dilution is called a quarter-logarithmic dilution or quarter-log dilution. Serial dilutions are widely used in experimental sciences, including biochemistry, pharmacology, microbiology, and physics.
"
Side_chain,Chemistry,1,"Notes
 In organic chemistry and biochemistry, a side chain is a chemical group that is attached to a core part of the molecule called the ""main chain"" or backbone. The side chain is a hydrocarbon branching element of a molecule that is attached to a larger hydrocarbon backbone. It is one factor in determining a molecule's properties and reactivity.[2] A side chain is also known as a pendant chain, but a pendant group (side group) has a different definition.
"
Multiple_bond,Chemistry,1,"Bond order, as introduced by Linus Pauling, is defined as the difference between the number of bonds and anti-bonds.
 The bond number itself is the number of electron pairs (bonds) between a pair of atoms.[1] For example, in diatomic nitrogen N≡N the bond number is 3, in ethyne H−C≡C−H the bond number between the two carbon atoms is also 3, and the C−H bond order is 1. Bond number gives an indication of the stability of a bond. Isoelectronic species have same bond number.[2] In molecules which have resonance or nonclassical bonding, bond number may not be an integer. In benzene, the delocalized molecular orbitals contain 6 pi electrons over six carbons essentially yielding half a pi bond together with the sigma bond for each pair of carbon atoms, giving a calculated bond number of 1.5. Furthermore, bond numbers of 1.1, for example, can arise under complex scenarios and essentially refer to bond strength relative to bonds with order 1.
"
Skeletal_formula,Chemistry,1,"The skeletal formula, also called line-angle formula or shorthand formula, of an organic compound is a type of molecular structural formula that serves as a shorthand representation of a molecule's bonding and some details of its molecular geometry. A skeletal formula shows the skeletal structure or skeleton of a molecule, which is composed of the skeletal atoms that make up the molecule.[1] It is represented in two dimensions, as on a piece of paper. It employs certain conventions to represent carbon and hydrogen atoms, which are the most common in organic chemistry.
 An early form of this representation was first developed by the organic chemist Friedrich August Kekulé von Stradonitz, while the modern form is closely related to and influenced by the Lewis (dot) structure of molecules and their valence electrons. For this reason, they are sometimes termed Kekulé structures[2] or Lewis-Kekulé structures.  Skeletal formulae have become ubiquitous in organic chemistry, partly because they are relatively quick and simple to draw, and also because the curved arrow notation used for discussions of reaction mechanism and/or delocalization can be readily superimposed.  
 Several other methods for depicting chemical structures are also commonly used in organic chemistry (though less frequently than skeletal formulae).  For example, conformational structures look similar to skeletal formulae and are used to depict the approximate positions of the atoms of a molecule in three-dimensional space, as a perspective drawing.  Other types of representations, e.g., Newman projections, Haworth projections and Fischer projections, also look somewhat similar to skeletal formulae.  However, there are slight differences in the conventions used, and the reader needs to be aware of them in order to understand the structural details that are encoded in these depictions.  While skeletal and conformational structures are also used in organometallic and inorganic chemistry, the conventions employed also differ somewhat.
"
Sol_(colloid),Chemistry,1,"A sol is a colloid made out of very small solid particles[1] in a continuous liquid medium.  Sols are quite stable and show the Tyndall effect. Examples include blood, pigmented ink, cell fluids, paint, antacids and mud.
 
Artificial sols may be prepared by dispersion or condensation. Dispersion techniques include grinding solids to colloidal dimensions by ball milling and Bredig's arc method. The stability of sols may be maintained by using dispersing agents.
 Sols are commonly used as part of the sol–gel process.
 A sol generally has a liquid as the dispersing medium and solid as a dispersed phase.
 Properties of a Colloid (applicable to sols)
"
Solid,Chemistry,1,"Solid is one of the four fundamental states of matter (the others being liquid, gas and plasma). The molecules in a solid are closely packed together and contain the least amount of kinetic energy. A solid is characterized by structural rigidity and resistance to a force applied to the surface. Unlike a liquid, a solid object does not flow to take on the shape of its container, nor does it expand to fill the entire available volume like a gas. The atoms in a solid are bound to each other, either in a regular geometric lattice (crystalline solids, which include metals and ordinary ice), or irregularly (an amorphous solid such as common window glass). Solids cannot be compressed with little pressure whereas gases can be compressed with little pressure because the molecules in a gas are loosely packed.
 The branch of physics that deals with solids is called solid-state physics, and is the main branch of condensed matter physics (which also includes liquids). Materials science is primarily concerned with the physical and chemical properties of solids. Solid-state chemistry is especially concerned with the synthesis of novel materials, as well as the science of identification and chemical composition.
"
Solubility,Chemistry,1,"
 Solubility is the property of a solid, liquid or gaseous chemical substance called solute to dissolve in a solid, liquid or gaseous solvent. The solubility of a substance fundamentally depends on the physical and chemical properties of the solute and solvent as well as on temperature, pressure and presence of other chemicals (including changes to the pH) of the solution. The extent of the solubility of a substance in a specific solvent is measured as the saturation concentration, where adding more solute does not increase the concentration of the solution and begins to precipitate the excess amount of solute.
 Insolubility is the inability to dissolve in a solid, liquid or gaseous solvent.
 Most often, the solvent is a liquid, which can be a pure substance or a mixture. One may also speak of solid solution, but rarely of solution in a gas (see vapor–liquid equilibrium instead).
 Under certain conditions, the equilibrium solubility can be exceeded to give a so-called supersaturated solution, which is metastable.[1] Metastability of crystals can also lead to apparent differences in the amount of a chemical that dissolves depending on its crystalline form or particle size. A supersaturated solution generally crystallises when 'seed' crystals are introduced and rapid equilibration occurs. Phenylsalicylate is one such simple observable substance when fully melted and then cooled below its fusion point.
 Solubility is not to be confused with the ability to dissolve a substance, because the solution might also occur because of a chemical reaction. For example, zinc dissolves (with effervescence) in hydrochloric acid as a result of a chemical reaction releasing hydrogen gas in a displacement reaction. The zinc ions are soluble in the acid.
 The solubility of a substance is an entirely different property from the rate of solution, which is how fast it dissolves. The smaller a particle is, the faster it dissolves although there are many factors to add to this generalization.
 Crucially, solubility applies to all areas of chemistry, geochemistry, inorganic, physical, organic and biochemistry. In all cases it will depend on the physical conditions (temperature, pressure and concentration) and the enthalpy and entropy directly relating to the solvents and solutes concerned.
By far the most common solvent in chemistry is water which is a solvent for most ionic compounds as well as a wide range of organic substances. This is a crucial factor in acidity and alkalinity and much environmental and geochemical work.
"
Solution,Chemistry,1,"
 In chemistry, a solution is a special type of homogeneous mixture composed of two or more substances. In such a mixture, a solute is a substance dissolved in another substance, known as a solvent. The mixing process of a solution happens at a scale where the effects of chemical polarity are involved, resulting in interactions that are specific to solvation. The solution usually has the state of the solvent when the solvent is the larger fraction of the mixture, as is commonly the case. One important parameter of a solution is the concentration, which is a measure of the amount of solute in a given amount of solution or solvent. The term ""aqueous solution"" is used when one of the solvents is water.[1]"
Solution,Chemistry,1,"
 In chemistry, a solution is a special type of homogeneous mixture composed of two or more substances. In such a mixture, a solute is a substance dissolved in another substance, known as a solvent. The mixing process of a solution happens at a scale where the effects of chemical polarity are involved, resulting in interactions that are specific to solvation. The solution usually has the state of the solvent when the solvent is the larger fraction of the mixture, as is commonly the case. One important parameter of a solution is the concentration, which is a measure of the amount of solute in a given amount of solution or solvent. The term ""aqueous solution"" is used when one of the solvents is water.[1]"
Solvated_electron,Chemistry,1,"A solvated electron is a free electron in (solvated in) a solution, and is the smallest possible anion. Solvated electrons occur widely, although it is difficult to observe them directly because their lifetimes are so short.[1] The deep color of solutions of alkali metals in liquid ammonia arises from the presence of solvated electrons: blue when dilute and copper-colored when more concentrated (> 3 molar).[2] Classically, discussions of solvated electrons focus on their solutions in ammonia, which are stable for days, but solvated electrons also occur in water and other solvents –  in fact, in any solvent that mediates outer-sphere electron transfer. The real hydration energy of the solvated electron can be estimated by using the hydration energy of a proton in water combined with kinetic data from pulse radiolysis experiments. The solvated electron forms an acid–base pair with atomic hydrogen.
 The solvated electron is responsible for a great deal of radiation chemistry.
 Alkali metals dissolve in liquid ammonia giving deep blue solutions, which conduct electricity. The blue colour of the solution is due to ammoniated electrons, which absorb energy in the visible region of light. Alkali metals also dissolve in some small primary amines, such as methylamine and ethylamine[3] and hexamethylphosphoramide, forming blue solutions.  Solvated electron solutions of the alkaline earth metals magnesium, calcium, strontium and barium in ethylenediamine have been used to intercalate graphite with these metals[4].
"
Solvation,Chemistry,1,"Solvation describes the interaction of solvent with dissolved molecules. Both ionized and uncharged molecules interact strongly with solvent, and the strength and nature of this interaction influence many properties of the solute, including solubility, reactivity, and color, as well as influencing the properties of the solvent such as the viscosity and density.[1] In the process of solvation, ions are surrounded by a concentric shell of solvent.  Solvation is the process of reorganizing solvent and solute molecules into solvation complexes.  Solvation involves bond formation, hydrogen bonding, and van der Waals forces.  Solvation of a solute by water is called hydration.[2] Solubility of solid compounds depends on a competition between lattice energy and solvation, including entropy effects related to changes in the solvent structure.[3]"
Solvation_shell,Chemistry,1,"A solvation shell is the solvent interface of any chemical compound or biomolecule that constitutes the solute.  When the solvent is water it is often referred to as a hydration shell or hydration sphere. The number of solvent molecules surrounding each unit of solute is called the hydration number of the solute.
 A classic example is when water molecules arrange around a metal ion. For example, if the latter were a cation, the electronegative oxygen atom of the water molecule would be attracted electrostatically to the positive charge on the metal ion. The result is a solvation shell of water molecules that surround the ion. This shell can be several molecules thick, dependent upon the charge of the ion, its distribution and spatial dimensions.
 A number of molecules of solvent are involved in the solvation shell around anions and cations from a dissolved salt in a solvent. Metal ions in aqueous solutions form metal aquo complexes. This number can be determined by various methods like compressibility and NMR measurements among others.
"
Solvent,Chemistry,1,"
 A solvent (from the Latin solvō, ""loosen, untie, solve"") is a substance that dissolves a solute, resulting in a solution. A solvent is usually a liquid but can also be a solid, a gas, or a supercritical fluid. The quantity of solute that can dissolve in a specific volume of solvent varies with temperature.  Major uses of solvents are in paints, paint removers, inks, dry cleaning.[1] Specific uses for organic solvents are in dry cleaning (e.g. tetrachloroethylene), as paint thinners (e.g. toluene, turpentine), as nail polish removers and glue solvents (acetone, methyl acetate, ethyl acetate), in spot removers (e.g. hexane, petrol ether), in detergents (citrus terpenes) and in perfumes (ethanol). Water is a solvent for polar molecules and the most common solvent used by living things; all the ions and proteins in a cell are dissolved in water within the cell. Solvents find various applications in chemical, pharmaceutical, oil, and gas industries, including in chemical syntheses and purification processes.
"
Spectrochemistry,Chemistry,1,"Spectrochemistry is the application of spectroscopy in any of several fields of chemistry.
 It includes the analysis of spectra in chemical terms, and the use of spectra to derive the structure of chemical compounds, and to qualitatively and quantitatively analyze their presence in samples.
"
Spectroscopy,Chemistry,1,"Spectroscopy is the study of the interaction between matter and electromagnetic radiation as a function of the wavelength or frequency of the radiation.[1][2][3][4][5][6] Historically, spectroscopy originated as the study of the wavelength dependence  of the absorption by gas phase matter of visible light dispersed by a prism. Matter waves and acoustic waves can also be considered forms of radiative energy, and recently gravitational waves have been associated with a spectral signature in the context of the Laser Interferometer Gravitational-Wave Observatory (LIGO).
 Spectroscopy, primarily in the electromagnetic spectrum, is a fundamental exploratory tool in the fields of physics, chemistry, and astronomy, allowing the composition, physical structure and electronic structure of matter to be investigated at the atomic, molecular and macro scale, and over astronomical distances. Important applications arise from biomedical spectroscopy in the areas of tissue analysis and medical imaging.
"
Standard_solution,Chemistry,1,"In analytical chemistry, a standard solution is a solution containing a precisely known concentration of an element or a substance. A known weight of solute is dissolved to make a specific volume. It is prepared using a standard substance, such as a primary standard. Standard solutions are used to determine the concentrations of other substances, such as solutions in titration. The concentrations of standard solutions are normally expressed in units of moles per litre (mol/L, often abbreviated to M for molarity), moles per cubic decimetre (mol/dm3), kilomoles per cubic metre (kmol/m3) or in terms related to those used in particular titrations (such as titres).
A simple standard is obtained by the dilution of a single element or a substance in a soluble solvent with which it reacts.
A primary standard is a reagent that is extremely pure, stable, has no waters of hydration and has high molecular weight. Some primary standards of titration of acids include sodium carbonate.
"
Standard_conditions_of_temperature_and_pressure,Chemistry,1,"Standard temperature and pressure are standard sets of conditions for experimental measurements to be established to allow comparisons to be made between different sets of data. The most used standards are those of the International Union of Pure and Applied Chemistry (IUPAC) and the National Institute of Standards and Technology (NIST), although these are not universally accepted standards. Other organizations have established a variety of alternative definitions for their standard reference conditions.
 In chemistry, IUPAC changed the definition of standard temperature and pressure (STP) in 1982: [1][2] STP should not be confused with the standard state commonly used in thermodynamic evaluations of the Gibbs energy of a reaction.
 NIST uses a temperature of 20 °C (293.15 K, 68 °F) and an absolute pressure of 1 atm (14.696 psi, 101.325 kPa). This standard is also called normal temperature and pressure (abbreviated as NTP). Please note that these stated values of STP used by NIST have not been verified and require a source. However, values cited in Modern Thermodynamics with Statistical Mechanics by Carl S. Helrich and A Guide to the NIST Chemistry WebBook by Peter J. Linstrom suggest a common STP in use by NIST for thermodynamic experiments is 298.15 K (25°C, 77°F) and 1 bar (14.5038 psi, 100 kPa).[3][4] The International Standard Metric Conditions for natural gas and similar fluids are 288.15 K (15.00 °C; 59.00 °F) and 101.325 kPa.[5] In industry and commerce, standard conditions for temperature and pressure are often necessary to define the standard reference conditions to express the volumes of gases and liquids and related quantities such as the rate of volumetric flow (the volumes of gases vary significantly with temperature and pressure): standard cubic meters per second (sm3/s), and normal cubic meters per second (nm3/s).
 However, many technical publications (books, journals, advertisements for equipment and machinery) simply state ""standard conditions"" without specifying them; often substituting the term with older ""normal conditions"", or ""NC"". In special cases this can lead to confusion and errors. Good practice always incorporates the reference conditions of temperature and pressure. If not stated, some room environment conditions are supposed, close to 1 atm pressure,  293 К (20 °C), and 0% humidity.
"
State_of_matter,Chemistry,1,"
 In physics, a state of matter is one of the distinct forms in which matter can exist. Four states of matter are observable in everyday life: solid, liquid, gas, and plasma. Many intermediate states are known to exist, such as liquid crystal, and some states only exist under extreme conditions, such as Bose–Einstein condensates, neutron-degenerate matter, and quark–gluon plasma, which only occur, respectively, in situations of extreme cold, extreme density, and extremely high energy. For a complete list of all exotic states of matter, see the list of states of matter.
 Historically, the distinction is made based on qualitative differences in properties. Matter in the solid state maintains a fixed volume and shape, with component particles (atoms, molecules or ions) close together and fixed into place. Matter in the liquid state maintains a fixed volume, but has a variable shape that adapts to fit its container. Its particles are still close together but move freely. Matter in the gaseous state has both variable volume and shape, adapting both to fit its container. Its particles are neither close together nor fixed in place. Matter in the plasma state has variable volume and shape, and contains neutral atoms as well as a significant number of ions and electrons, both of which can move around freely.
 The term phase is sometimes used as a synonym for state of matter, but a system can contain several immiscible phases of the same state of matter.
"
Stepwise_reaction,Chemistry,1,"A stepwise reaction[1] is a chemical reaction with one or more reaction intermediates and involving at least two consecutive elementary reactions.
 In a stepwise reaction, not all bonds are broken and formed at the same time. Hence, intermediates appear in the reaction pathway going from the reactants to the products. A stepwise reaction distinguishes itself from an elementary reaction in which the transformation is assumed to occur in a single step and to pass through a single transition state.[2] Many other terminologies are used for stepwise reactions: overall reaction, global reaction, apparent reaction, operational reaction, complex reaction, composite reaction, multiple step reaction, multistep reaction, etc.
 In contrast to elementary reactions which follow the law of mass action, the rate law of stepwise reactions is obtained by combining the rate laws of the multiple elementary steps, and can become rather complex. Moreover, when speaking about catalytic reactions, the diffusion may also limit the reaction. In general, however, there is one very slow step, which is the rate-determining step, i.e. the reaction doesn't proceed any faster than the rate-determining step proceeds.
 Organic reactions, especially when involving catalysis, are often stepwise. For example, a typical enol reaction consists of at least these elementary steps:
 Rδ+ is an electron acceptor, for example, the carbon of a carbonyl (C=O). A very strong base, usually an alkoxide, is needed for the first step.
 Reaction intermediates may be trapped in a trapping reaction. This proves the stepwise nature of the reaction and the structure of the intermediate. For example, superacids were used to prove the existence of carbocations.
"
Stereochemistry,Chemistry,1,"Stereochemistry, a subdiscipline of chemistry, involves the study of the relative spatial arrangement of atoms that form the structure of molecules and their manipulation.[1] The study of stereochemistry focuses on stereoisomers, which by definition have the same molecular formula and sequence of bonded atoms (constitution), but differ in the three-dimensional orientations of their atoms in space. For this reason, it is also known as 3D chemistry—the prefix ""stereo-"" means ""three-dimensionality"".[2] An important branch of stereochemistry is the study of chiral molecules.[3] Stereochemistry spans the entire spectrum of organic, inorganic, biological, physical and especially supramolecular chemistry. Stereochemistry includes methods for determining and describing these relationships; the effect on the physical or biological properties these relationships impart upon the molecules in question, and the manner in which these relationships influence the reactivity of the molecules in question (dynamic stereochemistry).
"
Stereoisomer,Chemistry,1,"In stereochemistry, stereoisomerism, or spatial isomerism, is a form of isomerism in which molecules have the same molecular formula and sequence of bonded atoms (constitution), but differ in the three-dimensional orientations of their atoms in space.[1][2] This contrasts with structural isomers, which share the same molecular formula, but the bond connections or their order differs. By definition, molecules that are stereoisomers of each other represent the same structural isomer.
"
Stoichiometry,Chemistry,1,"Stoichiometry /ˌstɔɪkiˈɒmɪtri/  is the calculation of reactants and products in chemical reactions in chemistry.
 Stoichiometry  is founded on the law of conservation of mass where the total mass of the reactants equals the total mass of the products, leading to the insight that the relations among quantities of reactants and products typically form a ratio of positive integers.   This means that if the amounts of the separate reactants are known, then the amount of the product can be calculated. Conversely, if one reactant has a known quantity and the quantity of the products can be empirically determined, then the amount of the other reactants can also be calculated.
 This is illustrated in the image here, where the balanced equation is:
 Here, one molecule of methane reacts with two molecules of oxygen gas to yield one molecule of carbon dioxide and two molecules  of water.  This particular chemical equation is an example of complete combustion. Stoichiometry measures these quantitative relationships, and is used to determine the amount of products and reactants that are produced or needed in a given reaction.  Describing the quantitative relationships among substances as they participate in chemical reactions is known as reaction stoichiometry.  In the example above, reaction stoichiometry measures the relationship between the quantities of methane and oxygen that react to form carbon dioxide and water.
 Because of the well known relationship of moles to atomic weights, the ratios that are arrived at by stoichiometry can be used to determine quantities by weight in a reaction described by a balanced equation.  This is called composition stoichiometry.
 Gas stoichiometry deals with reactions involving gases, where the gases are at a known temperature, pressure, and volume and can be assumed to be ideal gases. For gases, the volume ratio is ideally the same by the ideal gas law, but the mass ratio of a single reaction has to be calculated from the molecular masses of the reactants and products. In practice, due to the existence of isotopes, molar masses are used instead when calculating the mass ratio.
"
Strong_acid,Chemistry,1,"Acid strength is the tendency of an acid, symbolised by the chemical formula HA, to dissociate into a proton, H+, and an anion, A−. The dissociation of a strong acid in solution is effectively complete, except in its most concentrated solutions.
 Examples of strong acids are hydrochloric acid (HCl), perchloric acid (HClO4), nitric acid (HNO3) and sulfuric acid (H2SO4).
 A weak acid is only partially dissociated, with both the undissociated acid and its dissociation products being present, in solution, in equilibrium with each other. 
 Acetic acid (CH3COOH) is an example of a weak acid. The strength of a weak acid is quantified by its acid dissociation constant, pKa value.
 The strength of a weak organic acid may depend on substituent effects. The strength of an inorganic acid is dependent on the oxidation state for the atom to which the proton may be attached. Acid strength is solvent-dependent. For example, hydrogen chloride is a strong acid in aqueous solution, but is a weak acid when dissolved in glacial acetic acid.
"
Strong_base,Chemistry,1,"
 
 In chemistry, there are three definitions in common use of the word base, known as Arrhenius bases, Brønsted bases and Lewis bases. All definitions agree that bases are substances which react with acids as originally proposed by G.-F. Rouelle in the mid-18th century.
 Arrhenius proposed in 1884 that a base is a substance which dissociates in aqueous solution to form hydroxide ions OH−. These ions can react with hydrogen ions (H+ according to Arrhenius) from the dissociation of acids to form water in an acid-base reaction. A base was therefore a metal hydroxide such as NaOH or Ca(OH)2. Such aqueous hydroxide solutions were also described by certain characteristic properties. They are  slippery to the touch, can taste bitter[1] and change the color of pH indicators (e.g., turn red litmus paper blue).
 In water, by altering the autoionization equilibrium, bases yield solutions in which the hydrogen ion activity is lower than it is in pure water, i.e., the water has a pH higher than 7.0 at standard conditions. A soluble base is called an alkali if it contains and releases OH− ions quantitatively. Metal oxides, hydroxides, and especially alkoxides are basic, and conjugate bases of weak acids are weak bases.
 Bases and acids are seen as chemical opposites because the effect of an acid is to increase the hydronium (H3O+) concentration in water, whereas bases reduce this concentration. A reaction between aqueous solutions of an acid and a base is called neutralization, producing a solution of water and a salt in which the salt separates into its component ions. If the aqueous solution is saturated with a given salt solute, any additional such salt precipitates out of the solution.
 In the more general Brønsted–Lowry acid–base theory (1923), a base is a substance that can accept hydrogen cations (H+)—otherwise known as protons. This does include aqueous hydroxides since OH− does react with H+ to form water, so that Arrhenius bases are a subset of Brønsted bases. However there are also other Brønsted bases which accept protons, such as aqueous solutions of ammonia (NH3) or its organic derivatives (amines).[2] These bases do not contain a hydroxide ion but nevertheless react with water, resulting in an increase in the concentration of hydroxide ion.[3] Also, some non-aqueous solvents contain Brønsted bases which react with solvated protons. For example in liquid ammonia, NH2− is the basic ion species which accepts protons from NH4+, the acidic species in this solvent.
 G. N. Lewis realized that water, ammonia and other bases can form a bond with a proton due to the unshared pair of electrons that the bases possess.[3]  In the Lewis theory, a base is an electron pair donor which can share a pair of electrons with an electron acceptor which is described as a Lewis acid.[4] The Lewis theory is more general than the Brønsted model because the Lewis acid is not necessarily a proton, but can be another molecule (or ion) with a vacant low-lying orbital which can accept a pair of electrons. One notable example is boron trifluoride (BF3).
 Some other definitions of both bases and acids have been proposed in the past, but are not commonly used today.
"
Structural_formula,Chemistry,1,"The structural formula of a chemical compound is a graphic representation of the molecular structure (determined by structural chemistry methods), showing how the atoms are possibly arranged in the real three-dimensional space. The chemical bonding within the molecule is also shown, either explicitly or implicitly. Unlike chemical formulas, which have a limited number of symbols and are capable of only limited descriptive power, structural formulas provide a more complete geometric representation of the molecular structure. For example, many chemical compounds exist in different isomeric forms, which have different enantiomeric structures but the same chemical formula.
 Several systematic chemical naming formats, as in chemical databases, are used that are equivalent to, and as powerful as, geometric structures. These chemical nomenclature systems include SMILES, InChI and CML. These systematic chemical names can be converted to structural formulas and vice versa, but chemists nearly always describe a chemical reaction or synthesis using structural formulas rather than chemical names, because the structural formulas allow the chemist to visualize the molecules and the structural changes that occur in them during chemical reactions.
"
Structural_isomer,Chemistry,1,"In chemistry, a structural isomer (or constitutional isomer  in the  IUPAC nomenclature[1]) of a compound is another compound whose molecule has the same number of atoms of each element, but with logically distinct bonds between them.[2][3]  The term metamer was formerly used for the same concept.[4] For example, butanol H3C–(CH2)3–OH, methyl propyl ether H3C–(CH2)2–O–CH3, and diethyl ether (H3C–CH2–)2O have the same molecular formula C4H10O but are three distinct structural isomers.
 The concept applies also to polyatomic ions with the same total charge.  A classical example is the cyanate ion O=C=N− and the fulminate ion C−≡N+O−. It is also extended to ionic compounds, so that (for example) ammonium cyanate  [NH4]+ [O=C=N]− and urea (H2N–)2C=O are considered structural isomers,[4] and so are methylammonium formate [H3C–NH3]+ [HCO2]− and ammonium acetate [NH4]+ [H3C–CO2]−.
 Structural isomerism is the most radical type of isomerism.  It is opposed to stereoisomerism, in which the atoms and bonding scheme are the same, but only the relative spatial arrangement of the atoms is different.[5][6] Examples of the latter are the enantiomers, whose molecules are mirror images of each other, and the cis and trans versions of 2-butene.
 Among the structural isomers, one can distinguish several classes including skeletal isomers, positional  isomers (or regioisomers), functional isomers, tautomers, and structural topoisomers.[7]"
Subatomic_particle,Chemistry,1,"In the physical sciences, subatomic particles are smaller than atoms.[1] They can be composite particles, such as the neutron and proton; or elementary particles, which according to the standard model are not made of other particles.[2] Particle physics and nuclear physics study these particles and how they interact.[3]
The concept of a subatomic particle was refined when experiments showed that light could behave like a stream of particles (called photons) as well as exhibiting wave-like properties. This led to the concept of wave–particle duality to reflect that quantum-scale particles behave like both particles and waves (they are sometimes described as wavicles to reflect this[citation needed]). Another concept, the uncertainty principle, states that some of their properties taken together, such as their simultaneous position and momentum, cannot be measured exactly.[4] The wave–particle duality has been shown to apply not only to photons but to more massive particles as well.[5] Interactions of particles in the framework of quantum field theory are understood as creation and annihilation of quanta of corresponding fundamental interactions. This blends particle physics with field theory.
 Even among particle physicists, the exact definition of a particle has diverse descriptions.  These professional attempts at the definition of a particle include:  
"
Sublimation_(chemistry),Chemistry,1,"
 Sublimation is the transition of a substance directly from the solid to the gas state,[1] without passing through the liquid state.[2] Sublimation is an endothermic process that occurs at temperatures and pressures below a substance's triple point in its phase diagram, which corresponds to the lowest pressure at which the substance can exist as a liquid. The reverse process of sublimation is deposition or desublimation, in which a substance passes directly from a gas to a solid phase.[3] Sublimation has also been used as a generic term to describe a solid-to-gas transition (sublimation) followed by a gas-to-solid transition (deposition).[4] While vaporization from liquid to gas occurs as evaporation from the surface if it occurs below the boiling point of the liquid, and as boiling with formation of bubbles in the interior of the liquid if it occurs at the boiling point, there is no such distinction for the solid-to-gas transition which always occurs as sublimation from the surface.
 At normal pressures, most chemical compounds and elements possess three different states at different temperatures. In these cases, the transition from the solid to the gaseous state requires an intermediate liquid state. The pressure referred to is the partial pressure of the substance, not the total (e.g. atmospheric) pressure of the entire system. So, all solids that possess an appreciable vapour pressure at a certain temperature usually can sublime in air (e.g. water ice just below 0 °C). For some substances, such as carbon and arsenic, sublimation is much easier than evaporation from the melt, because the pressure of their triple point is very high, and it is difficult to obtain them as liquids.
 The term sublimation refers to a physical change of state and is not used to describe the transformation of a solid to a gas in a chemical reaction. For example, the dissociation on heating of solid ammonium chloride into hydrogen chloride and ammonia is not sublimation but a chemical reaction. Similarly the combustion of candles, containing paraffin wax, to carbon dioxide and water vapor is not sublimation but a chemical reaction with oxygen.
 Sublimation is caused by the absorption of heat which provides enough energy for some molecules to overcome the attractive forces of their neighbors and escape into the vapor phase. Since the process requires additional energy, it is an endothermic change. The enthalpy of sublimation (also called heat of sublimation) can be calculated by adding the enthalpy of fusion and the enthalpy of vaporization.
"
Chemical_substance,Chemistry,1,"A chemical substance is a form of matter having constant chemical composition and characteristic properties.[1][2] Some references add that chemical substance cannot be separated into its constituent elements by physical separation methods, i.e., without breaking chemical bonds.[3] Chemical substances can be simple substances,[4] chemical compounds, or alloys. Chemical elements may or may not be included in the definition, depending on expert viewpoint.[4] Chemical substances are often called 'pure' to set them apart from mixtures. A common example of a chemical substance is pure water; it has the same properties and the same ratio of hydrogen to oxygen whether it is isolated from a river or made in a laboratory. Other chemical substances commonly encountered in pure form are diamond (carbon), gold, table salt (sodium chloride) and refined sugar (sucrose). However, in practice, no substance is entirely pure, and chemical purity is specified according to the intended use of the chemical.
 Chemical substances exist as solids, liquids, gases, or plasma, and may change between these phases of matter with changes in temperature or pressure. Chemical substances may be combined or converted to others by means of chemical reactions.
 Forms of energy, such as light and heat, are not matter, and are thus not ""substances"" in this regard.
"
Substituent,Chemistry,1,"
 In organic chemistry and biochemistry, a substituent is an atom or group of atoms which replaces one or more hydrogen atoms on the parent chain of a hydrocarbon, becoming a moiety of the resultant new molecule. The terms substituent and functional group, as well as other ones (e.g. side chain, pendant group) are used almost interchangeably to describe branches from a parent structure,[1] though certain distinctions are made in the context of polymer chemistry.[2] In polymers, side chains extend from a backbone structure. In proteins, side chains are attached to the alpha carbon atoms of the amino acid backbone.
 The suffix -yl is used when naming organic compounds that contain a single bond replacing one hydrogen; -ylidene and -ylidyne are used with double bonds and triple bonds, respectively.  In addition, when naming hydrocarbons that contain a substituent, positional numbers are used to indicate which carbon atom the substituent attaches to when such information is needed to distinguish between isomers. The polar effect exerted by a substituent is a combination of the inductive effect and the mesomeric effect. Additional steric effects result from the volume occupied by a substituent.
 The phrases most-substituted and least-substituted are frequently used to describe or compare molecules that are products of a chemical reaction. In this terminology, methane is used as a reference of comparison. Using methane as a reference, for each hydrogen atom that is replaced or ""substituted"" by something else, the molecule can be said to be more highly substituted. For example:
"
Suspension_(chemistry),Chemistry,1,"In chemistry, a suspension is a heterogeneous mixture that contains solid particles sufficiently large for sedimentation. The particles may be visible to the naked eye, usually must be larger than one micrometer, and will eventually settle, although the mixture is only classified as a suspension when and while the particles have not settled out.
"
Tarnish,Chemistry,1,"Tarnish is a thin layer of corrosion that forms over copper, brass, aluminum, magnesium, neodymium and other similar metals as their outermost layer undergoes a chemical reaction. Tarnish does not always result from the sole effects of oxygen in the air. For example, silver needs hydrogen sulfide to tarnish, although it may tarnish with oxygen over time. It often appears as a dull, gray or black film or coating over metal. Tarnish is a surface phenomenon that is self-limiting, unlike rust. Only the top few layers of the metal react, and the layer of tarnish seals and protects the underlying layers from reacting.
 Tarnish actually preserves the underlying metal in outdoor use, and in this form is called patina. The formation of patina is necessary in applications such as copper roofing, and outdoor copper, bronze, and brass statues and fittings. Patina is the name given to tarnish on copper based metals, while Toning is a term for the type of tarnish which forms on coins.
"
Temperature,Chemistry,1,"
 Temperature is a physical quantity that expresses hot and cold. It is the manifestation of thermal energy, present in all matter, which is the source of the occurrence of heat, a flow of energy, when a body is in contact with another that is colder.
 Temperature is measured with a thermometer. Thermometers are calibrated in various temperature scales that historically have used various reference points and thermometric substances for definition. The most common scales are the Celsius scale (formerly called centigrade, denoted °C), the Fahrenheit scale (denoted °F), and the Kelvin scale (denoted K), the last of which is predominantly used for scientific purposes by conventions of the International System of Units (SI). 
 The lowest theoretical temperature is absolute zero, at which no more thermal energy can be extracted from a body. Experimentally, it can only be approached very closely, but not reached, which is recognized in the third law of thermodynamics.
 Temperature is important in all fields of natural science, including physics, chemistry, Earth science, astronomy, medicine, biology, ecology and geography as well as most aspects of daily life.
"
Yield_(chemistry),Chemistry,1,"
 In chemistry, yield, also referred to as reaction yield, is a measure of the quantity of moles of a product formed in relation to the reactant consumed, obtained in a chemical reaction, usually expressed as a percentage. [1]  Yield is one of the primary factors that scientists must consider in organic and inorganic chemical synthesis processes.[2] In chemical reaction engineering, ""yield"",  ""conversion"" and ""selectivity"" are terms used to describe ratios of how much of a reactant was consumed (conversion),  how much desired product was formed (yield) in relation to the undesired product (selectivity), represented as X, Y, and S.
"
Thermal_conductivity,Chemistry,1,"The thermal conductivity of a material is a measure of its ability to conduct heat. It is commonly denoted by 



k


{  k}
, 



λ


{  \lambda }
, or 



κ


{  \kappa }
.
 Heat transfer occurs at a lower rate in materials of low thermal conductivity than in materials of high thermal conductivity. For instance, metals typically have high thermal conductivity and are very efficient at conducting heat, while the opposite is true for insulating materials like Styrofoam. Correspondingly, materials of high thermal conductivity are widely used in heat sink applications, and materials of low thermal conductivity are used as thermal insulation. The reciprocal of thermal conductivity is called thermal resistivity.
 The defining equation for thermal conductivity is 




q

=
−
k
∇
T


{  \mathbf {q} =-k\nabla T}
, where 




q



{  \mathbf {q} }
 is the heat flux, 



k


{  k}
 is the thermal conductivity, and 



∇
T


{  \nabla T}
 is the temperature gradient. This is known as Fourier's Law for heat conduction. Although commonly expressed as a scalar, the most general form of thermal conductivity is a second-rank tensor. However, the tensorial description only becomes necessary in materials which are anisotropic.
"
Thermochemistry,Chemistry,1,"Thermochemistry is the study of the heat energy which is associated with chemical reactions and/or physical transformations.  A reaction may release or absorb energy, and a phase change may do the same, such as in melting and boiling.  Thermochemistry focuses on these energy changes, particularly on the system's energy exchange with its surroundings.  Thermochemistry is useful in predicting reactant and product quantities throughout the course of a given reaction. In combination with entropy determinations, it is also used to predict whether a reaction is spontaneous or non-spontaneous, favorable or unfavorable.
 Endothermic reactions absorb heat, while  exothermic reactions release heat.  Thermochemistry coalesces the concepts of thermodynamics with the concept of energy in the form of chemical bonds.  The subject commonly includes calculations of such quantities as heat capacity, heat of combustion, heat of formation, enthalpy, entropy, free energy, and calories.
"
Thermodynamics,Chemistry,1,"
 Thermodynamics is a branch of physics that deals with heat, work, and temperature, and their relation to energy, radiation, and physical properties of matter. The behavior of these quantities is governed by the four laws of thermodynamics which convey a quantitative description using measurable macroscopic physical quantities, but may be explained in terms of microscopic constituents by statistical mechanics. Thermodynamics applies to a wide variety of topics in science and engineering, especially physical chemistry, chemical engineering and mechanical engineering, but also in other complex fields such as meteorology.
 Historically, thermodynamics developed out of a desire to increase the efficiency of early steam engines, particularly through the work of French physicist Nicolas Léonard Sadi Carnot (1824) who believed that engine efficiency was the key that could help France win the Napoleonic Wars.[1] Scots-Irish physicist Lord Kelvin was the first to formulate a concise definition of thermodynamics in 1854[2] which stated, ""Thermo-dynamics is the subject of the relation of heat to forces acting between contiguous parts of bodies, and the relation of heat to electrical agency.""
 The initial application of thermodynamics to mechanical heat engines was quickly extended to the study of chemical compounds and chemical reactions. Chemical thermodynamics studies the nature of the role of entropy in the process of chemical reactions and has provided the bulk of expansion and knowledge of the field.[3][4][5][6][7][8][9][10][11] Other formulations of thermodynamics emerged. Statistical thermodynamics, or statistical mechanics, concerns itself with statistical predictions of the collective motion of particles from their microscopic behavior. In 1909, Constantin Carathéodory presented a purely mathematical approach in an axiomatic formulation, a description often referred to as geometrical thermodynamics.
"
Chemical_stability,Chemistry,1,"Chemical stability when used in the technical sense in chemistry, means thermodynamic stability of a chemical system.[1] Thermodynamic stability occurs when a system is in its lowest energy state, or chemical equilibrium with its environment.  This may be a dynamic equilibrium, where individual atoms or molecules change form, but their overall number in a particular form is conserved.  This type of chemical thermodynamic equilibrium will persist indefinitely unless the system is changed.  Chemical  systems might include changes in the phase of matter or a set of chemical reactions.
 State A is said to be more thermodynamically stable than state B if the Gibbs energy of the change from A to B is positive.
"
Thermometer,Chemistry,1,"A thermometer is a device that measures temperature or a temperature gradient (the degree of hotness or coldness of an object). A thermometer has two important elements: (1) a temperature sensor (e.g. the bulb of a mercury-in-glass thermometer or the pyrometric sensor in an infrared thermometer) in which some change occurs with a change in temperature; and (2) some means of converting this change into a numerical value (e.g. the visible scale that is marked on a mercury-in-glass thermometer or the digital readout on an infrared model). Thermometers are widely used in technology and industry to monitor processes, in meteorology, in medicine, and in scientific research.
 Some of the principles of the thermometer were known to Greek philosophers of two thousand years ago. As Henry Carrington Bolton (1900) noted, the thermometer's ""development from a crude toy to an instrument of precision occupied more than a century, and its early history is encumbered with erroneous statements that have been reiterated with such dogmatism that they have received the false stamp of authority.""[2] The Italian physician Santorio Santorio (Sanctorius, 1561-1636)[3] is commonly credited with the invention of the first thermometer, but its standardisation was completed through the 17th and 18th centuries.[4][5][6] In the first decades of the 18th century in the Dutch Republic, Daniel Gabriel Fahrenheit[7] made two revolutionary breakthroughs in the history of thermometry. He invented the mercury-in-glass thermometer (first widely used, accurate, practical thermometer)[2][1] and Fahrenheit scale (first standardized temperature scale to be widely used).[2]"
Titration,Chemistry,1,"Titration (also known as titrimetry[1] and volumetric analysis) is a common laboratory method of quantitative chemical analysis to determine the concentration of an identified analyte (a substance to be analyzed).  A reagent, termed the titrant or titrator,[2] is prepared as a standard solution of known concentration and volume. The titrant reacts with a solution of analyte (which may also be termed the titrand[3]) to determine the analyte's concentration. The volume of titrant that reacted with the analyte is termed the titration volume.
"
Torr,Chemistry,1,"
 The torr (symbol: Torr) is a unit of pressure based on an absolute scale, defined as exactly 1/760 of a standard atmosphere (101325 Pa).  Thus one torr is exactly 101325/760 pascals (≈ 133.32 Pa).
 Historically, one torr was intended to be the same as one ""millimeter of mercury"", but subsequent redefinitions of the two units made them slightly different (by less than 0.000015%).  The torr is not part of the International System of Units (SI). It is often combined with the metric prefix milli to name one millitorr (mTorr) or 0.001 Torr.
 The unit was named after Evangelista Torricelli, an Italian physicist and mathematician who discovered the principle of the barometer in 1644.[1]"
Transition_metal,Chemistry,1,"
 In chemistry, the term transition metal (or transition element) has three possible definitions:
 English chemist Charles Bury (1890–1968) first used the word transition in this context in 1921, when he referred to a transition series of elements during the change of an inner layer of electrons (for example n = 3 in the 4th row of the periodic table) from a stable group of 8 to one of 18, or from 18 to 32.[5][6][7] These elements are now known as the d-block.
"
Transuranic_element,Chemistry,1,"The transuranium elements (also known as transuranic elements) are the chemical elements with atomic numbers greater than 92, which is the atomic number of uranium. All of these elements are unstable and decay radioactively into other elements.
"
Multiple_bond,Chemistry,1,"Bond order, as introduced by Linus Pauling, is defined as the difference between the number of bonds and anti-bonds.
 The bond number itself is the number of electron pairs (bonds) between a pair of atoms.[1] For example, in diatomic nitrogen N≡N the bond number is 3, in ethyne H−C≡C−H the bond number between the two carbon atoms is also 3, and the C−H bond order is 1. Bond number gives an indication of the stability of a bond. Isoelectronic species have same bond number.[2] In molecules which have resonance or nonclassical bonding, bond number may not be an integer. In benzene, the delocalized molecular orbitals contain 6 pi electrons over six carbons essentially yielding half a pi bond together with the sigma bond for each pair of carbon atoms, giving a calculated bond number of 1.5. Furthermore, bond numbers of 1.1, for example, can arise under complex scenarios and essentially refer to bond strength relative to bonds with order 1.
"
Triple_point,Chemistry,1,"In thermodynamics, the triple point of a substance is the temperature and pressure at which the three phases (gas, liquid, and solid) of that substance coexist in thermodynamic equilibrium.[1] It is that temperature and pressure at which the sublimation curve, fusion curve and the vaporisation curve meet. For example, the triple point of mercury occurs at a temperature of −38.83440 °C (−37.90192 °F) and a pressure of 0.2 mPa.
 In addition to the triple point for solid, liquid, and gas phases, a triple point may involve more than one solid phase, for substances with multiple polymorphs. Helium-4 is a special case that presents a triple point involving two different fluid phases (lambda point).[1] The triple point of water was used to define the kelvin, the base unit of thermodynamic temperature in the International System of Units (SI).[2] The value of the triple point of water was fixed by definition, rather than measured, but that changed with the 2019 redefinition of SI base units. The triple points of several substances are used to define points in the ITS-90 international temperature scale, ranging from the triple point of hydrogen (13.8033 K) to the triple point of water (273.16 K, 0.01 °C, or 32.018 °F).
 The term ""triple point"" was coined in 1873 by James Thomson, brother of Lord Kelvin.[3]"
Tyndall_effect,Chemistry,1,"The Tyndall effect is light scattering by particles in a colloid or in a very fine suspension. Also known as Tyndall scattering, it is similar to Rayleigh scattering, in that the intensity of the scattered light is inversely proportional to the fourth power of the wavelength, so blue light is scattered much more strongly than red light. An example in everyday life is the blue colour sometimes seen in the smoke emitted by motorcycles, in particular two-stroke machines where the burnt engine oil provides these
particles. 
 Under the Tyndall effect, the longer wavelengths are more transmitted while the shorter wavelengths are more diffusely reflected via scattering. The Tyndall effect is seen when light-scattering particulate matter is dispersed in an otherwise light-transmitting medium, when the diameter of an individual particle is the range of roughly between 40 and 900 nm, i.e. somewhat below or near the wavelengths of visible light (400–750 nm).
 It is particularly applicable to colloidal mixtures and fine suspensions; for example, the Tyndall effect is used in nephelometers to determine the size and density of particles in aerosols and other colloidal matter (see ultramicroscope and turbidimeter).
 It is named after the 19th-century physicist John Tyndall.
"
UN_number,Chemistry,1,"A UN number (United Nations number) is a four-digit number that identifies hazardous materials, and articles (such as explosives, flammable liquids, oxidizers, toxic liquids, etc.) in the framework of international transport. Some hazardous substances have their own UN numbers (e.g. acrylamide has UN 2074), while sometimes groups of chemicals or products with similar properties receive a common UN number (e.g. flammable liquids, not otherwise specified, have UN 1993). A chemical in its solid state may receive a different UN number than the liquid phase if their hazardous properties differ significantly; substances with different levels of purity (or concentration in solution) may also receive different UN numbers.
 UN numbers range from UN 0004 to about UN 3548 (UN 0001 – UN 0003 are no longer in use) and are assigned by the United Nations Committee of Experts on the Transport of Dangerous Goods. They are published as part of their Recommendations on the Transport of Dangerous Goods, also known as the Orange Book. These recommendations are  adopted by the regulatory organization responsible for the different modes of transport. There is no UN number allocated to non-hazardous substances.
 An NA number (North America number) is issued by the United States Department of Transportation and is identical to UN numbers, except that some substances without a UN number may have an NA number. These additional NA numbers use the range NA 9000 - NA 9279. There are some exceptions, for example NA 2212 is all asbestos with UN 2212 limited to Asbestos, amphibole amosite, tremolite, actinolite, anthophyllite, or crocidolite. Another exception, NA 3334, is self-defense spray, non-pressurized while UN 3334 is aviation regulated liquid, not otherwise specified. For the complete list, see NA/UN exceptions.
 An ID number is a third type of identification number used for hazardous substances being offered for Air transport. Substances with an ID number are associated with proper shipping names recognized by the ICAO Technical Instructions.[1]  ID 8000, Consumer commodity does not have a UN or NA number, and is classed as a Class 9 hazardous material. 
"
Uncertainty,Chemistry,1,"Uncertainty refers to epistemic situations involving imperfect or unknown information. It applies to predictions of future events, to physical measurements that are already made, or to the unknown. Uncertainty arises in partially observable and/or stochastic environments, as well as due to ignorance, indolence, or both.[1] It arises in any number of fields, including insurance, philosophy, physics, statistics, economics, finance, psychology, sociology, engineering, metrology, meteorology, ecology and information science.
"
Uncertainty_principle,Chemistry,1,"
 In quantum mechanics, the uncertainty principle (also known as Heisenberg's uncertainty principle) is any of a variety of mathematical inequalities[1] asserting a fundamental limit to the accuracy with which the values for certain pairs of physical quantities of a particle, such as position, x, and momentum, p, can be predicted from initial conditions. 
 Such variable pairs are known as complementary variables or canonically conjugate variables; and, depending on interpretation, the uncertainty principle limits to what extent such conjugate properties maintain their approximate meaning, as the mathematical framework of quantum physics does not support the notion of simultaneously well-defined conjugate properties expressed by a single value. The uncertainty principle implies that it is in general not possible to predict the value of a quantity with arbitrary certainty, even if all initial conditions are specified.
 Introduced first in 1927 by the German physicist Werner Heisenberg, the uncertainty principle states that the more precisely the position of some particle is determined, the less precisely its momentum can be predicted from initial conditions, and vice versa.[2] The formal inequality relating the standard deviation of position σx and the standard deviation of momentum σp was derived by Earle Hesse Kennard[3] later that year and by Hermann Weyl[4] in 1928:
 




σ

x



σ

p


≥


ℏ
2


 
 


{  \sigma _{x}\sigma _{p}\geq {\frac {\hbar }{2}}~~}

 where ħ is the reduced Planck constant, h/(2π).
 Historically, the uncertainty principle has been confused[5][6] with a related effect in physics, called the observer effect, which notes that measurements of certain systems cannot be made without affecting the system, that is, without changing something in a system. Heisenberg utilized such an observer effect at the quantum level (see below) as a physical ""explanation"" of quantum uncertainty.[7] It has since become clearer, however, that the uncertainty principle is inherent in the properties of all wave-like systems,[8] and that it arises in quantum mechanics simply due to the matter wave nature of all quantum objects. Thus, the uncertainty principle actually states a fundamental property of quantum systems and is not a statement about the observational success of current technology.[9] It must be emphasized that measurement does not mean only a process in which a physicist-observer takes part, but rather any interaction between classical and quantum objects regardless of any observer.[10][note 1] [note 2] Since the uncertainty principle is such a basic result in quantum mechanics, typical experiments in quantum mechanics routinely observe aspects of it. Certain experiments, however, may deliberately test a particular form of the uncertainty principle as part of their main research program. These include, for example, tests of number–phase uncertainty relations in superconducting[12] or quantum optics[13] systems. Applications dependent on the uncertainty principle for their operation include extremely low-noise technology such as that required in gravitational wave interferometers.[14]"
Unified_atomic_mass_unit,Chemistry,1,"The dalton or unified atomic mass unit (symbols: Da or u) is a unit of mass widely used in physics and chemistry. It is defined as 1/12 of the mass of an unbound neutral atom of carbon-12 in its nuclear and electronic ground state and at rest.[1][2] The atomic mass constant, denoted mu  is defined identically, giving mu = m(12C)/12 = 1 Da.[3] This unit is commonly used in physics and chemistry to express the mass of atomic-scale objects, such as atoms, molecules, and elementary particles, both for discrete instances and multiple types of ensemble averages. For example, an atom of helium-4 has a mass of 4.0026 Da. This is an intrinsic property of the isotope and all helium-4 have the same mass. Acetylsalicylic acid (aspirin), C9H8O4, has an average mass of approximately 180.157 Da. However, there are no acetylsalicylic acid molecules with this mass. The two most common masses of individual acetylsalicylic acid molecules are 180.04228 Da and 181.04565 Da. 
 The molecular masses of proteins, nucleic acids, and other large polymers are often expressed with the units kilodaltons (kDa), megadaltons (MDa), etc.[4] Titin, one of the largest known proteins, has a molecular mass of between 3 and 3.7 megadaltons.[5] The DNA of chromosome 1 in the human genome has about 249 million base pairs, each with an average mass of about 650 Da, or 156 GDa total.[6] The mole is a unit of amount of substance, widely used in chemistry and physics, which was originally defined so that the mass of one mole of a substance, measured in grams, would be numerically equal to the average mass of one of its constituent particles, measured in daltons. That is, the molar mass of a chemical compound was meant to be numerically equal to its average molecular mass.  For example, the average mass of one molecule of water is about 18.0153 daltons, and one mole of water is about 18.0153 grams.  A protein whose molecule has an average mass of 64 kDa would have a molar mass of 64 kg/mol. However, while this equality can be assumed for almost all practical purposes, it is now only approximate, because of the way mole was redefined on 20 May 2019.[4][1] In general, the mass in daltons of an atom is numerically close, but not exactly equal to the number of nucleons A contained in its nucleus. It follows that the molar mass of a compound (grams per mole) is numerically close to the average number of nucleons contained in each molecule. By definition, the mass of an atom of carbon-12 is 12 daltons, which corresponds with the number of nucleons that it has (6 protons and 6 neutrons). However, the mass of an atomic-scale object is affected by the binding energy of the nucleons in its atomic nuclei, as well as the mass and binding energy of its electrons.  Therefore, this equality holds only for the carbon-12 atom in the stated conditions, and will vary for other substances. For example, the mass of one unbound atom of the common hydrogen isotope (hydrogen-1, protium) is 1.007825032241(94) Da, the mass of one free neutron is 1.00866491595(49) Da,[7] and the mass of one hydrogen-2 (deuterium) atom is 2.014101778114(122) Da.[8]  In general, the difference (mass defect) is less than 0.1%; exceptions include hydrogen-1 (about 0.8%), helium-3 (0.5%), lithium (0.25%) and beryllium (0.15%). 
 The unified atomic mass unit and the dalton should not be confused with the unit of mass in the atomic units systems, which is instead the electron rest mass (me).
"
Crystal_structure,Chemistry,1,"In crystallography, crystal structure is a description of the ordered arrangement of atoms, ions or molecules in a crystalline material.[3] Ordered structures occur from the intrinsic nature of the constituent particles to form symmetric patterns that repeat along the principal directions of three-dimensional space in matter.
 The smallest group of particles in the material that constitutes this repeating pattern is the unit cell of the structure. The unit cell completely reflects the symmetry and structure of the entire crystal, which is built up by repetitive translation of the unit cell along its principal axes. The translation vectors define the nodes of the Bravais lattice.
 The lengths of the principal axes, or edges, of the unit cell and the angles between them are the lattice constants, also called lattice parameters or cell parameters. The symmetry properties of the crystal are described by the concept of space groups.[3] All possible symmetric arrangements of particles in three-dimensional space may be described by the 230 space groups.
 The crystal structure and symmetry play a critical role in determining many physical properties, such as cleavage, electronic band structure, and optical transparency.
"
Units_conversion_by_factor-label,Chemistry,1,"In engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometres, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. The conversion of units from one dimensional unit to another is often easier within the metric or SI system than in others, due to the regular 10-base in all units. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.[1][2][3] The concept of physical dimension was introduced by Joseph Fourier in 1822.[4] Physical quantities that are of the same kind (also called commensurable) (e.g., length or time or mass) have the same dimension and can be directly compared to other physical quantities of the same kind, even if they are originally expressed in differing units of measure (such as yards and metres). If physical quantities have different dimensions (such as length vs. mass), they cannot be expressed in terms of similar units and cannot be compared in quantity (also called incommensurable). For example, asking whether a kilogram is larger than an hour is meaningless.
 Any physically meaningful equation (and any inequality) will have the same dimensions on its left and right sides, a property known as dimensional homogeneity. Checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. It also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation.
"
Unpaired_electron,Chemistry,1,"In chemistry, an unpaired electron is an electron that occupies an orbital of an atom singly, rather than as part of an electron pair.  Each atomic orbital of an atom (specified by the three quantum numbers n, l and m)  has a capacity to contain two electrons (electron pair) with opposite spins. As the formation of electron pairs is often energetically favourable, either in the form of a chemical bond or as a lone pair, unpaired electrons are relatively uncommon in chemistry, because an entity that carries an unpaired electron is usually rather reactive. In organic chemistry they typically only occur briefly during a reaction on an entity called a radical; however, they play an important role in explaining reaction pathways.
 Radicals are uncommon in s- and p-block chemistry, since the unpaired electron occupies a valence p orbital or an sp, sp2 or sp3 hybrid orbital. These orbitals are strongly directional and therefore overlap to form strong covalent bonds, favouring dimerisation of radicals. Radicals can be stable if dimerisation would result in a weak bond or the unpaired electrons are stabilised by delocalisation. In contrast, radicals in d- and f-block chemistry are very common. The less directional, more diffuse d and f orbitals, in which unpaired electrons reside, overlap less effectively, form weaker bonds and thus dimerisation is generally disfavoured. These d and f orbitals also have comparatively smaller radial extension, disfavouring overlap to form dimers.[1] Relatively more stable entities with unpaired electrons do exist, e.g. the nitric oxide molecule has one. According to Hund's rule, the spins of unpaired electrons are aligned parallel and this gives these molecules paramagnetic properties.
 The most stable examples of unpaired electrons are found on the atoms and ions of lanthanides and actinides. The incomplete f-shell of these entities does not interact very strongly with the environment they are in and this prevents them from being paired. The ions with the largest number of unpaired electrons are Gd3+ and Cm3+ with seven unpaired electrons.
 An unpaired electron has a magnetic dipole moment, while an electron pair has no dipole moment because the two electrons have opposite spins so their magnetic dipole fields are in opposite directions and cancel. Thus an atom with unpaired electrons acts as a magnetic dipole and interacts with a magnetic field.  Only elements with unpaired electrons exhibit paramagnetism, ferromagnetism, and antiferromagnetism.
"
Vacuum_flask,Chemistry,1,"A vacuum flask (also known as a Dewar flask, Dewar bottle or thermos) is an insulating storage vessel that greatly lengthens the time over which its contents remain hotter or cooler than the flask's surroundings. Invented by Sir James Dewar in 1892, the vacuum flask consists of two flasks, placed one within the other and joined at the neck. The gap between the two flasks is partially evacuated of air, creating a near-vacuum which significantly reduces heat transfer by conduction or convection.
 Vacuum flasks are used domestically, to keep beverages hot or cold for extended periods of time, and for many purposes in industry.
"
Valence_electron,Chemistry,1,"In chemistry and physics, a valence electron is an outer shell electron that is associated with an atom, and that can participate in the formation of a chemical bond if the outer shell is not closed; in a single covalent bond, both atoms in the bond contribute one valence electron in order to form a shared pair.
 The presence of valence electrons can determine the element's chemical properties, such as its valence—whether it may bond with other elements and, if so, how readily and with how many. In this way, a given element's reactivity is highly dependent upon its electronic configuration. For a main group element, a valence electron can exist only in the outermost electron shell; for a transition metal, a valence electron can also be in an inner shell. 
 An atom with a closed shell of valence electrons (corresponding to an electron configuration s2p6 for main group elements or d10s2p6 for transition metals) tends to be chemically inert. Atoms with one or two valence electrons more than a closed shell are highly reactive due to the relatively low energy to remove the extra valence electrons to form a positive ion. An atom with one or two electrons less than a closed shell is reactive due to its tendency either to gain the missing valence electrons and form a negative ion, or else to share valence electrons and form a covalent bond.
 Similar to a core electron, a valence electron has the ability to absorb or release energy in the form of a photon. An energy gain can trigger the electron to move (jump) to an outer shell; this is known as atomic excitation. Or the electron can even break free from its associated atom's shell; this is ionization to form a positive ion. When an electron loses energy (thereby causing a photon to be emitted), then it can move to an inner shell which is not fully occupied.
"
Valence_bond_theory,Chemistry,1,"
 In chemistry, valence bond (VB) theory is one of the two basic theories, along with molecular orbital (MO) theory, that were developed to use the methods of quantum mechanics to explain chemical bonding. It focuses on how the atomic orbitals of the dissociated atoms combine to give individual chemical bonds when a molecule is formed. In contrast, molecular orbital theory has orbitals that cover the whole molecule.[1]"
Valence_(chemistry),Chemistry,1,"In chemistry, the valence or valency of an element is a measure of its combining capacity with other atoms when it forms chemical compounds or molecules.
"
Van_der_Waals_force,Chemistry,1,"In molecular physics, the van der Waals force, named after Dutch scientist Johannes Diderik van der Waals, is a distance-dependent interaction between atoms or molecules. Unlike ionic or covalent bonds, these attractions do not result from a chemical electronic bond; they are comparatively weak and therefore more susceptible to disturbance. The van der Waals force quickly vanishes at longer distances between interacting molecules.
 Van der Waals force plays a fundamental role in fields as diverse as supramolecular chemistry, structural biology, polymer science, nanotechnology, surface science, and condensed matter physics. It also underlies many properties of organic compounds and molecular solids, including their solubility in polar and non-polar media.
 If no other force is present, the distance between atoms at which the force becomes repulsive rather than attractive as the atoms approach one another is called the van der Waals contact distance; this phenomenon results from the mutual repulsion between the atoms' electron clouds.[1] The van der Waals force has the same origin as the Casimir effect, which arises from quantum interactions with the zero-point field.[2] The term van der Waals force is sometimes used loosely for all intermolecular forces.[3] The term always includes the London dispersion force between instantaneously induced dipoles.[citation needed] It is sometimes applied to the Debye force between a permanent dipole and a corresponding induced dipole[citation needed] or to the Keesom force between permanent molecular dipoles.[citation needed]"
Van_%27t_Hoff_factor,Chemistry,1,"
 The van 't Hoff factor i (named after Dutch chemist Jacobus Henricus van 't Hoff) is a measure of the effect of a solute on colligative properties such as osmotic pressure, relative lowering in vapor pressure, boiling-point elevation and freezing-point depression. The van 't Hoff factor  is the ratio between the actual concentration of particles produced when the substance is dissolved and the concentration of a substance as calculated from its mass.   For most non-electrolytes dissolved in water, the van 't Hoff factor is essentially 1.  For most ionic compounds dissolved in water, the van 't Hoff factor is equal to the number of discrete ions in a formula unit of the substance.  This is true for ideal solutions only, as occasionally ion pairing occurs in solution.  At a given instant a small percentage of the ions are paired and count as a single particle.  Ion pairing occurs to some extent in all electrolyte solutions.  This causes the measured van 't Hoff factor to be less than that predicted in an ideal solution.  The deviation for the van 't Hoff factor tends to be greatest where the ions have multiple charges.
"
Vapor,Chemistry,1,"
 In physics, a vapor (American English) or vapour (British English and Canadian English; see spelling differences) is a substance in the gas phase at a temperature lower than its critical temperature,[1] which means that the vapor can be condensed to a liquid by increasing the pressure on it without reducing the temperature. A vapor is different from an aerosol.[2] An aerosol is a suspension of tiny particles of liquid, solid, or both within a gas.[2] For example, water has a critical temperature of 647 K (374 °C; 705 °F), which is the highest temperature at which liquid water can exist. In the atmosphere at ordinary temperatures, therefore, gaseous water (known as water vapor) will condense into a liquid if its partial pressure is increased sufficiently.
 A vapor may co-exist with a liquid (or a solid). When this is true, the two phases will be in equilibrium, and the gas-partial pressure will be equal to the equilibrium vapor pressure of the liquid (or solid).[1]"
Vapor_pressure,Chemistry,1,"Vapor pressure (or vapour pressure in British English; see spelling differences) or equilibrium vapor pressure is defined as the pressure exerted by a vapor in thermodynamic equilibrium with its condensed phases (solid or liquid) at a given temperature in a closed system. The equilibrium vapor pressure is an indication of a liquid's evaporation rate. It relates to the tendency of particles to escape from the liquid (or a solid). A substance with a high vapor pressure at normal temperatures is often referred to as volatile. The pressure exhibited by vapor present above a liquid surface is known as vapor pressure. As the temperature of a liquid increases, the kinetic energy of its molecules also increases. As the kinetic energy of the molecules increases, the number of molecules transitioning into a vapor also increases, thereby increasing the vapor pressure.
 
 The vapor pressure of any substance increases non-linearly with temperature according to the Clausius–Clapeyron relation. The atmospheric pressure boiling point of a liquid (also known as the normal boiling point) is the temperature at which the vapor pressure equals the ambient atmospheric pressure. With any incremental increase in that temperature, the vapor pressure becomes sufficient to overcome atmospheric pressure and lift the liquid to form vapor bubbles inside the bulk of the substance. Bubble formation deeper in the liquid requires a higher temperature due to the higher fluid pressure, because fluid pressure increases above the atmospheric pressure as the depth increases. More important at shallow depths is the higher temperature required to start bubble formation. The surface tension of the bubble wall leads to an overpressure in the very small, initial bubbles. 
 The vapor pressure that a single component in a mixture contributes to the total pressure in the system is called partial pressure. For example, air at sea level, and saturated with water vapor at 20 °C, has partial pressures of about 2.3 kPa of water, 78 kPa of nitrogen, 21 kPa of oxygen and 0.9 kPa of argon, totaling 102.2 kPa, making the basis for standard atmospheric pressure.
"
Vaporization,Chemistry,1,"Vaporization (or vaporisation) of an element or compound is a phase transition from the liquid phase to vapor.[1] There are two types of vaporization: evaporation and boiling.  Evaporation is a surface phenomenon, whereas boiling is a bulk phenomenon.
 Evaporation is a phase transition from the liquid phase to vapour (a state of  substance below critical temperature) that occurs at temperatures below the boiling temperature at a given pressure. Evaporation occurs on the surface. Evaporation only occurs when the partial pressure of vapour of a substance is less than the equilibrium vapor pressure. For example, due to constantly decreasing pressures, vapor pumped out of a solution will eventually leave behind a cryogenic liquid.
 Boiling is also a phase transition from the liquid phase to gas phase, but boiling is the formation of vapor as bubbles of vapor below the surface of the liquid.  Boiling occurs when the equilibrium vapor pressure of the substance is greater than or equal to the environmental pressure.  The temperature at which boiling occurs is the boiling temperature, or boiling point.  The boiling point varies with the pressure of the environment.
 Sublimation is a direct phase transition from the solid phase to the gas phase, skipping the intermediate liquid phase.  Because it does not involve the liquid phase, it is not a form of vaporization.
 The term vaporization has also been used in a colloquial or hyperbolic way to refer to the physical destruction of an object that is exposed to intense heat or explosive force, where the object is actually blasted into small pieces rather than literally converted to gaseous form. Examples of this usage include the ""vaporization"" of the uninhabited Marshall Island of Elugelab in the 1952 Ivy Mike thermonuclear test.[2] At the moment of a large enough meteor or comet impact, bolide detonation, a nuclear fission, thermonuclear fusion, or theoretical antimatter weapon detonation, a flux of so many gamma ray, x-ray, ultraviolet, visual light and heat photons strikes matter in a such brief amount of time (a great number of high-energy photons, many overlapping in the same physical space) that all molecules lose their atomic bonds and ""fly apart"". All atoms lose their electron shells and become positively charged ions, in turn emitting photons of a slightly lower energy than they had absorbed. All such matter becomes a gas of nuclei and electrons which rise into the air due to the extremely high temperature or bond to each other as they cool. The matter vaporized this way is immediately a plasma in a state of maximum entropy and this state steadily reduces via the factor of passing time due to natural processes in the biosphere and the effects of physics at normal temperatures and pressures.
 A similar process occurs during ultrashort pulse Laser ablation, where the high flux of incoming electromagnetic radiation strips the target material's surface of electrons, leaving positively charged atoms which undergo a coulomb explosion.[3]"
Viscosity,Chemistry,1,"The viscosity of a fluid is a measure of its resistance to deformation at a given rate. For liquids, it corresponds to the informal concept of ""thickness"": for example, syrup has a higher viscosity than water.[1] Viscosity can be conceptualized as quantifying the internal frictional force that arises between adjacent layers of fluid that are in relative motion. For instance, when a fluid is forced through a tube, it flows more quickly near the tube's axis than near its walls. In such a case, experiments show that some stress (such as a pressure difference between the two ends of the tube) is needed to sustain the flow through the tube.  This is because a force is required to overcome the friction between the layers of the fluid which are in relative motion: the strength of this force is proportional to the viscosity.
 A fluid that has no resistance to shear stress is known as an ideal or inviscid fluid. Zero viscosity is observed only at very low temperatures in superfluids. Otherwise, the second law of thermodynamics requires all fluids to have positive viscosity;[2][3] such fluids are technically said to be viscous or viscid. A fluid with a high viscosity, such as pitch, may appear to be a solid.
"
Volatility_(chemistry),Chemistry,1,"In chemistry, volatility is a material quality which describes how readily a substance vaporizes. At a given temperature and pressure, a substance with high volatility is more likely to exist as a vapor, while a substance with low volatility is more likely to be a liquid or solid. Volatility can also describe the tendency of a vapor to condense into a liquid or solid; less volatile substances will more readily condense from a vapor than highly volatile ones.[1] Differences in volatility can be observed by comparing how fast a group of substances evaporate (or sublime in the case of solids) when exposed to the atmosphere. A highly volatile substance such as rubbing alcohol (isopropyl alcohol) will quickly evaporate, while a substance with low volatility such as vegetable oil will remain condensed.[2] In general, solids are much less volatile than liquids, but there are some exceptions. Solids that sublime (change directly from solid to vapor) such as dry ice (solid carbon dioxide) or iodine can vaporize at a similar rate as some liquids under standard conditions.[3]"
Volt,Chemistry,1,"The volt (symbol: V) is the derived unit for electric potential, electric potential difference (voltage), and electromotive force.[1] It is named after the Italian physicist Alessandro Volta (1745–1827).
"
Voltmeter,Chemistry,1,"A voltmeter is an instrument used for measuring electric potential difference between two points in an electric circuit. It is  connected in parallel. It usually has a high resistance so that it takes a negligible current from the circuit.
 Analog voltmeters move a pointer across a scale in proportion to the voltage of the circuit; digital voltmeters give a numerical display of voltage by use of an analog-to-digital converter. 
Voltmeters are made in a wide range of styles. Instruments permanently mounted in a panel are used to monitor generators or other fixed apparatus. Portable instruments, usually equipped to also measure current and resistance in the form of a multimeter, are standard test instruments used in electrical and electronics work. Any measurement that can be converted to a voltage can be displayed on a meter that is suitably calibrated; for example, pressure, temperature, flow or level in a chemical process plant.
 General-purpose analog voltmeters may have an accuracy of a few percent of full scale, and are used with voltages from a fraction of a volt to several thousand volts. Digital meters can be made with high accuracy, typically better than 1%. Specially calibrated test instruments have higher accuracies, with laboratory instruments capable of measuring to accuracies of a few parts per million. Meters using amplifiers can measure tiny voltages of microvolts or less.
 Part of the problem of making an accurate voltmeter is that of calibration to check its accuracy. In laboratories, the Weston cell is used as a standard voltage for precision work. Precision voltage references are available based on electronic circuits.
"
Volume,Chemistry,1,"Volume is the quantity of three-dimensional space enclosed by a closed surface, for example, the space that a substance (solid, liquid, gas, or plasma) or shape occupies or contains.[1] Volume is often quantified numerically using the SI derived unit, the cubic metre. The volume of a container is generally understood to be the capacity of the container; i. e., the amount of fluid (gas or liquid) that the container could hold, rather than the amount of space the container itself displaces.
Three dimensional mathematical shapes are also assigned volumes. Volumes of some simple shapes, such as regular, straight-edged, and circular shapes can be easily calculated using arithmetic formulas. Volumes of complicated shapes can be calculated with integral calculus if a formula exists for the shape's boundary. One-dimensional figures (such as lines) and two-dimensional shapes (such as squares) are assigned zero volume in the three-dimensional space.
 The volume of a solid (whether regularly or irregularly shaped) can be determined by fluid displacement. Displacement of liquid can also be used to determine the volume of a gas. The combined volume of two substances is usually greater than the volume of just one of the substances. However, sometimes one substance dissolves in the other and in such cases the combined volume is not additive.[2] In differential geometry, volume is expressed by means of the volume form, and is an important global Riemannian invariant.
In thermodynamics, volume is a fundamental parameter, and is a conjugate variable to pressure.
"
Volumetric_flask,Chemistry,1,"A volumetric flask (measuring flask or graduated flask) is a piece of laboratory apparatus, a type of laboratory flask, calibrated to contain a precise volume at a certain temperature. Volumetric flasks are used for precise dilutions and preparation of standard solutions. These flasks are usually pear-shaped, with a flat bottom, and made of glass or plastic. The flask's mouth is either furnished with a plastic snap/screw cap or fitted with a joint to accommodate a PTFE or glass stopper. The neck of volumetric flasks is elongated and narrow with an etched ring graduation marking. The marking indicates the volume of liquid contained when filled up to that point. The marking is typically calibrated ""to contain"" (marked ""TC"" or ""IN"") at 20 °C and indicated correspondingly on a label. The flask's label also indicates the nominal volume, tolerance, precision class, relevant manufacturing standard and the manufacturer's logo. Volumetric flasks are of various sizes, containing from 1 milliliter to 20 liters of liquid.
"
Watch_glass,Chemistry,1,"A watch glass is a circular concave piece of glass used in chemistry as a surface to evaporate a liquid, to hold solids while being weighed, for heating a small amount of substance and as a cover for a beaker. The latter use is generally applied to prevent dust or other particles entering the beaker; the watch glass does not completely seal the beaker, so gas exchanges still occur. When used as an evaporation surface, a watch glass allows closer observation of precipitates or crystallization, and can be placed on a surface of contrasting color to improve the visibility overall. Watch glasses are also sometimes used to cover a glass of whisky, to concentrate the aromas in the glass, and to prevent spills when the whisky is swirled.[1] Watch glasses are named so because they are similar to the glass used for the front of old-fashioned pocket watches. In reference to this, large watch glasses are occasionally known as clock glasses.
"
Water_(molecule),Chemistry,1,"Water (H2O) is a polar inorganic compound that is at room temperature a tasteless and odorless liquid, which is nearly colorless apart from an inherent hint of blue. It is by far the most studied chemical compound[18] and is described as the ""universal solvent""[19] and the ""solvent of life.""[20] It is the most abundant substance on Earth[21] and the only common substance to exist as a solid, liquid, and gas on Earth's surface.[22] It is also the third most abundant molecule in the universe (behind molecular hydrogen and carbon monoxide).[21] Water intoxication
(see also Dihydrogen monoxide parody)
 Water molecules form hydrogen bonds with each other and are strongly polar. This polarity allows it to dissociate ions in salts and bond to other polar substances such as alcohols and acids, thus dissolving them. Its hydrogen bonding causes its many unique properties, such as having a solid form less dense than its liquid form,[c] a relatively high boiling point of 100 °C for its molar mass, and a high heat capacity.
 Water is amphoteric, meaning that it can exhibit properties of an acid or a base, depending on the pH of the solution that it is in; it readily produces both H+ and OH− ions.[c] Related to its amphoteric character, it undergoes self-ionization. The product of the activities, or approximately, the concentrations of H+ and OH− is a constant, so their respective concentrations are inversely proportional to each other.[23]"
Wave_function,Chemistry,1,"A wave function in quantum physics is a mathematical description of the quantum state of an isolated quantum system.  The wave function is a complex-valued probability amplitude, and the probabilities for the possible results of measurements made on the system can be derived from it.  The most common symbols for a wave function are the Greek letters ψ and Ψ (lower-case and capital psi, respectively).
 The wave function is a function of the degrees of freedom corresponding to some maximal set of commuting observables.  Once such a representation is chosen, the wave function can be derived from the quantum state.
 For a given system, the choice of which commuting degrees of freedom to use is not unique, and correspondingly the domain of the wave function is also not unique.  For instance, it may be taken to be a function of all the position coordinates of the particles over position space, or the momenta of all the particles over momentum space; the two are related by a Fourier transform.  Some particles, like electrons and photons, have nonzero spin, and the wave function for such particles includes spin as an intrinsic, discrete degree of freedom; other discrete variables can also be included, such as isospin.  When a system has internal degrees of freedom, the wave function at each point in the continuous degrees of freedom (e.g., a point in space) assigns a complex number for each possible value of the discrete degrees of freedom (e.g., z-component of spin) – these values are often displayed in a column matrix (e.g., a 2 × 1 column vector for a non-relativistic electron with spin ​1⁄2).
 According to the superposition principle of quantum mechanics, wave functions can be added together and multiplied by complex numbers to form new wave functions and form a Hilbert space.  The inner product between two wave functions is a measure of the overlap between the corresponding physical states, and is used in the foundational probabilistic interpretation of quantum mechanics, the Born rule, relating transition probabilities to inner products.  The Schrödinger equation determines how wave functions evolve over time, and a wave function behaves qualitatively like other waves, such as water waves or waves on a string, because the Schrödinger equation is mathematically a type of wave equation.  This explains the name ""wave function"", and gives rise to wave–particle duality.  However, the wave function in quantum mechanics describes a kind of physical phenomenon, still open to different interpretations, which fundamentally differs from that of classic mechanical waves.[1][2][3][4][5][6][7] In Born's statistical interpretation in non-relativistic quantum mechanics,[8][9][10]
the squared modulus of the wave function, |ψ|2, is a real number interpreted as the probability density of measuring a particle as being at a given place – or having a given momentum – at a given time, and possibly having definite values for discrete degrees of freedom.  The integral of this quantity, over all the system's degrees of freedom, must be 1 in accordance with the probability interpretation.  This general requirement that a wave function must satisfy is called the normalization condition.  Since the wave function is complex valued, only its relative phase and relative magnitude can be measured—its value does not, in isolation, tell anything about the magnitudes or directions of measurable observables; one has to apply quantum operators, whose eigenvalues correspond to sets of possible results of measurements, to the wave function ψ and calculate the statistical distributions for measurable quantities.
"
Weak_acid,Chemistry,1,"Acid strength is the tendency of an acid, symbolised by the chemical formula HA, to dissociate into a proton, H+, and an anion, A−. The dissociation of a strong acid in solution is effectively complete, except in its most concentrated solutions.
 Examples of strong acids are hydrochloric acid (HCl), perchloric acid (HClO4), nitric acid (HNO3) and sulfuric acid (H2SO4).
 A weak acid is only partially dissociated, with both the undissociated acid and its dissociation products being present, in solution, in equilibrium with each other. 
 Acetic acid (CH3COOH) is an example of a weak acid. The strength of a weak acid is quantified by its acid dissociation constant, pKa value.
 The strength of a weak organic acid may depend on substituent effects. The strength of an inorganic acid is dependent on the oxidation state for the atom to which the proton may be attached. Acid strength is solvent-dependent. For example, hydrogen chloride is a strong acid in aqueous solution, but is a weak acid when dissolved in glacial acetic acid.
"
Weak_base,Chemistry,1,"A weak base is a base that, upon dissolution in water, does not dissociate completely, so that the resulting aqueous solution contains only a small proportion of hydroxide ions and the concerned basic radical, and a large proportion of undissociated molecules of the base.
"
Wet_chemistry,Chemistry,1,"Wet chemistry is a form of analytical chemistry that uses classical methods such as observation to analyze materials. It is called wet chemistry since most analyzing is done in the liquid phase. Wet chemistry is also called bench chemistry since many tests are performed at lab benches. 
"
Mechanical_work,Chemistry,1,"
 In physics, work is the energy transferred to or from an object via the application of force along a displacement. In its simplest form, it is often represented as the product of force and displacement. A force is said to do positive work if (when applied) it has a component in the direction of the displacement of the point of application. A force does negative work if it has a component opposite to the direction of the displacement at the point of application of the force.
 For example, when a ball is held above the ground and then dropped, the work done by the gravitational force on the ball as it falls is equal to the weight of the ball (a force) multiplied by the distance to the ground (a displacement). When the force F is constant and the angle between the force and the displacement s is θ, then the work done is given by: 
 Work is a scalar quantity,[1] so it has only magnitude and no direction. Work transfers energy from one place to another, or one form to another. The SI unit of work is the joule (J).
"
Work-up_(chemistry),Chemistry,1,"In chemistry, work-up refers to the series of manipulations required to isolate and purify the product(s) of a chemical reaction.[1] Typically, these manipulations may include:
 For example, the Grignard reaction between phenylmagnesium bromide and carbon dioxide in the form of dry ice gives the conjugate base of benzoic acid. The desired product, benzoic acid, is obtained by the following work-up:[2] 
"
X-ray,Chemistry,1,"An X-ray, or X-radiation, is a penetrating form of high-energy electromagnetic radiation. Most X-rays have a wavelength ranging from 10 picometers to 10 nanometers, corresponding to frequencies in the range 30 petahertz to 30 exahertz (3×1016 Hz to 3×1019 Hz) and energies in the range 124 eV to 124 keV. X-ray wavelengths are shorter than those of UV rays and typically longer than those of gamma rays. In many languages, X-radiation is referred to as Röntgen radiation, after the German scientist Wilhelm Röntgen, who discovered it on November 8, 1895.[1] He named it X-radiation to signify an unknown type of radiation.[2] Spellings of X-ray(s) in English include the variants x-ray(s), xray(s), and X ray(s).[3]"
X-ray_scattering_techniques,Chemistry,1,"X-ray scattering techniques are a family of non-destructive analytical techniques which reveal information about the crystal structure, chemical composition, and physical properties of materials and thin films. These techniques are based on observing the scattered intensity of an X-ray beam hitting a sample as a function of incident and scattered angle, polarization, and wavelength or energy.
 Note that X-ray diffraction is now often considered a sub-set of X-ray scattering, where the scattering is elastic and the scattering object is crystalline, so that the resulting pattern contains sharp spots analyzed by X-ray crystallography (as in the Figure). However, both scattering and diffraction are related general phenomena and the distinction has not always existed.  Thus Guinier's classic text[1] from 1963 is titled ""X-ray diffraction in Crystals, Imperfect Crystals and Amorphous Bodies"" so 'diffraction' was clearly not restricted to crystals at that time.
"
X-ray_photoelectron_spectroscopy,Chemistry,1,"X-ray photoelectron spectroscopy (XPS) is a surface-sensitive quantitative spectroscopic technique based on the photoelectric effect that can identify the elements that exist within a material (elemental composition) or are covering its surface, as well as their chemical state, and the overall electronic structure and density of the electronic states in the material. XPS is a powerful measurement technique because it not only shows what elements are present, but also what other elements they are bonded to. The technique can be used in line profiling of the elemental composition across the surface, or in depth profiling when paired with ion-beam etching. It is often applied to study chemical processes in the materials in their as-received state or after cleavage, scraping, exposure to heat, reactive gasses or solutions, ultraviolet light, or during ion implantation.
 XPS belongs to the family of photoemission spectroscopies in which electron population spectra are obtained by irradiating a material with a beam of X-rays. Material properties are inferred from the measurement of the kinetic energy and the number of the ejected electrons. XPS requires high vacuum (residual gas pressure p ~ 10−6 Pa) or ultra-high vacuum (p < 10−7 Pa) conditions, although a current area of development is ambient-pressure XPS, in which samples are analyzed at pressures of a few tens of millibar.
 When laboratory X-ray sources are used, XPS easily detects all elements except hydrogen and helium. Detection limit is in the parts per thousand range, but parts per million (ppm) are achievable with long collection times and concentration at top surface. 
 XPS is routinely used to analyze inorganic compounds, metal alloys,[1] semiconductors,[2] polymers, elements, catalysts,[3][4][5][6] glasses, ceramics, paints, papers, inks, woods, plant parts, make-up, teeth, bones, medical implants, bio-materials,[7] coatings,[8] viscous oils, glues, ion-modified materials and many others. Somewhat less routinely XPS is used to analyze the hydrated forms of materials such as hydrogels and biological samples by freezing them in their hydrated state in an ultrapure environment, and allowing multilayers of ice to sublime away prior to analysis.
 "
Yield_(chemistry),Chemistry,1,"
 In chemistry, yield, also referred to as reaction yield, is a measure of the quantity of moles of a product formed in relation to the reactant consumed, obtained in a chemical reaction, usually expressed as a percentage. [1]  Yield is one of the primary factors that scientists must consider in organic and inorganic chemical synthesis processes.[2] In chemical reaction engineering, ""yield"",  ""conversion"" and ""selectivity"" are terms used to describe ratios of how much of a reactant was consumed (conversion),  how much desired product was formed (yield) in relation to the undesired product (selectivity), represented as X, Y, and S.
"
Zone_melting,Chemistry,1,"Zone melting (or zone refining or floating-zone process or travelling melting zone) is a group of similar methods of purifying crystals, in which a narrow region of a crystal is melted, and this molten zone is moved along the crystal. The molten region melts impure solid at its forward edge and leaves a wake of purer material solidified behind it as it moves through the ingot. The impurities concentrate in the melt, and are moved to one end of the ingot.  Zone refining was invented by John Desmond Bernal[1] and further developed by William Gardner Pfann in Bell Labs as a method to prepare high purity materials, mainly semiconductors, for manufacturing transistors. Its first commercial use was in germanium, refined to one atom of impurity per ten billion,[2] but the process can be extended to virtually any solute-solvent system having an appreciable concentration difference between solid and liquid phases at equilibrium.[3]  This process is also known as the float zone process, particularly in semiconductor materials processing.
"
Zwitterion,Chemistry,1,"In chemistry, a zwitterion (/ˈtsvɪtəˌraɪən/ TSVIT-ə-rye-ən; from German  Zwitter [ˈtsvɪtɐ] 'hermaphrodite'), also called an inner salt, is a molecule that contains an equal number of positively- and negatively-charged functional groups.[1] With amino acids, for example, in solution a chemical equilibrium will be established between the ""parent"" molecule and the zwitterion.
 Betaines are zwitterions that cannot isomerize to an all-neutral form, such as when the positive charge is located on a quaternary ammonium group. Similarly, a molecule containing a phosphonium group and a carboxylate group cannot isomerize.
"
Zinc,Chemistry,1,"
 Zinc is a chemical element with the symbol Zn and atomic number 30. Zinc is a slightly brittle metal at room temperature and has a blue-silvery appearance when oxidation is removed. It is the first element in group 12 of the periodic table. In some respects, zinc is chemically similar to magnesium: both elements exhibit only one normal oxidation state (+2), and the Zn2+ and Mg2+ ions are of similar size. Zinc is the 24th most abundant element in Earth's crust and has five stable isotopes. The most common zinc ore is sphalerite (zinc blende), a zinc sulfide mineral. The largest workable lodes are in Australia, Asia, and the United States. Zinc is refined by froth flotation of the ore, roasting, and final extraction using electricity (electrowinning).
 Brass, an alloy of copper and zinc in various proportions, was used as early as the third millennium BC in the Aegean area and the region which currently includes Iraq, the United Arab Emirates, Kalmykia, Turkmenistan and Georgia. In the second millennium BC in it was used in the regions currently including West India, Uzbekistan, Iran, Syria, Iraq, and Israel.[3][4][5] Zinc metal was not produced on a large scale until the 12th century in India, though it was known to the ancient Romans and Greeks.[6] The mines of Rajasthan have given definite evidence of zinc production going back to the 6th century BC.[7] To date, the oldest evidence of pure zinc comes from Zawar, in Rajasthan, as early as the 9th century AD when a distillation process was employed to make pure zinc.[8] Alchemists burned zinc in air to form what they called ""philosopher's wool"" or ""white snow"".
 The element was probably named by the alchemist Paracelsus after the German word Zinke (prong, tooth). German chemist Andreas Sigismund Marggraf is credited with discovering pure metallic zinc in 1746. Work by Luigi Galvani and Alessandro Volta uncovered the electrochemical properties of zinc by 1800. Corrosion-resistant zinc plating of iron (hot-dip galvanizing) is the major application for zinc. Other applications are in electrical batteries, small non-structural castings, and alloys such as brass. A variety of zinc compounds are commonly used, such as zinc carbonate and zinc gluconate (as dietary supplements), zinc chloride (in deodorants), zinc pyrithione (anti-dandruff shampoos), zinc sulfide (in luminescent paints), and dimethylzinc or diethylzinc in the organic laboratory.
 Zinc is an essential mineral, including to prenatal and postnatal development.[9] Zinc deficiency affects about two billion people in the developing world and is associated with many diseases.[10] In children, deficiency causes growth retardation, delayed sexual maturation, infection susceptibility, and diarrhea.[9] Enzymes with a zinc atom in the reactive center are widespread in biochemistry, such as alcohol dehydrogenase in humans.[11] Consumption of excess zinc may cause ataxia, lethargy, and copper deficiency.
"
Ab_initio_methods_(nuclear_physics),Physics,2,"In nuclear physics, ab initio methods seek to describe the atomic nucleus from the bottom up by solving the non-relativistic Schrödinger equation for all constituent nucleons and the forces between them. This is done either exactly for very light nuclei (up to four nucleons) or by employing certain well-controlled approximations for heavier nuclei. Ab initio methods constitute a more fundamental approach compared to e.g. the nuclear shell model. Recent progress has enabled ab initio treatment of heavier nuclei such as nickel.[1] A significant challenge in the ab initio treatment stems from the complexities of the inter-nucleon interaction. The strong nuclear force is believed to emerge from the strong interaction described by quantum chromodynamics (QCD), but QCD is non-perturbative in the low-energy regime relevant to nuclear physics. This makes the direct use of QCD for the description of the inter-nucleon interactions very difficult (see lattice QCD), and a model must be used instead. The most sophisticated models available are based on chiral effective field theory. This effective field theory (EFT) includes all interactions compatible with the symmetries of QCD, ordered by the size of their contributions. The degrees of freedom in this theory are nucleons and pions, as opposed to quarks and gluons as in QCD. The effective theory contains parameters called low-energy constants, which can be determined from scattering data.[1][2] Chiral EFT implies the existence of many-body forces, most notably the three-nucleon interaction which is known to be an essential ingredient in the nuclear many-body problem.[1][2] After arriving at a Hamiltonian 



H


{  H}
 (based on chiral EFT or other models) one must solve the Schrödinger equation
 where 



|

Ψ

⟩


{  \vert {\Psi }\rangle }
 is the many-body wavefunction of the A nucleons in the nucleus. Various ab initio methods have been devised to numerically find solutions to this equation:
"
Abbe_number,Physics,2,"In optics and lens design, the Abbe number, also known as the V-number or constringence of a transparent material, is an approximate measure of the material's dispersion (change of refractive index versus wavelength), with high values of V indicating low dispersion. It is named after Ernst Abbe (1840–1905), the German physicist who defined it. The term V-number should not be confused with the normalized frequency in fibers.
 The Abbe number,[1][2] VD, of a material is defined as
 where nC, nD and nF are the refractive indices of the material at the wavelengths of the Fraunhofer C, D1, and F spectral lines (656.3 nm, 589.3 nm, and 486.1 nm respectively). This formulation only applies to the visible spectrum. Outside this range requires the use of different spectral lines. For non-visible spectral lines the term V-number is more commonly used. The more general formulation defined as,
 where nshort, ncenter and nlong are the refractive indices of the material at three different wavelengths. The shortest wavelength index is nshort and the longest is nlong.
 Abbe numbers are used to classify glass and other optical materials in terms of their chromaticity. For example, the higher dispersion flint glasses have V < 55 whereas the lower dispersion crown glasses have larger Abbe numbers. Values of V range from below 25 for very dense flint glasses, around 34 for polycarbonate plastics, up to 65 for common crown glasses, and 75 to 85 for some fluorite and phosphate crown glasses. 
 Abbe numbers are used in the design of achromatic lenses, as their reciprocal is proportional to dispersion (slope of refractive index versus wavelength) in the wavelength region where the human eye is most sensitive (see graph). For different wavelength regions, or for higher precision in characterizing a system's chromaticity (such as in the design of apochromats), the full dispersion relation (refractive index as a function of wavelength) is used.
"
Absolute_electrode_potential,Physics,2,"
 Absolute electrode potential, in electrochemistry, according to an IUPAC definition,[1] is the electrode potential of a metal measured with respect to a universal reference system (without any additional metal–solution interface).
"
Absolute_humidity,Physics,2,"Humidity is the concentration of water vapor present in the air. Water vapor, the gaseous state of water, is generally invisible to the human eye.[1] Humidity indicates the likelihood for precipitation, dew, or fog to be present.
 Humidity depends on temperature and the pressure of the system of interest. The same amount of water vapor results in higher humidity in cool air than warm air. A related parameter is the dew point. The amount of water vapor needed to achieve saturation increases as the temperature increases. As the temperature of a parcel of air decreases it will eventually reach the saturation point without adding or losing water mass. The amount of water vapor contained within a parcel of air can vary significantly. For example, a parcel of air near saturation may contain 28 grams of water per cubic metre of air at 30 °C, but only 8 grams of water per cubic metre of air at 8 °C.
 Three primary measurements of humidity are widely employed: absolute, relative and specific. Absolute humidity describes the water content of air and is expressed in either grams per cubic metre[2] or grams per kilogram.[3] Relative humidity, expressed as a percentage, indicates a present state of absolute humidity relative to a maximum humidity given the same temperature. Specific humidity is the ratio of water vapor mass to total moist air parcel mass.
 Humidity plays an important role for surface life. For animal life dependent on perspiration (sweating) to regulate internal body temperature, high humidity impairs heat exchange efficiency by reducing the rate of moisture evaporation from skin surfaces. This effect can be calculated using a heat index table, also known as a humidex.
 The notion of air ""holding"" water vapor or being ""saturated"" by it is often mentioned in connection with the concept of relative humidity. This, however, is misleading—the amount of water vapor that enters (or can enter) a given space at a given temperature is almost independent of the amount of air (nitrogen, oxygen, etc.) that is present. Indeed, a vacuum has approximately the same equilibrium capacity to hold water vapor as the same volume filled with air; both are given by the equilibrium vapor pressure of water at the given temperature.[4][5] There is a very small difference described under ""Enhancement factor"" below, which can be neglected in many calculations unless high accuracy is required.
"
Absolute_motion,Physics,2,"Absolute space and time is a concept in physics and philosophy about the properties of the universe. In physics, absolute space and time may be a preferred frame.
"
Absolute_pressure,Physics,2,"Pressure measurement is the analysis of an applied force by a fluid (liquid or gas) on a surface. Pressure is typically measured in units of force per unit of surface area. Many techniques have been developed for the measurement of pressure and vacuum. Instruments used to measure and display pressure in an integral unit are called pressure meters or pressure gauges or vacuum gauges. A manometer is a good example, as it uses the surface area and weight of a column of liquid to both measure and indicate pressure. Likewise the widely used Bourdon gauge is a mechanical device, which both measures and indicates and is probably the best known type of gauge.
 A vacuum gauge is a pressure gauge used to measure pressures lower than the ambient atmospheric pressure, which is set as the zero point, in negative values (e.g.: −15 psig or −760 mmHg equals total vacuum). Most gauges measure pressure relative to atmospheric pressure as the zero point, so this form of reading is simply referred to as ""gauge pressure"". However, anything greater than total vacuum is technically a form of pressure. For very accurate readings, especially at very low pressures, a gauge that uses total vacuum as the zero point may be used, giving pressure readings in an absolute scale.
 Other methods of pressure measurement involve sensors that can transmit the pressure reading to a remote indicator or control system (telemetry).
"
Absolute_scale,Physics,2,"An absolute scale is a system of measurement that begins at a minimum, or zero point, and progresses in only one direction. An absolute scale differs from an arbitrary, or ""relative"", scale, which begins at some point selected by a person and can progress in both directions. An absolute scale begins at a natural minimum, leaving only one direction in which to progress.
 An absolute scale can only be applied to measurements in which a true minimum is known to exist. Time, for example, which does not have a clearly known beginning, is measured on a relative scale, with an arbitrary zero-point such as the conventional date of the birth of Jesus (see Anno Domini) or the accession of an emperor. Temperature, on the other hand, has a known minimum, absolute zero (where volume of an ideal gas becomes zero), and therefore, can be measured either in absolute terms (e.g. kelvin), or relative to a reference temperature (e.g. degree Celsius).
 Absolute scales are used when precise values are needed in comparison to a natural, unchanging zero point. Measurements of length, area and volume are inherently absolute, although measurements of distance are often based on an arbitrary starting point. Measurements of weight can be absolute, such as atomic weight, but more often they are measurements of the relationship between two masses, while measurements of speed are relative to an arbitrary reference frame. (Unlike many other measurements without a known, absolute minimum, speed has a known maximum and can be measured from a purely relative scale.) Absolute scales can be used for measuring a variety of things, from the flatness of an optical flat to neuroscientific tests.[1][2][3]"
Absolute_zero,Physics,2,"
 Absolute zero is the lowest limit of the thermodynamic temperature scale, a state at which the enthalpy and entropy of a cooled ideal gas reach their minimum value, taken as zero kelvins. The fundamental particles of nature have minimum vibrational motion, retaining only quantum mechanical, zero-point energy-induced particle motion. The theoretical temperature is determined by extrapolating the ideal gas law; by international agreement, absolute zero is taken as −273.15° on the Celsius scale (International System of Units),[1][2] which equals −459.67° on the Fahrenheit scale (United States customary units or Imperial units).[3] The corresponding Kelvin and Rankine temperature scales set their zero points at absolute zero by definition.
 It is commonly thought of as the lowest temperature possible, but it is not the lowest enthalpy state possible, because all real substances begin to depart from the ideal gas when cooled as they approach the change of state to liquid, and then to solid; and the sum of the enthalpy of vaporization (gas to liquid) and enthalpy of fusion (liquid to solid) exceeds the ideal gas's change in enthalpy to absolute zero. In the quantum-mechanical description, matter (solid) at absolute zero is in its ground state, the point of lowest internal energy.
 The laws of thermodynamics indicate that absolute zero cannot be reached using only thermodynamic means, because the temperature of the substance being cooled approaches the temperature of the cooling agent asymptotically,[4] and a system at absolute zero still possesses quantum mechanical zero-point energy, the energy of its ground state at absolute zero. The kinetic energy of the ground state cannot be removed. 
 Scientists and technologists routinely achieve temperatures close to absolute zero, where matter exhibits quantum effects such as Bose–Einstein condensate, superconductivity and superfluidity.
"
Absorption_spectroscopy,Physics,2,"Absorption spectroscopy refers to spectroscopic techniques that measure the absorption of radiation, as a function of frequency or wavelength, due to its interaction with a sample. The sample absorbs energy, i.e., photons, from the radiating field. The intensity of the absorption varies as a function of frequency, and this variation is the absorption spectrum. Absorption spectroscopy is performed across the electromagnetic spectrum.
 Absorption spectroscopy is employed as an analytical chemistry tool to determine the presence of a particular substance in a sample and, in many cases, to quantify the amount of the substance present. Infrared and ultraviolet–visible spectroscopy are particularly common in analytical applications. Absorption spectroscopy is also employed in studies of molecular and atomic physics, astronomical spectroscopy and remote sensing.
 There are a wide range of experimental approaches for measuring absorption spectra. The most common arrangement is to direct a generated beam of radiation at a sample and detect the intensity of the radiation that passes through it. The transmitted energy can be used to calculate the absorption. The source, sample arrangement and detection technique vary significantly depending on the frequency range and the purpose of the experiment.
 Following are the major types of absorption spectroscopy:[1] Nuclear magnetic resonance spectroscopy
"
Absorbance,Physics,2,"In optics, absorbance or decadic absorbance is the common logarithm of the ratio of incident to transmitted radiant power through a material, and spectral absorbance or spectral decadic absorbance is the common logarithm of the ratio of incident to transmitted spectral radiant power through a material.[1] Absorbance is dimensionless, and in particular is not a length, though it is a monotonically increasing function of path length, and approaches zero as the path length approaches zero. The use of the term ""optical density"" for absorbance is discouraged.[1]
In physics, a closely related quantity called ""optical depth"" is used instead of absorbance: the natural logarithm of the ratio of incident to transmitted radiant power through a material. The optical depth equals the absorbance times ln(10).
 The term absorption refers to the physical process of absorbing light, while absorbance does not always measure absorption: it measures attenuation (of transmitted radiant power). Attenuation can be caused by absorption, but also reflection, scattering, and other physical processes.
"
Accelerating_expansion_of_the_universe,Physics,2,"The accelerating expansion of the universe is the observation that the expansion of the universe is such that the velocity at which a distant galaxy is receding from the observer is continuously increasing with time.[1][2][3] The accelerated expansion was discovered during 1998, by two independent projects, the Supernova Cosmology Project and the High-Z Supernova Search Team, which both used distant type Ia supernovae to measure the acceleration.[4][5][6] The idea was that as type Ia supernovae have almost the same intrinsic brightness (a standard candle), and since objects that are further away appear dimmer, we can use the observed brightness of these supernovae to measure the distance to them. The distance can then be compared to the supernovae's cosmological redshift, which measures how much the universe has expanded since the supernova occurred.[7] The unexpected result was that objects in the universe are moving away from one another at an accelerated rate. Cosmologists at the time expected that recession velocity would always be decelerating, due to the gravitational attraction of the matter in the universe. Three members of these two groups have subsequently been awarded Nobel Prizes for their discovery.[8] Confirmatory evidence has been found in baryon acoustic oscillations, and in analyses of the clustering of galaxies.
 The accelerated expansion of the universe is thought to have begun since the universe entered its dark-energy-dominated era roughly 4 billion years ago.[9][notes 1]
Within the framework of general relativity, an accelerated expansion can be accounted for by a positive value of the cosmological constant Λ, equivalent to the presence of a positive vacuum energy, dubbed ""dark energy"". While there are alternative possible explanations, the description assuming dark energy (positive Λ) is used in the current standard model of cosmology, which also includes cold dark matter (CDM) and is known as the Lambda-CDM model.
"
Acceleration,Physics,2,"
 In mechanics, acceleration is the rate of change of the velocity of an object with respect to time.
Accelerations are vector quantities (in that they have magnitude and direction).[1] The orientation of an object's acceleration is given by the orientation of the net force acting on that object.  The magnitude of an object's acceleration, as described by Newton's Second Law,[2] is the combined effect of two causes:
 The SI unit for acceleration is metre per second squared (m⋅s−2, 






m

s

2







{  {\tfrac {\operatorname {m} }{\operatorname {s} ^{2}}}}
).
 For example, when a vehicle starts from a standstill (zero velocity, in an inertial frame of reference) and travels in a straight line at increasing speeds, it is accelerating in the direction of travel.  If the vehicle turns, an acceleration occurs toward the new direction and changes its motion vector.  The acceleration of the vehicle in its current direction of motion is called a linear (or tangential during circular motions) acceleration, the reaction to which the passengers on board experience as a force pushing them back into their seats.  When changing direction, the effecting acceleration is called radial (or orthogonal during circular motions) acceleration, the reaction to which the passengers experience as a centrifugal force.  If the speed of the vehicle decreases, this is an acceleration in the opposite direction and mathematically a negative, sometimes called deceleration, and passengers experience the reaction to deceleration as an inertial force pushing them forward.  Such negative accelerations are often achieved by retrorocket burning in spacecraft.[3]  Both acceleration and deceleration are treated the same, they are both changes in velocity.  Each of these accelerations (tangential, radial, deceleration) is felt by passengers until their relative (differential) velocity are neutralized in reference to the vehicle.
"
Gravitational_acceleration,Physics,2,"In physics, gravitational acceleration is the free fall acceleration of an object in vacuum — without any drag. This is the steady gain in speed caused exclusively by the force of gravitational attraction. At given GPS coordinates on the Earth's surface and a given altitude, all bodies accelerate in vacuum at the same rate.[1] This equality is true regardless of the masses or compositions of the bodies.
 At different points on Earth surface, the free fall acceleration ranges from 9.764 m/s2 to 9.834 m/s2[2] depending on altitude and latitude, with a conventional standard value of exactly 9.80665 m/s2 (approximately 32.17405 ft/s2). This does not take into account other effects, such as buoyancy or drag.
"
Accelerometer,Physics,2,"An accelerometer is a tool that measures proper acceleration.[1] Proper acceleration is the acceleration (the rate of change of velocity) of a body in its own instantaneous rest frame;[2] this is different from coordinate acceleration, which is acceleration in a fixed coordinate system. For example, an accelerometer at rest on the surface of the Earth will measure an acceleration due to Earth's gravity, straight upwards[3] (by definition) of g ≈ 9.81 m/s2. By contrast, accelerometers in free fall (falling toward the center of the Earth at a rate of about 9.81 m/s2) will measure zero.
 Accelerometers have many uses in industry and science. Highly sensitive accelerometers are used in inertial navigation systems for aircraft and missiles. Vibration in rotating machines is monitored by accelerometers. They are used in tablet computers and digital cameras so that images on screens are always displayed upright. In unmanned aerial vehicles, accelerometers help to stabilise flight.
 When two or more accelerometers are coordinated with one another, they can measure differences in proper acceleration, particularly gravity, over their separation in space—that is, the gradient of the gravitational field. Gravity gradiometry is useful because absolute gravity is a weak effect and depends on the local density of the Earth, which is quite variable.
 Single- and multi-axis accelerometers can detect both the magnitude and the direction of the proper acceleration, as a vector quantity, and can be used to sense orientation (because the direction of weight changes), coordinate acceleration, vibration, shock, and falling in a resistive medium (a case in which the proper acceleration changes, increasing from zero). Micromachined microelectromechanical systems (MEMS) accelerometers are increasingly present in portable electronic devices and video-game controllers, to detect changes in the positions of these devices.
"
Acoustics,Physics,2,"Acoustics is a branch of physics that deals with the study of mechanical waves in gases, liquids, and solids including topics such as vibration, sound, ultrasound and infrasound. A scientist who works in the field of acoustics is an acoustician while someone working in the field of acoustics technology may be called an acoustical engineer. The application of acoustics is present in almost all aspects of modern society with the most obvious being the audio and noise control industries.
 Hearing is one of the most crucial means of survival in the animal world and speech is one of the most distinctive characteristics of human development and culture. Accordingly, the science of acoustics spreads across many facets of human society—music, medicine, architecture, industrial production, warfare and more. Likewise, animal species such as songbirds and frogs use sound and hearing as a key element of mating rituals or marking territories. Art, craft, science and technology have provoked one another to advance the whole, as in many other fields of knowledge. Robert Bruce Lindsay's ""Wheel of Acoustics"" is a well accepted overview of the various fields in acoustics.[1]"
Adhesion,Physics,2,"Adhesion is the tendency of dissimilar particles or surfaces to cling to one another (cohesion refers to the tendency of similar or identical particles/surfaces to cling to one another). 
 Note 1: Adhesion requires energy that can come from chemical and/or physicallinkages, the latter being reversible when enough energy is applied.
 Note 2: In biology, adhesion reflects the behavior of cells shortly after contactto the surface.
 The forces that cause adhesion and cohesion can be divided into several types. The intermolecular forces responsible for the function of various kinds of stickers and sticky tape fall into the categories of chemical adhesion, dispersive adhesion, and diffusive adhesion. In addition to the cumulative magnitudes of these intermolecular forces, there are also certain emergent mechanical effects.
"
Adiabatic_cooling,Physics,2,"In thermodynamics, an adiabatic process is a type of thermodynamic process which occurs without transferring heat or mass between the system and its surroundings. Unlike an isothermal process, an adiabatic process transfers energy to the surroundings only as work.[1][2] It also conceptually supports the theory used to explain the first law of thermodynamics and is therefore a key thermodynamic concept.
 Some chemical and physical processes occur too rapidly for energy to enter or leave the system as heat, allowing a convenient ""adiabatic approximation"".[3] For example, the adiabatic flame temperature uses this approximation to calculate the upper limit of flame temperature by assuming combustion loses no heat to its surroundings.
 In meteorology and oceanography, adiabatic cooling  produces condensation of moisture or salinity, oversaturating the parcel. Therefore, the excess must be removed. There, the process becomes a pseudo-adiabatic process whereby the liquid water or salt that condenses is assumed to be removed upon formation by idealized instantaneous precipitation. The pseudoadiabatic process is only defined for expansion because a compressed parcel  becomes warmer and remains undersaturated.[4]"
Adiabatic_heating,Physics,2,"In thermodynamics, an adiabatic process is a type of thermodynamic process which occurs without transferring heat or mass between the system and its surroundings. Unlike an isothermal process, an adiabatic process transfers energy to the surroundings only as work.[1][2] It also conceptually supports the theory used to explain the first law of thermodynamics and is therefore a key thermodynamic concept.
 Some chemical and physical processes occur too rapidly for energy to enter or leave the system as heat, allowing a convenient ""adiabatic approximation"".[3] For example, the adiabatic flame temperature uses this approximation to calculate the upper limit of flame temperature by assuming combustion loses no heat to its surroundings.
 In meteorology and oceanography, adiabatic cooling  produces condensation of moisture or salinity, oversaturating the parcel. Therefore, the excess must be removed. There, the process becomes a pseudo-adiabatic process whereby the liquid water or salt that condenses is assumed to be removed upon formation by idealized instantaneous precipitation. The pseudoadiabatic process is only defined for expansion because a compressed parcel  becomes warmer and remains undersaturated.[4]"
Adiabatic_process,Physics,2,"In thermodynamics, an adiabatic process is a type of thermodynamic process which occurs without transferring heat or mass between the system and its surroundings. Unlike an isothermal process, an adiabatic process transfers energy to the surroundings only as work.[1][2] It also conceptually supports the theory used to explain the first law of thermodynamics and is therefore a key thermodynamic concept.
 Some chemical and physical processes occur too rapidly for energy to enter or leave the system as heat, allowing a convenient ""adiabatic approximation"".[3] For example, the adiabatic flame temperature uses this approximation to calculate the upper limit of flame temperature by assuming combustion loses no heat to its surroundings.
 In meteorology and oceanography, adiabatic cooling  produces condensation of moisture or salinity, oversaturating the parcel. Therefore, the excess must be removed. There, the process becomes a pseudo-adiabatic process whereby the liquid water or salt that condenses is assumed to be removed upon formation by idealized instantaneous precipitation. The pseudoadiabatic process is only defined for expansion because a compressed parcel  becomes warmer and remains undersaturated.[4]"
Aerodynamics,Physics,2,"Aerodynamics, from Greek ἀήρ aero (air) + δυναμική (dynamics), is the study of motion of air, particularly when affected by a solid object, such as an airplane wing. It is a sub-field of fluid dynamics and gas dynamics, and many aspects of aerodynamics theory are common to these fields.  The term aerodynamics is often used synonymously with gas dynamics, the difference being that ""gas dynamics"" applies to the study of the motion of all gases, and is not limited to air. 
The formal study of aerodynamics began in the modern sense in the eighteenth century, although observations of fundamental concepts such as aerodynamic drag were recorded much earlier.  Most of the early efforts in aerodynamics were directed toward achieving heavier-than-air flight, which was first demonstrated by Otto Lilienthal in 1891.[1]  Since then, the use of aerodynamics through mathematical analysis, empirical approximations, wind tunnel experimentation, and computer simulations has formed a rational basis for the development of heavier-than-air flight and a number of other technologies.  Recent work in aerodynamics has focused on issues related to compressible flow, turbulence, and boundary layers and has become increasingly computational in nature.
"
Afocal_system,Physics,2,"In optics an afocal system (a system without focus) is an optical system that produces no net convergence or divergence of the beam, i.e. has an infinite effective focal length.[1] This type of system can be created with a pair of optical elements where the distance between the elements is equal to the sum of each element's focal length (d = f1+f2). A simple example of an afocal optical system is an optical telescope imaging a star, the light entering the system is at infinity and the image it forms is at infinity (the light is collimated).[2] Although the system does not alter the divergence of a collimated beam, it does alter the width of the beam, increasing magnification. The magnification of such a telescope is given by
 Afocal systems are used in laser optics, for instance as beam expanders, Infrared and forward looking infrared systems, camera zoom lenses and telescopic lens attachments such as teleside converters,[3] and photography setups combining cameras and telescopes (Afocal photography).
"
Air_mass,Physics,2,"
 In meteorology, an air mass is a volume of air defined by its temperature and water vapor content.  Air masses cover many hundreds or thousands of  miles, and adapt to the characteristics of the surface below them. They are classified according to latitude and their continental or maritime source regions.  Colder air masses are termed polar or arctic, while warmer air masses are deemed tropical. Continental and superior air masses are dry while maritime and monsoon air masses are moist.   Weather fronts separate air masses with different density (temperature or moisture) characteristics. Once an air mass moves away from its source region, underlying vegetation and water bodies can quickly modify its character. Classification schemes tackle an air mass' characteristics, as well as modification.
"
Air_mass_(solar_energy),Physics,2,"The air mass coefficient defines the direct optical path length through the Earth's atmosphere, expressed as a ratio relative to the path length vertically upwards, i.e. at the zenith. The air mass coefficient can be used to help characterize the solar spectrum after solar radiation has traveled through the atmosphere.
 The air mass coefficient is commonly used to characterize the performance of solar cells under standardized conditions, and is often referred to using the syntax ""AM"" followed by a number. ""AM1.5"" is almost universal when characterizing terrestrial power-generating panels.
"
Albedo,Physics,2,"
 Albedo (/ælˈbiːdoʊ/) (Latin: albedo, meaning 'whiteness') is the measure of the diffuse reflection of solar radiation out of the total solar radiation and measured on a scale from 0, corresponding to a black body that absorbs all incident radiation, to 1, corresponding to a body that reflects all incident radiation.
 Surface albedo is defined as the ratio of radiosity to the irradiance (flux per unit area) received by a surface.[1] The proportion reflected is not only determined by properties of the surface itself, but also by the spectral and angular distribution of solar radiation reaching the Earth's surface.[2] These factors vary with atmospheric composition, geographic location and time (see position of the Sun). While bi-hemispherical reflectance is calculated for a single angle of incidence (i.e., for a given position of the Sun), albedo is the directional integration of reflectance over all solar angles in a given period. The temporal resolution may range from seconds (as obtained from flux measurements) to daily, monthly, or annual averages.
 Unless given for a specific wavelength (spectral albedo), albedo refers to the entire spectrum of solar radiation.[3] Due to measurement constraints, it is often given for the spectrum in which most solar energy reaches the surface (between 0.3 and 3 μm). This spectrum includes visible light (0.4–0.7 μm), which explains why surfaces with a low albedo appear dark (e.g., trees absorb most radiation), whereas surfaces with a high albedo appear bright (e.g., snow reflects most radiation).
 Albedo is an important concept in climatology, astronomy, and environmental management (e.g., as part of the Leadership in Energy and Environmental Design (LEED) program for sustainable rating of buildings). The average albedo of the Earth from the upper atmosphere, its planetary albedo, is 30–35% because of cloud cover, but widely varies locally across the surface because of different geological and environmental features.[4] The term albedo was introduced into optics by Johann Heinrich Lambert in his 1760 work Photometria.
"
Alloy,Physics,2,"
 An alloy is a combination of metals or metals combined with one or more other elements. For example, combining the metallic elements gold and copper produces red gold, gold and silver becomes white gold, and silver combined with copper produces sterling silver. Elemental iron, combined with non-metallic carbon or silicon, produces alloys called steel or silicon steel. The resulting mixture forms a substance with properties that often differ from those of the pure metals, such as increased strength or hardness. Unlike other substances that may contain metallic bases but do not behave as metals, such as aluminium oxide (sapphire), beryllium aluminium silicate (emerald) or sodium chloride (salt), an alloy will retain all the properties of a metal in the resulting material, such as electrical conductivity, ductility, opaqueness, and luster. Alloys are used in a wide variety of applications, from the steel alloys, used in everything from buildings to automobiles to surgical tools, to exotic titanium-alloys used in the aerospace industry, to beryllium-copper alloys for non-sparking tools. In some cases, a combination of metals may reduce the overall cost of the material while preserving important properties. In other cases, the combination of metals imparts synergistic properties to the constituent metal elements such as corrosion resistance or mechanical strength. Examples of alloys are steel, solder, brass, pewter, duralumin, bronze and amalgams.
 An alloy may be a solid solution of metal elements (a single phase, where all metallic grains (crystals) are of the same composition) or a mixture of metallic phases (two or more solutions, forming a microstructure of different crystals within the metal). Intermetallic compounds are alloys with a defined stoichiometry and crystal structure. Zintl phases are also sometimes considered alloys depending on bond types (see Van Arkel–Ketelaar triangle for information on classifying bonding in binary compounds).
 Alloys are defined by a metallic bonding character.[1] The alloy constituents are usually measured by mass percentage for practical applications, and in atomic fraction for basic science studies. Alloys are usually classified as substitutional or interstitial alloys, depending on the atomic arrangement that forms the alloy. They can be further classified as homogeneous (consisting of a single phase), or heterogeneous (consisting of two or more phases) or intermetallic.
"
Alpha_decay,Physics,2,"Alpha decay or α-decay is a type of radioactive decay in which an atomic nucleus emits an alpha particle (helium nucleus) and thereby transforms or 'decays' into a different atomic nucleus, with a mass number that is reduced by four and an atomic number that is reduced by two. An alpha particle is identical to the nucleus of a helium-4 atom, which consists of two protons and two neutrons. It has a charge of +2 e and a mass of 4 u.  For example, uranium-238 decays to form thorium-234. Alpha particles have a charge +2 e, but as a nuclear equation describes a nuclear reaction without considering the electrons – a convention that does not imply that the nuclei necessarily occur in neutral atoms – the charge is not usually shown.
Alpha decay typically occurs in the heaviest nuclides. Theoretically, it can occur only in nuclei somewhat heavier than nickel (element 28), where the overall binding energy per nucleon is no longer a minimum and the nuclides are therefore unstable toward spontaneous fission-type processes. In practice, this mode of decay has only been observed in nuclides considerably heavier than nickel, with the lightest known alpha emitters being the lightest isotopes (mass numbers 104–109) of tellurium (element 52). Exceptionally, however, beryllium-8 decays to two alpha particles.
Alpha decay is by far the most common form of cluster decay, where the parent atom ejects a defined daughter collection of nucleons, leaving another defined product behind. It is the most common form because of the combined extremely high nuclear binding energy and a relatively small mass of the alpha particle. Like other cluster decays, alpha decay is fundamentally a quantum tunneling process. Unlike beta decay, it is governed by the interplay between both the nuclear force and the electromagnetic force.
Alpha particles have a typical kinetic energy of 5 MeV (or ≈ 0.13% of their total energy, 110 TJ/kg) and have a speed of about 15,000,000 m/s, or 5% of the speed of light. There is surprisingly small variation around this energy, due to the heavy dependence of the half-life of this process on the energy produced. Because of their relatively large mass, the electric charge of +2 e and relatively low velocity, alpha particles are very likely to interact with other atoms and lose their energy, and their forward motion can be stopped by a few centimeters of air. 
Approximately 99% of the helium produced on Earth is the result of the alpha decay of underground deposits of minerals containing uranium or thorium. The helium is brought to the surface as a by-product of natural gas production.
"
Alpha_particle,Physics,2,"
 Alpha particles, also called alpha rays or alpha radiation, consist of two protons and two neutrons bound together into a particle identical to a helium-4 nucleus. They are generally produced in the process of alpha decay, but may also be produced in other ways. Alpha particles are named after the first letter in the Greek alphabet, α. The symbol for the alpha particle is α or α2+. Because they are identical to helium nuclei, they are also sometimes written as He2+ or 42He2+ indicating a helium ion with a +2 charge (missing its two electrons). Once the ion gains electrons from its environment, the alpha particle becomes a normal (electrically neutral) helium atom 42He.
 4.001506179127(63) u Alpha particles have a net spin of zero. Due to the mechanism of their production in standard alpha radioactive decay, alpha particles generally have a kinetic energy of about 5 MeV, and a velocity in the vicinity of 4% of the speed of light. (See discussion below for the limits of these figures in alpha decay.) They are a highly ionizing form of particle radiation, and (when resulting from radioactive alpha decay) usually have low penetration depth (stopped by a few centimeters of air, or by the skin). 
 However, so-called long range alpha particles from ternary fission are three times as energetic, and penetrate three times as far. The helium nuclei that form 10–12% of cosmic rays are also usually of much higher energy than those produced by nuclear decay processes, and thus may be highly penetrating and able to traverse the human body and also many meters of dense solid shielding, depending on their energy. To a lesser extent, this is also true of very high-energy helium nuclei produced by particle accelerators.
"
Alternating_current,Physics,2,"
 Alternating current (AC) is an electric current which periodically reverses direction and changes its magnitude continuously with time in contrast to direct current (DC) which flows only in one direction. Alternating current is the form in which electric power is delivered to businesses and residences, and it is the form of electrical energy that consumers typically use when they plug kitchen appliances, televisions, fans and electric lamps into a wall socket. A common source of DC power is a battery cell in a flashlight. The abbreviations AC and DC are often used to mean simply alternating and direct, as when they modify current or voltage.[1][2] The usual waveform of alternating current in most electric power circuits is a sine wave, whose positive half-period corresponds with positive direction of the current and vice versa. In certain applications, like guitar amplifiers, different waveforms are used, such as triangular waves or square waves. Audio and radio signals carried on electrical wires are also examples of alternating current. These types of alternating current carry information such as sound (audio) or images (video) sometimes carried by modulation of an AC carrier signal. These currents typically alternate at higher frequencies than those used in power transmission.
"
Ammeter,Physics,2,"An ammeter (from ampere meter) is a measuring instrument used to measure the current in a circuit. Electric currents are measured in amperes (A), hence the name. The ammeter is usually connected in series with the circuit in which the current is to be measured. Since the entire current passes through the ammeter, therefore, an ammeter usually has low resistance so that it may not change the value of the current flowing in the circuit.  
 Instruments used to measure smaller currents, in the milliampere or microampere range, are designated as milliammeters or microammeters. Early ammeters were laboratory instruments that relied on the Earth's magnetic field for operation. By the late 19th century, improved instruments were designed which could be mounted in any position and allowed accurate measurements in electric power systems. It is generally represented by letter 'A' in a circuit. 
"
Amorphous_solid,Physics,2,"In condensed matter physics and materials science, an amorphous (from the Greek a, without, morphé, shape, form) or non-crystalline solid is a solid that lacks the long-range order that is characteristic of a crystal. In some older books, the term has been used synonymously with glass. Nowadays, ""glassy solid"" or ""amorphous solid"" is considered to be the overarching concept, and glass the more special case: Glass is an amorphous solid that exhibits a glass transition.[1] Polymers are often amorphous. Other types of amorphous solids include gels, thin films, and nanostructured materials such as glass.
 Amorphous materials have an internal structure made of interconnected structural blocks. These blocks can be similar to the basic structural units found in the corresponding crystalline phase of the same compound.[2] Whether a material is liquid or solid depends primarily on the connectivity between its elementary building blocks so that solids are characterized by a high degree of connectivity whereas structural blocks in fluids have lower connectivity.[3] In the pharmaceutical industry, the amorphous drugs were shown to have higher bio-availability than their crystalline counterparts due to the high solubility of amorphous phase. Moreover, certain compounds can undergo precipitation in their amorphous form in vivo, and they can decrease each other's bio-availability if administered together.[4][5]"
Ampere,Physics,2,"
 The ampere (/ˈæmpɛər/, US: /ˈæmpɪər/;[1][2][3] symbol: A),[4] often shortened to ""amp"",[5] is the base unit of electric current in the International System of Units (SI).[6][7] It is named after André-Marie Ampère (1775–1836), French mathematician and physicist, considered the father of electromagnetism.
 The International System of Units defines the ampere in terms of other base units by measuring the electromagnetic force between electrical conductors carrying electric current. The earlier CGS system had two different definitions of current, one essentially the same as the SI's and the other using electric charge as the base unit, with the unit of charge defined by measuring the force between two charged metal plates. The ampere was then defined as one coulomb of charge per second.[8] In SI, the unit of charge, the coulomb, is defined as the charge carried by one ampere during one second.
 New definitions, in terms of invariant constants of nature, specifically the elementary charge, took effect on 20 May 2019.[9]"
Amplifier,Physics,2,"An amplifier, electronic amplifier or (informally) amp is an electronic device that can increase the power of a signal (a time-varying voltage or current). It is a two-port electronic circuit that uses electric power from a power supply to increase the amplitude of a signal applied to its input terminals, producing a proportionally greater amplitude signal at its output.  The amount of amplification provided by an amplifier is measured by its gain: the ratio of output voltage, current, or power to input.  An amplifier is a circuit that has a power gain greater than one.[1][2][3] An amplifier can either be a separate piece of equipment or an electrical circuit contained within another device. Amplification is fundamental to modern electronics, and amplifiers are widely used in almost all electronic equipment. Amplifiers can be categorized in different ways. One is by the frequency of the electronic signal being amplified. For example, audio amplifiers amplify signals in the audio (sound) range of less than 20 kHz, RF amplifiers amplify frequencies in the radio frequency range between 20 kHz and 300 GHz, and servo amplifiers and instrumentation amplifiers may work with very low frequencies down to direct current. Amplifiers can also be categorized by their physical placement in the signal chain; a preamplifier may precede other signal processing stages, for example.[4]  The first practical electrical device which could amplify was the triode vacuum tube, invented in 1906 by Lee De Forest, which led to the first amplifiers around 1912. Today most amplifiers use transistors.
"
Amplitude,Physics,2,"The amplitude of a periodic variable is a measure of its change in a single period (such as time or spatial period). There are various definitions of amplitude (see below), which are all functions of the magnitude of the differences between the variable's extreme values. In older texts, the phase of a period function is sometimes called the amplitude.[1]"
Angle_of_incidence_(optics),Physics,2,"In geometric optics, the angle of incidence is the angle between a ray incident on a surface and the line perpendicular to the surface at the point of incidence, called the normal.  The ray can be formed by any wave: optical, acoustic, microwave, X-ray and so on. In the figure below, the line representing a ray makes an angle θ with the normal (dotted line). The angle of incidence at which light is first totally internally reflected is known as the critical angle. The angle of reflection and angle of refraction are other angles related to beams.
 Determining the angle of reflection with respect to a planar surface is trivial, but the computation for almost any other surface is significantly more difficult. The exact solution for a sphere (which has important applications in astronomy and computer graphics) is treated in Alhazen's problem.
"
Angle_of_reflection,Physics,2,"
 Reflection is the change in direction of a wavefront at an interface between two different media so that the wavefront returns into the medium from which it originated. Common examples include the reflection of light, sound and water waves. The law of reflection says that for specular reflection the angle at which the wave is incident on the surface equals the angle at which it is reflected. Mirrors exhibit specular reflection.
 In acoustics, reflection causes echoes and is used in sonar. In geology, it is important in the study of seismic waves. Reflection is observed with surface waves in bodies of water. Reflection is observed with many types of electromagnetic wave, besides visible light. Reflection of VHF and higher frequencies is important for radio transmission and for radar. Even hard X-rays and gamma rays can be reflected at shallow angles with special ""grazing"" mirrors.
"
%C3%85ngstr%C3%B6m,Physics,2,"The angstrom[1][2][3][4] (/ˈæŋstrəm/, /ˈæŋstrʌm/;[3][5][6] ANG-strəm, ANG-strum[5]) or ångström[1][7][8][9] is a metric unit of length equal to 10−10 m; that is, one ten-billionth of a metre, 0.1 nanometre, or 100 picometres. Its symbol is Å, a letter of the Swedish alphabet.
 The ångström is not a part of the SI system of units, but it can be considered part of the metric system in general.[8][9] Although deprecated by both the International Bureau of Weights and Measures (BIPM) and the US National Institute of Standards and Technology (NIST), the unit is still often used in the natural sciences and technology to express sizes of atoms, molecules, microscopic biological structures, and lengths of chemical bonds, arrangement of atoms in crystals, wavelengths of electromagnetic radiation, and dimensions of integrated circuit parts. The atomic (covalent) radii of phosphorus, sulfur, and chlorine are about 1 ångström, while that of hydrogen is about 0.5 ångströms. Visible light has wavelengths in the range of 4000–7000 Å.[citation needed] The unit is named after 19th-century Swedish physicist Anders Jonas Ångström (Swedish: [ˈɔ̂ŋːstrœm]). The BIPM and NIST use the spelling ångström, including Swedish letters;[8][9] but this form is rare in English texts. Some popular US dictionaries list only the spelling angstrom.[2][3] The symbol should always be ""Å"", no matter how the unit is spelled.[1][4][3] Nonetheless, ""A"" is often used in less formal contexts or typographically limited media.[citation needed]"
Angular_acceleration,Physics,2,"In physics, angular acceleration refers to the time rate of change of angular velocity. As there are two types of angular velocity, namely spin angular velocity and orbital angular velocity, there are naturally also two types of angular acceleration, called spin angular acceleration and orbital angular acceleration respectively. Spin angular acceleration refers to the angular acceleration of a rigid body about its centre of rotation, and orbital angular acceleration refers to the angular acceleration of a point particle about a fixed origin. 
 Angular acceleration is measured in units of angle per unit time squared (which in SI units is radians per second squared), and is usually represented by the symbol alpha (α). In two dimensions, angular acceleration is a pseudoscalar whose sign is taken to be positive if the angular speed increases counterclockwise or decreases clockwise, and is taken to be negative if the angular speed increases clockwise or decreases counterclockwise. In three dimensions, angular acceleration is a pseudovector.[1] For rigid bodies, angular acceleration must be caused by a net external torque. However, this is not so for non-rigid bodies: For example, a figure skater can speed up her rotation (thereby obtaining an angular acceleration) simply by contracting her arms and legs inwards, which involves no external torque.
"
Angular_displacement,Physics,2,"Angular displacement of a body is the angle in radians (degrees, revolutions) through which a point revolves around a centre or line has been rotated in a specified sense about a specified axis. When a body rotates about its axis, the motion cannot simply be analyzed as a particle, as in circular motion it undergoes a changing velocity and acceleration at any time (t).  When dealing with the rotation of a body, it becomes simpler to consider the body itself rigid.  A body is generally considered rigid when the separations between all the particles remains constant throughout the body's motion, so for example parts of its mass are not flying off.  In a realistic sense, all things can be deformable, however this impact is minimal and negligible.  Thus the rotation of a rigid body over a fixed axis is referred to as rotational motion.
"
Angular_frequency,Physics,2,"In physics, angular frequency ω (also referred to by the terms angular speed, radial frequency, circular frequency, orbital frequency, radian frequency, and pulsatance) is a scalar measure of rotation rate. It refers to the angular displacement per unit time (e.g., in rotation) or the rate of change of the phase of a sinusoidal waveform (e.g., in oscillations and waves), or as the rate of change of the argument of the sine function.
Angular frequency (or angular speed) is the magnitude of the vector quantity angular velocity. The term angular frequency vector 






ω
→





{  {\vec {\omega }}}
 is sometimes used as a synonym for the vector quantity angular velocity.[1] One revolution is equal to 2π radians, hence[1][2] where:
"
Angular_momentum,Physics,2,"In physics, angular momentum (rarely, moment of momentum or rotational momentum) is the rotational equivalent of linear momentum. It is an important quantity in physics because it is a conserved quantity—the total angular momentum of a closed system remains constant.
 In three dimensions, the angular momentum for a point particle is a pseudovector r × p, the cross product of the particle's position vector r (relative to some origin) and its momentum vector; the latter is p = mv in Newtonian mechanics. This definition can be applied to each point in continua like solids or fluids, or physical fields. Unlike momentum, angular momentum does depend on where the origin is chosen, since the particle's position is measured from it.
 Just like for angular velocity, there are two special types of angular momentum: the spin angular momentum and orbital angular momentum. The spin angular momentum of an object is defined as the angular momentum about its centre of mass coordinate. The orbital angular momentum of an object about a chosen origin is defined as the angular momentum of the centre of mass about the origin. The total angular momentum of an object is the sum of the spin and orbital angular momenta. The orbital angular momentum vector of a point particle is always parallel and directly proportional to the orbital angular velocity vector ω of the particle, where the constant of proportionality depends on both the mass of the particle and its distance from origin. The spin angular momentum vector of a rigid body is proportional but not always parallel to the spin angular velocity vector Ω, making the constant of proportionality a second-rank tensor rather than a scalar.
 Angular momentum is an extensive quantity; i.e. the total angular momentum of any composite system is the sum of the angular momenta of its constituent parts. For a continuous rigid body, the total angular momentum is the volume integral of angular momentum density (i.e. angular momentum per unit volume in the limit as volume shrinks to zero) over the entire body.
 Torque can be defined as the rate of change of angular momentum, analogous to force. The net external torque on any system is always equal to the total torque on the system; in other words, the sum of all internal torques of any system is always 0 (this is the rotational analogue of Newton's Third Law). Therefore, for a closed system (where there is no net external torque), the total torque on the system must be 0, which means that the total angular momentum of the system is constant. The conservation of angular momentum helps explain many observed phenomena, for example the increase in rotational speed of a spinning figure skater as the skater's arms are contracted, the high rotational rates of neutron stars, the Coriolis effect, and the precession of gyroscopes. In general, conservation does limit the possible motion of a system, but does not uniquely determine what the exact motion is.
 In quantum mechanics, angular momentum (like other quantities) is expressed as an operator, and its one-dimensional projections have quantized eigenvalues. Angular momentum is subject to the Heisenberg uncertainty principle, implying that at any time, only one projection (also called ""component"") can be measured with definite precision; the other two then remain uncertain. Because of this, the notion of a quantum particle literally ""spinning"" about an axis does not exist. Quantum particles do possess a type of non-orbital angular momentum called ""spin"", but this angular momentum does not correspond to actual physical spinning motion.[1]"
Angular_velocity,Physics,2,"In physics, angular velocity refers to how fast an object rotates or revolves relative to another point, i.e. how fast the angular position or orientation of an object changes with time. There are two types of angular velocity: orbital angular velocity and spin angular velocity. Spin angular velocity refers to how fast a rigid body rotates with respect to its centre of rotation. Orbital angular velocity refers to how fast a point object revolves about a fixed origin, i.e. the time rate of change of its angular position relative to the origin. Spin angular velocity is independent of the choice of origin, in contrast to orbital angular velocity  which depends on the choice of origin.
 In general, angular velocity is measured in angle per unit time, e.g. radians per second (angle replacing distance from linear velocity with time in common). The  SI unit of angular velocity is expressed as radians per second with the radian having a dimensionless value of unity, thus the SI units of angular velocity are listed as 1/s or s−1. Angular velocity is usually represented by the symbol omega (ω, sometimes Ω). By convention, positive angular velocity indicates counter-clockwise rotation, while negative is clockwise.
 For example, a geostationary satellite completes one orbit per day above the equator, or 360 degrees per 24 hours, and has angular velocity ω = (360°)/(24 h) = 15°/h, or (2π rad)/(24 h) ≈ 0.26 rad/h. If angle is measured in radians, the linear velocity is the radius times the angular velocity, 



v
=
r
ω


{  v=r\omega }
. With orbital radius 42,000 km from the earth's center, the satellite's speed through space is thus v = 42,000 km × 0.26/h ≈ 11,000 km/h. The angular velocity is positive since the satellite travels eastward with the Earth's rotation (counter-clockwise from above the north pole.)
 In three dimensions, angular velocity is a pseudovector, with its magnitude measuring the rate at which an object rotates or revolves, and its direction pointing perpendicular to the instantaneous plane of rotation or angular displacement. The orientation of angular velocity is conventionally specified by the right-hand rule.[1]"
Anion,Physics,2,"
 An ion (/ˈaɪɒn, -ən/)[1] is an particle,atom or molecule with a net electrical charge. 
 The charge of the electron is considered negative by convention.  The negative charge of an ion is equal and opposite to charged proton(s) considered positive by convention.  The net charge of an ion is non-zero due to its total number of electrons being unequal to its total number of protons. 
 A cation is a positively charged ion, with fewer electrons than protons, while an anion is negatively charged, with more electrons than protons. Because of their opposite electric charges, cations and anions attract each other and readily form ionic compounds.
 Ions consisting of only a single atom are termed atomic or monatomic ions, while two or more atoms form molecular ions or polyatomic ions. In the case of physical ionization in a fluid (gas or liquid), ""ion pairs"" are created by spontaneous molecule collisions, where each generated pair consists of a free electron and a positive ion.[2] Ions are also created by chemical interactions, such as the dissolution of a salt in liquids, or by other means, such as passing a direct current through a conducting solution, dissolving an anode via ionization.
"
Annihilation,Physics,2,"In particle physics, annihilation is the process that occurs when a subatomic particle collides with its respective antiparticle to produce other particles, such as an electron colliding with a positron to produce two photons.[1] The total energy and momentum of the initial pair are conserved in the process and distributed among a set of other particles in the final state. Antiparticles have exactly opposite additive quantum numbers from particles, so the sums of all quantum numbers of such an original pair are zero. Hence, any set of particles may be produced whose total quantum numbers are also zero as long as conservation of energy and conservation of momentum are obeyed.[2] During a low-energy annihilation, photon production is favored, since these particles have no mass. However, high-energy particle colliders produce annihilations where a wide variety of exotic heavy particles are created.
 The word annihilation takes use informally for the interaction of two particles that are not mutual antiparticles - not charge conjugate.  Some quantum numbers may then not sum to zero in the initial state, but conserve with the same totals in the final state. An example is the ""annihilation"" of a high-energy electron antineutrino with an electron to produce a W−.
 If the annihilating particles are composite, such as mesons or baryons, then several different particles are typically produced in the final state.
"
Anode,Physics,2,"
 An anode is an electrode through which the conventional current enters into a polarized electrical device.  This contrasts with a cathode, an electrode through which conventional current leaves an electrical device.  A common mnemonic is ACID, for ""anode current into device"".[1] The direction of conventional current (the flow of positive charges) in a circuit is opposite to the direction of electron flow, so (negatively charged) electrons flow out the anode of a galvanic cell, into the outside circuit. In both a galvanic cell and an electrolytic cell, the anode is the electrode at which the oxidation reaction occurs.
 In an electrolytic cell, the anode is the wire or plate having excess positive charge.[2] Consequently, anions will tend to move towards the anode where they can undergo oxidation.
 Historically, the anode has also been known as the zincode.
"
Anti-gravity,Physics,2,"
Anti-gravity (also known as non-gravitational field) is a hypothetical phenomenon of creating a place or object that is free from the force of gravity. It does not refer to the lack of weight under gravity experienced in free fall or orbit, or to balancing the force of gravity with some other force, such as electromagnetism or aerodynamic lift. Anti-gravity is a recurring concept in science fiction, particularly in the context of spacecraft propulsion.  Examples are the gravity blocking substance ""Cavorite"" in H. G. Wells's The First Men in the Moon and the Spindizzy machines in James Blish's Cities in Flight.
 ""Anti-gravity"" is often used to refer to devices that look as if they reverse gravity even though they operate through other means, such as lifters, which fly in the air by moving air with electromagnetic fields.[1][2] What's commonly misconstrued is that while anti-gravity is the nullification of gravity, it is not repulsive gravity or negative gravity. Gravity plates and compensators as envisioned in contemporary science fiction also are not anti-gravity.
"
Antimatter,Physics,2,"
 In modern physics, antimatter is defined as matter which is composed of the antiparticles (or ""partners"") of the corresponding particles of ""ordinary"" matter. Minuscule numbers of antiparticles are generated daily at particle accelerators – total production has been only a few nanograms[1] (ng)– and in natural processes like cosmic ray collisions and some types of radioactive decay, but only a tiny fraction of these have successfully been bound together in experiments to form anti-atoms. No macroscopic amount of antimatter has ever been assembled due to the extreme cost and difficulty of production and handling.
 In theory, a particle and its anti-particle (for example, a proton and an antiproton) have the same mass, but opposite electric charge, and other differences in quantum numbers. For example, a proton has positive charge while an antiproton has negative charge.
 A collision between any particle and its anti-particle partner leads to their mutual annihilation, giving rise to various proportions of intense photons (gamma rays), neutrinos, and sometimes less-massive particle–antiparticle pairs. The majority of the total energy of annihilation emerges in the form of ionizing radiation. If surrounding matter is present, the energy content of this radiation will be absorbed and converted into other forms of energy, such as heat or light. The amount of energy released is usually proportional to the total mass of the collided matter and antimatter, in accordance with the mass–energy equivalence equation, E=mc2.[2] Antimatter particles bind with each other to form antimatter, just as ordinary particles bind to form normal matter. For example, a positron (the antiparticle of the electron) and an antiproton (the antiparticle of the proton) can form an antihydrogen atom. The nuclei of antihelium have been artificially produced, albeit with difficulty, and are the most complex anti-nuclei so far observed.[3] Physical principles indicate that complex antimatter atomic nuclei are possible, as well as anti-atoms corresponding to the known chemical elements.
 There is strong evidence that the observable universe is composed almost entirely of ordinary matter, as opposed to an equal mixture of matter and antimatter.[4] This asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics.[5] The process by which this inequality between matter and antimatter particles developed is called baryogenesis.
"
Antineutron,Physics,2,"The antineutron is the antiparticle of the neutron with symbol n. It differs from the neutron only in that some of its properties have equal magnitude but opposite sign. It has the same mass as the neutron, and no net electric charge, but has opposite baryon number (+1 for neutron, −1 for the antineutron). This is because the antineutron is composed of antiquarks, while neutrons are composed of quarks. The antineutron consists of one up antiquark and two down antiquarks.
 Since the antineutron is electrically neutral, it cannot easily be observed directly. Instead, the products of its annihilation with ordinary matter are observed. In theory, a free antineutron should decay into an antiproton, a positron and a neutrino in a process analogous to the beta decay of free neutrons. There are theoretical proposals of neutron–antineutron oscillations, a process that implies the violation of the baryon number conservation.[1][2][3] The antineutron was discovered in proton–antiproton collisions at the Bevatron (Lawrence Berkeley National Laboratory) by Bruce Cork in 1956, one year after the antiproton was discovered.
"
Antiparticle,Physics,2,"In particle physics, every type of particle is associated with an antiparticle with the same mass but with opposite physical charges (such as electric charge). For example, the antiparticle of the electron is the antielectron (which is often referred to as positron). While the electron has a negative electric charge, the positron has a positive electric charge, and is produced naturally in certain types of radioactive decay.  The opposite is also true: the antiparticle of the positron is the electron.
 Some particles, such as the photon, are their own antiparticle. Otherwise, for each pair of antiparticle partners, one is designated as the normal particle (a kind of all particles usually interacted with is made of), and the other (usually given the prefix ""anti-"") as the antiparticle.
 Particle–antiparticle pairs can annihilate each other, producing photons; since the charges of the particle and antiparticle are opposite, total charge is conserved. For example, the positrons produced in natural radioactive decay quickly annihilate themselves with electrons, producing pairs of gamma rays, a process exploited in positron emission tomography.
 The laws of nature are very nearly symmetrical with respect to particles and antiparticles. For example, an antiproton and a positron can form an antihydrogen atom, which is believed to have the same properties as a hydrogen atom. This leads to the question of why the formation of matter after the Big Bang resulted in a universe consisting almost entirely of matter, rather than being a half-and-half mixture of matter and antimatter. The discovery of charge parity violation helped to shed light on this problem by showing that this symmetry, originally thought to be perfect, was only approximate.
 Because charge is conserved, it is not possible to create an antiparticle without either destroying another particle of the same charge (as is for instance the case when antiparticles are produced naturally via beta decay or the collision of cosmic rays with Earth's atmosphere), or by the simultaneous creation of both a particle and its antiparticle, which can occur in particle accelerators such as the Large Hadron Collider at CERN.
 Although particles and their antiparticles have opposite charges, electrically neutral particles need not be identical to their antiparticles. The neutron, for example, is made out of quarks, the antineutron from antiquarks, and they are distinguishable from one another because neutrons and antineutrons annihilate each other upon contact. However, other neutral particles are their own antiparticles, such as photons, Z0 bosons, π0 mesons, and hypothetical gravitons and some hypothetical WIMPs.
"
Antiproton,Physics,2,"The antiproton, p, (pronounced p-bar) is the antiparticle of the proton. Antiprotons are stable, but they are typically short-lived, since any collision with a proton will cause both particles to be annihilated in a burst of energy.
 The existence of the antiproton with −1 electric charge, opposite to the +1 electric charge of the proton, was predicted by Paul Dirac in his 1933 Nobel Prize lecture.[3] Dirac received the Nobel Prize for his 1928 publication of his Dirac equation that predicted the existence of positive and negative solutions to Einstein's energy equation (



E
=
m

c

2




{  E=mc^{2}}
) and the existence of the positron, the antimatter analog of the electron, with opposite charge and spin.
 The antiproton was first experimentally confirmed in 1955 at the Bevatron particle accelerator by University of California, Berkeley physicists Emilio Segrè and Owen Chamberlain, for which they were awarded the 1959 Nobel Prize in Physics. In terms of valence quarks, an antiproton consists of two up antiquarks and one down antiquark (uud). The properties of the antiproton that have been measured all match the corresponding properties of the proton, with the exception that the antiproton has electric charge and magnetic moment that are the opposites of those in the proton. The questions of how matter is different from antimatter, and the relevance of antimatter in explaining how our universe survived the Big Bang, remain open problems—open, in part, due to the relative scarcity of antimatter in today's universe.
"
Antiquark,Physics,2,"
 A quark (/kwɔːrk, kwɑːrk/) is a type of elementary particle and a fundamental constituent of matter. Quarks combine to form composite particles called hadrons, the most stable of which are protons and neutrons, the components of atomic nuclei.[1] All commonly observable matter is composed of up quarks, down quarks and electrons. Due to a phenomenon known as color confinement, quarks are never found in isolation; they can be found only within hadrons, which include baryons (such as protons and neutrons) and mesons, or in quark–gluon plasmas.[2][3][nb 1] For this reason, much of what is known about quarks has been drawn from observations of hadrons.
 Quarks have various intrinsic properties, including electric charge, mass, color charge, and spin. They are the only elementary particles in the Standard Model of particle physics to experience all four fundamental interactions, also known as fundamental forces (electromagnetism, gravitation, strong interaction, and weak interaction), as well as the only known particles whose electric charges are not integer multiples of the elementary charge.
 There are six types, known as flavors, of quarks: up, down, charm, strange, top, and bottom.[4] Up and down quarks have the lowest masses of all quarks. The heavier quarks rapidly change into up and down quarks through a process of particle decay: the transformation from a higher mass state to a lower mass state. Because of this, up and down quarks are generally stable and the most common in the universe, whereas strange, charm, bottom, and top quarks can only be produced in high energy collisions (such as those involving cosmic rays and in particle accelerators). For every quark flavor there is a corresponding type of antiparticle, known as an antiquark, that differs from the quark only in that some of its properties (such as the electric charge) have equal magnitude but opposite sign.
 The quark model was independently proposed by physicists Murray Gell-Mann and George Zweig in 1964.[5] Quarks were introduced as parts of an ordering scheme for hadrons, and there was little evidence for their physical existence until deep inelastic scattering experiments at the Stanford Linear Accelerator Center in 1968.[6][7] Accelerator experiments have provided evidence for all six flavors. The top quark, first observed at Fermilab in 1995, was the last to be discovered.[5]"
Arc_length,Physics,2,"Arc length is the distance between two points along a section of a curve.
 Determining the length of an irregular arc segment is also called rectification of a curve. The advent of infinitesimal calculus led to a general formula that provides closed-form solutions in some cases.
"
Archimedes%27_principle,Physics,2,"Archimedes' principle states that the upward buoyant force that is exerted on a body immersed in a fluid, whether fully or partially, is proportional to the weight of the fluid that the body displaces.[1] Archimedes' principle is a law of physics fundamental to fluid mechanics. It was formulated by Archimedes of Syracuse.[2]"
Area_moment_of_inertia,Physics,2,"
 The 2nd moment of area, or second area moment and also known as the area moment of inertia, is a geometrical property of an area which reflects how its points are distributed with regard to an arbitrary axis. The second moment of area is typically denoted with either an 



I


{  I}
 (for an axis that lies in the plane) or with a 



J


{  J}
 (for an axis perpendicular to the plane). In both cases, it is calculated with a multiple integral over the object in question. Its dimension is L (length) to the fourth power. Its unit of dimension, when working with the International System of Units, is meters to the fourth power, m4, or inches to the fourth power, in4, when working in the Imperial System of Units.
 In structural engineering, the second moment of area of a beam is an important property used in the calculation of the beam's deflection and the calculation of stress caused by a moment applied to the beam. In order to maximize the second moment of area, a large fraction of the cross-sectional area of an I-beam is located at the maximum possible distance from the centroid of the I-beam's cross-section. The planar second moment of area provides insight into a beam's resistance to bending due to an applied moment, force, or distributed load perpendicular to its neutral axis, as a function of its shape. The polar second moment of area provides insight into a beam's resistance to torsional deflection, due to an applied moment parallel to its cross-section, as a function of its shape.
"
Astrophysics,Physics,2,"Astrophysics is a science that employs the methods and principles of physics in the study of astronomical objects and phenomena.[1][2] Among the subjects studied are the Sun, other stars, galaxies, extrasolar planets, the interstellar medium and the cosmic microwave background.[3][4] Emissions from these objects are examined across all parts of the electromagnetic spectrum, and the properties examined include luminosity, density, temperature, and chemical composition. Because astrophysics is a very broad subject, astrophysicists apply concepts and methods from many disciplines of physics, including classical mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.
 In practice, modern astronomical research often involves a substantial amount of work in the realms of theoretical and observational physics. Some areas of study for astrophysicists include their attempts to determine the properties of dark matter, dark energy, black holes, and other celestial bodies; and the origin and ultimate fate of the universe.[3] Topics also studied by theoretical astrophysicists include Solar System formation and evolution; stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity, special relativity, quantum and physical cosmology, including string cosmology and astroparticle physics.
"
Attenuation_coefficient,Physics,2,"The linear attenuation coefficient, attenuation coefficient, or narrow-beam attenuation coefficient characterizes how easily a volume of material can be penetrated by a beam of light, sound, particles, or other energy or matter.[1] A large attenuation coefficient means that the beam is quickly ""attenuated"" (weakened) as it passes through the medium, and a small attenuation coefficient means that the medium is relatively transparent to the beam. The SI unit of attenuation coefficient is the reciprocal metre (m−1). Extinction coefficient is an old term for this quantity[1] but is still used in meteorology and climatology.[2] Most commonly, the quantity measures the value of downward e-folding distance of the original intensity as the energy of the intensity passes through a unit (e.g. one meter) thickness of material, so that an attenuation coefficient of 1 m−1 means that after passing through 1 metre, the radiation will be reduced by a factor of e, and for material with a coefficient of 2 m−1, it will be reduced twice by e, or e2. Other measures may use a different factor than e, such as the decadic attenuation coefficient below. The broad-beam attenuation coefficient counts forward-scattered radiation as transmitted rather than attenuated, and is more applicable to radiation shielding.
"
Atom,Physics,2,"
 An atom is the smallest unit of ordinary matter that forms a chemical element. Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms. Atoms are extremely small, typically around 100 picometers across. They are so small that accurately predicting their behavior using classical physics—as if they were tennis balls, for example—is not possible due to quantum effects.
 Every atom is composed of a nucleus and one or more electrons bound to the nucleus. The nucleus is made of one or more protons and a number of neutrons. Only the most common variety of hydrogen has no neutrons. More than 99.94% of an atom's mass is in the nucleus. The protons have a positive electric charge, the electrons have a negative electric charge, and the neutrons have no electric charge. If the number of protons and electrons are equal, then the atom is electrically neutral. If an atom has more or fewer electrons than protons, then it has an overall negative or positive charge, respectively – such atoms are called ions.
 The electrons of an atom are attracted to the protons in an atomic nucleus by the electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by the nuclear force. This force is usually stronger than the electromagnetic force that repels the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force. In this case, the nucleus splits and leaves behind different elements. This is a form of nuclear decay.
 The number of protons in the nucleus is the atomic number and it defines to which chemical element the atom belongs. For example, any atom that contains 29 protons is copper. The number of neutrons defines the isotope of the element. Atoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules or crystals. The ability of atoms to associate and dissociate is responsible for most of the physical changes observed in nature. Chemistry is the discipline that studies these changes.
"
Atomic_line_filter,Physics,2,"An atomic line filter (ALF) is a more effective optical band-pass filter used in the physical sciences for filtering electromagnetic radiation with precision, accuracy, and minimal signal strength loss. Atomic line filters work via the absorption or resonance lines of atomic vapors and so may also be designated an atomic resonance filter (ARF).[2] The three major types of atomic line filters are absorption-re-emission ALFs, Faraday filters and Voigt filters.[3] Absorption-re-emission filters were the first type developed, and so are commonly called simply ""atomic line filters""; the other two types are usually referred to specifically as ""Faraday filters"" or ""Voigt filters"".  Atomic line filters use different mechanisms and designs for different applications, but the same basic strategy is always employed: by taking advantage of the narrow lines of absorption or resonance in a metallic vapor, a specific frequency of light bypasses a series of filters that block all other light.[4] Atomic line filters can be considered the optical equivalent of lock-in amplifiers; they are used in scientific applications requiring the effective detection of a narrowband signal (almost always laser light) that would otherwise be obscured by broadband sources, such as daylight.[3]  They are used regularly in Laser Imaging Detection and Ranging (LIDAR) and are being studied for their potential use in laser communication systems.[5] Atomic line filters are superior to conventional dielectric optical filters such as interference filters and Lyot filters, but their greater complexity makes them practical only in background-limited detection, where a weak signal is detected while suppressing a strong background.[6]   Compared to etalons, another high-end optical filter, Faraday filters are significantly sturdier and may be six times cheaper at around US$15,000 per unit.[7][8]"
Atomic_mass,Physics,2,"The atomic mass (ma or m) is the mass of an atom. Although the SI unit of mass is kilogram (symbol: kg), the atomic mass is often expressed in the non-SI unit dalton (symbol: Da, or u) where 1 dalton is defined as ​1⁄12 of the mass of a single carbon-12 atom, at rest.[1] The protons and neutrons of the nucleus account for nearly all of the total mass of atoms, with the electrons and nuclear binding energy making minor contributions. Thus, the numeric value of the atomic mass when expressed in daltons has nearly the same value as the mass number. Conversion between mass in kilograms and mass in daltons can be done using the atomic mass constant 




m


u



=



m
(





12


C


)


12



=
1
 


D
a




{  m_{\rm {u}}={{m({\rm {^{12}C}})} \over {12}}=1\ {\rm {Da}}}
.
 The formula used for conversion is:[2][3] where 




M


u





{  M_{\rm {u}}}
 is the molar mass constant, 




N


A





{  N_{\rm {A}}}
 is the Avogadro constant and 



M

(

12



C

)


{  M(^{12}\mathrm {C} )}
 is the experimentally determined molar mass of carbon-12.
 The relative isotopic mass (see section below) can be obtained by dividing the atomic mass ma of an isotope by the atomic mass constant mu yielding a dimensionless value. Thus, the atomic mass of a carbon-12 atom is 12 Da, but the relative isotopic mass of a carbon-12 atom is simply 12. The sum of relative isotopic masses of all atoms in a molecule is the relative molecular mass.
 The atomic mass of an isotope and the relative isotopic mass refers to a certain specific isotope of an element. Because substances are usually not isotopically pure, it is convenient to use the elemental atomic mass which is the average (mean) atomic mass of an element, weighted by the abundance of the isotopes. The dimensionless (standard) atomic weight is the weighted mean relative isotopic mass of a (typical naturally-occurring) mixture of isotopes.
 The atomic mass of atoms, ions, or atomic nuclei is slightly less than the sum of the masses of their constituent protons, neutrons, and electrons, due to binding energy mass loss (as per E = mc2).
"
Atomic_mass_unit,Physics,2,"The dalton or unified atomic mass unit (symbols: Da or u) is a unit of mass widely used in physics and chemistry. It is defined as 1/12 of the mass of an unbound neutral atom of carbon-12 in its nuclear and electronic ground state and at rest.[1][2] The atomic mass constant, denoted mu  is defined identically, giving mu = m(12C)/12 = 1 Da.[3] This unit is commonly used in physics and chemistry to express the mass of atomic-scale objects, such as atoms, molecules, and elementary particles, both for discrete instances and multiple types of ensemble averages. For example, an atom of helium-4 has a mass of 4.0026 Da. This is an intrinsic property of the isotope and all helium-4 have the same mass. Acetylsalicylic acid (aspirin), C9H8O4, has an average mass of approximately 180.157 Da. However, there are no acetylsalicylic acid molecules with this mass. The two most common masses of individual acetylsalicylic acid molecules are 180.04228 Da and 181.04565 Da. 
 The molecular masses of proteins, nucleic acids, and other large polymers are often expressed with the units kilodaltons (kDa), megadaltons (MDa), etc.[4] Titin, one of the largest known proteins, has a molecular mass of between 3 and 3.7 megadaltons.[5] The DNA of chromosome 1 in the human genome has about 249 million base pairs, each with an average mass of about 650 Da, or 156 GDa total.[6] The mole is a unit of amount of substance, widely used in chemistry and physics, which was originally defined so that the mass of one mole of a substance, measured in grams, would be numerically equal to the average mass of one of its constituent particles, measured in daltons. That is, the molar mass of a chemical compound was meant to be numerically equal to its average molecular mass.  For example, the average mass of one molecule of water is about 18.0153 daltons, and one mole of water is about 18.0153 grams.  A protein whose molecule has an average mass of 64 kDa would have a molar mass of 64 kg/mol. However, while this equality can be assumed for almost all practical purposes, it is now only approximate, because of the way mole was redefined on 20 May 2019.[4][1] In general, the mass in daltons of an atom is numerically close, but not exactly equal to the number of nucleons A contained in its nucleus. It follows that the molar mass of a compound (grams per mole) is numerically close to the average number of nucleons contained in each molecule. By definition, the mass of an atom of carbon-12 is 12 daltons, which corresponds with the number of nucleons that it has (6 protons and 6 neutrons). However, the mass of an atomic-scale object is affected by the binding energy of the nucleons in its atomic nuclei, as well as the mass and binding energy of its electrons.  Therefore, this equality holds only for the carbon-12 atom in the stated conditions, and will vary for other substances. For example, the mass of one unbound atom of the common hydrogen isotope (hydrogen-1, protium) is 1.007825032241(94) Da, the mass of one free neutron is 1.00866491595(49) Da,[7] and the mass of one hydrogen-2 (deuterium) atom is 2.014101778114(122) Da.[8]  In general, the difference (mass defect) is less than 0.1%; exceptions include hydrogen-1 (about 0.8%), helium-3 (0.5%), lithium (0.25%) and beryllium (0.15%). 
 The unified atomic mass unit and the dalton should not be confused with the unit of mass in the atomic units systems, which is instead the electron rest mass (me).
"
Atomic_number,Physics,2,"
 The atomic number or proton number (symbol Z) of a chemical element is the number of protons found in the nucleus of every atom of that element. The atomic number uniquely identifies a chemical element. It is identical to the charge number of the nucleus. In an uncharged atom, the atomic number is also equal to the number of electrons.
 The sum of the atomic number Z and the number of neutrons N gives the mass number A of an atom. Since protons and neutrons have approximately the same mass (and the mass of the electrons is negligible for many purposes) and the mass defect of nucleon binding is always small compared to the nucleon mass, the atomic mass of any atom, when expressed in unified atomic mass units (making a quantity called the ""relative isotopic mass""), is within 1% of the whole number A.
 Atoms with the same atomic number but different neutron numbers, and hence different mass numbers, are known as isotopes. A little more than three-quarters of naturally occurring elements exist as a mixture of isotopes (see monoisotopic elements), and the average isotopic mass of an isotopic mixture for an element (called the relative atomic mass) in a defined environment on Earth, determines the element's standard atomic weight. Historically, it was these atomic weights of elements (in comparison to hydrogen) that were the quantities measurable by chemists in the 19th century.
 The conventional symbol Z comes from the German word Zahl meaning number, which, before the modern synthesis of ideas from chemistry and physics, merely denoted an element's numerical place in the periodic table, whose order is approximately, but not completely, consistent with the order of the elements by atomic weights. Only after 1915, with the suggestion and evidence that this Z number was also the nuclear charge and a physical characteristic of atoms, did the word Atomzahl (and its English equivalent atomic number) come into common use in this context.
"
Atomic_orbital,Physics,2,"In atomic theory and quantum mechanics, an atomic orbital is a mathematical function describing the location and wave-like behavior of an electron in an atom.[1] This function can be used to calculate the probability of finding any electron of an atom in any specific region around the atom's nucleus. The term atomic orbital may also refer to the physical region or space where the electron can be calculated to be present, as predicted by the particular mathematical form of the orbital.[2] Each orbital in an atom is characterized by a unique set of values of the three quantum numbers n, ℓ, and m,[dubious  – discuss] which respectively correspond to the electron's energy, angular momentum, and an angular momentum vector component (the magnetic quantum number). Each such orbital can be occupied by a maximum of two electrons, each with its own spin quantum number s. The simple names s orbital, p orbital, d orbital, and f orbital refer to orbitals with angular momentum quantum number ℓ = 0, 1, 2, and 3 respectively. These names, together with the value of n, are used to describe the electron configurations of atoms. They are derived from the description by early spectroscopists of certain series of alkali metal spectroscopic lines as sharp, principal, diffuse, and fundamental. Orbitals for ℓ > 3 continue alphabetically, omitting j (g, h, i, k, ...)[3][4][5] because some languages do not distinguish between the letters ""i"" and ""j"".[6] Atomic orbitals are the basic building blocks of the atomic orbital model (alternatively known as the electron cloud or wave mechanics model), a modern framework for visualizing the submicroscopic behavior of electrons in matter. In this model the electron cloud of a multi-electron atom may be seen as being built up (in approximation) in an electron configuration that is a product of simpler hydrogen-like atomic orbitals. The repeating periodicity of the blocks of 2, 6, 10, and 14 elements within sections of the periodic table arises naturally from the total number of electrons that occupy a complete set of s, p, d, and f atomic orbitals, respectively, although for higher values of the quantum number n, particularly when the atom in question bears a positive charge, the energies of certain sub-shells become very similar and so the order in which they are said to be populated by electrons (e.g. Cr = [Ar]4s13d5 and Cr2+ = [Ar]3d4) can only be rationalized somewhat arbitrarily.
"
Atomic_packing_factor,Physics,2,"In crystallography, atomic packing factor (APF), packing efficiency, or packing fraction is the fraction of volume in a crystal structure that is occupied by constituent particles. It is a dimensionless quantity and always less than unity. In atomic systems, by convention, the APF is determined by assuming that atoms are rigid spheres. The radius of the spheres is taken to be the maximum value such that the atoms do not overlap. For one-component crystals (those that contain only one type of particle), the packing fraction is represented mathematically by
 where Nparticle is the number of particles in the unit cell, Vparticle is the volume of each particle, and Vunit cell is the volume occupied by the unit cell. It can be proven mathematically that for one-component structures, the most dense arrangement of atoms has an APF of about 0.74 (see Kepler conjecture), obtained by the close-packed structures. For multiple-component structures (such as with interstitial alloys), the APF can exceed 0.74.
 The atomic packing factor of a unit cell is relevant to the study of materials science, where it explains many properties of materials. For example, metals with a high atomic packing factor will have a higher ""workability"" (malleability or ductility), similar to how a road is smoother when the stones are closer together, allowing metal atoms to slide past one another more easily.
"
Atomic_physics,Physics,2,"Atomic physics is the field of physics that studies atoms as an isolated system of electrons and an atomic nucleus. It is primarily concerned with the arrangement of electrons around the nucleus and
the processes by which these arrangements change. This comprises ions, neutral atoms and, unless otherwise stated, it can be assumed that the term atom includes ions.
 The term atomic physics can be associated with nuclear power and nuclear weapons, due to the synonymous use of atomic and nuclear in standard English. Physicists distinguish between atomic physics—which deals with the atom as a system consisting of a nucleus and electrons—and nuclear physics, which studies nuclear reactions and special properties of atomic nuclei.
 As with many scientific fields, strict delineation can be highly contrived and atomic physics is often considered in the wider context of atomic, molecular, and optical physics. Physics research groups are usually so classified.
"
Atomic_structure,Physics,2,"
 An atom is the smallest unit of ordinary matter that forms a chemical element. Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms. Atoms are extremely small, typically around 100 picometers across. They are so small that accurately predicting their behavior using classical physics—as if they were tennis balls, for example—is not possible due to quantum effects.
 Every atom is composed of a nucleus and one or more electrons bound to the nucleus. The nucleus is made of one or more protons and a number of neutrons. Only the most common variety of hydrogen has no neutrons. More than 99.94% of an atom's mass is in the nucleus. The protons have a positive electric charge, the electrons have a negative electric charge, and the neutrons have no electric charge. If the number of protons and electrons are equal, then the atom is electrically neutral. If an atom has more or fewer electrons than protons, then it has an overall negative or positive charge, respectively – such atoms are called ions.
 The electrons of an atom are attracted to the protons in an atomic nucleus by the electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by the nuclear force. This force is usually stronger than the electromagnetic force that repels the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force. In this case, the nucleus splits and leaves behind different elements. This is a form of nuclear decay.
 The number of protons in the nucleus is the atomic number and it defines to which chemical element the atom belongs. For example, any atom that contains 29 protons is copper. The number of neutrons defines the isotope of the element. Atoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules or crystals. The ability of atoms to associate and dissociate is responsible for most of the physical changes observed in nature. Chemistry is the discipline that studies these changes.
"
Atomic_weight,Physics,2,"Relative atomic mass (symbol: Ar) or atomic weight is a dimensionless physical quantity defined as the ratio of the average mass of atoms of a chemical element in a given sample to the atomic mass constant. The atomic mass constant (symbol: mu) is defined as being 1/12 of the mass of a carbon-12 atom.[1][2] Since both quantities in the ratio are masses, the resulting value is dimensionless; hence the value is said to be relative.
 For a single given sample, the relative atomic mass of a given element is the weighted arithmetic mean of the masses of the individual atoms (including their isotopes) that are present in the sample. This quantity can vary substantially between samples because the sample's origin (and therefore its radioactive history or diffusion history) may have produced unique combinations of isotopic abundances. For example, due to a different mixture of stable carbon-12 and carbon-13 isotopes, a sample of elemental carbon from volcanic methane will have a different relative atomic mass than one collected from plant or animal tissues.
 The more common, and more specific quantity known as standard atomic weight (Ar, standard) is an application of the relative atomic mass values obtained from multiple different samples. It is sometimes interpreted as the expected range of the relative atomic mass values for the atoms of a given element from all terrestrial sources, with the various sources being taken from Earth.[3] ""Atomic weight"" is often loosely and incorrectly used as a synonym for standard atomic weight (incorrectly because standard atomic weights are not from a single sample). Standard atomic weight is nevertheless the most widely published variant of relative atomic mass.
 Additionally, the continued use of the term ""atomic weight"" (for any element) as opposed to ""relative atomic mass"" has attracted considerable controversy since at least the 1960s, mainly due to the technical difference between weight and mass in physics.[4] Still, both terms are officially sanctioned by the IUPAC. The term ""relative atomic mass"" now seems to be replacing ""atomic weight"" as the preferred term, although the term ""standard atomic weight"" (as opposed to the more correct ""standard relative atomic mass"") continues to be used.
"
Audio_frequency,Physics,2,"An audio frequency or audible frequency (AF) is a periodic vibration whose frequency is in the band audible to the average human, the human hearing range. The SI unit of frequency is the hertz (Hz). It is the property of sound that most determines pitch.[1] The generally accepted standard hearing range for humans is 20 to 20,000 Hz.[2][3][4]  In air at atmospheric pressure, these represent sound waves with wavelengths of 17 meters (56 ft) to 1.7 centimetres (0.67 in).  Frequencies below 20 Hz are generally felt rather than heard, assuming the amplitude of the vibration is great enough. High frequencies are the first to be affected by hearing loss due to age or prolonged exposure to very loud noises.[5][failed verification]"
Speed#Average_speed,Physics,2,"
 In everyday use and in kinematics, the speed of an object is the magnitude of the change of its position; it is thus a scalar quantity.[1] The average speed of an object in an interval of time is the distance travelled by the object divided by the duration of the interval;[2] the instantaneous speed is the limit of the average speed as the duration of the time interval approaches zero.
 Speed has the dimensions of distance divided by time. The SI unit of speed is the metre per second, but the most common unit of speed in everyday usage is the kilometre per hour or, in the US and the UK, miles per hour. For air and marine travel the knot is commonly used.
 The fastest possible speed at which energy or information can travel, according to special relativity, is the speed of light in a vacuum c = 299792458 metres per second (approximately 1079000000 km/h or 671000000 mph). Matter cannot quite reach the speed of light, as this would require an infinite amount of energy. In relativity physics, the concept of rapidity replaces the classical idea of speed.
"
Avogadro_constant,Physics,2,"
 The Avogadro constant (NA[1] or L[2]) is the proportionality factor that relates the number of constituent particles (usually molecules, atoms or ions) in a sample with the amount of substance in that sample.  Its SI unit is the reciprocal mole, and it is defined as NA = 6.02214076×1023 mol−1.[3][1][4][5][6]  It is named after the Italian scientist Amedeo Avogadro.[7]Although this is called Avogadro's constant (or number), he is not the chemist who determined its value. Stanislao Cannizzarro explained this number four years after Avogadro's death while at the Karlsruhe Congress in 1860.[8] The numeric value of the Avogadro constant expressed in reciprocal mole, a dimensionless number, is called the Avogadro number, sometimes denoted N[9][10] or N0,[11][12] which is thus the number of particles that are contained in one mole, exactly 6.02214076×1023.[4][1] The value of the Avogadro constant was chosen so that the mass of one mole of a chemical compound, in grams, is numerically equal (for all practical purposes) to the average mass of one molecule of the compound, in daltons (universal atomic mass units); one dalton being 1/12 of the mass of one carbon-12 atom, which is approximately the mass of one nucleon (proton or neutron). Twelve grams of carbon contains one mole of carbon atoms.  
 For example, the average mass of one molecule of water is about 18.0153 daltons, and one mole of water (N molecules) is about 18.0153 grams. Thus, the Avogadro constant NA is the proportionality factor that relates the molar mass of a substance to the average mass of one molecule, and the Avogadro number is also the approximate number of nucleons in one gram of ordinary matter.[13] The Avogadro constant also relates the molar volume of a substance to the average volume nominally occupied by one of its particles, when both are expressed in the same units of volume.  For example, since the molar volume of water in ordinary conditions is about 18 mL/mol, the volume occupied by one molecule of water is about 18/6.022×10−23 mL, or about 30 Å3 (cubic angstroms).  For a crystalline substance, it similarly relates its molar volume (in mL/mol), the volume of the repeating unit cell of the crystals (in mL), and the number of molecules in that cell.
 The Avogadro number (or constant) has been defined in many different ways through its long history. Its approximate value was first determined, indirectly, by Josef Loschmidt in 1865.[14] (Avogadro's number is closely related to the Loschmidt constant, and the two concepts are sometimes confused.) It was initially defined by Jean Perrin as the number of atoms in 16 grams of oxygen.[7]  It was later redefined in the 14th conference of the International Bureau of Weights and Measures (BIPM) as the number of atoms in 12 grams of the isotope carbon-12 (12C).[15]  In each case, the mole was defined as the quantity of a substance that contained the same number of atoms as those reference samples.  In particular, when carbon-12 was the reference, one mole of carbon-12 was exactly 12 grams of the element.
 These definitions meant that the value of the Avogadro number depended on the experimentally determined value of the mass (in grams) of one atom of those elements, and therefore it was known only to a limited number of decimal digits.  However, in its 26th Conference, the BIPM adopted a different approach: effective 20 May 2019, it defined the Avogadro number as the exact value N = 6.02214076×1023, and redefined the mole as the amount of a substance under consideration that contains N constituent particles of the substance.  Under the new definition, the mass of one mole of any substance (including hydrogen, carbon-12, and oxygen-16) is N times the average mass of one of its constituent particles—a physical quantity whose precise value has to be determined experimentally for each substance.
"
Avogadro_number,Physics,2,"
 The Avogadro constant (NA[1] or L[2]) is the proportionality factor that relates the number of constituent particles (usually molecules, atoms or ions) in a sample with the amount of substance in that sample.  Its SI unit is the reciprocal mole, and it is defined as NA = 6.02214076×1023 mol−1.[3][1][4][5][6]  It is named after the Italian scientist Amedeo Avogadro.[7]Although this is called Avogadro's constant (or number), he is not the chemist who determined its value. Stanislao Cannizzarro explained this number four years after Avogadro's death while at the Karlsruhe Congress in 1860.[8] The numeric value of the Avogadro constant expressed in reciprocal mole, a dimensionless number, is called the Avogadro number, sometimes denoted N[9][10] or N0,[11][12] which is thus the number of particles that are contained in one mole, exactly 6.02214076×1023.[4][1] The value of the Avogadro constant was chosen so that the mass of one mole of a chemical compound, in grams, is numerically equal (for all practical purposes) to the average mass of one molecule of the compound, in daltons (universal atomic mass units); one dalton being 1/12 of the mass of one carbon-12 atom, which is approximately the mass of one nucleon (proton or neutron). Twelve grams of carbon contains one mole of carbon atoms.  
 For example, the average mass of one molecule of water is about 18.0153 daltons, and one mole of water (N molecules) is about 18.0153 grams. Thus, the Avogadro constant NA is the proportionality factor that relates the molar mass of a substance to the average mass of one molecule, and the Avogadro number is also the approximate number of nucleons in one gram of ordinary matter.[13] The Avogadro constant also relates the molar volume of a substance to the average volume nominally occupied by one of its particles, when both are expressed in the same units of volume.  For example, since the molar volume of water in ordinary conditions is about 18 mL/mol, the volume occupied by one molecule of water is about 18/6.022×10−23 mL, or about 30 Å3 (cubic angstroms).  For a crystalline substance, it similarly relates its molar volume (in mL/mol), the volume of the repeating unit cell of the crystals (in mL), and the number of molecules in that cell.
 The Avogadro number (or constant) has been defined in many different ways through its long history. Its approximate value was first determined, indirectly, by Josef Loschmidt in 1865.[14] (Avogadro's number is closely related to the Loschmidt constant, and the two concepts are sometimes confused.) It was initially defined by Jean Perrin as the number of atoms in 16 grams of oxygen.[7]  It was later redefined in the 14th conference of the International Bureau of Weights and Measures (BIPM) as the number of atoms in 12 grams of the isotope carbon-12 (12C).[15]  In each case, the mole was defined as the quantity of a substance that contained the same number of atoms as those reference samples.  In particular, when carbon-12 was the reference, one mole of carbon-12 was exactly 12 grams of the element.
 These definitions meant that the value of the Avogadro number depended on the experimentally determined value of the mass (in grams) of one atom of those elements, and therefore it was known only to a limited number of decimal digits.  However, in its 26th Conference, the BIPM adopted a different approach: effective 20 May 2019, it defined the Avogadro number as the exact value N = 6.02214076×1023, and redefined the mole as the amount of a substance under consideration that contains N constituent particles of the substance.  Under the new definition, the mass of one mole of any substance (including hydrogen, carbon-12, and oxygen-16) is N times the average mass of one of its constituent particles—a physical quantity whose precise value has to be determined experimentally for each substance.
"
Avogadro%27s_law,Physics,2,"Avogadro's law (sometimes referred to as Avogadro's hypothesis or Avogadro's principle) is an experimental gas law relating the volume of a gas to the amount of substance of gas present.[1] The law is a specific case of the ideal gas law. A modern statement is:
 Avogadro's law states that ""equal volumes of all gases, at the same temperature and pressure, have the same number of molecules.""[1] For a given mass of an ideal gas, the volume and amount (moles) of the gas are directly proportional if the temperature and pressure are constant.
 The law is named after Amedeo Avogadro who, in 1812,[2][3] hypothesized that two given samples of an ideal gas, of the same volume and at the same temperature and pressure, contain the same number of molecules. As an example, equal volumes of molecular hydrogen and nitrogen contain the same number of molecules when they are at the same temperature and pressure, and observe ideal gas behavior. In practice, real gases show small deviations from the ideal behavior and the law holds only approximately, but is still a useful approximation for scientists.
"
Axion,Physics,2,"The axion (/ˈæksiɒn/) is a hypothetical elementary particle postulated by the Peccei–Quinn theory in 1977 to resolve the strong CP problem in quantum chromodynamics (QCD). If axions exist and have low mass within a specific range, they are of interest as a possible component of cold dark matter.
"
Azimuthal_quantum_number,Physics,2,"The azimuthal quantum number is a quantum number for an atomic orbital that determines its orbital angular momentum and describes the shape of the orbital.  The azimuthal quantum number is the second of a set of quantum numbers which describe the unique quantum state of an electron (the others being the principal quantum number, the magnetic quantum number, and the spin quantum number).  It is also known as the orbital angular momentum quantum number, orbital quantum number or second quantum number, and is symbolized as ℓ (pronounced ell).
"
Babinet%27s_principle,Physics,2,"In physics, Babinet's principle[1] states that the diffraction pattern from an opaque body is identical to that from a hole of the same size and shape except for the overall forward beam intensity.  It was formulated in the 1800s by French physicist Jacques Babinet.
"
Background_radiation,Physics,2,"Background radiation is a measure of the level of ionizing radiation present in the environment at a particular location which is not due to deliberate introduction of radiation sources.
 Background radiation originates from a variety of sources, both natural and artificial. These include both cosmic radiation and environmental radioactivity from naturally occurring radioactive materials (such as radon and radium), as well as man-made medical X-rays, fallout from nuclear weapons testing and nuclear accidents.
"
Ballistics,Physics,2,"Ballistics is the field of mechanics concerned with the launching, flight behavior and impact effects of projectiles, especially ranged weapon munitions such as bullets, unguided bombs, rockets or the like; the science or art of designing and accelerating projectiles so as to achieve a desired performance.
 A ballistic body is a free-moving body with momentum which can be subject to forces such as the forces exerted by pressurized gases from a gun barrel or a propelling nozzle, normal force by rifling, and gravity and air drag during flight.
 A ballistic missile is a missile that is guided only during the relatively brief initial phase of powered flight and the trajectory is subsequently governed by the laws of classical mechanics; in contrast to (for example) a cruise missile which is aerodynamically guided in powered flight like a fixed-wing aircraft.
"
Balmer_series,Physics,2,"
 The Balmer series, or Balmer lines in atomic physics, is one of a set of six named series describing the spectral line emissions of the hydrogen atom. The Balmer series is calculated using the Balmer formula, an empirical equation discovered by Johann Balmer in 1885.
 The visible spectrum of light from hydrogen displays four wavelengths, 410 nm, 434 nm, 486 nm, and 656 nm, that correspond to emissions of photons by electrons in excited states transitioning to the quantum level described by the principal quantum number n equals 2.[1] There are several prominent ultraviolet Balmer lines with wavelengths shorter than 400 nm. The number of these lines is an infinite continuum as it approaches a limit of 364.6 nm in the ultraviolet.
 After Balmer's discovery, five other hydrogen spectral series were discovered, corresponding to electrons transitioning to values of n other than two .
"
Barometer,Physics,2,"
 A barometer is a scientific instrument that is used to measure air pressure in a certain environment. Pressure tendency can forecast short term changes in the weather. Many measurements of air pressure are used within surface weather analysis to help find surface troughs, pressure systems and frontal boundaries.
 Barometers and pressure altimeters (the most basic and common type of altimeter) are essentially the same instrument, but used for different purposes. An altimeter is intended to be used at different levels matching the corresponding atmospheric pressure to the altitude, while a barometer is kept at the same level and measures subtle pressure changes caused by weather and elements of weather.  The average atmospheric pressure on the earth's surface varies between 940 and 1040 hPa (mbar). The average atmospheric pressure at sea level is 1013 hPa (mbar).
"
Baryon,Physics,2,"In particle physics, a baryon is a type of composite subatomic particle which contains an odd number of valence quarks (at least 3).[1] Baryons belong to the hadron family of particles; hadrons are composed of quarks. Baryons are also classified as fermions because they have half-integer spin.
 The name ""baryon"", introduced by Abraham Pais,[2] comes from the Greek word for ""heavy"" (βαρύς, barýs), because, at the time of their naming, most known elementary particles had lower masses than the baryons. Each baryon has a corresponding antiparticle (antibaryon) where their corresponding antiquarks replace quarks. For example, a proton is made of two up quarks and one down quark; and its corresponding antiparticle, the antiproton, is made of two up antiquarks and one down antiquark.
 Because they are composed of quarks, baryons participate in the strong interaction, which is mediated by particles known as gluons. The most familiar baryons are protons and neutrons, both of which contain three quarks, and for this reason they are sometimes called triquarks.  These particles make up most of the mass of the visible matter in the universe and compose the nucleus of every atom. (Electrons, the other major component of the atom, are members of a different family of particles called leptons; leptons do not interact via the strong force.) Exotic baryons containing five quarks, called pentaquarks, have also been discovered and studied.
 A census of the Universe's baryons indicates that 10% of them could be found inside galaxies, 50 to 60% in the circumgalactic medium,[3] and the remaining 30 to 40% could be located in the warm–hot intergalactic medium (WHIM).[4]"
Battery_(electricity),Physics,2,"
 A battery is a device consisting of one or more electrochemical cells with external connections[1] for powering electrical devices such as flashlights, mobile phones, and electric cars. When a battery is supplying electric power, its positive terminal is the cathode and its negative terminal is the anode.[2] The terminal marked negative is the source of electrons that will flow through an external electric circuit to the positive terminal. When a battery is connected to an external electric load, a redox reaction converts high-energy reactants to lower-energy products, and the free-energy difference is delivered to the external circuit as electrical energy.[3] Historically the term ""battery"" specifically referred to a device composed of multiple cells, however the usage has evolved to include devices composed of a single cell.[4] Primary (single-use or ""disposable"") batteries are used once and discarded, as the electrode materials are irreversibly changed during discharge; a common example is the alkaline battery used for flashlights and a multitude of portable electronic devices. Secondary (rechargeable) batteries can be discharged and recharged multiple times using an applied electric current; the original composition of the electrodes can be restored by reverse current. Examples include the lead-acid batteries used in vehicles and lithium-ion batteries used for portable electronics such as laptops and mobile phones.
 Batteries come in many shapes and sizes, from miniature cells used to power hearing aids and wristwatches to small, thin cells used in smartphones, to large lead acid batteries or lithium-ion batteries in vehicles, and at the largest extreme, huge battery banks the size of rooms that provide standby or emergency power for telephone exchanges and computer data centers.
 Batteries have much lower specific energy (energy per unit mass) than common fuels such as gasoline. In automobiles, this is somewhat offset by the higher efficiency of electric motors in converting chemical energy to mechanical work, compared to combustion engines.
"
Beam_(structure),Physics,2,"A beam is a structural element that primarily resists loads applied laterally to  the beam's axis. Its mode of deflection is primarily by bending. The loads applied to the beam result in reaction forces at the beam's support points. The total effect of all the forces acting on the beam is to produce shear forces and bending moments within the beams, that in turn induce internal stresses, strains and deflections of the beam. Beams are characterized by their manner of support, profile (shape of cross-section), equilibrium conditions, length, and their material.
 Beams are traditionally descriptions of building or civil engineering structural elements, but any structures such as automotive automobile frames, aircraft components, machine frames, and other mechanical or structural systems contain beam structures that are designed to carry lateral loads are analyzed in a similar fashion.
"
Bending,Physics,2,"In applied mechanics, bending (also known as flexure) characterizes the behavior of a slender structural element subjected to an external load applied perpendicularly to a longitudinal axis of the element.
 The structural element is assumed to be such that at least one of its dimensions is a small fraction, typically 1/10 or less, of the other two.[1] When the length is considerably longer than the width and the thickness, the element is called a beam. For example, a closet rod sagging under the weight of clothes on clothes hangers is an example of a beam experiencing bending. On the other hand, a shell is a structure of any geometric form where the length and the width are of the same order of magnitude but the thickness of the structure (known as the 'wall') is considerably smaller. A large diameter, but thin-walled, short tube supported at its ends and loaded laterally is an example of a shell experiencing bending.
 In the absence of a qualifier, the term bending is ambiguous because bending can occur locally in all objects. Therefore, to make the usage of the term more precise, engineers refer to a specific object such as; the bending of rods,[2] the bending of beams,[1] the bending of plates,[3] the bending of shells[2] and so on.
"
Bending_moment,Physics,2,"In solid mechanics, a bending moment is the reaction induced in a structural element when an external force or moment is applied to the element, causing the element to bend.[1][2] The most common or simplest structural element subjected to bending moments is the beam.  The diagram shows a beam which is simply supported (free to rotate and therefore lacking bending moments) at both ends; the ends can only react to the shear loads.  Other beams can have both ends fixed; therefore each end support has both bending moments and shear reaction loads. Beams can also have one end fixed and one end simply supported. The simplest type of beam is the cantilever, which is fixed at one end and is free at the other end (neither simple or fixed). In reality, beam supports are usually neither absolutely fixed nor absolutely rotating freely.
 The internal reaction loads in a cross-section of the structural element can be resolved into a resultant force and a resultant couple.  For equilibrium, the moment created by external forces (and external moments) must be balanced by the couple induced by the internal loads. The resultant internal couple is called the bending moment while the resultant internal force is called the shear force (if it is transverse to the plane of element) or the normal force (if it is along the plane of the element).
 The bending moment at a section through a structural element may be defined as the sum of the moments about that section of all external forces acting to one side of that section. The forces and moments on either side of the section must be equal in order to counteract each other and maintain a state of equilibrium so the same bending moment will result from summing the moments, regardless of which side of the section is selected. If clockwise bending moments are taken as negative, then a negative bending moment within an element will cause ""hogging"", and a positive moment will cause ""sagging"". It is therefore clear that a point of zero bending moment within a beam is a point of contraflexure—that is, the point of transition from hogging to sagging or vice versa.
 Moments and torques are measured as a force multiplied by a distance so they have as unit newton-metres (N·m), or pound-foot (lbf·ft). The concept of bending moment is very important in engineering (particularly in civil and mechanical engineering) and physics.
"
Bernoulli%27s_equation,Physics,2,"In fluid dynamics, Bernoulli's principle states that an increase in the speed of a fluid occurs simultaneously with a decrease in static pressure or a decrease in the fluid's potential energy.[1](Ch.3)[2](§ 3.5) The principle is named after Daniel Bernoulli who published it in his book Hydrodynamica in 1738.[3] Although Bernoulli deduced that pressure decreases when the flow speed increases, it was Leonhard Euler who derived Bernoulli's equation in its usual form in 1752.[4][5] The principle is only applicable for isentropic flows: when the effects of irreversible processes (like turbulence) and non-adiabatic processes (e.g. heat radiation) are small and can be neglected.
 Bernoulli's principle can be applied to various types of fluid flow, resulting in various forms of Bernoulli's equation; there are different forms of Bernoulli's equation for different types of flow. The simple form of Bernoulli's equation is valid for incompressible flows (e.g. most liquid flows and gases moving at low Mach number). More advanced forms may  be applied to compressible flows at higher Mach numbers (see the derivations of the Bernoulli equation). 
 Bernoulli's principle can be derived from the principle of conservation of energy. This states that, in a steady flow, the sum of all forms of energy in a fluid along a streamline is the same at all points on that streamline. This requires that the sum of kinetic energy, potential energy and internal energy remains constant.[2](§  3.5) Thus an increase in the speed of the fluid – implying an increase in its kinetic energy (dynamic pressure) – occurs with a simultaneous decrease in (the sum of) its potential energy (including the static pressure) and internal energy. If the fluid is flowing out of a reservoir, the sum of all forms of energy is the same on all streamlines because in a reservoir the energy per unit volume (the sum of pressure and gravitational potential ρ g h) is the same everywhere.[6](Example 3.5) Bernoulli's principle can also be derived directly from Isaac Newton's Second Law of Motion.  If a small volume of fluid is flowing horizontally from a region of high pressure to a region of low pressure, then there is more pressure behind than in front. This gives a net force on the volume, accelerating it along the streamline.[a][b][c] Fluid particles are subject only to pressure and their own weight. If a fluid is flowing horizontally and along a section of a streamline, where the speed increases it can only be because the fluid on that section has moved from a region of higher pressure to a region of lower pressure; and if its speed decreases, it can only be because it has moved from a region of lower pressure to a region of higher pressure. Consequently, within a fluid flowing horizontally, the highest speed occurs where the pressure is lowest, and the lowest speed occurs where the pressure is highest.[10]"
Bernoulli%27s_principle,Physics,2,"In fluid dynamics, Bernoulli's principle states that an increase in the speed of a fluid occurs simultaneously with a decrease in static pressure or a decrease in the fluid's potential energy.[1](Ch.3)[2](§ 3.5) The principle is named after Daniel Bernoulli who published it in his book Hydrodynamica in 1738.[3] Although Bernoulli deduced that pressure decreases when the flow speed increases, it was Leonhard Euler who derived Bernoulli's equation in its usual form in 1752.[4][5] The principle is only applicable for isentropic flows: when the effects of irreversible processes (like turbulence) and non-adiabatic processes (e.g. heat radiation) are small and can be neglected.
 Bernoulli's principle can be applied to various types of fluid flow, resulting in various forms of Bernoulli's equation; there are different forms of Bernoulli's equation for different types of flow. The simple form of Bernoulli's equation is valid for incompressible flows (e.g. most liquid flows and gases moving at low Mach number). More advanced forms may  be applied to compressible flows at higher Mach numbers (see the derivations of the Bernoulli equation). 
 Bernoulli's principle can be derived from the principle of conservation of energy. This states that, in a steady flow, the sum of all forms of energy in a fluid along a streamline is the same at all points on that streamline. This requires that the sum of kinetic energy, potential energy and internal energy remains constant.[2](§  3.5) Thus an increase in the speed of the fluid – implying an increase in its kinetic energy (dynamic pressure) – occurs with a simultaneous decrease in (the sum of) its potential energy (including the static pressure) and internal energy. If the fluid is flowing out of a reservoir, the sum of all forms of energy is the same on all streamlines because in a reservoir the energy per unit volume (the sum of pressure and gravitational potential ρ g h) is the same everywhere.[6](Example 3.5) Bernoulli's principle can also be derived directly from Isaac Newton's Second Law of Motion.  If a small volume of fluid is flowing horizontally from a region of high pressure to a region of low pressure, then there is more pressure behind than in front. This gives a net force on the volume, accelerating it along the streamline.[a][b][c] Fluid particles are subject only to pressure and their own weight. If a fluid is flowing horizontally and along a section of a streamline, where the speed increases it can only be because the fluid on that section has moved from a region of higher pressure to a region of lower pressure; and if its speed decreases, it can only be because it has moved from a region of lower pressure to a region of higher pressure. Consequently, within a fluid flowing horizontally, the highest speed occurs where the pressure is lowest, and the lowest speed occurs where the pressure is highest.[10]"
Bessel_function,Physics,2,"
 Bessel functions, first defined by the mathematician Daniel Bernoulli and then generalized by Friedrich Bessel, are canonical solutions y(x) of Bessel's differential equation
 for an arbitrary complex number α, the order of the Bessel function. Although α and −α produce the same differential equation, it is conventional to define different Bessel functions for these two values in such a way that the Bessel functions are mostly smooth functions of α.
 The most important cases are when α is an integer or half-integer. Bessel functions for integer α are also known as cylinder functions or the cylindrical harmonics because they appear in the solution to Laplace's equation in cylindrical coordinates. Spherical Bessel functions with half-integer α are obtained when the Helmholtz equation is solved in spherical coordinates.
"
Beta_decay,Physics,2,"In nuclear physics, beta decay (β-decay) is a type of radioactive decay in which a beta particle (fast energetic electron or positron) is emitted from an atomic nucleus, transforming the original nuclide to an isobar. For example, beta decay of a neutron transforms it into a proton by the emission of an electron accompanied by an antineutrino; or, conversely a proton is converted into a neutron by the emission of a positron with a neutrino in so-called positron emission. Neither the beta particle nor its associated (anti-)neutrino exist within the nucleus prior to beta decay, but are created in the decay process. By this process, unstable atoms obtain a more stable ratio of protons to neutrons. The probability of a nuclide decaying due to beta and other forms of decay is determined by its nuclear binding energy. The binding energies of all existing nuclides form what is called the nuclear band or valley of stability.[1] For either electron or positron emission to be energetically possible, the energy release (see below) or Q value must be positive.
 Beta decay is a consequence of the weak force, which is characterized by relatively lengthy decay times. Nucleons are composed of up quarks and down quarks,[2] and the weak force allows a quark to change its flavour by emission of a W boson leading to creation of an electron/antineutrino or positron/neutrino pair. For example, a neutron, composed of two down quarks and an up quark, decays to a proton composed of a down quark and two up quarks.
 Electron capture is sometimes included as a type of beta decay,[3] because the basic nuclear process, mediated by the weak force, is the same. In electron capture, an inner atomic electron is captured by a proton in the nucleus, transforming it into a neutron, and an electron neutrino is released.
"
Beta_particle,Physics,2,"A beta particle, also called beta ray or beta radiation (symbol β), is a high-energy, high-speed electron or positron emitted by the radioactive decay of an atomic nucleus during the process of beta decay. There are two forms of beta decay, β− decay and β+ decay, which produce electrons and positrons respectively.[2] Beta particles with an energy of 0.5 MeV have a range of about one metre in air; the distance is dependent on the particle energy.
 Beta particles are a type of ionizing radiation and for radiation protection purposes are regarded as being more ionising than gamma rays, but less ionising than alpha particles. The higher the ionising effect, the greater the damage to living tissue, but also the lower the penetrating power of the radiation.
"
Big_Bang,Physics,2,"
 The Big Bang theory is a cosmological model of the observable universe from the earliest known periods through its subsequent large-scale evolution.[1][2][3] The model describes how the universe expanded from an initial state of extremely high density and high temperature,[4] and offers a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the cosmic microwave background (CMB) radiation, and large-scale structure.
 Crucially, the theory is compatible with Hubble-Lemaître law – the observation that the farther away galaxies are, the faster they are moving away from Earth. Extrapolating this cosmic expansion backwards in time using the known laws of physics, the theory describes a high density state preceded by a singularity in which space and time lose meaning.[5] There is no evidence of any phenomena prior to the singularity. Detailed measurements of the expansion rate of the universe place the Big Bang at around 13.8 billion years ago, which is thus considered the age of the universe.[6] After its initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, and later atoms. Giant clouds of these primordial elements – mostly hydrogen, with some helium and lithium – later coalesced through gravity, forming early stars and galaxies, the descendants of which are visible today. Besides these primordial building materials, astronomers observe the gravitational effects of an unknown dark matter surrounding galaxies. Most of the gravitational potential in the universe seems to be in this form, and the Big Bang theory and various observations indicate that this gravitational potential is not made of baryonic matter, such as normal atoms. Measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to dark energy's existence.[7] Georges Lemaître first noted in 1927 that an expanding universe could be traced back in time to an originating single point, which he called the ""primeval atom"". For several decades, the scientific community was divided between supporters of the Big Bang and the rival steady-state model, but a wide range of empirical evidence has strongly favored the Big Bang, which is now universally accepted.[8] Edwin Hubble confirmed through analysis of galactic redshifts in 1929 that galaxies are indeed drifting apart; this is important observational evidence for an expanding universe. In 1964, the CMB was discovered, which was crucial evidence in favor of the hot Big Bang model,[9] since that theory predicted a uniform background radiation throughout the universe.
"
Binding_energy,Physics,2,"
 In physics and chemistry, binding energy is the smallest amount of energy required to remove a particle from a system of particles or to disassemble a system of particles into individual parts.[1]  In the former meaning the term is predominantly used in condensed matter physics, atomic physics, and chemistry, whereas in nuclear physics the term separation energy is used.  
 A bound system is typically at a lower energy level than its unbound constituents. According to relativity theory, a ΔE decrease in the total energy of a system is accompanied by a decrease ΔM in the total mass, where ΔM⋅c2=ΔE.[2]"
Binomial_random_variable,Physics,2,"In probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own Boolean-valued outcome: success (with probability p) or failure (with probability q = 1 − p). A single success/failure experiment is also called a Bernoulli trial or Bernoulli experiment, and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., n = 1, the binomial distribution is a Bernoulli distribution. The binomial distribution is the basis for the popular binomial test of statistical significance.
 The binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size N. If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one.  However, for N much larger than n, the binomial distribution remains a good approximation, and is widely used.
"
Biocatalysis,Physics,2,"Biocatalysis refers to the use of living (biological) systems or their parts to speed up (catalyze) chemical reactions. In biocatalytic processes, natural catalysts, such as enzymes, perform chemical transformations on organic compounds. Both enzymes that have been more or less isolated and enzymes still residing inside living cells are employed for this task.[1][2][3] Modern biotechnology, specifically directed evolution, has made the production of modified or non-natural enzymes  possible. This has enabled the development of enzymes that can catalyze novel small molecule transformations that may be difficult or impossible using classical synthetic organic chemistry. Utilizing natural or modified enzymes to perform organic synthesis is termed chemoenzymatic synthesis; the reactions performed by the enzyme are classified as chemoenzymatic reactions.
"
Biophysics,Physics,2,"Biophysics is an interdisciplinary science that applies approaches and methods traditionally used in physics to study biological phenomena.[1][2][3] Biophysics covers all scales of biological organization, from molecular to organismic and populations. Biophysical research shares significant overlap with biochemistry, molecular biology, physical chemistry, physiology, nanotechnology, bioengineering, computational biology, biomechanics, developmental biology and systems biology.
 The term biophysics was originally introduced by Karl Pearson in 1892.[4][5] Ambiguously, the term biophysics is also regularly used in academia to indicate the study of the physical quantities (e.g. electric current, temperature, stress, entropy) in biological systems, which is, by definition, performed by physiology. Nevertheless, other biological sciences also perform research on the biophysical properties of living organisms including molecular biology, cell biology, biophysics, and biochemistry.
"
Black_body,Physics,2,"
 A black body or blackbody is an idealized physical body that absorbs all incident electromagnetic radiation, regardless of frequency or angle of incidence. The name ""black body"" is given because it absorbs radiation in all frequencies, not because it only absorbs: a black body can emit black-body radiation. On the contrary, a white body is one with a ""rough surface that reflects all incident rays completely and uniformly in all directions.""[1] A black body in thermal equilibrium (that is, at a constant temperature) emits electromagnetic black-body radiation. The radiation is emitted according to Planck's law, meaning that it has a spectrum that is determined by the temperature alone (see figure at right), not by the body's shape or composition.
 An ideal black body in thermal equilibrium has two notable properties:[2] An approximate realization of a black surface is a hole in the wall of a large insulated enclosure (an oven, for example). Any light entering the hole is reflected or absorbed at the internal surfaces of the body and is unlikely to re-emerge, making the hole a nearly perfect absorber. When the radiation confined in such an enclosure is in thermal equilibrium, the radiation emitted from the hole will be as great as from any body at that equilibrium temperature.
 Real materials emit energy at a fraction—called the emissivity—of black-body energy levels. By definition, a black body in thermal equilibrium has an emissivity ε = 1. A source with a lower emissivity, independent of frequency, is often referred to as a gray body.[3][4]
Constructing black bodies with an emissivity as close to 1 as possible remains a topic of current interest.[5] In astronomy, the radiation from stars and planets is sometimes characterized in terms of an effective temperature, the temperature of a black body that would emit the same total flux of electromagnetic energy.
"
Black-body_radiation,Physics,2,"Black-body radiation is the thermal electromagnetic radiation within or surrounding a body in thermodynamic equilibrium with its environment, emitted by a black body (an idealized opaque, non-reflective body).  It has a specific spectrum of wavelengths, inversely related to intensity that depend only on the body's temperature, which is assumed for the sake of calculations and theory to be uniform and constant.[1][2][3][4] The thermal radiation spontaneously emitted by many ordinary objects can be approximated as black-body radiation.  A perfectly insulated enclosure that is in thermal equilibrium internally contains black-body radiation and will emit it through a hole made in its wall, provided the hole is small enough to have a negligible effect upon the equilibrium.
 In a dark room, a black body at room temperature appears black because most of the energy it radiates is in the infrared spectrum and cannot be perceived by the human eye. Since the human eye cannot perceive light waves below the visible frequency, a black body at the lowest just faintly visible temperature subjectively appears grey, even though its objective physical spectrum peak is in the infrared range.[5] The human eye perceives only black and white at low light levels as the light-sensitive retinal rods are more sensitive than cones. When the object becomes a little hotter, it appears dull red. As its temperature increases further it becomes bright red, orange, yellow, white, and ultimately blue-white.
 Although planets and stars are neither in thermal equilibrium with their surroundings nor perfect black bodies, black-body radiation is used as a first approximation for the energy they emit.[6] Black holes are near-perfect black bodies, in the sense that they absorb all the radiation that falls on them. It has been proposed that they emit black-body radiation (called Hawking radiation), with a temperature that depends on the mass of the black hole.[7] The term black body was introduced by Gustav Kirchhoff in 1860.[8] Black-body radiation is also called thermal radiation, cavity radiation, complete radiation or temperature radiation.
"
Block_and_tackle,Physics,2,"A block and tackle[1][2] or only tackle [3] is a system of two or more pulleys with a rope or cable threaded between them, usually used to lift heavy loads.
 The pulleys are assembled to form blocks and then blocks are paired so that one is fixed and one moves with the load. The rope is threaded through the pulleys to provide mechanical advantage that amplifies the force applied to the rope.[4] Hero of Alexandria described cranes formed from assemblies of pulleys in the first century. Illustrated versions of Hero's Mechanica (a book on raising heavy weights) show early block and tackle systems.[5]"
Bohr_model,Physics,2,"In atomic physics, the Bohr model or Rutherford–Bohr model, presented by Niels Bohr and Ernest Rutherford in 1913, is a system consisting of a small, dense nucleus surrounded by orbiting electrons—similar to the structure of the Solar System, but with attraction provided by electrostatic forces in place of gravity. After the cubical model (1902), the plum pudding model (1904), the Saturnian model (1904), and the Rutherford model (1911) came the Rutherford–Bohr model or just Bohr model for short (1913). The improvement over the 1911 Rutherford model mainly concerned the new quantum physical interpretation.
 The model's key success lay in explaining the Rydberg formula for the spectral emission lines of atomic hydrogen. While the Rydberg formula had been known experimentally, it did not gain a theoretical underpinning until the Bohr model was introduced. Not only did the Bohr model explain the reasons for the structure of the Rydberg formula, it also provided a justification for the fundamental physical constants that make up the formula's empirical results.
 The Bohr model is a relatively primitive model of the hydrogen atom, compared to the valence shell atom model. As a theory, it can be derived as a first-order approximation of the hydrogen atom using the broader and much more accurate quantum mechanics and thus may be considered to be an obsolete scientific theory. However, because of its simplicity, and its correct results for selected systems (see below for application), the Bohr model is still commonly taught to introduce students to quantum mechanics or energy level diagrams before moving on to the more accurate, but more complex, valence shell atom. A related model was originally proposed by Arthur Erich Haas in 1910 but was rejected. The quantum theory of the period between Planck's discovery of the quantum (1900) and the advent of a mature quantum mechanics (1925) is often referred to as the old quantum theory.
"
Boiling_point,Physics,2,"The boiling point of a substance is the temperature at which the vapor pressure of a liquid equals the pressure surrounding the liquid[1][2] and the liquid changes into a vapor.
 The boiling point of a liquid varies depending upon the surrounding environmental pressure.  A liquid in a partial vacuum has a lower boiling point than when that liquid is at atmospheric pressure.  A liquid at high pressure has a higher boiling point than when that liquid is at atmospheric pressure. For example, water boils at 100 °C (212 °F) at sea level, but at 93.4 °C (200.1 °F) at 1,905 metres (6,250 ft) [3] altitude. For a given pressure, different liquids will boil at different temperatures. 
 The normal boiling point (also called the atmospheric boiling point or the atmospheric pressure boiling point) of a liquid is the special case in which the vapor pressure of the liquid equals the defined atmospheric pressure at sea level, one atmosphere.[4][5] At that temperature, the vapor pressure of the liquid becomes sufficient to overcome atmospheric pressure and allow bubbles of vapor to form inside the bulk of the liquid. The standard boiling point has been defined by IUPAC since 1982 as the temperature at which boiling occurs under a pressure of one bar.[6] The heat of vaporization is the energy required to transform a given quantity (a mol, kg, pound, etc.) of a substance from a liquid into a gas at a given pressure (often atmospheric pressure).
 Liquids may change to a vapor at temperatures below their boiling points through the process of evaporation. Evaporation is a surface phenomenon in which molecules located near the liquid's edge, not contained by enough liquid pressure on that side, escape into the surroundings as vapor. On the other hand, boiling is a process in which molecules anywhere in the liquid escape, resulting in the formation of vapor bubbles within the liquid.
"
Boiling_point_elevation,Physics,2,"Boiling-point elevation describes the phenomenon that the boiling point of a liquid (a solvent) will be higher when another compound is added, meaning that a solution has a higher boiling point than a pure solvent. This happens whenever a non-volatile solute, such as a salt, is added to a pure solvent, such as water. The boiling point can be measured accurately using an ebullioscope.
"
Boltzmann_constant,Physics,2,"
 The first and third values are exact; the second is exactly equal to 1380649/16021766340. See the linked section for details.
 The Boltzmann constant (kB or k) is the proportionality factor that relates the average relative kinetic energy of particles in a gas with the thermodynamic temperature of the gas.[2] It occurs in the definitions of the kelvin and the gas constant, and in Planck's law of black-body radiation and Boltzmann's entropy formula. The Boltzmann constant has the dimension energy divided by temperature, the same as entropy.  It is named after the Austrian scientist Ludwig Boltzmann.
 As part of the 2019 redefinition of SI base units, the Boltzmann constant is one of the seven ""defining constants"" that have been given exact definitions. They are used in various combinations to define the seven SI base units. The Boltzmann constant is defined to be exactly 1.380649×10−23 J⋅K−1[3].
"
Bose%E2%80%93Einstein_condensate,Physics,2,"
 In condensed matter physics, a Bose–Einstein condensate (BEC) is a state of matter (also called the fifth state of matter) which is typically formed when a gas of bosons at low densities is cooled to temperatures very close to absolute zero (-273.15 °C, -459.67 °F). Under such conditions, a large fraction of bosons occupy the lowest quantum state, at which point microscopic quantum mechanical phenomena, particularly wavefunction interference, become apparent macroscopically. A BEC is formed by cooling a gas of extremely low density (about one-hundred-thousandth (1/100,000) the density of normal air) to ultra-low temperatures.
 This state was first predicted, generally, in 1924–1925 by Albert Einstein[1] following and crediting a pioneering paper by Satyendra Nath Bose on the new field now known as quantum statistics.[2]"
Boson,Physics,2,"In quantum mechanics, a boson (/ˈboʊsɒn/,[1] /ˈboʊzɒn/[2]) is a particle that follows Bose–Einstein statistics. Bosons make up one of two classes of elementary particles, the other being fermions.[3] The name boson was coined by Paul Dirac[4][5] to commemorate the contribution of Satyendra Nath Bose, an Indian physicist and professor of physics at University of Calcutta and at University of Dhaka[6][7] in developing, with Albert Einstein, Bose–Einstein statistics, which theorizes the characteristics of elementary particles.[8] 
 Examples of bosons are fundamental particles such as photons, gluons, and W and Z bosons (the four force-carrying gauge bosons of the Standard Model), the recently discovered Higgs boson, and the hypothetical graviton of quantum gravity. Some composite particles are also bosons, such as mesons and stable nuclei of even mass number such as deuterium (with one proton and one neutron, atomic mass number = 2), helium-4, and lead-208;[a] as well as some quasiparticles (e.g. Cooper pairs, plasmons, and phonons).[9]:130 An important characteristic of bosons is that there is no restriction on the number of them that occupy the same quantum state. This property is exemplified by helium-4 when it is cooled to become a superfluid.[10] Unlike bosons, two identical fermions cannot occupy the same quantum state. Whereas the elementary particles that make up matter (i.e. leptons and quarks) are fermions, the elementary bosons are force carriers that function as the 'glue' holding matter together.[11] This property holds for all particles with integer spin (s = 0, 1, 2, etc.) as a consequence of the spin–statistics theorem.
When a gas of Bose particles is cooled down to temperatures very close to absolute zero, then the kinetic energy of the particles decreases to a negligible amount, and they condense into the lowest energy level state. This state is called a Bose–Einstein condensate. This property is also the explanation for superfluidity.
"
Boyle%27s_law,Physics,2,"Boyle's law, also referred to as the Boyle–Mariotte law, or Mariotte's law (especially in France), is an experimental gas law that describes how the pressure of a gas tends to increase as the volume of the container decreases. A modern statement of Boyle's law is:
 The absolute pressure exerted by a given mass of an ideal gas is inversely proportional to the volume it occupies if the temperature and amount of gas remain unchanged within a closed system.[1][2] Mathematically, Boyle's law can be stated as:
 or
 where P is the pressure of the gas, V is the volume of the gas, and k is a constant.
 The equation states that the product of pressure and volume is a constant for a given mass of confined gas and this holds as long as the temperature is constant. For comparing the same substance under two different sets of conditions, the law can be usefully expressed as:
 This equation shows that, as volume increases, the pressure of the gas decreases in proportion. Similarly, as volume decreases, the pressure of the gas increases. The law was named after chemist and physicist Robert Boyle, who published the original law in 1662.[3]"
Bra%E2%80%93ket_notation,Physics,2,"In quantum mechanics, bra–ket notation, or Dirac notation, is ubiquitous. The notation uses the angle brackets, ""



⟨


{  \langle }
"" and ""



⟩


{  \rangle }
"", and a vertical bar ""




|



{  |}
"", to construct ""bras"" /brɑː/ and ""kets"" /kɛt/. A ket looks like ""




|

v
⟩


{  |v\rangle }
"". Mathematically it denotes a vector, 




v



{  {\boldsymbol {v}}}
, in an abstract (complex) vector space 



V


{  V}
, and physically it represents a state of some quantum system. A bra looks like ""



⟨
f

|



{  \langle f|}
"", and mathematically it denotes a linear form 



f
:
V
→


C




{  f:V\rightarrow \mathbb {\mathbb {C} } }
, i.e. a linear map that maps each vector in 



V


{  V}
 to a number in the complex plane 





C




{  \mathbb {\mathbb {C} } }
.  Letting the linear functional 



⟨
f

|



{  \langle f|}
 act on a vector 




|

v
⟩


{  |v\rangle }
 is written as 



⟨
f

|

v
⟩
∈


C




{  \langle f|v\rangle \in \mathbb {\mathbb {C} } }
. 
 On 



V


{  V}
 we introduce a scalar product 



(
⋅
,
⋅
)


{  (\cdot ,\cdot )}
 with antilinear first argument, which makes 



V


{  V}
 a Hilbert space. With this scalar product each vector 




ϕ

≡

|

ϕ
⟩


{  {\boldsymbol {\phi }}\equiv |\phi \rangle }
 can be identified with a corresponding linear form, by placing the vector in the anti-linear first slot of the inner product: 



(

ϕ

,
⋅
)
≡
⟨
ϕ

|



{  ({\boldsymbol {\phi }},\cdot )\equiv \langle \phi |}
. The correspondence between these notations is then 



(

ϕ

,

ψ

)
≡
⟨
ϕ

|

ψ
⟩


{  ({\boldsymbol {\phi }},{\boldsymbol {\psi }})\equiv \langle \phi |\psi \rangle }
. The linear form 



⟨
ϕ

|



{  \langle \phi |}
 is a covector to 




|

ϕ
⟩


{  |\phi \rangle }
, and the set of all covectors form a dual vector space 




V

∨




{  V^{\vee }}
, to the initial vector space 



V


{  V}
. The purpose of this linear form 



⟨
ϕ

|



{  \langle \phi |}
 can now be understood in terms of making projections on the state 




ϕ



{  {\boldsymbol {\phi }}}
, to find how linearly dependent two states are, etc.
 For the vector space 






C



n




{  \mathbb {\mathbb {C} } ^{n}}
, kets can be identified with column vectors, and bras with row vectors. Combinations of bras, kets, and operators are interpreted using matrix multiplication. If 






C



n




{  \mathbb {\mathbb {C} } ^{n}}
 has the standard hermitian inner product 



(

v

,

w

)
=

v

†


w


{  ({\boldsymbol {v}},{\boldsymbol {w}})=v^{\dagger }w}
, under this identification, the identification of kets and bras and vice versa provided by the inner product is taking the Hermitian conjugate (denoted 



†


{  \dagger }
).
 It is common to suppress the vector or linear form from the bra–ket notation and only use a label inside the typography for the bra or ket. For example, the spin operator 







σ
^




z




{  {\hat {\sigma }}_{z}}
 on a two dimensional space 



Δ


{  \Delta }
 of spinors, has eigenvalues 



±


{  \pm }
½  with eigenspinors 





ψ


+


,


ψ


−


∈
Δ


{  {\boldsymbol {\psi }}_{+},{\boldsymbol {\psi }}_{-}\in \Delta }
. In bra-ket notation one typically denotes this as 





ψ


+


=

|

+
⟩


{  {\boldsymbol {\psi }}_{+}=|+\rangle }
, and 





ψ


−


=

|

−
⟩


{  {\boldsymbol {\psi }}_{-}=|-\rangle }
. Just as above, kets and  bras with the same label are interpreted as kets and bras corresponding to each other using the inner product. In particular when also identified with row and column vectors, kets and bras with the same label are identified with Hermitian conjugate column and row vectors. 
 Bra–ket notation was effectively established in 1939 by Paul Dirac[1][2] and is thus also known as the Dirac notation.  (Still, the bra-ket notation has a precursor in Hermann Grassmann's use of the notation 



[
ϕ

∣

ψ
]


{  [\phi {\mid }\psi ]}
 for his inner products nearly 100 years earlier.[3][4])
"
Bragg%27s_law,Physics,2,"In physics, Bragg's law, or Wulff–Bragg's condition, a special case of Laue diffraction, gives the angles for coherent and incoherent scattering from a crystal lattice. When X-rays are incident on an atom, they make the electronic cloud move, as does any electromagnetic wave. The movement of these charges re-radiates waves with the same frequency, blurred slightly due to a variety of effects; this phenomenon is known as Rayleigh scattering (or elastic scattering). The scattered waves can themselves be scattered but this secondary scattering is assumed to be negligible.
 A similar process occurs upon scattering neutron waves from the nuclei or by a coherent spin interaction with an unpaired electron. These re-emitted wave fields interfere with each other either constructively or destructively (overlapping waves either add up together to produce stronger peaks or are subtracted from each other to some degree), producing a diffraction pattern on a detector or film.  The resulting wave interference pattern is the basis of diffraction analysis. This analysis is called Bragg diffraction.
"
Bremsstrahlung,Physics,2,"Bremsstrahlung /ˈbrɛmʃtrɑːləŋ/[1] (German pronunciation: [ˈbʁɛms.ʃtʁaːlʊŋ] (listen)), from bremsen ""to brake"" and Strahlung ""radiation""; i.e., ""braking radiation"" or ""deceleration radiation"", is electromagnetic radiation produced by the deceleration of a charged particle when deflected by another charged particle, typically an electron by an atomic nucleus. The moving particle loses kinetic energy, which is converted into radiation (i.e., a photon), thus satisfying the law of conservation of energy. The term is also used to refer to the process of producing the radiation. Bremsstrahlung has a continuous spectrum, which becomes more intense and whose peak intensity shifts toward higher frequencies as the change of the energy of the decelerated particles increases.
 Broadly speaking, bremsstrahlung or braking radiation is any radiation produced due to the deceleration (negative acceleration) of a charged particle, which includes synchrotron radiation (i.e. photon emission by a relativistic particle), cyclotron radiation (i.e. photon emission by a non-relativistic particle), and the emission of electrons and positrons during beta decay. However, the term is frequently used in the more narrow sense of radiation from electrons (from whatever source) slowing in matter.
 Bremsstrahlung emitted from plasma is sometimes referred to as free-free radiation. This refers to the fact that the radiation in this case is created by charged particles that are free; i.e., not part of an ion, atom or molecule, both before and after the deflection (acceleration) that caused the emission.
"
Brewster%27s_angle,Physics,2,"
 Brewster's angle (also known as the polarization angle) is an angle of incidence at which light with a particular polarization is perfectly transmitted through a transparent dielectric surface, with no reflection. When unpolarized light is incident at this angle, the light that is reflected from the surface is therefore perfectly polarized. This special angle of incidence is named after the Scottish physicist Sir David Brewster (1781–1868).[1][2]"
British_thermal_unit,Physics,2,"
The British thermal unit (BTU or Btu) is a unit of heat; it is defined as the amount of heat required to raise the temperature of one pound of water by one degree Fahrenheit. It is also part of the United States customary units.[1] Its counterpart in the metric system is the calorie, which is defined as the amount of heat required to raise the temperature of one gram of water by one degree Celsius.[2] Heat is now known to be equivalent to energy, for which the SI unit is the joule; one BTU is about 1055 joules (depending on definition, see below). While units of heat are often supplanted by energy units in scientific work, they are still used in some fields. For example, in the United States the price of natural gas is quoted in dollars per million BTUs.[3][4]"
Brittleness,Physics,2,"A material is brittle if, when subjected to stress, it breaks with little elastic deformation and without significant plastic deformation. Brittle materials absorb relatively little energy prior to fracture, even those of high strength. Breaking is often accompanied by a snapping sound. Brittle materials include most ceramics and glasses (which do not deform plastically) and some polymers, such as PMMA and polystyrene. Many steels become brittle at low temperatures (see ductile-brittle transition temperature), depending on their composition and processing.
 When used in materials science, it is generally applied to materials that fail when there is little or no plastic deformation before failure. One proof is to match the broken halves, which should fit exactly since no plastic deformation has occurred.
 When a material has reached the limit of its strength, it usually has the option of either deformation or fracture.  A naturally malleable metal can be made stronger by impeding the mechanisms of plastic deformation (reducing grain size, precipitation hardening, work hardening, etc.), but if this is taken to an extreme, fracture becomes the more likely outcome, and the material can become brittle.  Improving material toughness is, therefore, a balancing act.
"
Brownian_motion,Physics,2,"
 Brownian motion, or pedesis (from Ancient Greek: πήδησις /pɛ̌ːdɛːsis/ ""leaping""), is the random motion of particles suspended in a medium (a liquid or a gas).[2] This pattern of motion typically consists of random fluctuations in a particle's position inside a fluid sub-domain, followed by a relocation to another sub-domain. Each relocation is followed by more fluctuations within the new closed volume. This pattern describes a fluid at thermal equilibrium, defined by a given temperature. Within such a fluid, there exists no preferential direction of flow (as in transport phenomena). More specifically, the fluid's overall linear and angular momenta remain null over time. The kinetic energies of the molecular Brownian motions, together with those of molecular rotations and vibrations, sum up to the caloric component of a fluid's internal energy (the Equipartition theorem).
 This motion is named after the botanist Robert Brown, who first described the phenomenon in 1827, while looking through a microscope at pollen of the plant Clarkia pulchella immersed in water. In 1905, almost eighty years later, theoretical physicist Albert Einstein published a paper where he modeled the motion of the pollen particles as being moved by individual water molecules, making one of his first major scientific contributions.[3] This explanation of Brownian motion served as convincing evidence that atoms and molecules exist and was further verified experimentally by Jean Perrin in 1908. Perrin was awarded the Nobel Prize in Physics in 1926 ""for his work on the discontinuous structure of matter"".[4] The direction of the force of atomic bombardment is constantly changing, and at different times the particle is hit more on one side than another, leading to the seemingly random nature of the motion.
 The many-body interactions that yield the Brownian pattern cannot be solved by a model accounting for every involved molecule. In consequence, only probabilistic models applied to molecular populations can be employed to describe it. Two such models of the statistical mechanics, due to Einstein and Smoluchowski are presented below. Another, pure probabilistic class of models is the class of the stochastic process models. There exist sequences of both simpler and more complicated stochastic processes which converge (in the limit) to Brownian motion (see random walk and Donsker's theorem).[5][6]"
Bulk_modulus,Physics,2,"The bulk modulus (



K


{  K}
 or 



B


{  B}
) of a substance is a measure of how resistant to compression that substance is. It is defined as the ratio of the infinitesimal pressure increase to the resulting relative decrease of the volume.[1] Other moduli describe the material's response (strain) to other kinds of stress: the shear modulus describes the response to shear, and Young's modulus describes the response to linear stress.  For a fluid, only the bulk modulus is meaningful.  For a complex anisotropic solid such as wood or paper, these three moduli do not contain enough information to describe its behaviour, and one must use the full generalized Hooke's law.
"
Buoyancy,Physics,2,"
 Buoyancy (/ˈbɔɪənsi, ˈbuːjənsi/)[1][2] or upthrust, is an upward force exerted by a fluid that opposes the weight of a partially or fully immersed object. In a column of fluid, pressure increases with depth as a result of the weight of the overlying fluid. Thus the pressure at the bottom of a column of fluid is greater than at the top of the column. Similarly, the pressure at the bottom of an object submerged in a fluid is greater than at the top of the object. The pressure difference results in a net upward force on the object. The magnitude of the force is proportional to the pressure difference, and (as explained by Archimedes' principle) is equivalent to the weight of the fluid that would otherwise occupy the submerged volume of the object, i.e. the displaced fluid.
 For this reason, an object whose average density is greater than that of the fluid in which it is submerged tends to sink. If the object is less dense than the liquid, the force can keep the object afloat. This can occur only in a non-inertial reference frame, which either has a gravitational field or is accelerating due to a force other than gravity defining a ""downward"" direction.[3] The center of buoyancy of an object is the centroid of the displaced volume of fluid.
"
Calculus,Physics,2,"
 Calculus, originally called infinitesimal calculus or ""the calculus of infinitesimals"", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations.
 It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while  integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Today, calculus has widespread uses in science, engineering, and economics.[4] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly  devoted to the study of functions and limits. The word calculus (plural calculi) is a  Latin word, meaning originally ""small pebble"" (this meaning is kept in medicine – see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.
"
Capacitance,Physics,2,"
 Capacitance is the ability of a body to hold an electrical charge. In numerical terms: it is the ratio of the amount of electric charge stored on a conductor to a difference in electric potential. There are two closely related notions of capacitance: self capacitance and mutual capacitance.[1]:237–238 Any object that can be electrically charged exhibits self capacitance. In this case the electric potential difference is measured between the object and ground. A material with a large self capacitance holds more electric charge at a given potential difference than one with low capacitance. The notion of mutual capacitance is particularly important for understanding the operations of the capacitor, one of the three elementary linear electronic components (along with resistors and inductors). In a typical capacitor, two conductors are used to separate electric charge, with one conductor being positively charged and the other negatively charged, but the system having a total charge of zero. The ratio in this case is the magnitude of the electric charge on either conductor and the potential difference is that measured between the two conductors.
 The capacitance is a function only of the geometry of the design (e.g. area of the plates and the distance between them) and the permittivity of the dielectric material between the plates of the capacitor. For many dielectric materials, the permittivity and thus the capacitance, is independent of the potential difference between the conductors and the total charge on them.
 The SI unit of capacitance is the farad (symbol: F), named after the English physicist Michael Faraday. A 1 farad capacitor, when charged with 1 coulomb of electrical charge, has a potential difference of 1 volt between its plates.[2] The reciprocal of capacitance is called elastance.
"
Capacitive_reactance,Physics,2,"In electric and electronic systems, reactance is the opposition of a circuit element to the flow of current due to that element's inductance or capacitance. Greater reactance leads to smaller currents for the same voltage applied. Reactance is similar to electric resistance in this respect, but differs in that reactance does not lead to dissipation of electrical energy as heat.  Instead, energy is stored in the reactance, and later returned to the circuit whereas a resistance continuously loses energy.
 Reactance is used to compute amplitude and phase changes of sinusoidal alternating current (AC) going through a circuit element. It is denoted by the symbol 





X




{  \scriptstyle {X}}
. An ideal resistor has zero reactance, whereas ideal inductors and capacitors have zero resistance – that is, respond to current only by reactance. As frequency increases, inductive reactance also increases and capacitive reactance decreases.
"
Carnot_cycle,Physics,2,"The Carnot cycle is a theoretical ideal thermodynamic cycle proposed by French physicist Sadi Carnot in 1824 and expanded upon by others in the 1830s and 1840s. It provides an upper limit on the efficiency that any classical thermodynamic engine can achieve during the conversion of heat into work, or conversely, the efficiency of a refrigeration system in creating a temperature difference by the application of work to the system. It is not an actual thermodynamic cycle but is a theoretical construct.
 Every single thermodynamic system exists in a particular state. When a system is taken through a series of different states and finally returned to its initial state, a thermodynamic cycle is said to have occurred. In the process of going through this cycle, the system may perform work on its surroundings, for example by moving a piston, thereby acting as a heat engine. A system undergoing a Carnot cycle is called a Carnot heat engine, although such a ""perfect"" engine is only a theoretical construct and cannot be built in practice.[1] However, a microscopic Carnot heat engine has been designed and run.[2] Essentially, there are two ""heat reservoirs"" forming part of the heat engine at temperatures Th and Tc (hot and cold respectively).  They have such large thermal capacity  that their temperatures are practically unaffected by a single cycle. Since the cycle is theoretically reversible, there is no generation of entropy during the cycle; entropy is conserved. During the cycle, an arbitrary amount of entropy ΔS is extracted from the hot reservoir, and deposited in the cold reservoir.[citation needed] Since there is no volume change in either reservoir, they do no work, and during the cycle, an amount of energy ThΔS is extracted from the hot reservoir and a smaller amount of energy TcΔS is deposited in the cold reservoir. The difference in the two energies (Th-Tc)ΔS is equal to the work done by the engine.
"
Cartesian_coordinate_system,Physics,2,"
 A Cartesian coordinate system (UK: /kɑːˈtiːzjən/, US: /kɑːrˈtiʒən/) is a coordinate system that specifies each point uniquely in a plane by a set of numerical coordinates, which are the signed distances to the point from two fixed perpendicular oriented lines, measured in the same unit of length. Each reference line is called a coordinate axis or just axis (plural axes) of the system, and the point where they meet is its origin, at ordered pair (0, 0). The coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes, expressed as signed distances from the origin.
 One can use the same principle to specify the position of any point in three-dimensional space by three Cartesian coordinates, its signed distances to three mutually perpendicular planes (or, equivalently, by its perpendicular projection onto three mutually perpendicular lines). In general, n Cartesian coordinates (an element of real n-space) specify the point in an n-dimensional Euclidean space for any dimension n. These coordinates are equal, up to sign, to distances from the point to n mutually perpendicular hyperplanes.
 The invention of Cartesian coordinates in the 17th century by René Descartes (Latinized name: Cartesius) revolutionized mathematics by providing the first systematic link between Euclidean geometry and algebra. Using the Cartesian coordinate system, geometric shapes (such as curves) can be described by Cartesian equations: algebraic equations involving the coordinates of the points lying on the shape. For example, a circle of radius 2, centered at the origin of the plane, may be described as the set of all points whose coordinates x and y satisfy the equation x2 + y2 = 4.
 Cartesian coordinates are the foundation of analytic geometry, and provide enlightening geometric interpretations for many other branches of mathematics, such as linear algebra, complex analysis, differential geometry, multivariate calculus, group theory and more. A familiar example is the concept of the graph of a function. Cartesian coordinates are also essential tools for most applied disciplines that deal with geometry, including astronomy, physics, engineering and many more. They are the most common coordinate system used in computer graphics, computer-aided geometric design and other geometry-related data processing.
"
Cathode,Physics,2,"
A cathode is the electrode from which a conventional current leaves a polarized electrical device. This definition can be recalled by using the mnemonic CCD for Cathode Current Departs. A conventional current describes the direction in which positive charges move. Electrons have a negative electrical charge, so the movement of electrons is opposite to that of the conventional current flow. Consequently, the mnemonic cathode current departs also means that electrons flow into the device's cathode from the external circuit.
 The electrode through which conventional current flows the other way, into the device, is termed an anode.
"
Cathode_ray,Physics,2,"Cathode rays (electron beam or e-beam) are streams of electrons observed in discharge tubes. If an evacuated glass tube is equipped with two electrodes and a voltage is applied, glass behind the positive electrode is observed to glow, due to electrons emitted from the cathode (the electrode connected to the negative terminal of the voltage supply).  They were first observed in 1869 by German physicist Julius Plücker and Johann Wilhelm Hittorf,[1] and were named in 1876 by Eugen Goldstein Kathodenstrahlen, or cathode rays.[2][3] In 1897, British physicist J. J. Thomson showed that cathode rays were composed of a previously unknown negatively charged particle, which was later named the electron. Cathode-ray tubes (CRTs) use a focused beam of electrons deflected by electric or magnetic fields to render an image on a screen.
"
Cation,Physics,2,"
 An ion (/ˈaɪɒn, -ən/)[1] is an particle,atom or molecule with a net electrical charge. 
 The charge of the electron is considered negative by convention.  The negative charge of an ion is equal and opposite to charged proton(s) considered positive by convention.  The net charge of an ion is non-zero due to its total number of electrons being unequal to its total number of protons. 
 A cation is a positively charged ion, with fewer electrons than protons, while an anion is negatively charged, with more electrons than protons. Because of their opposite electric charges, cations and anions attract each other and readily form ionic compounds.
 Ions consisting of only a single atom are termed atomic or monatomic ions, while two or more atoms form molecular ions or polyatomic ions. In the case of physical ionization in a fluid (gas or liquid), ""ion pairs"" are created by spontaneous molecule collisions, where each generated pair consists of a free electron and a positive ion.[2] Ions are also created by chemical interactions, such as the dissolution of a salt in liquids, or by other means, such as passing a direct current through a conducting solution, dissolving an anode via ionization.
"
Celestial_mechanics,Physics,2,"Celestial mechanics is the branch of astronomy that deals with the motions of objects in outer space. Historically, celestial mechanics applies principles of physics (classical mechanics) to astronomical objects, such as stars and planets, to produce ephemeris data.
"
Celsius_scale,Physics,2,"
 
 The degree Celsius is a unit of temperature on the Celsius scale,[1] a temperature scale originally known as the centigrade scale.[2] The degree Celsius (symbol: °C) can refer to a specific temperature on the Celsius scale or a unit to indicate a difference between two temperatures or an uncertainty. It is named after the Swedish astronomer Anders Celsius (1701–1744), who developed a similar temperature scale. Before being renamed to honor Anders Celsius in 1948, the unit was called centigrade, from the Latin centum, which means 100, and gradus, which means steps.
 Since 1743 the Celsius scale has been based on 0 °C for the freezing point of water and 100 °C for the boiling point of water at 1 atm pressure. Prior to 1743 the values were reversed (i.e. the boiling point was 0 degrees and the freezing point was 100 degrees). The 1743 scale reversal was proposed by Jean-Pierre Christin.
 By international agreement, between 1954 and 2019 the unit degree Celsius and the Celsius scale were defined by absolute zero and the triple point of Vienna Standard Mean Ocean Water (VSMOW), a precisely defined water standard. This definition also precisely related the Celsius scale to the Kelvin scale, which defines the SI base unit of thermodynamic temperature with symbol K. Absolute zero, the lowest temperature possible, is defined as being exactly 0 K and −273.15 °C. Until 19 May 2019, the temperature of the triple point of water was defined as exactly 273.16 K (0.01 °C).[3] This means that a temperature difference of one degree Celsius and that of one kelvin are exactly the same.[4] On 20 May 2019, the kelvin was redefined so that its value is now determined by the definition of the Boltzmann constant rather than being defined by the triple point of VSMOW. This means that the triple point is now a measured value, not a defined value. The newly-defined exact value of the Boltzmann constant was selected so that the measured value of the VSMOW triple point is exactly the same as the older defined value to within the limits of accuracy of contemporary metrology. The degree Celsius remains exactly equal to the kelvin, and 0 K remains exactly −273.15 °C.
"
Center_of_curvature,Physics,2,"
 In geometry, the center of curvature of a curve is found at a point that is at a distance from the curve equal to the radius of curvature lying on the normal vector.  It is the point at infinity if the curvature is zero.  The osculating circle to the curve is centered at the centre of curvature.  Cauchy defined the center of curvature C as the intersection point of two infinitely close normal lines to the curve.[1] The locus of centers of curvature for each point on the curve comprise the evolute of the curve. This term is generally used in physics regarding to study of lenses and mirrors.
 It can also be defined as the spherical distance between the point at which all the rays falling on a lens or mirror either seems to converge to (in the case of convex lenses and concave mirrors) or diverge from (in the case of concave lenses or convex mirrors) and the lens/mirror itself.[2]"
Center_of_gravity,Physics,2,"
 In physics, the center of mass of a distribution of mass in space (sometimes referred to as the balance point) is the unique point where the weighted relative position of the distributed mass sums to zero. This is the point to which a force may be applied to cause a linear acceleration without an angular acceleration. Calculations in mechanics are often simplified when formulated with respect to the center of mass. It is a hypothetical point where the entire mass of an object may be assumed to be concentrated to visualise its motion. In other words, the center of mass is the particle equivalent of a given object for application of Newton's laws of motion.
 In the case of a single rigid body, the center of mass is fixed in relation to the body, and if the body has uniform density, it will be located at the centroid.  The center of mass may be located outside the physical body, as is sometimes the case for hollow or open-shaped objects, such as a horseshoe.  In the case of a distribution of separate bodies, such as the planets of the Solar System, the center of mass may not correspond to the position of any individual member of the system.
 The center of mass is a useful reference point for calculations in mechanics that involve masses distributed in space, such as the linear and angular momentum of planetary bodies and rigid body dynamics. In orbital mechanics, the equations of motion of planets are formulated as point masses located at the centers of mass.  The center of mass frame is an inertial frame in which the center of mass of a system is at rest with respect to the origin of the coordinate system.
"
Center_of_mass,Physics,2,"
 In physics, the center of mass of a distribution of mass in space (sometimes referred to as the balance point) is the unique point where the weighted relative position of the distributed mass sums to zero. This is the point to which a force may be applied to cause a linear acceleration without an angular acceleration. Calculations in mechanics are often simplified when formulated with respect to the center of mass. It is a hypothetical point where the entire mass of an object may be assumed to be concentrated to visualise its motion. In other words, the center of mass is the particle equivalent of a given object for application of Newton's laws of motion.
 In the case of a single rigid body, the center of mass is fixed in relation to the body, and if the body has uniform density, it will be located at the centroid.  The center of mass may be located outside the physical body, as is sometimes the case for hollow or open-shaped objects, such as a horseshoe.  In the case of a distribution of separate bodies, such as the planets of the Solar System, the center of mass may not correspond to the position of any individual member of the system.
 The center of mass is a useful reference point for calculations in mechanics that involve masses distributed in space, such as the linear and angular momentum of planetary bodies and rigid body dynamics. In orbital mechanics, the equations of motion of planets are formulated as point masses located at the centers of mass.  The center of mass frame is an inertial frame in which the center of mass of a system is at rest with respect to the origin of the coordinate system.
"
Center_of_pressure_(fluid_mechanics),Physics,2,"
The center of pressure  is the point where the total sum of a pressure field acts on a body, causing a force to act through that point. The total force vector acting at the center of pressure is the value of the integrated vectorial pressure field.  The resultant force and center of pressure location produce equivalent force and moment on the body as the original pressure field. Pressure fields occur in both static and dynamic fluid mechanics.  Specification of the center of pressure, the reference point from which the center of pressure is referenced, and the associated force vector allows the moment generated about any point to be computed by a translation from the reference point to the desired new point. It is common for the center of pressure to be located on the body, but in fluid flows it is possible for the pressure field to exert a moment on the body of such magnitude that the center of pressure is located outside the body.[1]"
Centigrade_(temperature),Physics,2,"
 
 The degree Celsius is a unit of temperature on the Celsius scale,[1] a temperature scale originally known as the centigrade scale.[2] The degree Celsius (symbol: °C) can refer to a specific temperature on the Celsius scale or a unit to indicate a difference between two temperatures or an uncertainty. It is named after the Swedish astronomer Anders Celsius (1701–1744), who developed a similar temperature scale. Before being renamed to honor Anders Celsius in 1948, the unit was called centigrade, from the Latin centum, which means 100, and gradus, which means steps.
 Since 1743 the Celsius scale has been based on 0 °C for the freezing point of water and 100 °C for the boiling point of water at 1 atm pressure. Prior to 1743 the values were reversed (i.e. the boiling point was 0 degrees and the freezing point was 100 degrees). The 1743 scale reversal was proposed by Jean-Pierre Christin.
 By international agreement, between 1954 and 2019 the unit degree Celsius and the Celsius scale were defined by absolute zero and the triple point of Vienna Standard Mean Ocean Water (VSMOW), a precisely defined water standard. This definition also precisely related the Celsius scale to the Kelvin scale, which defines the SI base unit of thermodynamic temperature with symbol K. Absolute zero, the lowest temperature possible, is defined as being exactly 0 K and −273.15 °C. Until 19 May 2019, the temperature of the triple point of water was defined as exactly 273.16 K (0.01 °C).[3] This means that a temperature difference of one degree Celsius and that of one kelvin are exactly the same.[4] On 20 May 2019, the kelvin was redefined so that its value is now determined by the definition of the Boltzmann constant rather than being defined by the triple point of VSMOW. This means that the triple point is now a measured value, not a defined value. The newly-defined exact value of the Boltzmann constant was selected so that the measured value of the VSMOW triple point is exactly the same as the older defined value to within the limits of accuracy of contemporary metrology. The degree Celsius remains exactly equal to the kelvin, and 0 K remains exactly −273.15 °C.
"
Classical_central-force_problem,Physics,2,"In classical potential theory, the central-force problem is to determine the motion of a particle in a single central potential field. A central force is a force (possibly negative) that points from the particle directly towards a fixed point in space, the center, and whose magnitude only depends on the distance of the object to the center.  In many important cases, the problem can be solved analytically, i.e., in terms of well-studied functions such as trigonometric functions.
 The solution of this problem is important to classical mechanics, since many naturally occurring forces are central. Examples include gravity and electromagnetism as described by Newton's law of universal gravitation and Coulomb's law, respectively.  The problem is also important because some more complicated problems in classical physics (such as the two-body problem with forces along the line connecting the two bodies) can be reduced to a central-force problem.  Finally, the solution to the central-force problem often makes a good initial approximation of the true motion, as in calculating the motion of the planets in the Solar System.
"
Centrifugal_force,Physics,2,"
 In Newtonian mechanics, the centrifugal force is an inertial force (also called a ""fictitious"" or ""pseudo"" force) that appears to act on all objects when viewed in a rotating frame of reference. It is directed away from an axis which is parallel to the axis of rotation and passing through the coordinate system's origin. If the axis of rotation passes through the coordinate system's origin, the centrifugal force is directed radially outwards from that axis. The magnitude of centrifugal force F on an object of mass m at the distance r from the origin of a frame of reference rotating with angular velocity ω is:
 The concept of centrifugal force can be applied in rotating devices, such as centrifuges, centrifugal pumps, centrifugal governors, and centrifugal clutches, and in centrifugal railways, planetary orbits and banked curves, when they are analyzed in a rotating coordinate system. The term has sometimes also been used for the reactive centrifugal force that may be viewed as a reaction to a centripetal force in some circumstances.
"
Centripetal_force,Physics,2,"
 A centripetal force (from Latin centrum, ""center"" and petere, ""to seek""[1]) is a force that makes a body follow a curved path. Its direction is always orthogonal to the motion of the body and towards the fixed point of the instantaneous center of curvature of the path. Isaac Newton described it as ""a force by which bodies are drawn or impelled, or in any way tend, towards a point as to a centre"".[2] In Newtonian mechanics, gravity provides the centripetal force causing astronomical orbits.
 One common example involving centripetal force is the case in which a body moves with uniform speed along a circular path. The centripetal force is directed at right angles to the motion and also along the radius towards the centre of the circular path.[3][4] The mathematical description was derived in 1659 by the Dutch physicist Christiaan Huygens.[5]"
CGh_physics,Physics,2,"cGh physics refers to the mainstream attempts in physics to unify relativity, gravitation and quantum mechanics, in particular following the ideas of Matvei Petrovich Bronstein and George Gamow. The letters are the standard symbols for the speed of light (c), the gravitational constant (G), and Planck's constant (h).
 If one considers these three universal constants as the basis for a 3-D coordinate system and envisions a cube, then this pedagogic construction provides a framework, which is referred to as the cGh cube, or physics cube, or cube of theoretical physics (CTP).[1] This cube can used for organizing major subjects within physics as occupying each of the eight corners.[2][3] The eight corners of the cGh physics cube are:
 Other cGh subjects include Planck units, Hawking radiation and black hole thermodynamics.
 While there are several other physical constants, these three are given special consideration, because they can be used to define all Planck units and thus all physical quantities.[4] The three constants are therefore used sometimes as a framework for philosophical study and as one of pedagogical patterns.[5]"
Chain_reaction,Physics,2,"A chain reaction is a sequence of reactions where a reactive product or by-product causes additional reactions to take place. In a chain reaction, positive feedback leads to a self-amplifying chain of events.
 Chain reactions are one way that systems which are not in thermodynamic equilibrium can release energy or increase entropy in order to reach a state of higher entropy. For example, a system may not be able to reach a lower energy state by releasing energy into the environment, because it is hindered or prevented in some way from taking the path that will result in the energy release. If a reaction results in a small energy release making way for more energy releases in an expanding chain, then the system will typically collapse explosively until much or all of the stored energy has been released. 
 A macroscopic metaphor for chain reactions is thus a snowball causing a larger snowball until finally an avalanche results (""snowball effect""). This is a result of stored gravitational potential energy seeking a path of release over friction. Chemically, the equivalent to a snow avalanche is a spark causing a forest fire. In nuclear physics, a single stray neutron can result in a prompt critical event, which may finally be energetic enough for a nuclear reactor meltdown or (in a bomb) a nuclear explosion.
 Numerous chain reactions can be represented by a mathematical model based on Markov chains.
"
Change_of_base_rule,Physics,2,"
 In mathematics, the logarithm is the inverse function to exponentiation. That means the logarithm of a given number x is the exponent to which another fixed number, the base b, must be raised, to produce that number x. In the simplest case, the logarithm counts the number of occurrences of the same factor in repeated multiplication; e.g., since 1000 = 10 × 10 × 10 = 103, the ""logarithm base 10"" of 1000 is 3, or log10(1000) = 3. The logarithm of x to base b is denoted as logb(x), or without parentheses, logb x, or even without the explicit base, log x, when no confusion is possible, or when the base does not matter such as in big O notation.
 More generally, exponentiation allows any positive real number as base to be raised to any real power, always producing a positive result, so logb(x) for any two positive real numbers b and x, where b is not equal to 1, is always a unique real number y. More explicitly, the defining relation between exponentiation and logarithm is:
 For example, log2 64 = 6, as 26 = 64.
 The logarithm base 10 (that is b = 10) is called the common logarithm and is commonly used in science and engineering. The natural logarithm has the number e (that is b ≈ 2.718) as its base; its use is widespread in mathematics and physics, because of its simpler integral and derivative. The binary logarithm uses base 2 (that is b = 2) and is commonly used in computer science.  Logarithms are examples of concave functions.[1] Logarithms were introduced by John Napier in 1614 as a means of simplifying calculations.[2] They were rapidly adopted by navigators, scientists, engineers, surveyors and others to perform high-accuracy computations more easily. Using logarithm tables, tedious multi-digit multiplication steps can be replaced by table look-ups and simpler addition. This is possible because of the fact—important in its own right—that the logarithm of a product is the sum of the logarithms of the factors:
 provided that b, x and y are all positive and b ≠ 1. The slide rule, also based on logarithms, allows quick calculations without tables, but at lower precision.
The present-day notion of logarithms comes from Leonhard Euler, who connected them to the exponential function in the 18th century, and who also introduced the letter e as the base of natural logarithms.[3] Logarithmic scales reduce wide-ranging quantities to tiny scopes. For example, the decibel (dB) is a unit used to express ratio as logarithms, mostly for signal power and amplitude (of which sound pressure is a common example). In chemistry, pH is a logarithmic measure for the acidity of an aqueous solution. Logarithms are commonplace in scientific formulae, and in measurements of the complexity of algorithms and of geometric objects called fractals. They help to describe frequency ratios of musical intervals, appear in formulas counting prime numbers or approximating factorials, inform some models in psychophysics, and can aid in forensic accounting.
 In the same way as the logarithm reverses exponentiation, the complex logarithm is the inverse function of the exponential function, whether applied to real numbers or complex numbers. The modular discrete logarithm is another variant; it has uses in public-key cryptography.
"
Charge_carrier,Physics,2,"In physics, a charge carrier is a particle or quasiparticle that is free to move, carrying an electric charge, especially the particles that carry electric charges in electrical conductors. Examples are electrons, ions and holes. In a conducting medium, an electric field can exert force on these free particles, causing a net motion of the particles through the medium; this is what constitutes an electric current. In conducting media, particles serve to carry charge:
 In some conductors, such as ionic solutions and plasmas, positive and negative charge carriers coexist, so in these cases an electric current consists of the two types of carrier moving in opposite directions. In other conductors, such as metals, there are only charge carriers of one polarity, so an electric current in them simply consists of charge carriers moving in one direction.
"
Chemical_physics,Physics,2,"Chemical physics is a subdiscipline of chemistry and physics that investigates physicochemical phenomena using techniques from atomic and molecular physics and condensed matter physics; it is the branch of physics that studies chemical processes from the point of view of physics. While at the interface of physics and chemistry, chemical physics is distinct from physical chemistry in that it focuses more on the characteristic elements and theories of physics. Meanwhile, physical chemistry studies the physical nature of chemistry.  Nonetheless, the distinction between the two fields is vague, and scientists often practice in both fields during the course of their research.[1] The United States Department of Education defines chemical physics as ""A program that focuses on the scientific study of structural phenomena combining the disciplines of physical chemistry and atomic/molecular physics. Includes instruction in heterogeneous structures, alignment and surface phenomena, quantum theory, mathematical physics, statistical and classical mechanics, chemical kinetics, and laser physics.""[2]"
Chromatic_aberration,Physics,2,"In optics, chromatic aberration (CA), also called chromatic distortion and spherochromatism, is a failure of a lens to focus all colors to the same point.[1] It is caused by dispersion: the refractive index of the lens elements varies with the wavelength of light. The refractive index of most transparent materials decreases with increasing wavelength.[2] Since the focal length of a lens depends on the refractive index, this variation in refractive index affects focusing.[3] Chromatic aberration manifests itself as ""fringes"" of color along boundaries that separate dark and bright parts of the image.
  Tilt Spherical aberration Astigmatism Coma  Distortion Petzval field curvature Chromatic aberration
"
Circular_motion,Physics,2,"In physics, circular motion is a movement of an object along the circumference of a circle or rotation along a circular path. It can be uniform, with constant angular rate of rotation and constant speed, or non-uniform with a changing rate of rotation. The rotation around a fixed axis of a three-dimensional body involves circular motion of its parts. The equations of motion describe the movement of the center of mass of a body.
 Examples of circular motion include: an artificial satellite orbiting the Earth at a constant height, a ceiling fan's blades rotating around a hub, a stone which is tied to a rope and is being swung in circles, a car turning through a curve in a race track, an electron moving perpendicular to a uniform magnetic field, and a gear turning inside a mechanism.
 Since the object's velocity vector is constantly changing direction, the moving object is undergoing acceleration by a centripetal force in the direction of the center of rotation.  Without this acceleration, the object would move in a straight line, according to Newton's laws of motion.
"
Classical_mechanics,Physics,2,"
 Classical[note 1] mechanics is a physical theory describing the motion of macroscopic objects, from projectiles to parts of machinery, and astronomical objects, such as spacecraft, planets, stars and galaxies.  For objects governed by classical mechanics, if the present state is known, it is possible to predict how it will move in the future (determinism) and how it has moved in the past (reversibility).
 The earliest development of classical mechanics is often referred to as Newtonian mechanics. It consists of the physical concepts employed and the mathematical methods invented by Isaac Newton, Gottfried Wilhelm Leibniz and others in the 17th century to describe the motion of bodies under the influence of a system of forces.  Later, more abstract methods were developed, leading to the reformulations of classical mechanics known as Lagrangian mechanics and Hamiltonian mechanics. These advances, made predominantly in the 18th and 19th centuries, extend substantially beyond Newton's work, particularly through their use of analytical mechanics. They are, with some modification, also used in all areas of modern physics.
 Classical mechanics provides extremely accurate results when studying large objects that are not extremely massive and speeds not approaching the speed of light. When the objects being examined have about the size of an atom diameter, it becomes necessary to introduce the other major sub-field of mechanics: quantum mechanics. To describe velocities that are not small compared to the speed of light, special relativity is needed. In cases where objects become extremely massive,  general relativity becomes applicable. However, a number of modern sources do include relativistic mechanics in classical physics, which in their view represents classical mechanics in its most developed and accurate form.
"
Friction#Coefficient_of_friction,Physics,2,"
 Friction is the force resisting the relative motion of solid surfaces, fluid layers, and material elements sliding against each other.[2] There are several types of friction:
 When surfaces in contact move relative to each other, the friction between the two surfaces converts kinetic energy into thermal energy (that is, it converts work to heat). This property can have dramatic consequences, as illustrated by the use of friction created by rubbing pieces of wood together to start a fire. Kinetic energy is converted to thermal energy whenever motion with friction occurs, for example when a viscous fluid is stirred. Another important consequence of many types of friction can be wear, which may lead to performance degradation or damage to components. Friction is a component of the science of tribology.
 Friction is desirable and important in supplying traction to facilitate motion on land. Most land vehicles rely on friction for acceleration, deceleration and changing direction. Sudden reductions in traction can cause loss of control and accidents.
 Friction is not itself a fundamental force. Dry friction arises from a combination of inter-surface adhesion, surface roughness, surface deformation, and surface contamination. The complexity of these interactions makes the calculation of friction from first principles  impractical and necessitates the use of empirical methods for analysis and the development of theory.
 Friction is a non-conservative force - work done against friction is path dependent. In the presence of friction, some kinetic energy is always transformed to thermal energy, so mechanical energy is not conserved.
"
Coherence_(physics),Physics,2,"
 In physics, two wave sources are perfectly coherent if their frequency and waveform are identical and their phase difference is constant. Coherence is an ideal property of waves that enables stationary (i.e. temporally and spatially constant) interference. It contains several distinct concepts, which are limiting cases that never quite occur in reality but allow an understanding of the physics of waves, and has become a very important concept in quantum physics. More generally, coherence describes all properties of the correlation between physical quantities of a single wave, or between several waves or wave packets.
 Interference is the addition, in the mathematical sense, of wave functions. A single wave can interfere with itself, but this is still an addition of two waves (see Young's slits experiment). Constructive or destructive interferences are limit cases, and two waves always interfere, even if the result of the addition is complicated or not remarkable. When interfering, two waves can add together to create a wave of greater amplitude than either one (constructive interference) or subtract from each other to create a wave of lesser amplitude than either one (destructive interference), depending on their relative phase. Two waves are said to be coherent if they have a constant relative phase. The amount of coherence can readily be measured by the interference visibility, which looks at the size of the interference fringes relative to the input waves (as the phase offset is varied); a precise mathematical definition of the degree of coherence is given by means of correlation functions.
 Spatial coherence describes the correlation (or predictable relationship) between waves at different points in space, either lateral or longitudinal.[1] Temporal coherence describes the correlation between waves observed at different moments in time. Both are observed in the Michelson–Morley experiment and Young's interference experiment. Once the fringes are obtained in the Michelson interferometer, when one of the mirrors is moved away gradually, the time for the beam to travel increases and the fringes become dull and finally disappear, showing temporal coherence. Similarly, if in a double-slit experiment, the space between the two slits is increased, the coherence dies gradually and finally the fringes disappear, showing spatial coherence.  In both cases, the fringe amplitude slowly disappears, as the path difference increases past the coherence length.
"
Cohesion_(chemistry),Physics,2,"Cohesion (from Latin cohaesiō ""cling"" or ""unity"") or cohesive attraction or cohesive force is the action or property of like molecules sticking together, being mutually attractive. It is an intrinsic property of a substance that is caused by the shape and structure of its molecules, which makes the distribution of  surrounding electrons irregular when molecules get close to one another, creating electrical attraction that can maintain a microscopic structure such as a water drop. In other words, cohesion allows for surface tension, creating a ""solid-like"" state upon which light-weight or low-density materials can be placed.
 Water, for example, is strongly cohesive as each molecule may make four hydrogen bonds to other water molecules in a tetrahedral configuration. This results in a relatively strong Coulomb force between molecules. In simple terms, the polarity (a state in which a molecule is oppositely charged on its poles) of water molecules allows them to be attracted to each other. The polarity is due to the electronegativity of the atom of oxygen: oxygen is more electronegative than the atoms of hydrogen, so the electrons they share through the covalent bonds are more often close to oxygen rather than hydrogen. These are called polar covalent bonds, covalent bonds between atoms that thus become oppositely charged.[1] In the case of a water molecule, the hydrogen atoms carry positive charges while the oxygen atom has a negative charge. This charge polarization within the molecule allows it to align with adjacent molecules through strong intermolecular hydrogen bonding, rendering the bulk liquid cohesive. Van der Waals gases such as methane, however, have weak cohesion due only to van der Waals forces that operate by induced polarity  in non-polar molecules.
 Cohesion, along with adhesion (attraction between unlike molecules), helps explain phenomena such as meniscus, surface tension and capillary action.
 Mercury in a glass flask is a good example of the effects of the ratio between cohesive and adhesive forces. Because of its high cohesion and low adhesion to the glass, mercury does not spread out to cover the bottom of the flask, and if enough is placed in the flask to cover the bottom, it exhibits a strongly convex meniscus, whereas the meniscus of water is concave. Mercury will not wet the glass, unlike water and many other liquids,[2] and if the glass is tipped, it will 'roll' around inside.
"
Cold_fusion,Physics,2,"
 Cold fusion is a hypothesized type of nuclear reaction that would occur at, or near, room temperature. It would contrast starkly with the ""hot"" fusion that is known to take place naturally within stars and artificially in hydrogen bombs and prototype fusion reactors under immense pressure and at temperatures of millions of degrees, and be distinguished from muon-catalyzed fusion. There is currently no accepted theoretical model that would allow cold fusion to occur.
 In 1989, two electrochemists, Martin Fleischmann and Stanley Pons, reported that their apparatus had produced anomalous heat (""excess heat"") of a magnitude they asserted would defy explanation except in terms of nuclear processes.[1] They further reported measuring small amounts of nuclear reaction byproducts, including neutrons and tritium.[2] The small tabletop experiment involved electrolysis of heavy water on the surface of a palladium (Pd) electrode.[3] The reported results received wide media attention[3] and raised hopes of a cheap and abundant source of energy.[4] Many scientists tried to replicate the experiment with the few details available. Hopes faded with the large number of negative replications, the withdrawal of many reported positive replications, the discovery of flaws and sources of experimental error in the original experiment, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts.[5] By late 1989, most scientists considered cold fusion claims dead,[6][7] and cold fusion subsequently gained a reputation as pathological science.[8][9] In 1989 the United States Department of Energy (DOE) concluded that the reported results of excess heat did not present convincing evidence of a useful source of energy and decided against allocating funding specifically for cold fusion. A second DOE review in 2004, which looked at new research, reached similar conclusions and did not result in DOE funding of cold fusion.[10] A small community of researchers continues to investigate cold fusion,[6][11][12] now often preferring the designation low-energy nuclear reactions (LENR) or condensed matter nuclear science (CMNS).[13][14][15][16] Since articles about cold fusion are rarely published in peer-reviewed mainstream scientific journals any longer, they do not attract the level of scrutiny expected for mainstream scientific publications.[17]"
Complex_harmonic_motion,Physics,2,"In physics, complex harmonic motion is a complicated realm based on the simple harmonic motion. The word ""complex"" refers to different situations. Unlike simple harmonic motion, which is regardless of air resistance, friction, etc., complex harmonic motion often has additional forces to dissipate the initial energy and lessen the speed and amplitude of an oscillation until the energy of the system is totally drained and the system comes to rest at its equilibrium point.
"
Composite_particle,Physics,2,"This is a list of known and hypothesized particles.
"
Compton_scattering,Physics,2,"
 Compton scattering, discovered by Arthur Holly Compton, is the scattering of a photon by a charged particle, usually an electron. If it results in a decrease in energy (increase in wavelength) of the photon (which may be an X-ray or gamma ray photon), it is called the Compton effect. Part of the energy of the photon is transferred to the recoiling electron. Inverse Compton scattering occurs when a charged particle transfers part of its energy to a photon.
"
Concave_lens,Physics,2,"
 A lens is a transmissive optical device that focuses or disperses a light beam by means of refraction. A simple lens consists of a single piece of transparent material, while a compound lens consists of several simple lenses (elements), usually arranged along a common axis. Lenses are made from materials such as glass or plastic, and are ground and polished or molded to a desired shape. A lens can focus light to form an image, unlike a prism, which refracts light without focusing. Devices that similarly focus or disperse waves and radiation other than visible light are also called lenses, such as microwave lenses, electron lenses, acoustic lenses, or explosive lenses.
"
Condensation_point,Physics,2,"In mathematics, a condensation point p of a subset S of a topological space, is any point p such that every open neighborhood of p contains uncountably many points of S. Thus ""condensation point"" is synonymous with ""




ℵ

1




{  \aleph _{1}}
-accumulation point"".[1]"
Condensed_matter_physics,Physics,2,"Condensed matter physics is the field of physics that deals with the macroscopic and microscopic physical properties of matter, especially the solid and liquid phases which arise from electromagnetic forces between atoms. More generally, the subject deals with ""condensed"" phases of matter: systems of very many constituents with strong interactions between them. More exotic condensed phases include the superconducting phase exhibited by certain materials at low temperature, the ferromagnetic and antiferromagnetic phases of spins on crystal lattices of atoms, and the Bose–Einstein condensate found in ultracold atomic systems. Condensed matter physicists seek to understand the behavior of these phases by experiments to measure various material properties, and by applying the physical laws of quantum mechanics, electromagnetism, statistical mechanics, and other theories to develop mathematical models.
 The diversity of systems and phenomena available for study makes condensed matter physics the most active field of contemporary physics: one third of all American physicists self-identify as condensed matter physicists,[1] and the Division of Condensed Matter Physics is the largest division at the American Physical Society.[2]  The field overlaps with chemistry, materials science, engineering and nanotechnology, and relates closely to atomic physics and biophysics. The theoretical physics of condensed matter shares important concepts and methods with that of particle physics and nuclear physics.[3] A variety of topics in physics such as crystallography, metallurgy, elasticity, magnetism, etc., were treated as distinct areas until the 1940s, when they were grouped together as solid state physics. Around the 1960s, the study of physical properties of liquids was added to this list, forming the basis for the more comprehensive specialty of condensed matter physics.[4] The Bell Telephone Laboratories was one of the first institutes to conduct a research program in condensed matter physics.[4]"
Momentum#Conservation,Physics,2,"
 
 In Newtonian mechanics, linear momentum, translational momentum, or simply momentum (pl. momenta) is the product of the mass and velocity of an object. It is a vector quantity, possessing a magnitude and a direction. If m is an object's mass and v is its velocity (also a vector quantity), then the object's momentum is:




p

=
m

v

.


{  \mathbf {p} =m\mathbf {v} .}

In SI units, momentum is measured in kilogram meters per second (kg⋅m/s).
 Newton's second law of motion states that the rate of change of a body's momentum is equal to the net force acting on it. Momentum depends on the frame of reference, but in any inertial frame it is a conserved quantity, meaning that if a closed system is not affected by external forces, its total linear momentum does not change. Momentum is also conserved in special relativity (with a modified formula) and, in a modified form,  in electrodynamics, quantum mechanics, quantum field theory, and general relativity. It is an expression of one of the fundamental symmetries of space and time: translational symmetry.
 Advanced formulations of classical mechanics, Lagrangian and Hamiltonian mechanics, allow one to choose coordinate systems that incorporate symmetries and constraints. In these systems the conserved quantity is  generalized momentum, and in general this is different from the kinetic momentum defined above. The concept of generalized momentum is carried over into quantum mechanics, where it becomes an operator on a wave function. The momentum and position operators are related by the Heisenberg uncertainty principle.
 In continuous systems such as electromagnetic fields, fluid dynamics and deformable bodies, a momentum density can be defined, and a continuum version of the conservation of momentum leads to equations such as the Navier–Stokes equations for fluids or the Cauchy momentum equation for deformable solids or fluids.
"
Conservation_law,Physics,2,"In physics, a conservation law states that a particular measurable property of an isolated physical system does not change as the system evolves over time. Exact conservation laws include conservation of energy, conservation of linear momentum, conservation of angular momentum, and conservation of electric charge. There are also many approximate conservation laws, which apply to such quantities as mass, parity, lepton number, baryon number, strangeness, hypercharge, etc.  These quantities are conserved in certain classes of physics processes, but not in all.
 A local conservation law is usually expressed mathematically as a continuity equation, a partial differential equation which gives a relation between the amount of the quantity and the ""transport"" of that quantity.    It states that the amount of the conserved quantity at a point or within a volume can only change by the amount of the quantity which flows in or out of the volume.
 From Noether's theorem, each conservation law is associated with a symmetry in the underlying physics.
"
Constructive_interference,Physics,2,"In physics, interference is a phenomenon in which two waves superpose to form a resultant wave of greater, lower, or the same amplitude. Constructive and destructive interference result from the interaction of waves that are correlated or coherent with each other, either because they come from the same source or because they have the same or nearly the same frequency. Interference effects can be observed with all types of waves, for example, light, radio, acoustic, surface water waves, gravity waves, or matter waves. The resulting images or graphs are called interferograms.
"
Continuous_spectrum,Physics,2,"In physics, a continuous spectrum usually means a set of attainable values for some physical quantity (such as energy or wavelength) that is best described as an interval of real numbers, as opposed to a discrete spectrum, a set of attainable values that is discrete in the mathematical sense, where there is a positive gap between each value and the next one.
 The classical example of a continuous spectrum, from which the name is derived, is the part of the spectrum of the light emitted by excited atoms of hydrogen that is due to free electrons becoming bound to a hydrogen ion and emitting photons, which are smoothly spread over a wide range of wavelengths, in contrast to the discrete lines due to electrons falling from some bound quantum state to a state of lower energy.
 As in that classical example, the term is most often used when the range of values of a physical quantity may have both a continuous and a discrete part, whether at the same time or in different situations. In quantum systems, continuous spectra (as in bremsstrahlung and thermal radiation) are usually associated with free particles, such as atoms in a gas, electrons in an electron beam, or conduction band electrons in a metal. In particular, the position and momentum of a free particle has a continuous spectrum, but when the particle is confined to a limited space its spectrum becomes discrete.
 Often a continuous spectrum may be just a convenient model for a discrete spectrum whose values are too close to be distinguished, as in the phonons in a crystal.
 The continuous and discrete spectra of physical systems can be modeled in functional analysis as different parts in the decomposition of the spectrum of a linear operator acting on a function space, such as the Hamiltonian operator.
"
Continuum_mechanics,Physics,2,"
 Continuum mechanics is a branch of mechanics that deals with the mechanical behavior of materials modeled as a continuous mass rather than as discrete particles. The French mathematician Augustin-Louis Cauchy was the first to formulate such models in the 19th century.
"
Convection,Physics,2,"Convection is the transfer of heat due to the bulk movement of molecules within fluids (gases and liquids), including molten rock (rheid). Convection includes sub-mechanisms of advection (directional bulk-flow transfer of heat), and diffusion (non-directional transfer of energy or mass particles along a concentration gradient).
 Convection cannot take place in most solids because neither bulk current flows nor significant diffusion of matter can take place. Diffusion of heat takes place in rigid solids, but that is called heat conduction. Convection, additionally may take place in soft solids or mixtures where solid particles can move past each other.
 Thermal convection can be demonstrated by placing a heat source (e.g. a Bunsen burner) at the side of a glass filled with a liquid, and observing the changes in temperature in the glass caused by the warmer fluid circulating into cooler areas.
 Convective heat transfer is one of the major types of heat transfer, and convection is also a major mode of mass transfer in fluids. Convective heat and mass transfer takes place both by diffusion – the random Brownian motion of individual particles in the fluid – and by advection, in which matter or heat is transported by the larger-scale motion of currents in the fluid. In the context of heat and mass transfer, the term ""convection"" is used to refer to the combined effects of advective and diffusive transfer.[1] Sometimes the term ""convection"" is used to refer specifically to ""free heat convection"" (natural heat convection) where bulk-flow in a fluid is due to temperature-induced differences in buoyancy, as opposed to ""forced heat convection"" where forces other than buoyancy (such as pump or fan) move the fluid. However, in mechanics, the correct use of the word ""convection"" is the more general sense, and different types of convection should be further qualified, for clarity.
 Convection can be qualified in terms of being natural, forced, gravitational, granular, or thermomagnetic. It may also be said to be due to combustion, capillary action, or Marangoni and Weissenberg effects. Heat transfer by natural convection plays a role in the structure of Earth's atmosphere, its oceans, and its mantle. Discrete convective cells in the atmosphere can be seen as clouds, with stronger convection resulting in thunderstorms. Natural convection also plays a role in stellar physics.
 The convection mechanism is also used in cooking, when using a convection oven, which uses fans to circulate hot air around food in order to cook the food faster than a conventional oven.
"
Convex_lens,Physics,2,"
 A lens is a transmissive optical device that focuses or disperses a light beam by means of refraction. A simple lens consists of a single piece of transparent material, while a compound lens consists of several simple lenses (elements), usually arranged along a common axis. Lenses are made from materials such as glass or plastic, and are ground and polished or molded to a desired shape. A lens can focus light to form an image, unlike a prism, which refracts light without focusing. Devices that similarly focus or disperse waves and radiation other than visible light are also called lenses, such as microwave lenses, electron lenses, acoustic lenses, or explosive lenses.
"
Coulomb,Physics,2,"
 The coulomb (symbol: C) is the International System of Units (SI) unit of electric charge. Under the 2019 redefinition of the SI base units, which took effect on 20 May 2019,[2] the coulomb is exactly 1/(1.602176634×10−19) (which is approximately 6.2415090744×1018, or 1.036×10−5 mol) elementary charges. The same number of electrons has the same magnitude but opposite sign of charge, that is, a charge of −1 C.
"
Coulomb%27s_law,Physics,2,"
 Coulomb's law, or Coulomb's inverse-square law, is an experimental law[1] of physics that quantifies the amount of force between two stationary, electrically charged particles. The electric force between charged bodies at rest is conventionally called electrostatic force or Coulomb force.[2] The law was first discovered in 1785 by French physicist Charles-Augustin de Coulomb, hence the name. Coulomb's law was essential to the development of the theory of electromagnetism, maybe even its starting point,[1] as it made it possible to discuss the quantity of electric charge in a meaningful way.[3] The law states that the magnitude of the electrostatic force of attraction or repulsion between two point charges is directly proportional to the product of the magnitudes of charges and inversely proportional to the square of the distance between them,[4] Here, ke is Coulomb's constant (ke ≈ 8.988×109 N⋅m2⋅C−2),[1] q1 and q2 are the signed magnitudes of the charges, and the scalar r is the distance between the charges.  
 The force is along the straight line joining the two charges. If the charges have the same sign, the electrostatic force between them is repulsive; if they have different signs, the force between them is attractive.
 Being an inverse-square law, the law is analogous to Isaac Newton's inverse-square law of universal gravitation, but gravitational forces are always attractive, while electrostatic forces can be attractive or repulsive.[2] Coulomb's law can be used to derive Gauss's law, and vice versa. In the case of a single stationary point charge, the two laws are equivalent, expressing the same physical law in different ways.[5] The law has been tested extensively, and observations have upheld the law on the scale from 10−16 m to 108 m.[5]"
Lens_(optics)#Types_of_simple_lenses,Physics,2,"
 A lens is a transmissive optical device that focuses or disperses a light beam by means of refraction. A simple lens consists of a single piece of transparent material, while a compound lens consists of several simple lenses (elements), usually arranged along a common axis. Lenses are made from materials such as glass or plastic, and are ground and polished or molded to a desired shape. A lens can focus light to form an image, unlike a prism, which refracts light without focusing. Devices that similarly focus or disperse waves and radiation other than visible light are also called lenses, such as microwave lenses, electron lenses, acoustic lenses, or explosive lenses.
"
Cosmic_background_radiation,Physics,2,"Cosmic background radiation is electromagnetic radiation from the Big Bang.  The origin of this radiation depends on the region of the spectrum that is observed. One component is the cosmic microwave background. This component is redshifted photons that have freely streamed from an epoch when the Universe became transparent for the first time to radiation. Its discovery and detailed observations of its properties are considered one of the major confirmations of the Big Bang. The discovery (by chance in 1965) of the cosmic background radiation suggests that the early universe was dominated by a radiation field, a field of extremely high temperature and pressure.[1] The Sunyaev–Zel'dovich effect shows the phenomena of radiant cosmic background radiation interacting with ""electron"" clouds distorting the spectrum of the radiation.
 There is also background radiation in the infrared, x-rays, etc., with different causes, and they can sometimes be resolved into an individual source. See cosmic infrared background and X-ray background. See also cosmic neutrino background and extragalactic background light.
"
Creep_(deformation),Physics,2,"In materials science, creep (sometimes called cold flow) is the tendency of a solid material to move slowly or deform permanently under the influence of persistent mechanical stresses. It can occur as a result of long-term exposure to high levels of stress that are still below the yield strength of the material. Creep is more severe in materials that are subjected to heat for long periods and generally increases as they near their melting point.
 The rate of deformation is a function of the material's properties, exposure time, exposure temperature and the applied structural load. Depending on the magnitude of the applied stress and its duration, the deformation may become so large that a component can no longer perform its function — for example creep of a turbine blade could cause the blade to contact the casing, resulting in the failure of the blade. Creep is usually of concern to engineers and metallurgists when evaluating components that operate under high stresses or high temperatures. Creep is a deformation mechanism that may or may not constitute a failure mode. For example, moderate creep in concrete is sometimes welcomed because it relieves tensile stresses that might otherwise lead to cracking.
 Unlike brittle fracture, creep deformation does not occur suddenly upon the application of stress. Instead, strain accumulates as a result of long-term stress. Therefore, creep is a ""time-dependent"" deformation.
It works on the principle of Hooke's law (stress is directly proportional to strain).
"
Crest_(physics),Physics,2,"A crest point on a wave is the maximum value of upward displacement within a cycle. A crest is a point on a surface wave where the displacement of the medium is at a maximum. A trough is the opposite of a crest, so the minimum or lowest point in a cycle.
 When the crests and troughs of two sine waves of equal amplitude and frequency intersect or collide, while being in phase with each other, the result is called constructive interference and the magnitudes double (above and below the line). When in antiphase – 180° out of phase – the result is destructive interference: the resulting wave is the undisturbed line having zero amplitude.
"
Crest_factor,Physics,2,"Crest factor is a parameter of a waveform, such as alternating current or sound, showing the ratio of peak values to the effective value. In other words, crest factor indicates how extreme the peaks are in a waveform. Crest factor 1 indicates no peaks, such as direct current or a square wave. Higher crest factors indicate peaks, for example sound waves tend to have high crest factors.
 Crest factor is the peak amplitude of the waveform divided by the RMS value of the waveform.  This is equivalent to the ratio of the L∞ norm to the L2 norm of the function of the waveform:[1] The peak-to-average power ratio (PAPR) is the peak amplitude squared (giving the peak power) divided by the RMS value squared (giving the average power).[2] It is the square of the crest factor:
 When expressed in decibels, crest factor and PAPR are equivalent, due to the way decibels are calculated for power ratios vs amplitude ratios.
 Crest factor and PAPR are therefore dimensionless quantities.  While the crest factor is defined as a positive real number, in commercial products it is also commonly stated as the ratio of two whole numbers, e.g., 2:1. The PAPR is most used in signal processing applications. As it is a power ratio, it is normally expressed in decibels (dB). The crest factor of the test signal is a fairly important issue in loudspeaker testing standards; in this context it is usually expressed in dB.[3][4][5] The minimum possible crest factor is 1, 1:1 or 0 dB.
"
Critical_angle_(optics),Physics,2,"Total internal reflection (TIR) is the optical phenomenon in which the surface of the water in a fish-tank (for example) when viewed from below the water level, reflects the underwater scene like a mirror, with no loss of brightness (Fig. 1). In general, TIR occurs when waves in one medium reach the boundary with another medium at a sufficiently slanting angle, provided that the second (""external"") medium is transparent to the waves and allows them to travel faster than in the first (""internal"") medium. TIR occurs not only with electromagnetic waves such as light and microwaves, but also with other types of waves, including sound and water waves. In the case of a narrow train of waves, such as a laser beam (Fig. 2), we tend to describe the reflection in terms of ""rays"" rather than waves. In a medium whose properties are independent of direction, such as air, water, or glass, each ""ray"" is perpendicular to the associated wavefronts.[importance?] Refraction is generally accompanied by partial reflection. When waves are refracted from a medium of lower propagation speed to a medium of higher propagation speed (e.g., from water to air), the angle of refraction (between the refracted ray and the line perpendicular to the refracting surface) is greater than the angle of incidence (between the incident ray and the perpendicular). As the angle of incidence approaches a certain limit, called the critical angle, the angle of refraction approaches 90°, at which the refracted ray becomes parallel to the surface. As the angle of incidence increases beyond the critical angle, the conditions of refraction can no longer be satisfied; so there is no refracted ray, and the partial reflection becomes total. For visible light, the critical angle is about 49° for incidence at the water-to-air boundary, and about 42° for incidence at the common glass-to-air boundary.
 Details of the mechanism of TIR give rise to more subtle phenomena. While total reflection, by definition, involves no continuing flow of power across the interface between the two media, the external medium carries a so-called evanescent wave, which travels along the interface with an amplitude that falls off exponentially with distance from the interface. The ""total"" reflection is indeed total if the external medium is lossless (perfectly transparent), continuous, and of infinite extent, but can be conspicuously less than total if the evanescent wave is absorbed by a lossy external medium (""attenuated total reflectance""), or diverted by the outer boundary of the external medium or by objects embedded in that medium (""frustrated"" TIR). Unlike partial reflection between transparent media, total internal reflection is accompanied by a non-trivial phase shift (not just zero or 180°) for each component of polarization (perpendicular or parallel to the plane of incidence), and the shifts vary with the angle of incidence. The explanation of this effect by Augustin-Jean Fresnel, in 1823, added to the evidence in favor of the wave theory of light.
 The phase shifts are utilized by Fresnel's invention, the Fresnel rhomb, to modify polarization. The efficiency of the reflection is exploited by optical fibers (used in telecommunications cables and in image-forming fiberscopes), and by reflective prisms, such as erecting prisms for binoculars.
"
Critical_mass,Physics,2,"A critical mass is the smallest amount of fissile material needed for a sustained nuclear chain reaction. The critical mass of a fissionable material depends upon its nuclear properties (specifically, its nuclear fission cross-section), density, shape, enrichment, purity, temperature, and surroundings. The concept is important in nuclear weapon design.
"
Curie_temperature,Physics,2,"
 
 In physics and materials science, the Curie temperature (TC), or Curie point, is the temperature above which certain materials lose their permanent magnetic properties, which can (in most cases) be replaced by induced magnetism. The Curie temperature is named after Pierre Curie, who showed that magnetism was lost at a critical temperature.[1] The force of magnetism is determined by the magnetic moment, a dipole moment within an atom which originates from the angular momentum and spin of electrons. Materials have different structures of intrinsic magnetic moments that depend on temperature; the Curie temperature is the critical point at which a material's intrinsic magnetic moments change direction.
 Permanent magnetism is caused by the alignment of magnetic moments and induced magnetism is created when disordered magnetic moments are forced to align in an applied magnetic field. For example, the ordered magnetic moments (ferromagnetic, Figure 1) change and become disordered (paramagnetic, Figure 2) at the Curie temperature. Higher temperatures make magnets weaker, as spontaneous magnetism only occurs below the Curie temperature. Magnetic susceptibility above the Curie temperature can be calculated from the Curie–Weiss law, which is derived from Curie's law.
 In analogy to ferromagnetic and paramagnetic materials, the Curie temperature can also be used to describe the phase transition between ferroelectricity and paraelectricity. In this context, the order parameter is the electric polarization that goes from a finite value to zero when the temperature is increased above the Curie temperature.
"
Current_density,Physics,2,"In electromagnetism, current density is the amount of charge per unit time that flows through a unit area of a chosen cross section.[1] The current density vector is defined as a vector whose magnitude is the electric current per cross-sectional area at a given point in space, its direction being that of the motion of the positive charges at this point. In SI base units, the electric current density is measured in amperes per square metre.[2]"
Curvilinear_motion,Physics,2,"The motion of an object moving in a curved path is called curvilinear motion.
Example: A stone thrown into the air at an angle.
 Curvilinear motion describes the motion of a moving particle that conforms to a known or fixed curve. The study of such motion involves the use of two co-ordinate systems, the first being planar motion and the latter being cylindrical motion.
"
Cyclotron,Physics,2,"A cyclotron is a type of particle accelerator invented by Ernest O. Lawrence in 1929–1930 at the University of California, Berkeley,[1][2] and patented in 1932.[3][4] A cyclotron accelerates charged particles outwards from the center of a flat cylindrical vacuum chamber along a spiral path.[5][6] The particles are held to a spiral trajectory by a static magnetic field and accelerated by a rapidly varying (radio frequency) electric field. Lawrence was awarded the 1939 Nobel Prize in Physics for this invention.[6][7] Cyclotrons were the most powerful particle accelerator technology until the 1950s when they were superseded by the synchrotron, and are still used to produce particle beams in physics and nuclear medicine. The largest single-magnet cyclotron was the 4.67 m (184 in) synchrocyclotron built between 1940 and 1946 by Lawrence at the University of California, Berkeley,[1][6] which could accelerate protons to 730 mega electron volts (MeV). The largest cyclotron is the 17.1 m (56 ft) multimagnet TRIUMF accelerator at the University of British Columbia in Vancouver, British Columbia, which can produce 500 MeV protons.
 Over 1200 cyclotrons are used in nuclear medicine worldwide for the production of radionuclides.[8]"
Dalton%27s_law,Physics,2,"Dalton's law (also called Dalton's law of partial pressures) states that in a mixture of non-reacting gases, the total  pressure exerted is equal to the sum of the partial pressures of the individual gases.[1] This empirical law was observed by John Dalton in 1801 and published in 1802.[2] Dalton's law is related to the ideal gas laws.
"
Damped_vibration,Physics,2,"Vibration is a mechanical phenomenon whereby oscillations occur about an equilibrium point. The word comes from Latin vibrationem (""shaking, brandishing""). The oscillations may be periodic, such as the motion of a pendulum—or random, such as the movement of a tire on a gravel road.
 Vibration can be desirable: for example, the motion of a tuning fork, the reed in a woodwind instrument or harmonica, a mobile phone, or the cone of a loudspeaker.
 In many cases, however, vibration is undesirable, wasting energy and creating unwanted sound. For example, the vibrational motions of engines, electric motors, or any mechanical device in operation are typically unwanted. Such vibrations could be caused by imbalances in the rotating parts, uneven friction, or the meshing of gear teeth. Careful designs usually minimize unwanted vibrations.
 The studies of sound and vibration are closely related.  Sound, or pressure waves, are generated by vibrating structures (e.g. vocal cords); these pressure waves can also induce the vibration of structures (e.g. ear drum).  Hence, attempts to reduce noise are often related to issues of vibration.
"
Damping,Physics,2,"Damping is an influence within or upon an oscillatory system that has the effect of reducing, restricting or preventing its oscillations. In physical systems, damping is produced by processes that dissipate the energy stored in the oscillation.[1] Examples include viscous drag in mechanical systems, resistance in electronic oscillators, and absorption and scattering of light in optical oscillators. Damping not based on energy loss can be important in other oscillating systems such as those that occur in biological systems and bikes.[2] The damping ratio is a dimensionless measure describing how oscillations in a system decay after a disturbance. Many systems exhibit oscillatory behavior when they are disturbed from their position of static equilibrium. A mass suspended from a spring, for example, might, if pulled and released, bounce up and down. On each bounce, the system tends to return to its equilibrium position, but overshoots it. Sometimes losses (e.g. frictional) damp the system and can cause the oscillations to gradually decay in amplitude towards zero or attenuate. The damping ratio is a measure describing how rapidly the oscillations decay from one bounce to the next.
 The damping ratio is a system parameter, denoted by ζ (zeta), that can vary from undamped (ζ = 0), underdamped (ζ < 1) through critically damped (ζ = 1) to overdamped (ζ > 1).
 The behaviour of oscillating systems is often of interest in a diverse range of disciplines that include control engineering, chemical engineering, mechanical engineering, structural engineering, and electrical engineering. The physical quantity that is oscillating varies greatly, and could be the swaying of a tall building in the wind, or the speed of an electric motor, but a normalised, or non-dimensionalised approach can be convenient in describing common aspects of behavior.
"
Darcy%E2%80%93Weisbach_equation,Physics,2,"In fluid dynamics, the Darcy–Weisbach equation is an empirical equation, which relates the head loss, or pressure loss, due to friction along a given length of pipe to the average velocity of the fluid flow for an incompressible fluid. The equation is named after Henry Darcy and Julius Weisbach.
 The Darcy–Weisbach equation contains a dimensionless friction factor, known as the Darcy friction factor. This is also variously called the Darcy–Weisbach friction factor, friction factor, resistance coefficient, or flow coefficient.[a]"
Dark_energy,Physics,2,"
 In physical cosmology and astronomy, dark energy is an unknown form of energy that affects the universe on the largest scales. The first observational evidence for its existence came from supernovae measurements, which showed that the universe does not expand at a constant rate; rather, the expansion of the universe is accelerating.[1][2] Understanding the evolution of the universe requires knowledge of its starting conditions and its composition. Prior to these observations, the only forms of matter-energy known to exist were ordinary matter, antimatter, dark matter, and radiation.  Measurements of the cosmic microwave background suggest the universe began in a hot Big Bang, from which general relativity explains its evolution and the subsequent large scale motion. Without introducing a new form of energy, there was no way to explain how an accelerating universe could be measured. Since the 1990s, dark energy has been the most accepted premise to account for the accelerated expansion. As of 2020, there are active areas of cosmology research aimed at understanding the fundamental nature of dark energy.[3] Assuming that the lambda-CDM model of cosmology is correct, the best current measurements indicate that dark energy contributes 69% of the total energy in the present-day observable universe. The mass–energy of dark matter and ordinary (baryonic) matter contributes 26% and 5%, respectively, and other components such as neutrinos and photons contribute a very small amount.[4][5][6][7] The density of dark energy is very low (~ 7 × 10−30 g/cm3), much less than the density of ordinary matter or dark matter within galaxies. However, it dominates the mass–energy of the universe because it is uniform across space.[8][9][10] Two proposed forms of dark energy are the cosmological constant,[11][12] representing a constant energy density filling space homogeneously, and scalar fields such as quintessence or moduli, dynamic quantities having energy densities that can vary in time and space. Contributions from scalar fields that are constant in space are usually also included in the cosmological constant. The cosmological constant can be formulated to be equivalent to the zero-point radiation of space i.e. the vacuum energy.[13] Scalar fields that change in space can be difficult to distinguish from a cosmological constant because the change may be extremely slow.
 Due to the toy model nature of concordance cosmology, some experts believe[14] that a more accurate general relativistic treatment of the structures that exist on all scales[15] in the real Universe may do away with the need to invoke dark energy. Inhomogeneous cosmologies, which attempt to account for the backreaction of structure formation on the metric, generally do not acknowledge any dark energy contribution to the energy density of the Universe.
"
Dark_matter,Physics,2,"
 Dark matter is a form of matter thought to account for approximately 85% of the matter in the universe and about a quarter of its total mass–energy density or about 2.241×10−27 kg/m3.  Its presence is implied in a variety of astrophysical observations, including gravitational effects that cannot be explained by accepted theories of gravity unless more matter is present than can be seen. For this reason, most experts think that dark matter is abundant in the universe and that it has had a strong influence on its structure and evolution. Dark matter is called dark because it does not appear to interact with the electromagnetic field, which means it does not absorb, reflect or emit electromagnetic radiation, and is therefore difficult to detect.[1] Primary evidence for dark matter comes from calculations showing that many galaxies would fly apart, or that they would not have formed or would not move as they do, if they did not contain a large amount of unseen matter.[2] Other lines of evidence include observations in gravitational lensing[3] and in the cosmic microwave background, along with astronomical observations of the observable universe's current structure, the formation and evolution of galaxies, mass location during galactic collisions,[4] and the motion of galaxies within galaxy clusters. In the standard Lambda-CDM model of cosmology, the total mass–energy of the universe contains 5% ordinary matter and energy, 27% dark matter and 68% of a form of energy known as dark energy.[5][6][7][8] Thus, dark matter constitutes 85%[a] of total mass, while dark energy plus dark matter constitute 95% of total mass–energy content.[9][10][11][12] Because dark matter has not yet been observed directly, if it exists, it must barely interact with ordinary baryonic matter and radiation, except through gravity. Most dark matter is thought to be non-baryonic in nature; it may be composed of some as-yet undiscovered subatomic particles.[b] The primary candidate for dark matter is some new kind of elementary particle that has not yet been discovered, in particular, weakly interacting massive particles (WIMPs).[13] Many experiments to directly detect and study dark matter particles are being actively undertaken, but none have yet succeeded.[14] Dark matter is classified as ""cold"", ""warm"", or ""hot"" according to its velocity (more precisely, its free streaming length). Current models favor a cold dark matter scenario, in which structures emerge by gradual accumulation of particles.
 Although the existence of dark matter is generally accepted by the scientific community, some astrophysicists, intrigued by certain observations which do not fit some dark matter theories, argue for various modifications of the standard laws of general relativity, such as modified Newtonian dynamics, tensor–vector–scalar gravity, or entropic gravity. These models attempt to account for all observations without invoking supplemental non-baryonic matter.[15]"
DC_motor,Physics,2,"A DC motor is any of a class of rotary electrical motors that converts direct current electrical energy into mechanical energy. The most common types rely on the forces produced by magnetic fields. Nearly all types of DC motors have some internal mechanism, either electromechanical or electronic, to periodically change the direction of current in part of the motor. 
 DC motors were the first form of motor widely used, as they could be powered from existing direct-current lighting power distribution systems. A DC motor's speed can be controlled over a wide range, using either a variable supply voltage or by changing the strength of current in its field windings. Small DC motors are used in tools, toys, and appliances. The universal motor can operate on direct current but is a lightweight brushed motor used for portable power tools and appliances. Larger DC motors are currently used in propulsion of electric vehicles, elevator and hoists, and in drives for steel rolling  mills.  The advent of power electronics has made replacement of DC motors with AC motors possible in many applications.
"
Decibel,Physics,2,"
 The decibel (symbol: dB) is a relative unit of measurement corresponding to one tenth of a bel (B). It is used to express the ratio of one value of a power or root-power quantity to another, on a logarithmic scale.  A logarithmic quantity in decibels is called a level. Two signals whose levels differ by one decibel have a power ratio of 101/10 (approximately 1.25893) or (sometimes equivalently) an amplitude (field quantity) ratio of 10​1⁄20 (approximately 1.12202).[1][2] Decibels can be used to express a change in value (e.g., +1 dB or −1 dB) or an absolute value. In the latter case, the number of decibels expresses the ratio of a value to a fixed reference value; when used in this way, a suffix that indicates the reference value is often appended to the decibel symbol. For example, if the reference value is 1 volt, then the suffix is ""V"" (e.g., ""20 dBV""), and if the reference value is one milliwatt, then the suffix is ""m"" (e.g., ""20 dBm"") according to NIST.[3][4] Different definitions are used for expressing a ratio in decibels, depending on the nature of the quantities: power and root-power. When expressing a power ratio, the number of decibels is ten times its logarithm in base 10.[5] That is, a change in power by a factor of 10 corresponds to a 10 dB change in level. When expressing root-power quantities, a change in amplitude by a factor of 10 corresponds to a 20 dB change in level. The decibel scales differ by a factor of two so that the related power and root-power levels change by the same number of decibels in linear systems, where power is proportional to the square of amplitude.
 The definition of the decibel originated in the measurement of power in telephony of the early 20th century in the Bell System in the United States. One decibel is one tenth (deci-) of one bel, named in honor of Alexander Graham Bell; however, the bel is seldom used. Today, the decibel is used for a wide variety of measurements in science and engineering, most prominently in acoustics, electronics, and control theory. In electronics, the gains of amplifiers, attenuation of signals, and signal-to-noise ratios are often expressed in decibels.
 In the International System of Quantities, the decibel is defined as a unit of measurement for quantities of type level or level difference, which are defined as the logarithm of the ratio of power- or root-power type quantities.[6]"
Definite_integral,Physics,2,"In mathematics, an integral assigns numbers to functions in a way that can describe displacement, area, volume, and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus; its inverse operation, differentiation, is the other. Given a function f of a real variable x and an interval [a, b] of the real line, the definite integral of f from a to b can be interpreted informally as the signed area of the region in the xy-plane that is bounded by the graph of f, the x-axis and the vertical lines x = a and x = b. It is denoted
 The operation of integration, up to an additive constant, is the inverse of the operation of differentiation. For this reason, the term integral may also refer to the related notion of the antiderivative, called an indefinite integral, a function F whose derivative is the given function f. In this case, it is written:
 The integrals discussed in this article are those termed definite integrals. It is the fundamental theorem of calculus that connects differentiation with the definite integral: if f is a continuous real-valued function defined on a closed interval [a, b], then once an antiderivative F of f is known, the definite integral of f over that interval is given by
 The principles of integration were formulated independently by Isaac Newton and Gottfried Wilhelm Leibniz in the late 17th century, who thought of the integral as an infinite sum of rectangles of infinitesimal width. Bernhard Riemann later gave a rigorous mathematical definition of integrals, which is based on a limiting procedure that approximates the area of a curvilinear region by breaking the region into thin vertical slabs. Beginning in the 19th century, more sophisticated notions of integrals began to appear, where the type of the function as well as the domain over which the integration is performed has been generalized. A line integral is defined for functions of two or more variables, and the interval of integration [a, b] is replaced by a curve connecting the two endpoints. In a surface integral, the curve is replaced by a piece of a surface in three-dimensional space.
"
Deflection_(engineering),Physics,2,"In engineering, deflection is the degree to which a structural element is displaced under a load (due to its deformation). It may refer to an angle or a distance.
 The deflection distance of a member under a load can be calculated by integrating the function that mathematically describes the slope of the deflected shape of the member under that load. 
 Standard formulas exist for the deflection of common beam configurations and load cases at discrete locations.
Otherwise methods such as virtual work, direct integration, Castigliano's method, Macaulay's method or the direct stiffness method are used. The deflection of beam elements is usually calculated on the basis of the Euler–Bernoulli beam equation while that of a plate or shell element is calculated using plate or shell theory.
 An example of the use of deflection in this context is in building construction. Architects and engineers select materials for various applications.
"
Deformation_(mechanics),Physics,2,"In physics, deformation is the continuum mechanics transformation of a body from a reference configuration to a current configuration.[1]  A configuration is a set containing the positions of all particles of the body.
 A  deformation may be caused by external loads,[2] body forces (such as gravity or electromagnetic forces), or changes in temperature, moisture content, or chemical reactions, etc.
 Strain is a description of deformation in terms of relative displacement of particles in the body that excludes rigid-body motions.  Different equivalent choices may be made for the expression of a strain field depending on whether it is defined with respect to the initial or the final configuration of the body and on whether the metric tensor or its dual is considered.
 In a continuous body, a deformation field results from a stress field induced by applied forces or is due to changes in the temperature field inside the body. The relation between stresses and induced strains is expressed by constitutive equations, e.g., Hooke's law for linear elastic materials. Deformations which are recovered after the stress field has been removed are called elastic deformations. In this case, the continuum completely recovers its original configuration. On the other hand, irreversible deformations remain even after stresses have been removed. One type of irreversible deformation is plastic deformation, which occurs in material bodies after stresses have attained a certain threshold value known as the elastic limit or yield stress, and are the result of slip, or dislocation mechanisms at the atomic level. Another type of irreversible deformation is viscous deformation, which is the irreversible part of viscoelastic deformation.
 In the case of elastic deformations, the response function linking strain to the deforming stress is the compliance tensor of the material.
"
Density,Physics,2,"
 The density (more precisely, the volumetric mass density; also known as specific mass), of a substance is its mass per unit volume. The symbol most often used for density is ρ (the lower case Greek letter rho), although the Latin letter D can also be used. Mathematically, density is defined as mass divided by volume:[1] where ρ is the density, m is the mass, and V is the volume. In some cases (for instance, in the United States oil and gas industry), density is loosely defined as its weight per unit volume,[2] although this is scientifically inaccurate – this quantity is more specifically called specific weight.
 For a pure substance the density has the same numerical value as its mass concentration.
Different materials usually have different densities, and density may be relevant to buoyancy, purity and packaging. Osmium and iridium are the densest known elements at standard conditions for temperature and pressure.
 To simplify comparisons of density across different systems of units, it is sometimes replaced by the dimensionless quantity ""relative density"" or ""specific gravity"", i.e. the ratio of the density of the material to that of a standard material, usually water. Thus a relative density less than one means that the substance floats in water.
 The density of a material varies with temperature and pressure. This variation is typically small for solids and liquids but much greater for gases. Increasing the pressure on an object decreases the volume of the object and thus increases its density. Increasing the temperature of a substance (with a few exceptions) decreases its density by increasing its volume. In most materials, heating the bottom of a fluid results in convection of the heat from the bottom to the top, due to the decrease in the density of the heated fluid. This causes it to rise relative to more dense unheated material.
 The reciprocal of the density of a substance is occasionally called its specific volume, a term sometimes used in thermodynamics. Density is an intensive property in that increasing the amount of a substance does not increase its density; rather it increases its mass.
"
Derivative,Physics,2,"
 The derivative of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value). Derivatives are a fundamental tool of calculus.  For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time advances.
 The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value.  For this reason, the derivative is often described as the ""instantaneous rate of change"", the ratio of the instantaneous change in the dependent variable to that of the independent variable.
 Derivatives may be generalized to functions of several real variables. In this generalization, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables.  It can be calculated in terms of the partial derivatives with respect to the independent variables.  For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector.
 The process of finding a derivative is called differentiation. The reverse process is called antidifferentiation.  The fundamental theorem of calculus relates antidifferentiation with integration. Differentiation and integration constitute the two fundamental operations in single-variable calculus.[Note 1]"
Destructive_interference,Physics,2,"In physics, interference is a phenomenon in which two waves superpose to form a resultant wave of greater, lower, or the same amplitude. Constructive and destructive interference result from the interaction of waves that are correlated or coherent with each other, either because they come from the same source or because they have the same or nearly the same frequency. Interference effects can be observed with all types of waves, for example, light, radio, acoustic, surface water waves, gravity waves, or matter waves. The resulting images or graphs are called interferograms.
"
Diamagnetism,Physics,2,"Diamagnetic materials are repelled by a magnetic field; an applied magnetic field creates an induced magnetic field in them in the opposite direction, causing a repulsive force.  In contrast, paramagnetic and ferromagnetic materials are attracted by a magnetic field. Diamagnetism is a quantum mechanical effect that occurs in all materials; when it is the only contribution to the magnetism, the material is called diamagnetic. In paramagnetic and ferromagnetic substances, the weak diamagnetic force is overcome by the attractive force of magnetic dipoles in the material. The magnetic permeability of diamagnetic materials is less than the permeability of vacuum, μ0. In most materials, diamagnetism is a weak effect which can only be detected by sensitive laboratory instruments, but a superconductor acts as a strong diamagnet because it repels a magnetic field entirely from its interior.
 Diamagnetism was first discovered when Anton Brugmans observed in 1778 that bismuth was repelled by magnetic fields.[1] In 1845, Michael Faraday demonstrated that it was a property of matter and concluded that every material responded (in either a diamagnetic or paramagnetic way) to an applied magnetic field. On a suggestion by William Whewell, Faraday first referred to the phenomenon as diamagnetic (the prefix dia- meaning through or across), then later changed it to diamagnetism.[2][3] A simple rule of thumb is used in chemistry to determine whether a particle (atom, ion, or molecule) is paramagnetic or diamagnetic:[4] If all electrons in the particle are paired, then the substance made of this particle is diamagnetic; If it has unpaired electrons, then the substance is paramagnetic.
"
Dielectric,Physics,2,"A dielectric (or dielectric material) is an electrical insulator that can be polarized by an applied electric field. When a dielectric material is placed in an electric field, electric charges do not flow through  the material as they do in an electrical conductor but only slightly shift from their average equilibrium positions causing dielectric polarization. Because of dielectric polarization, positive charges are displaced in the direction of the field and negative charges shift in the direction opposite to the field (for example, if the field is moving in the positive x-axis, the negative charges will shift in the negative x-axis). This creates an internal electric field that reduces the overall field within the dielectric itself.[1] If a dielectric is composed of weakly bonded molecules, those molecules not only become polarized, but also reorient so that their symmetry axes align to the field.[1] The study of dielectric properties concerns storage and dissipation of electric and magnetic energy in materials.[2][3][4] Dielectrics are important for explaining various phenomena in electronics, optics, solid-state physics, and cell biophysics.
"
Diffraction,Physics,2,"Diffraction refers to various phenomena that occur when a wave encounters an obstacle or opening. It is defined as the bending of waves around  the corners of an obstacle or through an aperture into the region of geometrical shadow of the obstacle/aperture. The diffracting object or aperture effectively becomes a secondary source of the propagating wave. Italian scientist Francesco Maria Grimaldi coined the word diffraction and was the first to record accurate observations of the phenomenon in 1660.[1][2] In classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets.[3] The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength, as shown in the inserted image. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. However, if there are multiple, closely spaced openings, a complex pattern of varying intensity can result.
 These effects also occur when a light wave travels through a medium with a varying refractive index, or when a sound wave travels through a medium with varying acoustic impedance – all waves diffract, including gravitational waves[citation needed], water waves, and other electromagnetic waves such as X-rays and radio waves. Furthermore, quantum mechanics also demonstrates that matter possesses wave-like properties, and hence, undergoes diffraction (which is measurable at subatomic to molecular levels).[4] Diffraction and interference are closely related and are nearly – if not exactly – identical in meaning. Richard Feynman observes that ""diffraction"" tends to be used when referring to many wave sources, and ""interference"" when only a few are considered.[5]"
Direct_current,Physics,2,"Direct current (DC) is the one directional or unidirectional flow of electric charge. An electrochemical cell is a prime example of  DC power. Direct current may flow through a conductor such as a wire, but can also flow through semiconductors, insulators, or even through a vacuum as in electron or ion beams. The electric current flows in a constant direction, distinguishing it from alternating current (AC). A term formerly used for this type of current was galvanic current.[1] The abbreviations AC and DC are often used to mean simply alternating and direct, as when they modify current or voltage.[2][3] Direct current may be converted from an alternating current supply by use of a rectifier, which contains electronic elements (usually) or electromechanical elements (historically) that allow current to flow only in one direction. Direct current may be converted into alternating current via an inverter.
 Direct current has many uses, from the charging of batteries to large power supplies for electronic systems, motors, and more. Very large quantities of electrical energy provided via direct-current  are used in smelting of aluminum and other electrochemical processes. It is also used for some railways, especially in urban areas. High-voltage direct current is used to transmit large amounts of power from remote generation sites or to interconnect alternating current power grids.
"
Dispersion_(optics),Physics,2,"In optics, dispersion is the phenomenon in which the phase velocity of a wave depends on its frequency.[1]
Media having this common property may be termed dispersive media. Sometimes the term chromatic dispersion is used for specificity.
Although the term is used in the field of optics to describe light and other electromagnetic waves, dispersion in the same sense can apply to any sort of wave motion such as acoustic dispersion in the case of sound and seismic waves, in gravity waves (ocean waves), and for telecommunication signals along transmission lines (such as coaxial cable) or optical fiber.
 In optics, one important and familiar consequence of dispersion is the change in the angle of refraction of different colors of light,[2] as seen in the spectrum produced by a dispersive prism and in chromatic aberration of lenses. Design of compound achromatic lenses, in which chromatic aberration is largely cancelled, uses a quantification of a glass's dispersion given by its Abbe number V, where lower Abbe numbers correspond to greater dispersion over the visible spectrum. In some applications such as telecommunications, the absolute phase of a wave is often not important but only the propagation of wave packets or ""pulses""; in that case one is interested only in variations of group velocity with frequency, so-called group-velocity dispersion.
"
Displacement_(fluid),Physics,2,"In fluid mechanics, displacement occurs when an object is largely immersed in a fluid, pushing it out of the way and taking its place. The volume of the fluid displaced can then be measured, and from this, the volume of the immersed object can be deduced (the volume of the immersed object will be exactly equal to the volume of the displaced fluid).
 An object that sinks displaces an amount of fluid equal to the object's volume. Thus buoyancy is expressed through Archimedes' principle, which states that the weight of the object is reduced by its volume multiplied by the density of the fluid. If the weight of the object is less than this displaced quantity, the object floats; if more, it sinks. The amount of fluid displaced is directly related (via Archimedes' principle) to its volume.
 In the case of an object that sinks (is totally submerged), the volume of the object is displaced. In the case of an object that floats, the amount of fluid displaced will be equal in weight to the displacing object.
 Archimedes' principle, a physical law of buoyancy, states that any body completely or partially submerged in a fluid (gas or liquid) at rest is acted upon by an upward, or buoyant, force the magnitude of which is equal to the weight of the fluid displaced by the body. The volume of displaced fluid is equivalent to the volume of an object fully immersed in a fluid or to that fraction of the volume below the surface of an object partially submerged in a liquid. The weight of the displaced portion of the fluid is equivalent to the magnitude of the buoyant force. The buoyant force on a body floating in a liquid or gas is also equivalent in magnitude to the weight of the floating object and is opposite in direction; the object neither rises nor sinks. If the weight of an object is less than that of the displaced fluid, the object rises, as in the case of a block of wood that is released beneath the surface of water or a helium-filled balloon that is let loose in the air. An object heavier than the amount of the fluid it displaces, though it sinks when released, has an apparent weight loss equal to the weight of the fluid displaced. In fact, in some accurate weighing, a correction must be made in order to compensate for the buoyancy effect of the surrounding air. The buoyant force, which always opposes gravity, is nevertheless caused by buoyancy. Fluid pressure increases with depth because of the (gravitational) weight of the fluid above. This increasing pressure applies a force on a submerged object that increases with depth. The result is buoyancy.[1]"
Distance,Physics,2,"Distance is a numerical measurement of how far apart objects or points are. In physics or everyday usage, distance may refer to a physical length or an estimation based on other criteria (e.g. ""two counties over""). The distance from a point A to a point B is sometimes denoted as 




|

A
B

|



{  |AB|}
.[1] In most cases, ""distance from A to B"" is interchangeable with ""distance from B to A"".[2] In mathematics, a distance function or metric is a generalization of the concept of physical distance; it is a way of describing what it means for elements of some space to be ""close to"", or ""far away from"" each other.
In psychology and social sciences, distance is a non-numerical measurement; Psychological distance is defined as ""the different ways in which an object might be removed from"" the self along dimensions such as ""time, space, social distance, and hypotheticality.[3]"
Drift_velocity,Physics,2,"In physics a drift velocity is the average velocity attained by charged particles, such as electrons, in a material due to an electric field. In general, an electron  in a conductor will propagate randomly at the Fermi velocity, resulting in an average velocity of zero. Applying an electric field adds to this random motion a small net flow in one direction; this is the drift.
 Drift velocity is proportional to current. In a resistive material it is also proportional to the magnitude of an external electric field. Thus Ohm's law can be explained in terms of drift velocity. The law's most elementary expression is:
 where u is drift velocity, μ is the material's electron mobility, and E is the electric field. In the MKS system these quantities' units are m/s, m2/(V·s), and V/m, respectively.
 When a potential difference is applied across a conductor, free electrons gain velocity in the direction opposite to the electric field between successive collisions (and lose velocity when traveling in the direction of the field), thus acquiring a velocity component in that direction in addition to its random thermal velocity. As a result, there is a definite small drift velocity of electrons, which is superimposed on the random motion of free electrons. Due to this drift velocity, there is a net flow of electrons opposite to the direction of the field.
"
Doppler_effect,Physics,2,"The Doppler effect (or the Doppler shift) is the change in frequency of a wave in relation to an observer who is moving relative to the wave source.[1] It is named after the Austrian physicist Christian Doppler, who described the phenomenon in 1842.
 A common example of Doppler shift is the change of pitch heard when a vehicle sounding a horn approaches and recedes from an observer. Compared to the emitted frequency, the received frequency is higher during the approach, identical at the instant of passing by, and lower during the recession.[2] The reason for the Doppler effect is that when the source of the waves is moving towards the observer, each successive wave crest is emitted from a position closer to the observer than the crest of the previous wave.[2][3]  Therefore, each wave takes slightly less time to reach the observer than the previous wave. Hence, the time between the arrivals of successive wave crests at the observer is reduced, causing an increase in the frequency. While they are traveling, the distance between successive wave fronts is reduced, so the waves ""bunch together"".  Conversely, if the source of waves is moving away from the observer, each wave is emitted from a position farther from the observer than the previous wave, so the arrival time between successive waves is increased, reducing the frequency. The distance between successive wave fronts is then increased, so the waves ""spread out"".
 For waves that propagate in a medium, such as sound waves, the velocity of the observer and of the source are relative to the medium in which the waves are transmitted.[1] The total Doppler effect may therefore result from motion of the source, motion of the observer, or motion of the medium. Each of these effects is analyzed separately. For waves which do not require a medium, such as electromagnetic waves or gravitational waves, only the relative difference in velocity between the observer and the source needs to be considered, giving rise to the relativistic Doppler effect.
"
Drag_(physics),Physics,2,"In fluid dynamics, drag (sometimes called air resistance, a type of friction, or fluid resistance, another type of friction or fluid friction) is a force acting opposite to the relative motion of any object moving with respect to a surrounding fluid.[1] This can exist between two fluid layers (or surfaces) or a fluid and a solid surface. Unlike other resistive forces, such as dry friction, which are nearly independent of velocity, drag force depends on velocity.[2][3] Drag force is proportional to the velocity for a laminar flow and the squared velocity for a turbulent flow. Even though the ultimate cause of a drag is viscous friction, the turbulent drag is independent of viscosity.[4] Drag forces always decrease fluid velocity relative to the solid object in the fluid's path.
"
Ductility,Physics,2,"Ductility is a mechanical property commonly described as a material's amenability to drawing (e.g. into wire).[1] In materials science, ductility is defined by the degree to which a material can sustain plastic deformation under tensile stress before failure.[2][3] Ductility is an important consideration in engineering and manufacturing, defining a material's suitability for certain manufacturing operations (such as cold working) and its capacity to absorb mechanical overload.[4] Materials that are generally described as ductile include gold and copper.[5] Malleability, a similar mechanical property, is characterized by a material's ability to deform plastically without failure under compressive stress.[6][7] Historically, materials were considered malleable if they were amenable to forming by hammering or rolling.[1] Lead is an example of a material which is, relatively, malleable but not ductile.[5][8]"
Dynamics_(mechanics),Physics,2,"Dynamics is the branch of physics developed in classical mechanics concerned with the study of forces and their effects on motion. Isaac Newton was the first to formulate the fundamental physical laws that govern dynamics in classical non-relativistic physics, especially his second law of motion.
"
Dyne,Physics,2,"The dyne (symbol dyn, from Ancient Greek: δύναμις, romanized: dynamis, lit. 'power, force') is a derived unit of force specified in the centimetre–gram–second (CGS) system of units, a predecessor of the modern SI.
"
Econophysics,Physics,2,"Econophysics is a heterodox interdisciplinary research field, applying theories and methods originally developed by physicists in order to solve problems in economics, usually those including uncertainty or stochastic processes and nonlinear dynamics. Some of its application to the study of financial markets has also been termed statistical finance referring to its roots in statistical physics. Econophysics is closely related to social physics.
"
Elastic_collision,Physics,2,"An elastic collision is an encounter between two bodies  in which the total kinetic energy of the two bodies remains the same. In an ideal, perfectly elastic collision, there is no net conversion of kinetic energy into other forms such as heat, noise, or potential energy.
 During the collision of small objects, kinetic energy is first converted to potential energy associated with a repulsive force between the particles (when the particles move against this force, i.e. the angle between the force and the relative velocity is obtuse), then this potential energy is converted back to kinetic energy (when the particles move with this force, i.e. the angle between the force and the relative velocity is acute).
 Collisions of atoms are elastic, for example Rutherford backscattering.
 A useful special case of elastic collision is when the two bodies have equal mass, in which case they will simply exchange their momenta.
 The molecules—as distinct from atoms—of a gas or liquid rarely experience perfectly elastic collisions because kinetic energy is exchanged between the molecules’ translational motion and their internal degrees of freedom with each collision. At any instant, half the collisions are, to a varying extent, inelastic collisions (the pair possesses less kinetic energy in their translational motions after the collision than before), and half could be described as “super-elastic” (possessing more kinetic energy after the collision than before). Averaged across the entire sample, molecular collisions can be regarded as essentially elastic as long as Planck's law forbids black-body photons to carry away energy from the system.
 In the case of macroscopic bodies, perfectly elastic collisions are an ideal never fully realized, but approximated by the interactions of objects such as billiard balls.
 When considering energies, possible rotational energy before and/or after a collision may also play a role.
"
Elastic_energy,Physics,2,"Elastic energy is the mechanical potential energy stored in the configuration of a material or physical system as it is subjected to elastic deformation by work performed upon it.  Elastic energy occurs when objects are impermanently compressed, stretched or generally deformed in any manner.  Elasticity theory primarily develops formalisms for the mechanics of solid bodies and materials.[1] (Note however, the work done by a stretched rubber band is not an example of elastic energy. It is an example of entropic elasticity.) The elastic potential energy equation is used in calculations of positions of mechanical equilibrium.  The energy is potential as it will be converted into other forms of energy, such as kinetic energy and sound energy, when the object is allowed to return to its original shape (reformation) by its elasticity. 
 The essence of elasticity is reversibility. Forces applied to an elastic material transfer energy into the material which, upon yielding that energy to its surroundings, can recover its original shape.  However, all materials have limits to the degree of distortion they can endure without breaking or irreversibly altering their internal structure. Hence, the characterizations of solid materials include specification, usually in terms of strains, of its elastic limits.  Beyond the elastic limit, a material is no longer storing all of the energy from mechanical work performed on it in the form of elastic energy.
 Elastic energy of or within a substance is static energy of configuration. It corresponds to energy stored principally by changing the interatomic distances between nuclei. Thermal energy is the randomized distribution of kinetic energy within the material, resulting in statistical fluctuations of the material about the equilibrium configuration. There is some interaction, however.  For example, for some solid objects, twisting, bending, and other distortions may generate thermal energy, causing the material's temperature to rise. Thermal energy in solids is often carried by internal elastic waves, called phonons. Elastic waves that are large on the scale of an isolated object usually produce macroscopic vibrations sufficiently lacking in randomization that their oscillations are merely the repetitive exchange between (elastic) potential energy within the object and the kinetic energy of motion of the object as a whole.
 Although elasticity is most commonly associated with the mechanics of solid bodies or materials, even the early literature on classical thermodynamics defines and uses ""elasticity of a fluid"" in ways compatible with the broad definition provided in the Introduction above.[2]:107 et seq. Solids include complex crystalline materials with sometimes complicated behavior.  By contrast, the behavior of compressible fluids, and especially gases, demonstrates the essence of elastic energy with negligible complication. The simple thermodynamic formula: 



d
U
=
−
P

d
V
 
,


{  dU=-P\,dV\ ,}
 where dU is an infinitesimal change in recoverable internal energy U, P is the uniform pressure (a force per unit area) applied to the material sample of interest, and dV is the infinitesimal change in volume that corresponds to the change in internal energy.  The minus sign appears because dV is negative under compression by a positive applied pressure which also increases the internal energy. Upon reversal, the work that is done by a system is the negative of the change in its internal energy corresponding to the positive dV of an increasing volume. In other words, the system loses stored internal energy when doing work on its surroundings.  Pressure is stress and volumetric change corresponds to changing the relative spacing of points within the material.  The stress-strain-internal energy relationship of the foregoing formula is repeated in formulations for elastic energy of solid materials with complicated crystalline structure.
"
Elastic_instability,Physics,2,"Elastic instability is a form of instability occurring in elastic systems, such as buckling of beams and plates subject to large compressive loads.
 There are a lot of ways to study this kind of instability. One of them is to use the method of incremental deformations based on superposing a small perturbation on an equilibrium solution.
"
Elastic_modulus,Physics,2,"An elastic modulus (also known as modulus of elasticity) is a quantity that measures an object or substance's resistance to being deformed elastically (i.e., non-permanently) when a stress is applied to it. The elastic modulus of an object is defined as the slope of its stress–strain curve in the elastic deformation region:[1] A stiffer material will have a higher elastic modulus. An elastic modulus has the form:
 where stress is the force causing the deformation divided by the area to which the force is applied and strain is the ratio of the change in some parameter caused by the deformation to the original value of the parameter. Since strain is a dimensionless quantity, the units of 



δ


{  \delta }
 will be the same as the units of stress.[2] Specifying how stress and strain are to be measured, including directions, allows for many types of elastic moduli to be defined. The three primary ones are:
 Two other elastic moduli are Lamé's first parameter, λ, and P-wave modulus, M, as used in table of modulus comparisons given below references.
 Homogeneous and isotropic (similar in all directions) materials (solids) have their (linear) elastic properties fully described by two elastic moduli, and one may choose any pair.  Given a pair of elastic moduli, all other elastic moduli can be calculated according to formulas in the table below at the end of page.
 Inviscid fluids are special in that they cannot support shear stress, meaning that the shear modulus is always zero. This also implies that Young's modulus for this group is always zero.
 In some texts, the modulus of elasticity is referred to as the elastic constant, while the inverse quantity is referred to as elastic modulus.
"
Elasticity_(physics),Physics,2,"
 In physics and materials science, elasticity is the ability of a body to resist a distorting influence and to return to its original size and shape when that influence or force is removed. Solid objects will deform when adequate loads are applied to them; if the material is elastic, the object will return to its initial shape and size after removal. This is in contrast to plasticity, in which the object fails to do so and instead remains in its deformed state.
 The physical reasons for elastic behavior can be quite different for different materials. In metals, the atomic lattice changes size and shape when forces are applied (energy is added to the system).  When forces are removed, the lattice goes back to the original lower energy state. For rubbers and other polymers, elasticity is caused by the stretching of polymer chains when forces are applied.
 Hooke's law states that the force required to deform elastic objects should be directly proportional to the distance of deformation, regardless of how large that distance becomes. This is known as perfect elasticity, in which a given object will return to its original shape no matter how strongly it is deformed. This is an ideal concept only; most materials which possess elasticity in practice remain purely elastic only up to very small deformations, after which plastic (permanent) deformation occurs.
 In engineering, the elasticity of a material is quantified by the elastic modulus such as the Young's modulus, bulk modulus or shear modulus which measure the amount of stress needed to achieve a unit of strain; a higher modulus indicates that the material is harder to deform. The SI unit of this modulus is the pascal (Pa). The material's elastic limit or yield strength is the maximum stress that can arise before the onset of plastic deformation. Its SI unit is also the pascal (Pa).
"
Electric_charge,Physics,2,"Electric charge is the physical property of matter that causes it to experience a force when placed in an electromagnetic field. There are two types of electric charge: positive and negative (commonly carried by protons and electrons respectively). Like charges repel each other and unlike charges attract each other. An object with an absence of net charge is referred to as neutral. Early knowledge of how charged substances interact is now called classical electrodynamics, and is still accurate for problems that do not require consideration of quantum effects.
 Electric charge is a conserved property; the net charge of an isolated system, the amount of positive charge minus the amount of negative charge, cannot change. Electric charge is carried by subatomic particles. In ordinary matter, negative charge is carried by electrons, and positive charge is carried by the protons in the nuclei of atoms. If there are more electrons than protons in a piece of matter, it will have a negative charge, if there are fewer it will have a positive charge, and if there are equal numbers it will be neutral. Charge is quantized; it comes in integer multiples of individual small units called the elementary charge, e, about 1.602×10−19 coulombs,[1] which is the smallest charge which can exist freely (particles called quarks have smaller charges, multiples of 1/3e, but they are only found in combination, and always combine to form particles with integer charge). The proton has a charge of +e, and the electron has a charge of −e.
 Electric charges produce electric fields.[2] A moving charge also produces a magnetic field.[3] The interaction of electric charges with an electromagnetic field (combination of  electric and magnetic fields) is the source of the electromagnetic (or Lorentz) force,[4] which is one of the four fundamental forces in physics. The study of photon-mediated interactions among charged particles is called quantum electrodynamics.[5] The SI derived unit of electric charge is the coulomb (C) named after  French physicist Charles-Augustin de Coulomb. In electrical engineering, it is also common to use the ampere hour (Ah); in physics and chemistry, it is common to use the elementary charge (e as a unit). Chemistry also uses the Faraday constant as the charge on a mole of electrons. The lowercase symbol q often denotes charge.
"
Electric_circuit,Physics,2,"An electrical network is an interconnection of electrical components (e.g., batteries, resistors, inductors, capacitors, switches, transistors) or a model of such an interconnection, consisting of electrical elements (e.g., voltage sources, current sources, resistances, inductances, capacitances).   An electrical circuit is a network consisting of a closed loop, giving a return path for the current. Linear electrical networks, a special type consisting only of sources (voltage or current), linear lumped elements (resistors, capacitors, inductors), and linear distributed elements (transmission lines), have the property that signals are linearly superimposable.  They are thus more easily analyzed, using powerful frequency domain methods such as Laplace transforms, to determine DC response, AC response, and transient response.
 A resistive circuit is a circuit containing only resistors and ideal current and voltage sources.  Analysis of resistive circuits is less complicated than analysis of circuits containing capacitors and inductors.  If the sources are constant (DC) sources, the result is a DC circuit. The effective resistance and current distribution properties of arbitrary resistor networks can be modeled in terms of their graph measures and geometrical properties.[1] A network that contains active electronic components is known as an electronic circuit. Such networks are generally nonlinear and require more complex design and analysis tools.
"
Electric_current,Physics,2,"
 An electric current is a stream of charged particles, such as electrons or ions, moving through an electrical conductor or space. It is measured as the net rate of flow of electric charge past a region.[1]:2[2]:622 The moving particles are called charge carriers, which may be one of several types of particles, depending on the conductor.  In electric circuits the charge carriers are often electrons moving through a wire. In semiconductors they can be electrons or holes. In an electrolyte the charge carriers are ions, while in plasma, an ionized gas, electric current is formed by both electrons and ions.[3] The SI unit of electric current is the ampere, or amp, which is the flow of electric charge across a surface at the rate of one coulomb per second. The ampere (symbol: A) is an SI base unit[4]:15 Electric current is measured using a device called an ammeter.[2]:788 Electric currents create magnetic fields, which are used in motors, generators, inductors, and transformers. In ordinary conductors, they cause Joule heating, which creates light in incandescent light bulbs. Time-varying currents emit electromagnetic waves, which are used in telecommunications to broadcast information.
"
Electric_displacement_field,Physics,2,"
 In physics, the electric displacement field (denoted by D) or electric induction is a vector field that appears in Maxwell's equations.  It accounts for the effects of free and bound charge within materials. ""D"" stands for ""displacement"", as in the related concept of displacement current in dielectrics. In free space, the electric displacement field is equivalent to flux density, a concept that lends understanding to Gauss's law. In the International System of Units (SI), it is expressed in units of coulomb per meter square (C⋅m−2).
"
Electric_field,Physics,2,"An electric field (sometimes E-field[1]) is the physical field that surrounds each electric charge and exerts force on all other charges in the field, either attracting or repelling them.[2][3] Electric fields originate from electric charges, or from time-varying magnetic fields. Electric fields and magnetic fields are both manifestations of the electromagnetic force, one of the four fundamental forces (or interactions) of nature.
 Electric fields are important in many areas of physics, and are exploited practically in electrical technology. In atomic physics and chemistry, for instance, the electric field is used to model the attractive force holding the atomic nucleus and electrons together in atoms. It also models the forces in chemical bonding between atoms that result in molecules.
 The electric field is defined mathematically as a vector field that associates to each point in space the (electrostatic or Coulomb) force per unit of charge exerted on an infinitesimal positive test charge at rest at that point.[4][5][6] The derived SI units for the electric field are volts per meter (V/m), exactly equivalent to newtons per coulomb (N/C).[7]"
Electric_field_gradient,Physics,2,"In atomic, molecular, and solid-state physics, the electric field gradient (EFG) measures the rate of change of the electric field at an atomic nucleus generated by the electronic charge distribution and the other nuclei. The EFG couples with the nuclear electric quadrupole moment of quadrupolar nuclei (those with spin quantum number greater than one-half) to generate an effect which can be measured using several spectroscopic methods, such as nuclear magnetic resonance (NMR), microwave spectroscopy, electron paramagnetic resonance (EPR, ESR), nuclear quadrupole resonance (NQR), Mössbauer spectroscopy or perturbed angular correlation (PAC). The EFG is non-zero only if the charges surrounding the nucleus violate cubic symmetry and therefore generate an inhomogeneous electric field at the position of the nucleus.
 EFGs are highly sensitive to the electronic density in the immediate vicinity of a nucleus. This is because the EFG operator scales as r−3, where r is the distance from a nucleus. This sensitivity has been used to study effects on charge distribution resulting from substitution, weak interactions, and charge transfer. Especially in crystals, the local structure can be investigated with above methods using the EFG's sensitivity to local changes, like defects or phase changes. In crystals the EFG is in the order of 1021V/m². Density functional theory has become an important tool for methods of nuclear spectroscopy to calculate EFGs and provide a deeper understanding of specific EFGs in crystals from measurements.
"
Electric_intensity,Physics,2,"An electric field (sometimes E-field[1]) is the physical field that surrounds each electric charge and exerts force on all other charges in the field, either attracting or repelling them.[2][3] Electric fields originate from electric charges, or from time-varying magnetic fields. Electric fields and magnetic fields are both manifestations of the electromagnetic force, one of the four fundamental forces (or interactions) of nature.
 Electric fields are important in many areas of physics, and are exploited practically in electrical technology. In atomic physics and chemistry, for instance, the electric field is used to model the attractive force holding the atomic nucleus and electrons together in atoms. It also models the forces in chemical bonding between atoms that result in molecules.
 The electric field is defined mathematically as a vector field that associates to each point in space the (electrostatic or Coulomb) force per unit of charge exerted on an infinitesimal positive test charge at rest at that point.[4][5][6] The derived SI units for the electric field are volts per meter (V/m), exactly equivalent to newtons per coulomb (N/C).[7]"
Electric_generator,Physics,2,"In electricity generation, a generator[1] is a device that converts motive power (mechanical energy) into electrical power for use in an external circuit. Sources of mechanical energy include steam turbines, gas turbines, water turbines, internal combustion engines, wind turbines and even hand cranks. The first electromagnetic generator, the Faraday disk, was invented in 1831 by British scientist Michael Faraday. Generators provide nearly all of the power for electric power grids.
 The reverse conversion of electrical energy into mechanical energy is done by an electric motor, and motors and generators have many similarities. Many motors can be mechanically driven to generate electricity; frequently they make acceptable manual generators.
"
Electric_motor,Physics,2,"An electric motor is an electrical machine that converts electrical energy into mechanical energy. Most electric motors operate through the interaction between the motor's magnetic field and electric current in a wire winding to generate force in the form of torque applied on the motor's shaft. Electric motors can be powered by direct current (DC) sources, such as from batteries, motor vehicles or rectifiers, or by alternating current (AC) sources, such as a power grid, inverters or electrical generators. An electric generator is mechanically identical to an electric motor, but operates with a reversed flow of power, converting mechanical energy into electrical energy.
 Electric motors may be classified by considerations such as power source type, internal construction, application and type of motion output. In addition to AC versus DC types, motors may be brushed or brushless, may be of various phase (see single-phase, two-phase, or three-phase), and may be either air-cooled or liquid-cooled. General-purpose motors with standard dimensions and characteristics provide convenient mechanical power for industrial use. The largest electric motors are used for ship propulsion, pipeline compression and pumped-storage applications with ratings reaching 100 megawatts. Electric motors are found in industrial fans, blowers and pumps, machine tools, household appliances, power tools and disk drives. Small motors may be found in electric watches.
 In certain applications, such as in regenerative braking with traction motors, electric motors can be used in reverse as generators to recover energy that might otherwise be lost as heat and friction.
 Electric motors produce linear or rotary force (torque) intended to propel some external mechanism, such as a fan or an elevator. An electric motor is generally designed for continuous rotation, or for linear movement over a significant distance compared to its size.  Magnetic solenoids produce significant mechanical force, but over an operating distance comparable to their size. Transducers such as loudspeakers and microphones convert between electrical current and mechanical force to reproduce signals such as speech. When compared with common internal combustion engines (ICEs), electric motors are lightweight, physically smaller, provide more power output, are mechanically simpler and cheaper to build, while providing instant and consistent torque at any speed, with more responsiveness, higher overall efficiency and lower heat generation. However, electric motors are not as convenient or common as ICEs in mobile applications (i.e. cars and buses) as they require a large and expensive battery, while ICEs require a relatively small fuel tank.
"
Electric_potential,Physics,2,"An electric potential (also called the electric field potential, potential drop, or the electrostatic potential) is the amount of work needed to move a unit  of electric charge from a reference point to a specific point in an electric field without producing an acceleration. Typically, the reference point is the Earth or a point at infinity, although any point can be used.
 In classical electrostatics, the electrostatic field is a vector quantity which is expressed as the gradient of the electrostatic potential, which is a scalar quantity denoted by V or occasionally φ,[1] equal to the electric potential energy of any charged particle at any location (measured in joules) divided by the charge of that particle (measured in coulombs). By dividing out the charge on the particle a quotient is obtained that is a property of the electric field itself. In short, electric potential is the electric potential energy per unit charge.
 This value can be calculated in either a static (time-invariant) or a dynamic (varying with time) electric field at a specific time in units of joules per coulomb (J⋅C−1), or volts (V). The electric potential at infinity is assumed to be zero.
 In electrodynamics, when time-varying fields are present, the electric field cannot be expressed only in terms of a scalar potential. Instead, the electric field can be expressed in terms of both the scalar electric potential and the magnetic vector potential.[2] The electric potential and the magnetic vector potential together form a four vector, so that the two kinds of potential are mixed under Lorentz transformations.
 Practically, electric potential is always a continuous function in space; Otherwise, the spatial derivative of it will yield a field with infinite magnitude, which is practically impossible. Even an idealized point charge has 1 ⁄ r potential, which is continuous everywhere except the origin. The electric field is not continuous across an idealized surface charge, but it is not infinite at any point. Therefore, the electric potential is continuous across an idealized surface charge. An idealized linear charge has ln(r) potential, which is continuous everywhere except on the linear charge.
"
Electric_power,Physics,2,"Electric power is the rate, per unit time, at which electrical energy is transferred by an electric circuit. The SI unit of power is the watt, one joule per second.
 Electric power is usually produced by electric generators, but can also be supplied by  sources such as electric batteries.  It is usually supplied to businesses and homes (as domestic mains electricity) by the electric power industry through an electric power grid. 
 Electric power can be delivered over long distances by transmission lines and used for applications such as motion, light or heat with high efficiency.[1]"
Electrical_conductor,Physics,2,"In physics and electrical engineering, a conductor is an object or type of material that allows the flow of charge (electrical current) in one or more directions. Materials made of metal are common electrical conductors. Electrical current is generated by the flow of negatively charged electrons, positively charged holes, and positive or negative ions in some cases. 
 In order for current to flow, it is not necessary for one charged particle to travel from the machine producing the current to that consuming it. Instead, the charged particle simply needs to nudge its neighbor a finite amount who will nudge its neighbor and on and on until a particle is nudged into the consumer, thus powering the machine. Essentially what is occurring is a long chain of momentum transfer between mobile charge carriers; the Drude model of conduction describes this process more rigorously. This momentum transfer model makes metal an ideal choice for a conductor; metals, characteristically, possess a delocalized sea of electrons which gives the electrons enough mobility to collide and thus effect a momentum transfer. 
 As discussed above, electrons are the primary mover in metals; however, other devices such as the cationic electrolyte(s) of a battery, or the mobile protons of the proton conductor of a fuel cell rely on positive charge carriers. Insulators are non-conducting materials with few mobile charges that support only insignificant electric currents.
"
Electrical_insulator,Physics,2,"An electrical insulator is a material in which the electron does not flow freely or the atom of the insulator have tightly bound electrons whose internal electric charges do not flow freely; very little electric current will flow through it under the influence of an electric field. This contrasts with other materials, semiconductors and conductors, which conduct electric current more easily. The property that distinguishes an insulator is its resistivity; insulators have higher resistivity than semiconductors or conductors. The most common examples are non-metals.
 A perfect insulator does not exist because even insulators contain small numbers of mobile charges (charge carriers) which can carry current. In addition, all insulators become electrically conductive when a sufficiently large voltage is applied that the electric field tears electrons away from the atoms. This is known as the breakdown voltage of an insulator. Some materials such as glass, paper and Teflon, which have high resistivity, are very good electrical insulators. A much larger class of materials, even though they may have lower bulk resistivity, are still good enough to prevent significant current from flowing at normally used voltages, and thus are employed as insulation for electrical wiring and cables. Examples include rubber-like polymers and most plastics which can be thermoset or thermoplastic in nature.
 Insulators are used in electrical equipment to support and separate electrical conductors without allowing current through themselves. An insulating material used in bulk to wrap electrical cables or other equipment is called insulation. The term insulator is also used more specifically to refer to insulating supports used to attach electric power distribution or transmission lines to utility poles and transmission towers. They support the weight of the suspended wires without allowing the current to flow through the tower to ground.
"
Electrical_potential_energy,Physics,2,"Electric potential energy, or Electrostatic potential energy, is a potential energy (measured in joules) that results from conservative Coulomb forces and is associated with the configuration of a particular set of point charges within a defined system. An object may have electric potential energy by virtue of two key elements: its own electric charge and its relative position to other electrically charged objects.
 The term ""electric potential energy"" is used to describe the potential energy in systems with time-variant electric fields, while the term ""electrostatic potential energy"" is used to describe the potential energy in systems with time-invariant electric fields.
"
Electrical_and_electronics_engineering,Physics,2,"
 Electrical engineering is an engineering discipline concerned with the study, design and application of equipment, devices and systems which use electricity, electronics, and electromagnetism. It emerged as an identifiable occupation in the latter half of the 19th century after commercialization of the electric telegraph, the telephone, and electrical power generation, distribution and use.
 Electrical engineering is now divided into a wide range of fields, including computer engineering, systems engineering, power engineering, telecommunications, radio-frequency engineering, signal processing, instrumentation, and electronics. Many of these disciplines overlap with other engineering branches, spanning a huge number of specializations including hardware engineering, power electronics, electromagnetics and waves, microwave engineering, nanotechnology, electrochemistry, renewable energies, mechatronics, and electrical materials science.[a] Electrical engineers typically hold a degree in electrical engineering or electronic engineering.  Practising engineers may have professional certification and be members of a professional body or an international standards organization. These include the International Electrotechnical Commission (IEC), the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET) (formerly the IEE).
 Electrical engineers work in a very wide range of industries and the skills required are likewise variable. These range from circuit theory to the management skills of a project manager.  The tools and equipment that an individual engineer may need are similarly variable, ranging from a simple voltmeter to sophisticated design and manufacturing software.
"
Electrical_network,Physics,2,"An electrical network is an interconnection of electrical components (e.g., batteries, resistors, inductors, capacitors, switches, transistors) or a model of such an interconnection, consisting of electrical elements (e.g., voltage sources, current sources, resistances, inductances, capacitances).   An electrical circuit is a network consisting of a closed loop, giving a return path for the current. Linear electrical networks, a special type consisting only of sources (voltage or current), linear lumped elements (resistors, capacitors, inductors), and linear distributed elements (transmission lines), have the property that signals are linearly superimposable.  They are thus more easily analyzed, using powerful frequency domain methods such as Laplace transforms, to determine DC response, AC response, and transient response.
 A resistive circuit is a circuit containing only resistors and ideal current and voltage sources.  Analysis of resistive circuits is less complicated than analysis of circuits containing capacitors and inductors.  If the sources are constant (DC) sources, the result is a DC circuit. The effective resistance and current distribution properties of arbitrary resistor networks can be modeled in terms of their graph measures and geometrical properties.[1] A network that contains active electronic components is known as an electronic circuit. Such networks are generally nonlinear and require more complex design and analysis tools.
"
Electrical_resistance,Physics,2,"In electronics and electromagnetism, the electrical resistance of an object is a measure of its opposition to the flow of electric current. The reciprocal quantity is electrical conductance, and is the ease with which an electric current passes. Electrical resistance shares some conceptual parallels with the notion of mechanical friction. The SI unit of electrical resistance is the ohm (Ω), while electrical conductance is measured in siemens (S) (formerly called “mho”s and then represented by ℧).
 The resistance of an object depends in large part on the material it is made of. Objects made of electrical insulators like rubber tend to have very high resistance and low conductivity, while objects made of electrical conductors like metals tend to have very low resistance and high conductivity. This relationship is quantified by resistivity or conductivity. The nature of a material is not the only factor in resistance and conductance, however; it also depends on the size and shape of an object because these properties are extensive rather than intensive. For example, a wire's resistance is higher if it is long and thin, and lower if it is short and thick. All objects resist electrical current, except for superconductors, which have a resistance of zero.
 The resistance R of an object is defined as the ratio of voltage V across it to current I through it, while the conductance G is the reciprocal:
 For a wide variety of materials and conditions, V and I  are directly proportional to each other, and therefore R and G are constants (although they will depend on the size and shape of the object, the material it is made of, and other factors like temperature or strain). This proportionality is called Ohm's law, and materials that satisfy it are called ohmic materials.
 In other cases, such as a transformer, diode or battery, V and I are not directly proportional. The ratio V/I is sometimes still useful, and is referred to as a chordal resistance or static resistance,[1][2] since it corresponds to the inverse slope of a chord between the origin and an I–V curve. In other situations, the derivative 







d


V



d


I







{  {\frac {\mathrm {d} \,V}{\mathrm {d} \,I}}\,\!}
 may be most useful; this is called the differential resistance.
"
Electricity,Physics,2,"
 Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Electricity is related to magnetism, both being part of the phenomenon of electromagnetism, as described by Maxwell's equations. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.
 The presence of an electric charge, which can be either positive or negative, produces an electric field. The movement of electric charges is an electric current and produces a magnetic field.
 When a charge is placed in a location with a non-zero electric field, a force will act on it. The magnitude of this force is given by Coulomb's law. If the charge moves, the electric field would be doing work on the electric charge. Thus we can speak of electric potential at a certain point in space, which is equal to the work done by an external agent in carrying a unit of positive charge from an arbitrarily chosen reference point to that point without any acceleration and is typically measured in volts.
 Electricity is at the heart of many modern technologies, being used for:
 Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the seventeenth and eighteenth centuries. The theory of electromagnetism was developed in the 19th century, and by the end of that century electricity was being put to industrial and residential use by electrical engineers. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.[1]"
Electro-optic_effect,Physics,2,"An electro-optic effect is a change in the optical properties of a material in response to an electric field that varies slowly compared with the frequency of light.  The term encompasses a number of distinct phenomena, which can be subdivided into
 In December 2015, two further electro-optic effects of type (b) were theoretically predicted to exist [1] but have not, as yet, been experimentally observed.
 Changes in absorption can have a strong effect on refractive index for wavelengths near the absorption edge, due to the Kramers–Kronig relation.
 Using a less strict definition of the electro-optic effect allowing also electric fields oscillating at optical frequencies, one could also include nonlinear absorption (absorption depends on the light intensity) to category a) and the optical Kerr effect (refractive index depends on the light intensity) to category b). Combined with the photoeffect and photoconductivity, the electro-optic effect gives rise to the photorefractive effect.
 The term ""electro-optic"" is often erroneously used as a synonym for ""optoelectronic"".
"
Electrochemical_cell,Physics,2,"
 An electrochemical cell is a device capable of either generating electrical energy from chemical reactions or using electrical energy to cause chemical reactions. The electrochemical cells which generate an electric current are called voltaic cells or galvanic cells and those that generate chemical reactions, via electrolysis for example, are called electrolytic cells.[1][2][better source needed] A common example of a galvanic cell is a standard 1.5 volt[3][better source needed] cell meant for consumer use.  A battery consists of one or more cells, connected in parallel, series or series-and-parallel pattern.
"
Electrodynamics,Physics,2,"
 Electromagnetism is a branch of physics involving the study of the electromagnetic force, a type of physical interaction that occurs between electrically charged particles. The electromagnetic force is carried by electromagnetic fields composed of electric fields and magnetic fields, and it is responsible for electromagnetic radiation such as light.  It is one of the four fundamental interactions (commonly called forces) in nature, together with the strong interaction, the weak interaction, and gravitation.[1] At high energy the weak force and electromagnetic force are unified as a single electroweak force.
 Electromagnetic phenomena are defined in terms of the electromagnetic force, sometimes called the Lorentz force, which includes both electricity and magnetism as different manifestations of the same phenomenon. The electromagnetic force plays a major role in determining the internal properties of most objects encountered in daily life.  The electromagnetic attraction between atomic nuclei and their orbital electrons holds atoms together.  Electromagnetic forces are responsible for the chemical bonds between atoms which create molecules, and intermolecular forces.   The electromagnetic force governs all chemical processes, which arise from interactions between the electrons of neighboring atoms.
 There are numerous mathematical descriptions of the electromagnetic field.  In classical electrodynamics, electric fields are described as electric potential and electric current. In Faraday's law, magnetic fields are associated with electromagnetic induction and magnetism, and Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents.
 The theoretical implications of electromagnetism, particularly the establishment of the speed of light based on properties of the ""medium"" of propagation (permeability and permittivity), led to the development of special relativity by Albert Einstein in 1905.
"
Electrolytic_cell,Physics,2,"An electrolytic cell uses electrical energy to drive a non-spontaneous redox reaction.  An electrolytic cell is a kind of electrochemical cell. It is often used to decompose chemical compounds, in a process called electrolysis—the Greek word lysis means to break up. Important examples of electrolysis are the decomposition of water into hydrogen and oxygen, and bauxite into aluminium and other chemicals. Electroplating (e.g., of copper, silver, nickel or chromium) is done  using an electrolytic cell. Electrolysis is a technique that uses a direct electric current (DC).
 An electrolytic cell has three component parts: an electrolyte and two electrodes (a cathode and an anode). The electrolyte is usually a solution of water or other solvents in which ions are dissolved. Molten salts such as sodium chloride are also electrolytes. When driven by an external voltage applied to the electrodes, the ions in the electrolyte are attracted to an electrode with the opposite charge, where charge-transferring (also called faradaic or redox) reactions can take place. Only with an external electrical potential (i.e., voltage) of correct polarity and sufficient magnitude can an electrolytic cell decompose a normally stable, or inert chemical compound in the solution. The electrical energy provided can produce a chemical reaction which would not occur spontaneously otherwise.
"
Electromagnet,Physics,2,"
 An electromagnet is a type of magnet in which the magnetic field is produced by an electric current.  Electromagnets usually consist of wire wound into a coil.  A current through the wire creates a magnetic field which is concentrated in the hole, denoting the centre of the coil. The magnetic field disappears when the current is turned off.  The wire turns are often wound around a magnetic core made from a ferromagnetic or ferrimagnetic material such as iron; the magnetic core concentrates the magnetic flux and makes a more powerful magnet.
 The main advantage of an electromagnet over a permanent magnet is that the magnetic field can be quickly changed by controlling the amount of electric current in the winding. However, unlike a permanent magnet that needs no power, an electromagnet requires a continuous supply of current to maintain the magnetic field.
 Electromagnets are widely used as components of other electrical devices, such as motors, generators, electromechanical solenoids, relays, loudspeakers, hard disks, MRI machines, scientific instruments, and magnetic separation equipment.  Electromagnets are also employed in industry for picking up and moving heavy iron objects such as scrap iron and steel.[2]"
Electromagnetic_field,Physics,2,"An electromagnetic field (also EM field) is a classical (i.e. non-quantum) field produced by moving electric charges.[1] It is the field described by classical electrodynamics and is the classical counterpart to the quantized electromagnetic field tensor in quantum electrodynamics. The electromagnetic field propagates at the speed of light (in fact, this field can be identified as light) and interacts with charges and currents. Its quantum counterpart is one of the four fundamental forces of nature (the others are gravitation, weak interaction and strong interaction.)
 The field can be viewed as the combination of an electric field and a magnetic field. The electric field is produced by stationary charges, and the magnetic field by moving charges (currents); these two are often described as the sources of the field. The way in which charges and currents interact with the electromagnetic field is described by Maxwell's equations and the Lorentz force law.[2] The force created by the electric field is much stronger than the force created by the magnetic field.[3] From a classical perspective in the history of electromagnetism, the electromagnetic field can be regarded as a smooth, continuous field, propagated in a wavelike manner. By contrast, from the perspective of quantum field theory, this field is seen as quantized; meaning that the free quantum field (i.e. non-interacting field) can be expressed as the Fourier sum of creation and annihilation operators in energy-momentum space while the effects of the interacting quantum field may be analyzed in perturbation theory via the S-matrix with the aid of a whole host of mathematical technologies such as the Dyson series, Wick's theorem, correlation functions, time-evolution operators, Feynman diagrams etc. Note that the quantized field is still spatially continuous; its energy states however are discrete (the field's energy states must not be confused with its energy values, which are continuous; the quantum field's creation operators create multiple discrete states of energy called photons.)
"
Electromagnetic_induction,Physics,2,"
 Electromagnetic or magnetic induction is the production of an electromotive force across an electrical conductor in a changing magnetic field.
 Michael Faraday is generally credited with the discovery of induction in 1831, and James Clerk Maxwell mathematically described it as Faraday's law of induction. Lenz's law describes the direction of the induced field. Faraday's law was later generalized to become the Maxwell–Faraday equation, one of the four Maxwell equations in his theory of electromagnetism.
 Electromagnetic induction has found many applications, including electrical components such as inductors and transformers, and devices such as electric motors and generators.
"
Electromagnetic_radiation,Physics,2,"
 In physics, electromagnetic radiation (EM radiation or EMR) refers to the waves (or their quanta, photons) of the electromagnetic field, propagating (radiating) through space, carrying electromagnetic radiant energy.[1] It includes radio waves, microwaves, infrared, (visible) light, ultraviolet, X-rays, and gamma rays.[2] Classically, electromagnetic radiation consists of electromagnetic waves, which are synchronized oscillations of electric and magnetic fields. In a vacuum, electromagnetic waves travel at the speed of light, commonly denoted c. In homogeneous, isotropic media, the oscillations of the two fields are perpendicular to each other and perpendicular to the direction of energy and wave propagation, forming a transverse wave. The wavefront of electromagnetic waves emitted from a point source (such as a light bulb) is a sphere. The position of an electromagnetic wave within the electromagnetic spectrum can be characterized by either its frequency of oscillation or its wavelength.  Electromagnetic waves of different frequency are called by different names since they have different sources and effects on matter.  In order of increasing frequency and decreasing wavelength these are: radio waves, microwaves, infrared radiation, visible light, ultraviolet radiation, X-rays and gamma rays.[3] Electromagnetic waves are emitted by electrically charged particles undergoing acceleration,[4][5] and these waves can subsequently interact with other charged particles, exerting force on them.  EM waves carry energy, momentum and angular momentum away from their source particle and can impart those quantities to matter with which they interact.  Electromagnetic radiation is associated with those EM waves that are free to propagate themselves (""radiate"") without the continuing influence of the moving charges that produced them, because they have achieved sufficient distance from those charges. Thus, EMR is sometimes referred to as the far field. In this language, the near field refers to EM fields near the charges and current that directly produced them, specifically electromagnetic induction and electrostatic induction phenomena.
 In quantum mechanics, an alternate way of viewing EMR is that it consists of photons, uncharged elementary particles with zero rest mass which are the quanta of the electromagnetic force, responsible for all electromagnetic interactions.[6] Quantum electrodynamics is the theory of how EMR interacts with matter on an atomic level.[7]  Quantum effects provide additional sources of EMR, such as the transition of electrons to lower energy levels in an atom and black-body radiation.[8] The energy of an individual photon is quantized and is greater for photons of higher frequency. This relationship is given by Planck's equation E = hf, where E is the energy per photon, f is the frequency of the photon, and h is Planck's constant. A single gamma ray photon, for example, might carry ~100,000 times the energy of a single photon of visible light.
 The effects of EMR upon chemical compounds and biological organisms depend both upon the radiation's power and its frequency. EMR of visible or lower frequencies (i.e., visible light, infrared, microwaves, and radio waves) is called non-ionizing radiation, because its photons do not individually have enough energy to ionize atoms or molecules or break chemical bonds. The effects of these radiations on chemical systems and living tissue are caused primarily by heating effects from the combined energy transfer of many photons. In contrast, high frequency ultraviolet, X-rays and gamma rays are called ionizing radiation, since individual photons of such high frequency have enough energy to ionize molecules or break chemical bonds. These radiations have the ability to cause chemical reactions and damage living cells beyond that resulting from simple heating, and can be a health hazard.
"
Electromagnetic_spectrum,Physics,2,"
 The electromagnetic spectrum is the range of frequencies (the spectrum) of electromagnetic radiation and their respective wavelengths and photon energies.
 The electromagnetic spectrum covers electromagnetic waves with frequencies ranging from below one hertz to above 1025 hertz, corresponding to wavelengths from thousands of kilometers down to a fraction of the size of an atomic nucleus.  This frequency range is divided into separate bands, and the electromagnetic waves within each frequency band are called by different names; beginning at the low frequency (long wavelength) end of the spectrum these are: radio waves, microwaves, infrared, visible light, ultraviolet, X-rays, and gamma rays at the high-frequency (short wavelength) end.  The electromagnetic waves in each of these bands have different characteristics, such as how they are produced, how they interact with matter, and their practical applications.  The limit for long wavelengths is the size of the universe itself, while it is thought that the short wavelength limit is in the vicinity of the Planck length.[4]  Gamma rays, X-rays, and high ultraviolet are classified as ionizing radiation as their photons have enough energy to ionize atoms, causing chemical reactions.
 In most of the frequency bands above, a technique called spectroscopy can be used to physically separate waves of different frequencies, producing a spectrum showing the constituent frequencies.  Spectroscopy is used to study the interactions of electromagnetic waves with matter.[5]  Other technological uses are described under electromagnetic radiation.
"
Electromagnetic_wave_equation,Physics,2,"
 The electromagnetic wave equation is a second-order partial differential equation that describes the propagation of electromagnetic waves through a medium or in a vacuum.  It is a three-dimensional form of the wave equation. The homogeneous form of the equation, written in terms of either the electric field E or the magnetic field B, takes the form:
 where
 is the speed of light (i.e. phase velocity) in a medium with permeability μ, and permittivity ε, and ∇2 is the Laplace operator.  In a vacuum, vph = c0 = 299,792,458 meters per second, a fundamental physical constant.[1]  The electromagnetic wave equation derives from Maxwell's equations. In most older literature, B is called the magnetic flux density or magnetic induction.
"
Electromagnetism,Physics,2,"
 Electromagnetism is a branch of physics involving the study of the electromagnetic force, a type of physical interaction that occurs between electrically charged particles. The electromagnetic force is carried by electromagnetic fields composed of electric fields and magnetic fields, and it is responsible for electromagnetic radiation such as light.  It is one of the four fundamental interactions (commonly called forces) in nature, together with the strong interaction, the weak interaction, and gravitation.[1] At high energy the weak force and electromagnetic force are unified as a single electroweak force.
 Electromagnetic phenomena are defined in terms of the electromagnetic force, sometimes called the Lorentz force, which includes both electricity and magnetism as different manifestations of the same phenomenon. The electromagnetic force plays a major role in determining the internal properties of most objects encountered in daily life.  The electromagnetic attraction between atomic nuclei and their orbital electrons holds atoms together.  Electromagnetic forces are responsible for the chemical bonds between atoms which create molecules, and intermolecular forces.   The electromagnetic force governs all chemical processes, which arise from interactions between the electrons of neighboring atoms.
 There are numerous mathematical descriptions of the electromagnetic field.  In classical electrodynamics, electric fields are described as electric potential and electric current. In Faraday's law, magnetic fields are associated with electromagnetic induction and magnetism, and Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents.
 The theoretical implications of electromagnetism, particularly the establishment of the speed of light based on properties of the ""medium"" of propagation (permeability and permittivity), led to the development of special relativity by Albert Einstein in 1905.
"
Electromechanics,Physics,2,"In engineering, electromechanics[1][2][3][4] combines processes and procedures drawn from electrical engineering and mechanical engineering. Electromechanics focuses on the interaction of electrical and mechanical systems as a whole and how the two systems interact with each other. This process is especially prominent in systems such as those of DC or AC rotating electrical machines which can be designed and operated to generate power from a mechanical process (generator) or used to power a mechanical effect (motor). Electrical engineering in this context also encompasses electronics engineering.
 Electromechanical devices are ones which have both electrical and mechanical processes. Strictly speaking, a manually operated switch is an electromechanical component due to the mechanical movement causing an electrical output. Though this is true, the term is usually understood to refer to devices which involve an electrical signal to create mechanical movement, or vice versa mechanical movement to create an electric signal. Often involving electromagnetic principles such as in relays, which allow a voltage or current to control another, usually isolated circuit voltage or current by mechanically switching sets of contacts, and solenoids, by which a voltage can actuate a moving linkage as in solenoid valves.
 Before the development of modern electronics, electromechanical devices were widely used in complicated subsystems of parts, including electric typewriters, teleprinters, clocks, initial television systems, and the very early electromechanical digital computers.
"
Electromotive_force,Physics,2,"In electromagnetism and electronics, electromotive force (emf, denoted 





E




{  {\mathcal {E}}}
 and measured in volts),[1] is the electrical action produced by a non-electrical source.[2] Devices (known as transducers) provide an emf[3] by converting other forms of energy into electrical energy,[3] such as batteries (which convert chemical energy) or generators (which convert mechanical energy).[2]  Sometimes an analogy to water pressure is used to describe electromotive force.[4]  (The word ""force"" in this case is not used to mean forces of interaction between bodies).
 In electromagnetic induction, emf can be defined around a closed loop of conductor as the electromagnetic work that would be done on an  electric charge (an electron in this instance) if it travels once around the loop.[5] For a time-varying magnetic flux linking a loop, the electric potential's scalar field is not defined due to a circulating electric vector field, but an emf nevertheless does work that can be measured as a virtual electric potential around the loop.[6] In the case of a two-terminal device (such as an electrochemical cell) which is modeled as a Thévenin's equivalent circuit, the equivalent emf can be measured as the open-circuit potential difference, or voltage, between the two terminals. This potential difference can drive an electric current if an external circuit is attached to the terminals, in which case the device becomes the voltage source of that circuit.
"
Electron,Physics,2,"The electron  is a subatomic particle, symbol e− or β−, whose electric charge is negative one elementary charge.[9] Electrons belong to the first generation of the lepton particle family,[10] and are generally thought to be elementary particles because they have no known components or substructure.[1]  The electron has a mass that is approximately 1/1836 that of the proton.[11] Quantum mechanical properties of the electron include an intrinsic angular momentum (spin) of a half-integer value, expressed in units of the reduced Planck constant, ħ. Being fermions, no two electrons can occupy the same quantum state, in accordance with the Pauli exclusion principle.[10] Like all elementary particles, electrons exhibit properties of both particles and waves: they can collide with other particles and can be diffracted like light. The wave properties of electrons are easier to observe with experiments than those of other particles like neutrons and protons because electrons have a lower mass and hence a longer de Broglie wavelength for a given energy.
 Electrons play an essential role in numerous physical phenomena, such as electricity, magnetism, chemistry and thermal conductivity, and they also participate in gravitational, electromagnetic and weak interactions.[12] Since an electron has charge, it has a surrounding electric field, and if that electron is moving relative to an observer, said observer will observe it to generate a magnetic field. Electromagnetic fields produced from other sources will affect the motion of an electron according to the Lorentz force law. Electrons radiate or absorb energy in the form of photons when they are accelerated. Laboratory instruments are capable of trapping individual electrons as well as electron plasma by the use of electromagnetic fields. Special telescopes can detect electron plasma in outer space. Electrons are involved in many applications such as electronics, welding, cathode ray tubes, electron microscopes, radiation therapy, lasers, gaseous ionization detectors and particle accelerators.
 Interactions involving electrons with other subatomic particles are of interest in fields such as chemistry and nuclear physics. The Coulomb force interaction between the positive protons within atomic nuclei and the negative electrons without, allows the composition of the two known as atoms. Ionization or differences in the proportions of negative electrons versus positive nuclei changes the binding energy of an atomic system. The exchange or sharing of the electrons between two or more atoms is the main cause of chemical bonding.[13] In 1838, British natural philosopher Richard Laming first hypothesized the concept of an indivisible quantity of electric charge to explain the chemical properties of atoms.[3] Irish physicist George Johnstone Stoney named this charge 'electron' in 1891, and J. J. Thomson and his team of British physicists identified it as a particle in 1897.[5]  Electrons can also participate in nuclear reactions, such as nucleosynthesis in stars, where they are known as beta particles. Electrons can be created through beta decay of radioactive isotopes and in high-energy collisions, for instance when cosmic rays enter the atmosphere.  The antiparticle of the electron is called the positron; it is identical to the electron except that it carries electrical charge  of the opposite sign. When an electron collides with a positron, both particles can be annihilated, producing gamma ray photons.
"
Electron_capture,Physics,2,"
 Electron capture (K-electron capture, also K-capture, or L-electron capture, L-capture) is a process in which the proton-rich nucleus of an electrically neutral atom absorbs an inner atomic electron, usually from the K or L electron shells. This process thereby changes a nuclear proton to a neutron and simultaneously causes the emission of an electron neutrino. 
 Since this single emitted neutrino carries the entire decay energy, it has this single characteristic energy. Similarly, the momentum of the neutrino emission causes the daughter atom to recoil with a single characteristic momentum.
 The resulting daughter nuclide, if it is in an excited state, then transitions to its ground state. Usually, a gamma ray is emitted during this transition, but nuclear de-excitation may also take place by internal conversion.
 Following capture of an inner electron from the atom, an outer electron replaces the electron that was captured and one or more characteristic X-ray photons is emitted in this process. Electron capture sometimes also results in the Auger effect, where an electron is ejected from the atom's electron shell due to interactions between the atom's electrons in the process of seeking a lower energy electron state.
 Following electron capture, the atomic number is reduced by one, the neutron number is increased by one, and there is no change in mass number. Simple electron capture by itself results in a neutral atom, since the loss of the electron in the electron shell is balanced by a loss of positive nuclear charge. However, a positive atomic ion may result from further Auger electron emission.
 Electron capture is an example of weak interaction, one of the four fundamental forces.
 Electron capture is the primary decay mode for isotopes with a relative superabundance of protons in the nucleus, but with insufficient energy difference between the isotope and its prospective daughter (the isobar with one less positive charge) for the nuclide to decay by emitting a positron. Electron capture is always an alternative decay mode for radioactive isotopes that do have sufficient energy to decay by positron emission. Electron capture is sometimes included as a type of beta decay,[1] because the basic nuclear process, mediated by the weak force, is the same. In nuclear physics, beta decay is a type of radioactive decay in which a beta ray (fast energetic electron or positron) and a neutrino are emitted from an atomic nucleus.
Electron capture is sometimes called inverse beta decay, though this term usually refers to the interaction of an electron antineutrino with a proton.[2] If the energy difference between the parent atom and the daughter atom is less than 1.022 MeV, positron emission is forbidden as not enough decay energy is available to allow it, and thus electron capture is the sole decay mode. For example, rubidium-83 (37 protons, 46 neutrons) will decay to krypton-83 (36 protons, 47 neutrons) solely by electron capture (the energy difference, or decay energy, is about 0.9 MeV).
"
Atomic_orbital,Physics,2,"In atomic theory and quantum mechanics, an atomic orbital is a mathematical function describing the location and wave-like behavior of an electron in an atom.[1] This function can be used to calculate the probability of finding any electron of an atom in any specific region around the atom's nucleus. The term atomic orbital may also refer to the physical region or space where the electron can be calculated to be present, as predicted by the particular mathematical form of the orbital.[2] Each orbital in an atom is characterized by a unique set of values of the three quantum numbers n, ℓ, and m,[dubious  – discuss] which respectively correspond to the electron's energy, angular momentum, and an angular momentum vector component (the magnetic quantum number). Each such orbital can be occupied by a maximum of two electrons, each with its own spin quantum number s. The simple names s orbital, p orbital, d orbital, and f orbital refer to orbitals with angular momentum quantum number ℓ = 0, 1, 2, and 3 respectively. These names, together with the value of n, are used to describe the electron configurations of atoms. They are derived from the description by early spectroscopists of certain series of alkali metal spectroscopic lines as sharp, principal, diffuse, and fundamental. Orbitals for ℓ > 3 continue alphabetically, omitting j (g, h, i, k, ...)[3][4][5] because some languages do not distinguish between the letters ""i"" and ""j"".[6] Atomic orbitals are the basic building blocks of the atomic orbital model (alternatively known as the electron cloud or wave mechanics model), a modern framework for visualizing the submicroscopic behavior of electrons in matter. In this model the electron cloud of a multi-electron atom may be seen as being built up (in approximation) in an electron configuration that is a product of simpler hydrogen-like atomic orbitals. The repeating periodicity of the blocks of 2, 6, 10, and 14 elements within sections of the periodic table arises naturally from the total number of electrons that occupy a complete set of s, p, d, and f atomic orbitals, respectively, although for higher values of the quantum number n, particularly when the atom in question bears a positive charge, the energies of certain sub-shells become very similar and so the order in which they are said to be populated by electrons (e.g. Cr = [Ar]4s13d5 and Cr2+ = [Ar]3d4) can only be rationalized somewhat arbitrarily.
"
Electron_pair,Physics,2,"In chemistry, an electron pair or a Lewis pair consists of two electrons that occupy the same molecular orbital but have opposite spins. The electron pair concept was introduced in a 1916 paper of Gilbert N. Lewis.[1] Because electrons are fermions, the Pauli exclusion principle forbids these particles from having exactly the same quantum numbers. Therefore, the only way to occupy the same orbital, i.e. have the same orbital quantum numbers, is to differ in the spin quantum number. This limits the number of electrons in the same orbital to exactly two.
 The pairing of spins is often energetically favorable and electron pairs therefore play a very large role in chemistry. They can form a chemical bond between two atoms, or they can occur as a lone pair of valence electrons.  They also fill the core levels of an atom.
 Because the spins are paired, the magnetic moment of the electrons cancels and the contribution of the pair to the magnetic properties will in general be a diamagnetic one.
 Although a strong tendency to pair off electrons can be observed in chemistry, it is also possible that electrons occur as unpaired electrons.
 In the case of metallic bonding the magnetic moments also compensate to a large extent, but the bonding is more communal so that individual pairs of electrons cannot be distinguished and it is better to consider the electrons as a collective 'ocean'.
 A very special case of electron pair formation occurs in superconductivity: the formation of Cooper pairs.
"
Electron_paramagnetic_resonance,Physics,2,"Electron paramagnetic resonance (EPR) or electron spin resonance (ESR) spectroscopy is a method for studying materials with unpaired electrons. The basic concepts of EPR are analogous to those of nuclear magnetic resonance (NMR), but it is electron spins that are excited instead of the spins of atomic nuclei. EPR spectroscopy is particularly useful for studying metal complexes or organic radicals.
EPR was first observed in Kazan State University by Soviet physicist Yevgeny Zavoisky in 1944,[1][2] and was developed independently at the same time by Brebis Bleaney at the University of Oxford.
"
Electronvolt,Physics,2,"In physics, an electronvolt (symbol eV, also written electron-volt and electron volt) is the amount of kinetic energy gained by a single electron accelerating from rest through an electric potential difference of one volt in vacuum. When used as a unit of energy, the numerical value of 1 eV in joules (symbol J) is equivalent to the numerical value of the charge of an electron in coulombs (symbol C). Under the 2019 redefinition of the SI base units, this sets 1 eV equal to the exact value 1.602176634×10−19 J.[1] Historically, the electronvolt was devised as a standard unit of measure through its usefulness in electrostatic particle accelerator sciences, because a particle with electric charge q has an energy E = qV after passing through the potential V; if q is quoted in integer units of the elementary charge and the potential in volts, one gets an energy in eV.
 It is a common unit of energy within physics, widely used in solid state, atomic, nuclear, and particle physics. It is commonly used with the metric prefixes milli-, kilo-, mega-, giga-, tera-, peta- or exa- (meV, keV, MeV, GeV, TeV, PeV and EeV respectively). In some older documents, and in the name Bevatron, the symbol BeV is used, which stands for billion (109) electronvolts; it is equivalent to the GeV.
"
Electronegativity,Physics,2,"Electronegativity, symbol χ, measures the tendency of an atom to attract a shared pair of electrons (or electron density).[1] An atom's electronegativity is affected by both its atomic number and the distance at which its valence electrons reside from the charged nucleus. The higher the associated electronegativity, the more an atom or a substituent group attracts electrons.
 On the most basic level, electronegativity is determined by factors like the nuclear charge (the more protons an atom has, the more ""pull"" it will have on electrons) and the number and location of other electrons in the atomic shells (the more electrons an atom has, the farther from the nucleus the valence electrons will be, and as a result the less positive charge they will experience—both because of their increased distance from the nucleus, and because the other electrons in the lower energy core orbitals will act to shield the valence electrons from the positively charged nucleus).
 The opposite of electronegativity is electropositivity: a measure of an element's ability to donate electrons.
 The term ""electronegativity"" was introduced by Jöns Jacob Berzelius in 1811,[2]
though the concept was known before that and was studied by many chemists including Avogadro.[2]
In spite of its long history, an accurate scale of electronegativity was not developed until 1932, when Linus Pauling proposed an electronegativity scale which depends on bond energies, as a development of valence bond theory.[3] It has been shown to correlate with a number of other chemical properties. Electronegativity cannot be directly measured and must be calculated from other atomic or molecular properties. Several methods of calculation have been proposed, and although there may be small differences in the numerical values of the electronegativity, all methods show the same periodic trends between elements.
 The most commonly used method of calculation is that originally proposed by Linus Pauling. This gives a dimensionless quantity, commonly referred to as the Pauling scale (χr), on a relative scale running from 0.79 to 3.98 (hydrogen = 2.20). When other methods of calculation are used, it is conventional (although not obligatory) to quote the results on a scale that covers the same range of numerical values: this is known as an electronegativity in Pauling units.
 As it is usually calculated, electronegativity is not a property of an atom alone, but rather a property of an atom in a molecule.[4] Properties of a free atom include ionization energy and electron affinity. It is to be expected that the electronegativity of an element will vary with its chemical environment,[5] but it is usually considered to be a transferable property, that is to say that similar values will be valid in a variety of situations.
 Caesium is the least electronegative element (0.79); fluorine is the most (3.98). Francium and caesium were originally both assigned 0.7; caesium's value was later refined to 0.79, but no experimental data allows a similar refinement for francium. However, francium's ionization energy is known to be slightly higher than caesium's, in accordance with the relativistic stabilization of the 7s orbital, and this in turn implies that francium is in fact more electronegative than caesium.
"
Electronics,Physics,2," Electronics comprises the physics, engineering, technology and applications that deal with the emission, flow and control of electrons in vacuum and matter.[1] It uses active devices to control electron flow by amplification and rectification, which distinguishes it from  classical electrical engineering which uses passive effects such as resistance, capacitance and inductance to control current flow.
 Electronics has had a major effect on the development of modern society.
The identification of the electron in 1897, along with the subsequent invention of the vacuum tube which could amplify and rectify small electrical signals,  inaugurated the field of electronics and the electron age.[2] This distinction started around 1906 with the invention by Lee De Forest of the triode, which made electrical amplification of weak radio signals and audio signals possible with a non-mechanical device.  Until 1950, this field was called ""radio technology"" because its principal application was the design and theory of radio transmitters, receivers, and vacuum tubes.
 The term ""solid-state electronics"" emerged after the first working transistor was invented by William Shockley, Walter Houser Brattain and John Bardeen at Bell Labs in 1947. The MOSFET (MOS transistor) was later invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959. The MOSFET was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses, revolutionizing the electronics industry, and playing a central role in the microelectronics revolution and Digital Revolution. The MOSFET has since become the basic element in most modern electronic equipment, and is the most widely used electronic device in the world.
 Electronics is widely used in information processing, telecommunication, and signal processing.  The ability of electronic devices to act as switches makes digital information-processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed electronic components into a regular working system, called an electronic system; examples are computers or control systems. An electronic system may be a component of another engineered system or a standalone device. As of 2019[update] most electronic devices[3] use semiconductor components to perform electron control. Commonly, electronic devices contain circuitry consisting of active semiconductors supplemented with passive elements; such a circuit is described as an electronic circuit. Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes, integrated circuits, optoelectronics, and sensors, associated passive electrical components, and interconnection technologies. The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible.
 The study of semiconductor devices and related technology is considered a branch of solid-state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering.  This article focuses on engineering aspects of electronics.
"
Electrostatics,Physics,2,"Electrostatics is a branch of physics that studies electric charges at rest.
 Since classical physics, it has been known that some materials, such as amber, attract lightweight particles after rubbing. The Greek word for amber, ήλεκτρον, or electron, was thus the source of the word 'electricity'. Electrostatic phenomena arise from the forces that electric charges exert on each other. Such forces are described by Coulomb's law.
Even though electrostatically induced forces seem to be rather weak, some electrostatic forces such as the one between an electron and a proton, that together make up a hydrogen atom, is about 36 orders of magnitude stronger than the gravitational force acting between them.
 There are many examples of electrostatic phenomena, from those as simple as the attraction of the plastic wrap to one's hand after it is removed from a package to the apparently spontaneous explosion of grain silos, the damage of electronic components during manufacturing, and photocopier & laser printer operation. Electrostatics involves the buildup of charge on the surface of objects due to contact with other surfaces. Although charge exchange happens whenever any two surfaces contact and separate, the effects of charge exchange are usually only noticed when at least one of the surfaces has a high resistance to electrical flow. This is because the charges that transfer are trapped there for a time long enough for their effects to be observed. These charges then remain on the object until they either bleed off to ground or are quickly neutralized by a discharge: e.g., the familiar phenomenon of a static ""shock"" is caused by the neutralization of charge built up in the body from contact with insulated surfaces.
"
Electrostriction,Physics,2,"Electrostriction (cf. magnetostriction) is a property of all electrical non-conductors, or dielectrics, that causes them to change their shape under the application of an electric field.
"
Elementary_charge,Physics,2,"The elementary charge, usually denoted by e or sometimes qe, is the electric charge carried by a single proton or, equivalently, the magnitude of the negative electric charge carried by a single electron, which has charge −1 e.[2] This elementary charge is a fundamental physical constant. To avoid confusion over its sign, e is sometimes called the elementary positive charge.
 From the 2019 redefinition of SI base units, which took effect on 20 May 2019, its value is exactly 1.602176634×10−19 C[1], by definition of the coulomb. In the centimetre–gram–second system of units (CGS), it is 4.80320425(10)×10−10 statcoulombs.[3] Making the value of the elementary charge  exact  implies that the value of ε0 (electric constant), which was an exact value before, is now subject to experimental determination: ε0 had an exactly defined value until the 2019 SI redefinition, after which it has become a subject of experimental refinement over time.[4] The SI committees (CGPM, CIPM, etc.) had long  considered redefining the SI base units entirely in terms of physical constants so as to remove their dependence on physical artifacts (such as the International Prototype of the Kilogram): for this to work, it was necessary to define fixed values for the physical constants.[5] Robert A. Millikan's oil drop experiment first measured the magnitude of the elementary charge in 1909.[6]"
Elementary_particle,Physics,2,"In particle physics, an elementary particle or fundamental particle is a subatomic particle with no substructure, i.e. it is not composed of other particles.[1](pp1–3) Particles currently thought to be elementary include the fundamental fermions (quarks, leptons, antiquarks, and antileptons), which generally are ""matter particles"" and ""antimatter particles"", as well as the fundamental bosons (gauge bosons and the Higgs boson), which generally are ""force particles"" that mediate interactions among fermions.[1](pp1–3) A particle containing two or more elementary particles is called a composite particle.
 Ordinary matter is composed of atoms, once presumed to be elementary particles—atom meaning ""unable to cut"" in Greek—although the atom's existence remained controversial until about 1905, as some leading physicists regarded molecules as mathematical illusions, and matter as ultimately composed of energy.[1](pp1–3)[2] Subatomic constituents of the atom were first identified in the early 1930s; the electron and the proton, along with the photon, the particle of electromagnetic radiation.[1](pp1–3) At that time, the recent advent of quantum mechanics was radically altering the conception of particles, as a single particle could seemingly span a field as would a wave, a paradox still eluding satisfactory explanation.[3][4] Via quantum theory, protons and neutrons were found to contain quarks – up quarks and down quarks – now considered elementary particles.[1](pp1–3) And within a molecule, the electron's three degrees of freedom (charge, spin, orbital) can separate via the wavefunction into three quasiparticles (holon, spinon, and orbiton).[5] Yet a free electron – one which is not orbiting an atomic nucleus and hence lacks orbital motion – appears unsplittable and remains regarded as an elementary particle.[5] Around 1980, an elementary particle's status as indeed elementary – an ultimate constituent of substance – was mostly discarded for a more practical outlook,[1](pp1–3) embodied in particle physics' Standard Model, what's known as science's most experimentally successful theory.[4][6] Many elaborations upon and theories beyond the Standard Model, including the popular supersymmetry, double the number of elementary particles by hypothesizing that each known particle associates with a ""shadow"" partner far more massive,[7][8] although all such superpartners remain undiscovered.[6][9] Meanwhile, an elementary boson mediating gravitation – the graviton – remains hypothetical.[1](pp1–3) Also, as hypotheses indicate, spacetime is probably quantized, so there most likely exist ""atoms"" of space and time itself.[10]"
Emission_spectrum,Physics,2,"
 The emission spectrum of a chemical element or chemical compound is the spectrum of frequencies of electromagnetic radiation emitted due to an atom or molecule making a transition from a high energy state to a lower energy state. The photon energy of the emitted photon is equal to the energy difference between the two states. There are many possible electron transitions for each atom, and each transition has a specific energy difference. This collection of different transitions, leading to different radiated wavelengths, make up an emission spectrum. Each element's emission spectrum is unique. Therefore, spectroscopy can be used to identify elements in matter of unknown composition. Similarly, the emission spectra of molecules can be used in chemical analysis of substances.
"
Emissivity,Physics,2,"The emissivity of the surface of a material is its effectiveness in emitting energy as thermal radiation. Thermal radiation is electromagnetic radiation that may include both visible radiation (light) and infrared radiation, which is not visible to human eyes. The thermal radiation from very hot objects (see photograph) is easily visible to the eye. Quantitatively, emissivity is the ratio of the thermal radiation from a surface to the radiation from an ideal black surface at the same temperature as given by the Stefan–Boltzmann law. The ratio varies from 0 to 1. The surface of a perfect black body (with an emissivity of 1) emits thermal radiation at the rate of approximately 448 watts per square metre at room temperature (25 °C, 298.15 K); all real objects have emissivities less than 1.0, and emit radiation at correspondingly lower rates.[1] Emissivities are important in several contexts:
"
Energy,Physics,2,"
 
 In physics, energy is the quantitative property that must be transferred to an object in order to perform work on, or to heat, the object.[note 1]  Energy is a conserved quantity; the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The SI unit of energy is the joule, which is the energy transferred to an object by the work of moving it a distance of 1 metre against a force of 1 newton.
 Common forms of energy include the kinetic energy of a moving object, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), the elastic energy stored by stretching solid objects, the chemical energy released when a fuel burns, the radiant energy carried by light, and the thermal energy due to an object's temperature.
 Mass and energy are closely related. Due to mass–energy equivalence, any object that has mass when stationary (called rest mass) also has an equivalent amount of energy whose form is called rest energy, and any additional energy (of any form) acquired by the object above that rest energy will increase the object's total mass just as it increases its total energy. For example, after heating an object, its increase in energy could be measured as a small increase in mass, with a sensitive enough scale.
 Living organisms require energy to stay alive, such as the energy humans get from food.  Human civilization requires energy to function, which it gets from  energy resources such as fossil fuels, nuclear fuel, or renewable energy. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.
"
Energy_level,Physics,2,"A quantum mechanical system or particle that is bound—that is, confined spatially—can only take on certain discrete values of energy, called energy levels. This contrasts with classical particles, which can have any amount of energy. The term is commonly used for the energy levels of the electrons in atoms, ions, or molecules, which are bound by the electric field of the nucleus, but can also refer to energy levels of nuclei or vibrational or rotational energy levels in molecules. The energy spectrum of a system with such discrete energy levels is said to be quantized.(And the energy levels don’t have to be equal )
 In chemistry and atomic physics, an electron shell, or principal energy level, may be thought of as the orbit of one or more electrons around an atom's nucleus. The closest shell to the nucleus is called the ""1 shell"" (also called ""K shell""), followed by the ""2 shell"" (or ""L shell""), then the ""3 shell"" (or ""M shell""), and so on farther and farther from the nucleus. The shells correspond with the principal quantum numbers (n = 1, 2, 3, 4 ...) or are labeled alphabetically with letters used in the X-ray notation (K, L, M, N…).
 Each shell can contain only a fixed number of electrons: The first shell can hold up to two electrons, the second shell can hold up to eight (2 + 6) electrons, the third shell can hold up to 18 (2 + 6 + 10) and so on. The general formula is that the nth shell can in principle hold up to 2(n2) electrons.[1] Since electrons are electrically attracted to the nucleus, an atom's electrons will generally occupy outer shells only if the more inner shells have already been completely filled by other electrons. However, this is not a strict requirement: atoms may have two or even three incomplete outer shells. (See Madelung rule for more details.) For an explanation of why electrons exist in these shells see electron configuration.[2] If the potential energy is set to zero at infinite distance from the atomic nucleus or molecule, the usual convention, then bound electron states have negative potential energy.
 If an atom, ion, or molecule is at the lowest possible energy level, it and its electrons are said to be in the ground state. If it is at a higher energy level, it is said to be excited, or any electrons that have higher energy than the ground state are excited. If more than one quantum mechanical state is at the same energy, the energy levels are ""degenerate"". They are then called degenerate energy levels.
"
Endothermic,Physics,2,"An endothermic process is any process with an increase in the enthalpy H (or internal energy U) of the system.[1] In such a process, a closed system usually absorbs thermal energy from its surroundings, which is heat transfer into the system. It may be a chemical process, such as dissolving ammonium nitrate in water, or a physical process, such as the melting of ice cubes. 
 The term was coined by Marcellin Berthelot from the Greek roots endo-, derived from the word ""endon"" (ἔνδον) meaning ""within"", and the root ""therm"" (θερμ-), meaning ""hot"" or ""warm"" in the sense that a reaction depends on absorbing heat if it is to proceed. The opposite of an endothermic process is an exothermic process, one that releases or ""gives out"" energy, usually in the form of heat and sometimes as electrical energy.[2] Thus in each term (endothermic and exothermic) the prefix refers to where heat (or electrical energy) goes as the process occurs.
"
Engineering_physics,Physics,2,"Engineering physics, or engineering science, refers to the study of the combined disciplines of physics, mathematics, biology, social science, and engineering, particularly computer, nuclear, electrical, electronic, aerospace, materials or mechanical engineering. By focusing on the scientific method as a rigorous basis, it seeks ways to apply, design, and develop new solutions in engineering.[1][2][3][4]"
Enthalpy,Physics,2,"Enthalpy /ˈɛnθəlpi/ (listen) is a property of a thermodynamic system, defined as the sum of the system's internal energy and the product of its pressure and volume.[1][2] 
It is a convenient state function preferred in many measurements in chemical, biological, and physical systems at a constant pressure. The pressure-volume term expresses the work required to establish the system's physical dimensions, i.e. to make room for it by displacing its surroundings.[3][4] As a state function, enthalpy depends only on the final configuration of internal energy, pressure, and volume, and not on the path taken to achieve it.
 The unit of measurement for enthalpy in the International System of Units (SI) is the joule. Other historical conventional units still in use include the British thermal unit (BTU) and the calorie.
 The total enthalpy of a system cannot be measured directly, because the internal energy contains components that are unknown, not easily accessible, or are not of interest in thermodynamics. In practice, a change in enthalpy (ΔH) is the preferred expression for measurements at constant pressure, because it simplifies the description of energy transfer. When matter transfer into or out of the system is also prevented, the enthalpy change equals the energy exchanged with the environment by heat. For calibration of enthalpy changes a specific and convenient reference point is established. Enthalpies for chemical substances at constant pressure usually refer to standard state: most commonly 1 bar (100 kPa) pressure. Standard state does not strictly specify a temperature, but expressions for enthalpy generally reference the standard heat of formation at 25 °C (298 K). For endothermic processes, the change ΔH is a positive value, and is negative in exothermic (heat-releasing) processes.
 The enthalpy of an ideal gas is independent of its pressure, and depends only on its temperature, which correlates to its internal energy. Real gases at common temperatures and pressures often closely approximate this behavior, which simplifies practical thermodynamic design and analysis.
"
Entropy,Physics,2,"
 Entropy is an extensive property of a thermodynamic system. It quantifies the number Ω of microscopic configurations (known as microstates) that are consistent with the macroscopic quantities that characterize the system (such as its volume, pressure and temperature).[1] Under the assumption that each microstate of a large ensemble is equally probable, the entropy 



S


{  S}
 is the natural logarithm of the number of microstates, multiplied by the Boltzmann constant kB.
 The second law of thermodynamics states that the entropy of an isolated system never decreases over time. Isolated systems spontaneously evolve towards thermodynamic equilibrium, the state with maximum entropy. Non-isolated systems, like organisms, may lose entropy, provided their environment's entropy increases by at least that amount so that the total entropy either increases or remains constant. Therefore, the entropy in a specific system can decrease as long as the total entropy of the Universe does not. Entropy is a function of the state of the system, so the change in entropy of a system is determined by its initial and final states. In the idealization that a process is reversible, the entropy does not change, while irreversible processes always increase the total entropy.
 Because it is determined by the number of random microstates, entropy is related to the amount of additional information needed to specify the exact physical state of a system, given its macroscopic specification. For this reason, it is often said that entropy is an expression of the disorder, or randomness of a system, or of the lack of information about it.[2] The concept of entropy plays a central role in information theory.
"
Equilibrant_force,Physics,2,"An equilibrant force is a force which brings a body into mechanical equilibrium.[1] According to Newton's second law, a body has zero acceleration when the vector sum of all the forces acting upon it is zero. Therefore, an equilibrant force is equal in magnitude and opposite in direction to the resultant of all the other forces acting on a body. The term has been attested since the late 19th century.[2]"
Equipartition,Physics,2,"In classical statistical mechanics, the equipartition theorem relates the temperature of a system to its average energies. The equipartition theorem is also known as the law of equipartition, equipartition of energy, or simply equipartition. The original idea of equipartition was that, in thermal equilibrium, energy is shared equally among all of its various forms; for example, the average kinetic energy per degree of freedom in translational motion of a molecule should equal that in rotational motion.
 The equipartition theorem makes quantitative predictions. Like the virial theorem, it gives the total average kinetic and potential energies for a system at a given temperature, from which the system's heat capacity can be computed. However, equipartition also gives the average values of individual components of the energy, such as the kinetic energy of a particular particle or the potential energy of a single spring. For example, it predicts that every atom in a monatomic ideal gas has an average kinetic energy of (3/2)kBT in thermal equilibrium, where kB is the Boltzmann constant and T is the (thermodynamic) temperature. More generally, equipartition can be applied to any classical system in thermal equilibrium, no matter how complicated. It can be used to derive the ideal gas law, and the Dulong–Petit law for the specific heat capacities of solids. The equipartition theorem can also be used to predict the properties of stars, even white dwarfs and neutron stars, since it holds even when relativistic effects are considered.
 Although the equipartition theorem makes accurate predictions in certain conditions, it is inaccurate when quantum effects are significant, such as at low temperatures.  When the thermal energy kBT is smaller than the quantum energy spacing in a particular degree of freedom, the average energy and heat capacity of this degree of freedom are less than the values predicted by equipartition. Such a degree of freedom is said to be ""frozen out"" when the thermal energy is much smaller than this spacing. For example, the heat capacity of a solid decreases at low temperatures as various types of motion become frozen out, rather than remaining constant as predicted by equipartition. Such decreases in heat capacity were among the first signs to physicists of the 19th century that classical physics was incorrect and that a new, more subtle, scientific model was required. Along with other evidence, equipartition's failure to model black-body radiation—also known as the ultraviolet catastrophe—led Max Planck to suggest that energy in the oscillators in an object, which emit light, were quantized, a revolutionary hypothesis that spurred the development of quantum mechanics and quantum field theory.
"
Escape_velocity,Physics,2,"
 In physics (specifically, celestial mechanics), escape velocity is the minimum speed needed for a free, non-propelled object to escape from the gravitational influence of a massive body, that is, to achieve an infinite distance from it. Escape velocity is a function of the mass of the body and distance to the center of mass of the body. 
 A rocket, continuously accelerated by its exhaust, need not reach ballistic escape velocity at any distance since it is supplied with additional kinetic energy by the expulsion of its reaction mass. It can achieve escape at any speed, given a suitable mode of propulsion and sufficient propellant to provide the accelerating force on the object to escape. 
 The escape velocity from Earth's surface is about 11,186 m/s (6.951 mi/s; 40,270 km/h; 36,700 ft/s; 25,020 mph; 21,744 kn).[1] More generally, escape velocity is the speed at which the sum of an object's kinetic energy and its gravitational potential energy is equal to zero;[nb 1] an object which has achieved escape velocity is neither on the surface, nor in a closed orbit (of any radius). With escape velocity in a direction pointing away from the ground of a massive body, the object will move away from the body, slowing forever and approaching, but never reaching, zero speed.  Once escape velocity is achieved, no further impulse need be applied for it to continue in its escape. In other words, if given escape velocity, the object will move away from the other body, continually slowing, and will asymptotically approach zero speed as the object's distance approaches infinity, never to come back.[2] Speeds higher than escape velocity have a positive speed at infinity. Note that the minimum escape velocity assumes that there is no friction (e.g., atmospheric drag), which would increase the required instantaneous velocity to escape the gravitational influence, and that there will be no future acceleration or deceleration (for example from thrust or gravity from other objects), which would change the required instantaneous velocity.
 For a spherically symmetric, massive body such as a star, or planet, the escape velocity for that body, at a given distance, is calculated by the formula[3] where G is the universal gravitational constant (G ≈ 6.67×10−11 m3·kg−1·s−2), M the mass of the body to be escaped from, and r the distance from the center of mass of the body to the object.[nb 2] The relationship is independent of the mass of the object escaping the massive body. Conversely, a body that falls under the force of gravitational attraction of mass M, from infinity, starting with zero velocity, will strike the massive object with a velocity equal to its escape velocity given by the same formula.
 When given an initial speed 



V


{  V}
 greater than the escape speed 




v

e


,


{  v_{e},}
 the object will asymptotically approach the hyperbolic excess speed 




v

∞


,


{  v_{\infty },}
 satisfying the equation:[4] In these equations atmospheric friction (air drag) is not taken into account.
"
Excited_state,Physics,2,"
 In quantum mechanics, an excited state of a system (such as an atom, molecule or nucleus) is any quantum state of the system that has a higher energy than the ground state (that is, more energy than the absolute minimum). Excitation is an elevation in energy level above an arbitrary baseline energy state. In physics there is a specific technical definition for energy level which is often associated with an atom being raised to an excited state.[citation needed][definition needed] The temperature of a group of particles is indicative of the level of excitation (with the notable exception of systems that exhibit negative temperature).
 The lifetime of a system in an excited state is usually short: spontaneous or induced emission of a quantum of energy (such as a photon or a phonon) usually occurs shortly after the system is promoted to the excited state, returning the system to a state with lower energy (a less excited state or the ground state). This return to a lower energy level is often loosely described as decay and is the inverse of excitation.
 Long-lived excited states are often called metastable. Long-lived nuclear isomers and singlet oxygen are two examples of this.
"
Exothermic,Physics,2,"In thermodynamics, the term exothermic process (exo- : ""outside"") describes a process or reaction that releases energy from the system to its surroundings, usually in the form of heat, but also in a form of light (e.g. a spark, flame, or flash), electricity (e.g. a battery), or sound (e.g. explosion heard when burning hydrogen). Its etymology stems from the Greek prefix έξω (exō, which means ""outwards"") and the Greek word θερμικός (thermikόs, which means ""thermal"").[1] The term exothermic was first coined by Marcellin Berthelot. 
 The opposite of an exothermic process is an endothermic process, one that absorbs energy usually in the form of heat.  The concept is frequently applied in the physical sciences to chemical reactions where chemical bond energy is converted to thermal energy (heat).
"
Experimental_physics,Physics,2,"Experimental physics is the category of disciplines and sub-disciplines in the field of physics that are concerned with the observation of physical phenomena and experiments. Methods vary from discipline to discipline, from simple experiments and observations, such as the Cavendish experiment, to more complicated ones, such as the Large Hadron Collider.
"
Farad,Physics,2,"[1]The farad (symbol: F) is the SI derived unit of electrical capacitance, the ability of a body to store an electrical charge. It is named after the English physicist Michael Faraday.
"
Falling_bodies,Physics,2,"A set of equations describe the resultant trajectories when objects move owing to a constant gravitational force under normal Earth-bound conditions. For example, Newton's law of universal gravitation simplifies to F = mg, where m is the mass of the body. This assumption is reasonable for objects falling to earth over the relatively short vertical distances of our everyday experience, but is untrue over larger distances, such as spacecraft trajectories.
"
Faraday_(unit),Physics,2,"The Faraday constant, denoted by the symbol F and sometimes stylized as ℱ, is named after Michael Faraday. In chemistry and physics, this constant represents the magnitude of electric charge per mole of electrons.[1] It has the currently accepted value
 Since 1 mol electrons = 6.02214076×1023 electrons (Avogadro's number),[3] the Faraday constant is equal to the elementary charge e, the magnitude of the charge of an electron multiplied by 1 mole:[4] One common use of the Faraday constant is in electrolysis calculations. One can divide the amount of charge in coulombs by the Faraday constant in order to find the chemical amount (in moles) of the element that has been oxidized.
 The value of F was first determined by weighing the amount of silver deposited in an electrochemical reaction in which a measured current was passed for a measured time, and using Faraday's law of electrolysis.[5]"
Faraday_constant,Physics,2,"The Faraday constant, denoted by the symbol F and sometimes stylized as ℱ, is named after Michael Faraday. In chemistry and physics, this constant represents the magnitude of electric charge per mole of electrons.[1] It has the currently accepted value
 Since 1 mol electrons = 6.02214076×1023 electrons (Avogadro's number),[3] the Faraday constant is equal to the elementary charge e, the magnitude of the charge of an electron multiplied by 1 mole:[4] One common use of the Faraday constant is in electrolysis calculations. One can divide the amount of charge in coulombs by the Faraday constant in order to find the chemical amount (in moles) of the element that has been oxidized.
 The value of F was first determined by weighing the amount of silver deposited in an electrochemical reaction in which a measured current was passed for a measured time, and using Faraday's law of electrolysis.[5]"
Fermat%27s_principle,Physics,2," Fermat's principle, also known as the principle of least time, is the link between ray optics and wave optics. In its original ""strong"" form,[1] Fermat's principle states that the path taken by a ray between two given points is the path that can be traversed in the least time. In order to be true in all cases, this statement must be weakened by replacing the ""least"" time with a time that is ""stationary"" with respect to variations of the path — so that a deviation in the path causes, at most, a second-order change in the traversal time. To put it loosely, a ray path is surrounded by close paths that can be traversed in very close times. It can be shown that this technical definition corresponds to more intuitive notions of a ray, such as a line of sight or the path of a narrow beam.
 First proposed by the French mathematician Pierre de Fermat in 1662, as a means of explaining the ordinary law of refraction of light (Fig. 1), Fermat's principle was initially controversial because it seemed to ascribe knowledge and intent to nature. Not until the 19th century was it understood that nature's ability to test alternative paths is merely a fundamental property of waves.[2] If points A and B are given, a wavefront expanding from A sweeps all possible ray paths radiating from A, whether they pass through B or not. If the wavefront reaches point B, it sweeps not only the ray path(s) from A to B, but also an infinitude of nearby paths with the same endpoints. Fermat's principle describes any ray that happens to reach point B; there is no implication that the ray ""knew"" the quickest path or ""intended"" to take that path.
  For the purpose of comparing traversal times, the time from one point to the next nominated point is taken as if the first point were a point-source.[3] Without this condition, the traversal time would be ambiguous; for example, if the propagation time from P to P′ were reckoned from an arbitrary wavefront W containing P  (Fig. 2), that time could be made arbitrarily small by suitably angling the wavefront.
 Treating a point on the path as a source is the minimum requirement of Huygens' principle, and is part of the explanation of Fermat's principle. But it can also be shown that the geometric construction by which Huygens tried to apply his own principle (as distinct from the principle itself) is simply an invocation of Fermat's principle.[4] Hence all the conclusions that Huygens drew from that construction — including, without limitation, the laws of rectilinear propagation of light, ordinary reflection, ordinary refraction, and the extraordinary refraction of ""Iceland crystal"" (calcite) — are also consequences of Fermat's principle.
"
Fermi_surface,Physics,2,"In condensed matter physics, the Fermi surface is the surface in reciprocal space which separates occupied from unoccupied electron states at zero temperature.[1] The shape of the Fermi surface is derived from the periodicity and symmetry of the crystalline lattice and from the occupation of electronic energy bands. The existence of a Fermi surface is a direct consequence of the Pauli exclusion principle, which allows a maximum of one electron per quantum state.[2][3][4][5]"
Fermion,Physics,2,"In particle physics, a fermion is a particle that follows Fermi–Dirac statistics and generally has half odd integer spin 1/2, 3/2 etc. These particles obey the Pauli exclusion principle. Fermions include all quarks and leptons, as well as all composite particles made of an odd number of these, such as all baryons and many atoms and nuclei. Fermions differ from bosons, which obey Bose–Einstein statistics.
 Some fermions are elementary particles, such as the electrons, and some are composite particles, such as the protons. According to the spin-statistics theorem in relativistic quantum field theory, particles with integer spin are bosons, while particles with half-integer spin are fermions.
 In addition to the spin characteristic, fermions have another specific property: they possess conserved baryon or lepton quantum numbers. Therefore, what is usually referred to as the spin statistics relation is in fact a spin statistics-quantum number relation.[1] As a consequence of the Pauli exclusion principle, only one fermion can occupy a particular quantum state at a given time. If multiple fermions have the same spatial probability distribution, then at least one property of each fermion, such as its spin, must be different. Fermions are usually associated with matter, whereas bosons are generally force carrier particles, although in the current state of particle physics the distinction between the two concepts is unclear.  Weakly interacting fermions can also display bosonic behavior under extreme conditions. At low temperature fermions show superfluidity for uncharged particles and superconductivity for charged particles.
 Composite fermions, such as protons and neutrons, are the key building blocks of everyday matter.
 The name fermion was coined by English theoretical physicist Paul Dirac from the surname of Italian physicist Enrico Fermi.[2]"
Ferrimagnetism,Physics,2,"
 In physics, a ferrimagnetic material is one that has populations of atoms with opposing magnetic moments, as in antiferromagnetism; however, in ferrimagnetic materials, the opposing moments are unequal and a spontaneous magnetization remains.[1] This happens when the populations consist of different materials or ions (such as Fe2+ and Fe3+).
 Ferrimagnetism is exhibited by ferrites and magnetic garnets. The oldest known magnetic substance, magnetite (iron(II,III) oxide; Fe3O4), is a ferrimagnet; it was originally classified as a ferromagnet before Néel's discovery of ferrimagnetism and antiferromagnetism in 1948.[2] Known ferrimagnetic materials include yttrium iron garnet (YIG); cubic ferrites composed of iron oxides with other elements such as aluminum, cobalt, nickel, manganese, and zinc; and hexagonal ferrites such as PbFe12O19 and BaFe12O19 and pyrrhotite, Fe1−xS.[3]"
Ferromagnetism,Physics,2,"Ferromagnetism is the basic mechanism by which certain materials (such as iron) form permanent magnets, or are attracted to magnets. In physics, several different types of magnetism are distinguished. Ferromagnetism (along with the similar effect ferrimagnetism)  is the strongest type and is responsible for the common phenomenon of magnetism in magnets encountered in everyday life.[1] Substances respond weakly to magnetic fields with three other types of magnetism—paramagnetism, diamagnetism, and antiferromagnetism—but the forces are usually so weak that they can be detected only by sensitive instruments in a laboratory.  An everyday example of ferromagnetism is a refrigerator magnet used to hold notes on a refrigerator door. The attraction between a magnet and ferromagnetic material is ""the quality of magnetism first apparent to the ancient world, and to us today"".[2] Permanent magnets (materials that can be magnetized by an external magnetic field and remain magnetized after the external field is removed) are either ferromagnetic or ferrimagnetic, as are the materials that are noticeably attracted to them.  Only a few substances are ferromagnetic.  The common ones are iron, cobalt, nickel and most of their alloys, and some compounds of rare earth metals.
Ferromagnetism is very important in industry and modern technology, and is the basis for many electrical and electromechanical devices such as electromagnets, electric motors, generators, transformers, and magnetic storage such as tape recorders, and hard disks, and nondestructive testing of ferrous materials.
 Ferromagnetic materials can be divided into magnetically ""soft"" materials like annealed iron, which can be magnetized but do not tend to stay magnetized, and magnetically ""hard"" materials, which do. Permanent magnets are made from ""hard"" ferromagnetic materials such as alnico and ferrite that are subjected to special processing in a strong magnetic field during manufacture to align their internal microcrystalline structure, making them very hard to demagnetize. To demagnetize a saturated magnet, a certain magnetic field must be applied, and this threshold depends on coercivity of the respective material. ""Hard"" materials have high coercivity, whereas ""soft"" materials have low coercivity. The overall strength of a magnet is measured by its magnetic moment or, alternatively, the total magnetic flux it produces. The local strength of magnetism in a material is measured by its magnetization.
"
Field_line,Physics,2,"A field line is a graphical visual aid for visualizing vector fields. It consists of a directed line which is tangent to the field vector at each point along its length.[1][2]   A diagram showing a representative set of neighboring field lines is a common way of depicting a vector field in scientific and mathematical literature; this is called a field line diagram.  They are used to show electric fields, magnetic fields, and gravitational fields among many other types.  In fluid mechanics field lines showing the velocity field of a fluid flow are called streamlines.
"
For_Inspiration_and_Recognition_of_Science_and_Technology,Physics,2,"For Inspiration and Recognition of Science and Technology (FIRST) is an international youth organization that operates the FIRST Robotics Competition, FIRST LEGO League Challenge, FIRST LEGO League Explore, FIRST LEGO League Discover, and FIRST Tech Challenge competitions.[4]
Founded by Dean Kamen and Woodie Flowers in 1989, its expressed goal is to develop ways to inspire students in engineering and technology fields. Its philosophy is expressed by the organization as Coopertition and Gracious Professionalism.
FIRST also operates FIRST Place, a research facility at FIRST Headquarters in Manchester, New Hampshire, where it holds educational programs and day camps for students and teachers.[5]"
First_law_of_thermodynamics,Physics,2,"
 The first law of thermodynamics is a version of the law of conservation of energy, adapted for thermodynamic processes, distinguishing two kinds of transfer of energy, as heat and as thermodynamic work, and relating them to a function of a body's state, called Internal energy.
 The law of conservation of energy states that the total energy of an isolated system is constant; energy can be transformed from one form to another, but can be neither created nor destroyed.
 For a thermodynamic process without transfer of matter, the first law is often formulated[1][nb 1] where ΔU denotes the change in the internal energy of a closed system, Q denotes the quantity of energy supplied to the system as heat, and W denotes the amount of thermodynamic work done by the system on its surroundings. An equivalent statement is that perpetual motion machines of the first kind are impossible.
 For processes that include transfer of matter, a further statement is needed: 'With due account of the respective reference states of the systems, when two systems, which may be of different chemical compositions, initially separated only by an impermeable wall, and otherwise isolated, are combined into a new system by the thermodynamic operation of removal of the wall, then
 where U0 denotes the internal energy of the combined system, and U1 and U2 denote the internal energies of the respective separated systems.'
"
Nuclear_fission,Physics,2,"{{Redirect2|Splitting the atom|Split the atom|the EP by Massive Attack|Splitting the Atom}
 In nuclear physics and nuclear chemistry, nuclear fission is a nuclear reaction or a radioactive decay process in which the nucleus of an atom splits into two or more smaller, lighter nuclei. The fission process often produces  gamma photons, and releases a very large amount of energy even by the energetic standards of radioactive decay.
 Nuclear fission of heavy elements was discovered on December 17, 1938 by German Otto Hahn and his assistant Fritz Strassmann, and explained theoretically in January 1939 by Lise Meitner and her nephew Otto Robert Frisch. Frisch named the process by analogy with biological fission of living cells. For heavy nuclides, it is an exothermic reaction which can release large amounts of energy both as electromagnetic radiation and as kinetic energy of the fragments (heating the bulk material where fission takes place). Like nuclear fusion, in order for fission to produce energy, the total binding energy of the resulting elements must have a greater binding energy than that of the starting element.
 Fission is a form of nuclear transmutation because the resulting fragments are not the same element as the original atom. The two (or more) nuclei produced are most often of comparable but slightly different sizes, typically with a mass ratio of products of about 3 to 2, for common fissile isotopes.[1][2] Most fissions are binary fissions (producing two charged fragments), but occasionally (2 to 4 times per 1000 events), three positively charged fragments are produced, in a ternary fission. The smallest of these fragments in ternary processes ranges in size from a proton to an argon nucleus.
 Apart from fission induced by a neutron, harnessed and exploited by humans, a natural form of spontaneous radioactive decay (not requiring a neutron) is also referred to as fission, and occurs especially in very high-mass-number isotopes. Spontaneous fission was discovered in 1940 by Flyorov, Petrzhak, and Kurchatov[3] in Moscow, when they confirmed that, without bombardment by neutrons, the fission rate of uranium was indeed negligible, as predicted by Niels Bohr; it was not.[3][clarification needed] The unpredictable composition of the products (which vary in a broad probabilistic and somewhat chaotic manner) distinguishes fission from purely quantum tunneling processes such as proton emission, alpha decay, and cluster decay, which give the same products each time. Nuclear fission produces energy for nuclear power and drives the explosion of nuclear weapons. Both uses are possible because certain substances called nuclear fuels undergo fission when struck by fission neutrons, and in turn emit neutrons when they break apart. This makes a self-sustaining nuclear chain reaction possible, releasing energy at a controlled rate in a nuclear reactor or at a very rapid, uncontrolled rate in a nuclear weapon.
 The amount of free energy contained in nuclear fuel is millions of times the amount of free energy contained in a similar mass of chemical fuel such as gasoline, making nuclear fission a very dense source of energy. The products of nuclear fission, however, are on average far more radioactive than the heavy elements which are normally fissioned as fuel, and remain so for significant amounts of time, giving rise to a nuclear waste problem. Concerns over nuclear waste accumulation and the destructive potential of nuclear weapons are a counterbalance to the peaceful desire to use fission as an energy source.
"
Flavour_(particle_physics),Physics,2,"
 In particle physics, flavour or flavor refers to the species of an elementary particle. The Standard Model counts six flavours of quarks and six flavours of leptons. They are conventionally parameterized with flavour quantum numbers that are assigned to all subatomic particles. They can also be described by some of the family symmetries proposed for the quark-lepton generations.
"
Fluid,Physics,2,"In physics, a fluid is a substance that continually deforms (flows) under an applied shear stress, or external force.  Fluids are a phase of matter and include liquids, gases and plasmas. They are substances with zero shear modulus, or, in simpler terms, substances which cannot resist any shear force applied to them. 
 Although the term ""fluid"" includes both the liquid and gas phases, in common usage, ""fluid"" is often used as a synonym for ""liquid"", with no implication that gas could also be present. This colloquial usage of the term is also common in medicine and in nutrition (""take plenty of fluids"").
 Liquids form a free surface (that is, a surface not created by the container) while gases do not. Viscoelastic fluids like Silly Putty appear to behave similar to a solid when a sudden force is applied. Also substances with a very high viscosity such as pitch appear to behave like a solid (see pitch drop experiment).
"
Fluid_mechanics,Physics,2,"Fluid mechanics is the branch of physics concerned with the mechanics of fluids (liquids, gases, and plasmas) and the forces on them.[1]:3
It has applications in a wide range of disciplines, including mechanical, civil, chemical and biomedical engineering, geophysics, oceanography, meteorology, astrophysics, and biology.
 It can be divided into fluid statics, the study of fluids at rest; and fluid dynamics, the study of the effect of forces on fluid motion.[1]:3
It is a branch of continuum mechanics, a subject which models matter without using the information that it is made out of atoms; that is, it models matter from a macroscopic viewpoint rather than from microscopic. Fluid mechanics, especially fluid dynamics, is an active field of research, typically mathematically complex.  Many problems are partly or wholly unsolved and are best addressed by numerical methods, typically using computers.  A modern discipline, called computational fluid dynamics (CFD), is devoted to this approach.[2] Particle image velocimetry, an experimental method for visualizing and analyzing fluid flow, also takes advantage of the highly visual nature of fluid flow.
"
Fluid_physics,Physics,2,"Fluid mechanics is the branch of physics concerned with the mechanics of fluids (liquids, gases, and plasmas) and the forces on them.[1]:3
It has applications in a wide range of disciplines, including mechanical, civil, chemical and biomedical engineering, geophysics, oceanography, meteorology, astrophysics, and biology.
 It can be divided into fluid statics, the study of fluids at rest; and fluid dynamics, the study of the effect of forces on fluid motion.[1]:3
It is a branch of continuum mechanics, a subject which models matter without using the information that it is made out of atoms; that is, it models matter from a macroscopic viewpoint rather than from microscopic. Fluid mechanics, especially fluid dynamics, is an active field of research, typically mathematically complex.  Many problems are partly or wholly unsolved and are best addressed by numerical methods, typically using computers.  A modern discipline, called computational fluid dynamics (CFD), is devoted to this approach.[2] Particle image velocimetry, an experimental method for visualizing and analyzing fluid flow, also takes advantage of the highly visual nature of fluid flow.
"
Fluid_statics,Physics,2,"Fluid statics or hydrostatics is the branch of fluid mechanics that studies ""fluids at rest and the pressure in a fluid or exerted by a fluid on an immersed body"".[1] It encompasses the study of the conditions under which fluids are at rest in stable equilibrium as opposed to fluid dynamics, the study of fluids in motion. Hydrostatics are categorized as a part of the fluid statics, which is the study of all fluids, incompressible or not, at rest.
 Hydrostatics is fundamental to hydraulics, the engineering of equipment for storing, transporting and using fluids.  It is also relevant to geophysics and astrophysics (for example, in understanding plate tectonics and the anomalies of the Earth's gravitational field), to meteorology, to medicine (in the context of blood pressure), and many other fields.
 Hydrostatics offers physical explanations for many phenomena of everyday life, such as why atmospheric pressure changes with altitude, why wood and oil float on water, and why the surface of still water is always level.
"
Fluorescence,Physics,2,"
 Fluorescence is the emission of light by a substance that has absorbed light or other electromagnetic radiation. It is a form of luminescence. In most cases, the emitted light has a longer wavelength, and therefore lower energy, than the absorbed radiation. The most striking example of fluorescence occurs when the absorbed radiation is in the ultraviolet region of the spectrum, and thus invisible to the human eye, while the emitted light is in the visible region, which gives the fluorescent substance a distinct color that can be seen only when exposed to UV light. Fluorescent materials cease to glow nearly immediately when the radiation source stops, unlike phosphorescent materials, which continue to emit light for some time after.
 Fluorescence has many practical applications, including mineralogy, gemology, medicine, chemical sensors (fluorescence spectroscopy), fluorescent labelling, dyes, biological detectors, cosmic-ray detection, vacuum fluorescent displays, and cathode-ray tubes. Its most common everyday application is in energy-saving fluorescent lamps and LED lamps, where fluorescent coatings are used to convert short-wavelength UV light or blue light into longer-wavelength yellow light, thereby mimicking the warm light of energy-inefficient incandescent lamps. Fluorescence also occurs frequently in nature in some minerals and in various biological forms in many branches of the animal kingdom.
"
Flux,Physics,2,"Flux describes any effect that appears to pass or travel (whether it actually moves or not) through a surface or substance.   A flux is a concept in applied mathematics and vector calculus which has many applications to physics.  For transport phenomena, flux is a vector quantity, describing the magnitude and direction of the flow of a substance or property. In vector calculus flux is a scalar quantity, defined as the surface integral of the perpendicular component of a vector field over a surface.[1]"
Flux#Terminology,Physics,2,"Flux describes any effect that appears to pass or travel (whether it actually moves or not) through a surface or substance.   A flux is a concept in applied mathematics and vector calculus which has many applications to physics.  For transport phenomena, flux is a vector quantity, describing the magnitude and direction of the flow of a substance or property. In vector calculus flux is a scalar quantity, defined as the surface integral of the perpendicular component of a vector field over a surface.[1]"
Focal_length,Physics,2,"The focal length of an optical system is a measure of how strongly the system converges or diverges light; it is the inverse of the system's optical power. A positive focal length indicates that a system converges light, while a negative focal length indicates that the system diverges light. A system with a shorter focal length bends the rays more sharply, bringing them to a focus in a shorter distance or diverging them more quickly. For the special case of a thin lens in air, a positive focal length is the distance over which initially collimated (parallel) rays are brought to a focus, or alternatively a negative focal length indicates how far in front of the lens a point source must be located to form a collimated beam. For more general optical systems, the focal length has no intuitive meaning; it is simply the inverse of the system's optical power.
 In most photography and all telescopy, where the subject is essentially infinitely far away, longer focal length (lower optical power) leads to higher magnification and a narrower angle of view; conversely, shorter focal length or higher optical power is associated with lower magnification and a wider angle of view. On the other hand, in applications such as microscopy in which magnification is achieved by bringing the object close to the lens, a shorter focal length (higher optical power) leads to higher magnification because the subject can be brought closer to the center of projection.
"
Focus_(optics),Physics,2,"In geometrical optics, a focus, also called an image point, is the point where light rays originating from a point on the object converge.[1] Although the focus is conceptually a point, physically the focus has a spatial extent, called the blur circle. This non-ideal focusing may be caused by aberrations of the imaging optics. In the absence of significant aberrations, the smallest possible blur circle is the Airy disc, which is caused by diffraction from the optical system's aperture. Aberrations tend worsen as the aperture diameter increases, while the Airy circle is smallest for large apertures.
 An image, or image point or region, is in focus if light from object points is converged almost as much as possible in the image, and out of focus if light is not well converged. The border between these is sometimes defined using a ""circle of confusion"" criterion.
 A principal focus or focal point is a special focus:
 Diverging (negative) lenses and convex mirrors do not focus a collimated beam to a point. Instead, the focus is the point from which the light appears to be emanating, after it travels through the lens or reflects from the mirror. A convex parabolic mirror will reflect a beam of collimated light to make it appear as if it were radiating from the focal point, or conversely, reflect rays directed toward the focus as a collimated beam. A convex elliptical mirror will reflect light directed towards one focus as if it were radiating from the other focus, both of which are behind the mirror. A convex hyperbolic mirror will reflect rays emanating from the focal point in front of the mirror as if they were emanating from the focal point behind the mirror. Conversely, it can focus rays directed at the focal point that is behind the mirror towards the focal point that is in front of the mirror as in a Cassegrain telescope.
"
Force,Physics,2,"
 In physics, a force is any interaction that, when unopposed, will change the motion of an object. A force can cause an object with mass to change its velocity (which includes to begin moving from a state of rest), i.e., to accelerate. Force can also be described intuitively as a push or a pull. A force has both magnitude and direction, making it a vector quantity.  It is measured in the SI unit of newtons and represented by the symbol F.
 The original form of Newton's second law states that the net force acting upon an object is equal to the rate at which its momentum changes with time. If the mass of the object is constant, this law implies that the acceleration of an object is directly proportional to the net force acting on the object, is in the direction of the net force, and is inversely proportional to the mass of the object.
 Concepts related to force include: thrust, which increases the velocity of an object; drag, which decreases the velocity of an object; and torque, which produces changes in rotational speed of an object. In an extended body, each part usually applies forces on the adjacent parts; the distribution of such forces through the body is the internal mechanical stress. Such internal mechanical stresses cause no acceleration of that body as the forces balance one another. Pressure, the distribution of many small forces applied over an area of a body, is a simple type of stress that if unbalanced can cause the body to accelerate. Stress usually causes deformation of solid materials, or flow in fluids.
"
Force_carrier,Physics,2,"In quantum field theory, force carriers or messenger particles or intermediate particles are particles that give rise to forces between other particles. These particles are bundles of energy (quanta) of a particular kind of field. There is one kind of field for every type of elementary particle. For instance, there is an electromagnetic field whose quanta are photons.[1]  The concept is especially important in particle physics where the force carrier particles that mediate the electromagnetic, weak, and strong interactions are called gauge bosons.
"
Frame_of_reference,Physics,2,"In physics, a frame of reference (or reference frame) consists of an abstract coordinate system and the set of physical reference points that uniquely fix (locate and orient) the coordinate system and standardize measurements within that frame.
 For n dimensions, n + 1 reference points are sufficient to fully define a reference frame. Using rectangular (Cartesian) coordinates, a reference frame may be defined with a reference point at the origin and a reference point at one unit distance along each of the n coordinate axes.
 In Einsteinian relativity, reference frames are used to specify the relationship between a moving observer and the phenomenon or phenomena under observation. In this context, the phrase often becomes ""observational frame of reference"" (or ""observational reference frame""), which implies that the observer is at rest in the frame, although not necessarily located at its origin. A relativistic reference frame includes (or implies) the coordinate time, which does not equate across different frames moving relatively to each other. The situation thus differs from Galilean relativity, where all possible coordinate times are essentially equivalent.
"
Fraunhofer_lines,Physics,2,"In physics and optics, the Fraunhofer lines are a set of spectral absorption lines named after the German physicist Joseph von Fraunhofer (1787–1826). The lines were originally observed as dark features (absorption lines) in the optical spectrum of the Sun.
"
Free_body_diagram,Physics,2,"In physics and engineering, a free body diagram (force diagram,[1] or FBD) is a graphical illustration used to visualize the applied forces, moments, and resulting reactions on a body in a given condition. They depict a body or connected bodies with all the applied forces and moments, and reactions, which act on the body(ies). The body may consist of multiple internal members (such as a truss), or be a compact body (such as a beam). A series of free bodies and other diagrams may be necessary to solve complex problems.
"
Frequency,Physics,2,"Frequency is the number of occurrences of a repeating event per unit of time.[1] It is also referred to as temporal frequency, which emphasizes the contrast to spatial frequency and angular frequency. Frequency is measured in units of hertz (Hz) which is equal to one occurrence of a repeating event per second.  The period is the duration of time of one cycle in a repeating event, so the period is the reciprocal of the frequency.[2]  For example: if a newborn baby's heart beats at a frequency of 120 times a minute (2 hertz), its period, T, — the time interval between beats—is half a second (60 seconds divided by 120 beats). Frequency is an important parameter used in science and engineering to specify the rate of oscillatory and vibratory phenomena, such as mechanical vibrations, audio signals (sound), radio waves, and light.
"
Frequency_modulation,Physics,2,"Frequency modulation (FM) is the encoding of information in a carrier wave by varying the instantaneous frequency of the wave. The technology is used in telecommunications, radio broadcasting, signal processing, and computing.
 In analog frequency modulation, such as radio broadcasting, of an audio signal representing voice or music, the instantaneous frequency deviation, i.e. the difference between the frequency of the carrier and its center frequency, has a functional relation to the modulating signal amplitude.
 Digital data can be encoded and transmitted with a type of frequency modulation known as frequency-shift keying (FSK), in which the instantaneous frequency of the carrier is shifted among a set of frequencies. The frequencies may represent digits, such as 0 and 1. FSK is widely used in computer modems, such as fax modems, telephone caller ID systems, garage door openers, and other low frequency transmissions.[1] Radioteletype also uses FSK.[2] Frequency modulation is widely used for FM radio broadcasting.  It is also used in telemetry, radar, seismic prospecting, and monitoring newborns for seizures via EEG,[3] two-way radio systems, sound synthesis, magnetic tape-recording systems and some video-transmission systems.  In radio transmission, an advantage of frequency modulation is that it has a larger signal-to-noise ratio and therefore rejects radio frequency interference better than an equal power amplitude modulation (AM) signal.  For this reason, most music is broadcast over FM radio.
 Frequency modulation and phase modulation are the two complementary principal methods of angle modulation; phase modulation is often used as an intermediate step to achieve frequency modulation. These methods contrast with amplitude modulation, in which the amplitude of the carrier wave varies, while the frequency and phase remain constant.
"
Free_fall,Physics,2,"In Newtonian physics, free fall is any motion of a body where gravity is the only force acting upon it. In the context of general relativity, where gravitation is reduced to a space-time curvature, a body in free fall has no force acting on it.
 An object in the technical sense of the term ""free fall"" may not necessarily be falling down in the usual sense of the term. An object moving upwards would not normally be considered to be falling, but if it is subject to the force of gravity only, it is said to be in free fall. The moon is thus in free fall.
 In a roughly uniform gravitational field, in the absence of any other forces, gravitation acts on each part of the body roughly equally, which results in the sensation of weightlessness, a condition that also occurs when the gravitational field is weak (such as when far away from any source of gravity).
 The term ""free fall"" is often used more loosely than in the strict sense defined above. Thus, falling through an atmosphere without a deployed parachute, or lifting device, is also often referred to as free fall. The aerodynamic drag forces in such situations prevent them from producing full weightlessness, and thus a skydiver's ""free fall"" after reaching terminal velocity produces the sensation of the body's weight being supported on a cushion of air.
"
Freezing_point,Physics,2,"The melting point (or, rarely, liquefaction point) of a substance is the temperature at which it changes state from solid to liquid. At the melting point the solid and liquid phase exist in equilibrium. The melting point of a substance depends on pressure and is usually specified at a standard pressure such as 1 atmosphere or 100 kPa.
 When considered as the temperature of the reverse change from liquid to solid, it is referred to as the freezing point or crystallization point. Because of the ability of some substances to supercool, the freezing point is not considered as a characteristic property of a substance. When the ""characteristic freezing point"" of a substance is determined, in fact the actual methodology is almost always ""the principle of observing the disappearance rather than the formation of ice, that is, the melting point.[1]"
Friction,Physics,2,"
 Friction is the force resisting the relative motion of solid surfaces, fluid layers, and material elements sliding against each other.[2] There are several types of friction:
 When surfaces in contact move relative to each other, the friction between the two surfaces converts kinetic energy into thermal energy (that is, it converts work to heat). This property can have dramatic consequences, as illustrated by the use of friction created by rubbing pieces of wood together to start a fire. Kinetic energy is converted to thermal energy whenever motion with friction occurs, for example when a viscous fluid is stirred. Another important consequence of many types of friction can be wear, which may lead to performance degradation or damage to components. Friction is a component of the science of tribology.
 Friction is desirable and important in supplying traction to facilitate motion on land. Most land vehicles rely on friction for acceleration, deceleration and changing direction. Sudden reductions in traction can cause loss of control and accidents.
 Friction is not itself a fundamental force. Dry friction arises from a combination of inter-surface adhesion, surface roughness, surface deformation, and surface contamination. The complexity of these interactions makes the calculation of friction from first principles  impractical and necessitates the use of empirical methods for analysis and the development of theory.
 Friction is a non-conservative force - work done against friction is path dependent. In the presence of friction, some kinetic energy is always transformed to thermal energy, so mechanical energy is not conserved.
"
Function_(mathematics),Physics,2,"In mathematics, a function[note 1] is a binary relation between two sets that associates every element of the first set to exactly one element of the second set. Typical examples are functions from integers to integers, or from the real numbers to real numbers.
 Functions were originally the idealization of how a varying quantity depends on another quantity. For example, the position of a planet is a function of time. Historically, the concept was elaborated with the infinitesimal calculus at the end of the 17th century, and, until the 19th century, the functions that were considered were differentiable (that is, they had a high degree of regularity). The concept of a function was formalized at the end of the 19th century in terms of set theory, and this greatly enlarged the domains of application of the concept.
 A function is a process or a relation that associates each element x of a set X,  the domain of the function, to a single element y of another set Y (possibly the same set), the codomain of the function. It is customarily denoted by letters such as 



f


{  f}
, 



g


{  g}
 and 



h


{  h}
.[1] If the function is called f, this relation is denoted by y = f (x) (which reads ""f of x""), where the element x is the argument or input of the function, and y is the value of the function, the output, or the image of x by f.[2] The symbol that is used for representing the input is the variable of the function (e.g., f is a function of the variable x).[3] A function is uniquely represented by the set of all pairs (x, f (x)), called the graph of the function.[note 2][4] When the domain and the codomain are sets of real numbers, each such pair may be thought of as the Cartesian coordinates of a point in the plane. The set of these points is called the graph of the function; it is a popular means of illustrating the function.
 Functions are widely used in science, and in most fields of mathematics. It has been said that functions are ""the central objects of investigation"" in most fields of mathematics.[5]"
Fundamental_forces,Physics,2,"
 In physics, the fundamental interactions, also known as fundamental forces, are the interactions that do not appear to be reducible to more basic interactions. There are four fundamental interactions known to exist:  the gravitational and electromagnetic interactions, which produce significant long-range forces whose effects can be seen directly in everyday life, and the strong and weak interactions, which produce forces at minuscule, subatomic distances and govern nuclear interactions. Some scientists hypothesize that a fifth force might exist, but these hypotheses remain speculative.[1][2][3] Each of the known fundamental interactions can be described mathematically as a field. The gravitational force is attributed to the curvature of spacetime, described by Einstein's general theory of relativity. The other three are discrete quantum fields, and their interactions are mediated by elementary particles described by the Standard Model of particle physics.[4] Within the Standard Model, the strong  interaction is carried by a particle called the gluon, and is responsible for quarks binding together to form hadrons, such as protons and neutrons.  As a residual effect, it creates the nuclear force that binds the latter particles to form atomic nuclei. The weak interaction is carried by particles called W and Z bosons, and also acts on the nucleus of atoms, mediating radioactive decay.  The electromagnetic force, carried by the photon, creates electric and magnetic fields, which are responsible for the attraction between orbital electrons and atomic nuclei which holds atoms together, as well as chemical bonding and electromagnetic waves, including visible light, and forms the basis for electrical technology. Although the electromagnetic force is far stronger than  gravity, it tends to cancel itself out within large objects, so over large (astronomical) distances gravity tends to be the dominant force, and is responsible for holding together the large scale structures in the universe, such as planets, stars, and galaxies.
 Many theoretical physicists believe these fundamental forces to be related and to become unified into a single force at very high energies on a minuscule scale, the Planck scale, but particle accelerators cannot produce the enormous energies required to experimentally probe this.[5] Devising a common theoretical framework that would explain the relation between the forces in a single theory is perhaps the greatest goal of today's theoretical physicists. The weak and electromagnetic forces have already been unified with the electroweak theory of Sheldon Glashow, Abdus Salam, and Steven Weinberg for which they received the 1979 Nobel Prize in physics.[6][7][8] Progress is currently being made in uniting the electroweak and strong fields within what is called a Grand Unified Theory (GUT).[citation needed]  A bigger challenge is to find a way to quantize the gravitational field, resulting in a theory of quantum gravity (QG) which would unite gravity in a common theoretical framework with the other three forces.  Some theories, notably string theory, seek both QG and GUT within one framework, unifying all four fundamental interactions along with mass generation within a theory of everything (ToE).
"
Fundamental_frequency,Physics,2,"
 The natural frequency, or fundamental frequency (FF), often referred to simply as the fundamental, is defined as the lowest frequency of a periodic waveform. In music, the fundamental is the musical pitch of a note that is perceived as the lowest partial present. In terms of a superposition of sinusoids, the fundamental frequency is the lowest frequency sinusoidal in the sum.  In some contexts, the fundamental is usually abbreviated as f0, indicating the lowest frequency counting from zero.[1][2][3] In other contexts, it is more common to abbreviate it as f1, the first harmonic.[4][5][6][7][8] (The second harmonic is then f2 = 2⋅f1, etc.  In this context, the zeroth harmonic would be 0 Hz.)
 According to Benward's and Saker's Music: In Theory and Practice:[9] Since the fundamental is the lowest frequency and is also perceived as the loudest, the ear identifies it as the specific pitch of the musical tone [harmonic spectrum].... The individual partials are not heard separately but are blended together by the ear into a single tone."
Fundamental_theorem_of_calculus,Physics,2,"The fundamental theorem of calculus is a theorem that links the concept of differentiating a function with the concept of integrating a function.
 The first part of the theorem, sometimes called the first fundamental theorem of calculus, states that one of the antiderivatives (also called indefinite integral), say F, of some function f may be obtained as the integral of f with a variable bound of integration. This implies the existence of antiderivatives for continuous functions.[1] Conversely, the second part of the theorem, sometimes called the second fundamental theorem of calculus, states that the integral of a function f over some interval can be computed by using any one, say F, of its infinitely many antiderivatives. This part of the theorem has key practical applications, because explicitly finding the antiderivative of a function by symbolic integration avoids numerical integration to compute integrals. This provides generally a better numerical accuracy.
"
Nuclear_fusion,Physics,2,"
 Nuclear fusion is a reaction in which two or more atomic nuclei are combined to form one or more different atomic nuclei and subatomic particles (neutrons or protons). The difference in mass between the reactants and products is manifested as either the release or the absorption of energy. This difference in mass arises due to the difference in atomic binding energy between the nuclei before and after the reaction. Fusion is the process that powers active or main sequence stars and other high-magnitude stars, where large amounts of energy are released.
 A fusion process that produces nuclei lighter than iron-56 or nickel-62 will generally release energy. These elements have relatively small mass per nucleon and large binding energy per nucleon. Fusion of nuclei lighter than these releases energy (an exothermic process), while fusion of heavier nuclei results in energy retained by the product nucleons, and the resulting reaction is endothermic. The opposite is true for the reverse process, nuclear fission. This means that the lighter elements, such as hydrogen and helium, are in general more fusible; while the heavier elements, such as uranium, thorium and plutonium, are more fissionable. The extreme astrophysical event of a supernova can produce enough energy to fuse nuclei into elements heavier than iron.
 In 1920, Arthur Eddington suggested hydrogen-helium fusion could be the primary source of stellar energy. Quantum tunneling was discovered by Friedrich Hund in 1929, and shortly afterwards Robert Atkinson and Fritz Houtermans used the measured masses of light elements to show that large amounts of energy could be released by fusing small nuclei. Building on the early experiments in nuclear transmutation by Ernest Rutherford, laboratory fusion of hydrogen isotopes was accomplished by Mark Oliphant in 1932. In the remainder of that decade, the theory of the main cycle of nuclear fusion in stars was worked out by Hans Bethe.  Research into fusion for military purposes began in the early 1940s as part of the Manhattan Project.  Fusion was accomplished in 1951 with the Greenhouse Item nuclear test.  Nuclear fusion on a large scale in an explosion was first carried out on 1 November 1952, in the Ivy Mike hydrogen bomb test.
 Research into developing controlled fusion inside fusion reactors has been ongoing since the 1940s, but the technology is still in its development phase.
"
Gamma_ray,Physics,2,"
 A gamma ray, or gamma radiation (symbol γ or 



γ


{  \gamma }
), is a penetrating form of electromagnetic radiation arising from the radioactive decay of atomic nuclei. It consists of the shortest wavelength electromagnetic waves and so imparts the highest photon energy. Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900 while studying radiation emitted by radium. In 1903, Ernest Rutherford named this radiation gamma rays based on their relatively strong penetration of matter; in 1900 he had already named two less penetrating types of decay radiation (discovered by Henri Becquerel) alpha rays and beta rays in ascending order of penetrating power.
 Gamma rays from radioactive decay are in the energy range from a few kiloelectronvolts (keV) to approximately 8 megaelectronvolts (~8 MeV), corresponding to the typical energy levels in nuclei with reasonably long lifetimes. The energy spectrum of gamma rays can be used to identify the decaying radionuclides using gamma spectroscopy. Very-high-energy gamma rays in the 100–1000 teraelectronvolt (TeV) range have been observed from sources such as the Cygnus X-3 microquasar.
 Natural sources of gamma rays originating on Earth are mostly as a result of radioactive decay and secondary radiation from atmospheric interactions with cosmic ray particles. However, there are other rare natural sources, such as terrestrial gamma-ray flashes, which produce gamma rays from electron action upon the nucleus. Notable artificial sources of gamma rays include fission, such as that which occurs in nuclear reactors, and high energy physics experiments, such as neutral pion decay and nuclear fusion.
 Gamma rays and X-rays are both electromagnetic radiation, and since they overlap in the electromagnetic spectrum, the terminology varies between scientific disciplines. In some fields of physics, they are distinguished by their origin: Gamma rays are created by nuclear decay, while in the case of X-rays, the origin is outside the nucleus. In astrophysics, gamma rays are conventionally defined as having photon energies above 100 keV and are the subject of gamma ray astronomy, while radiation below 100 keV is classified as X-rays and is the subject of X-ray astronomy. This convention stems from the early man-made X-rays, which had energies only up to 100 keV, whereas many gamma rays could go to higher energies. A large fraction of astronomical gamma rays are screened by Earth's atmosphere.
 Gamma rays are ionizing radiation and are thus biologically hazardous. Due to their high penetration power, they can damage bone marrow and internal organs. Unlike alpha and beta rays, they pass easily through the body and thus pose a formidable radiation protection challenge, requiring shielding made from dense materials such as lead or concrete.
 Gamma rays cannot be reflected off a mirror and their wavelengths are so small that they will pass between atoms in a detector. This means that gamma ray detectors often contain densely packed diamonds.
"
Gas,Physics,2,"
 Gas is one of the four fundamental states of matter (the others being solid, liquid, and plasma). A pure gas may be made up of individual atoms (e.g. a noble gas like neon), elemental molecules made from one type of atom (e.g. oxygen), or compound molecules made from a variety of atoms (e.g. carbon dioxide). A gas mixture, such as air, contains a variety of pure gases. What distinguishes a gas from liquids and solids is the vast separation of the individual gas particles. This separation usually makes a colorless gas invisible to the human observer. The interaction of gas particles in the presence of  electric and  gravitational fields are considered[by whom?] negligible, as indicated by the constant velocity vectors in the image.
 The gaseous state of matter occurs between the liquid and plasma states,[1] the latter of which provides the upper temperature boundary for gases. Bounding the lower end of the temperature scale lie degenerative quantum gases[2] which are gaining increasing attention.[3]
High-density atomic gases super-cooled to very low temperatures are classified by their statistical behavior as either Bose gases or Fermi gases. For a comprehensive listing of these exotic states of matter see list of states of matter.
"
General_relativity,Physics,2,"
 General relativity, also known as the general theory of relativity, is the geometric theory of gravitation published by Albert Einstein in 1915 and is the current description of gravitation in modern physics. General relativity generalizes special relativity and refines Newton's law of universal gravitation, providing a unified description of gravity as a geometric property of space and time or four-dimensional spacetime. In particular, the curvature of spacetime is directly related to the energy and momentum of whatever matter and radiation are present. The relation is specified by the Einstein field equations, a system of partial differential equations.
 Some predictions of general relativity differ significantly from those of classical physics, especially concerning the passage of time, the geometry of space, the motion of bodies in free fall, and the propagation of light. Examples of such differences include gravitational time dilation, gravitational lensing, the gravitational redshift of light, the gravitational time delay and singularities/black holes. The predictions of general relativity in relation to classical physics have been confirmed in all observations and experiments to date. Although general relativity is not the only relativistic theory of gravity, it is the simplest theory that is consistent with experimental data. However, unanswered questions remain, the most fundamental being how general relativity can be reconciled with the laws of quantum physics to produce a complete and self-consistent theory of quantum gravity, how gravity can be unified with the three non-gravitational forces—strong nuclear, weak nuclear, and electromagnetic force.
 Einstein's theory has important astrophysical implications. For example, it implies the existence of black holes—regions of space in which space and time are distorted in such a way that nothing, not even light, can escape—as an end-state for massive stars. There is ample evidence that the intense radiation emitted by certain kinds of astronomical objects is due to black holes. For example, microquasars and active galactic nuclei result from the presence of stellar black holes and supermassive black holes, respectively. The bending of light by gravity can lead to the phenomenon of gravitational lensing, in which multiple images of the same distant astronomical object are visible in the sky. General relativity also predicts the existence of gravitational waves, which have since been observed directly by the physics collaboration LIGO. In addition, general relativity is the basis of current cosmological models of a consistently expanding universe.
 Widely acknowledged as a theory of extraordinary beauty, general relativity has often been described as the most beautiful of all existing physical theories.[2]"
Geophysics,Physics,2,"
 Geophysics (/ˌdʒiːoʊˈfɪzɪks/) is a subject of natural science concerned with the physical processes and physical properties of the Earth and its surrounding space environment, and the use of quantitative methods for their analysis. The term geophysics sometimes refers only to geological applications: Earth's shape; its gravitational and magnetic fields; its internal structure and composition; its dynamics and their surface expression in plate tectonics, the generation of magmas, volcanism and rock formation.[3] However, modern geophysics organizations and pure scientists use a broader definition that includes the water cycle including snow and ice; fluid dynamics of the oceans and the atmosphere; electricity and magnetism in the ionosphere and magnetosphere and solar-terrestrial relations; and analogous problems associated with the Moon and other planets.[3][4][5][6][7] Although geophysics was only recognized as a separate discipline in the 19th century, its origins date back to ancient times. The first magnetic compasses were made from lodestones, while more modern magnetic compasses played an important role in the history of navigation. The first seismic instrument was built in 132 AD. Isaac Newton applied his theory of mechanics to the tides and the precession of the equinox; and instruments were developed to measure the Earth's shape, density and gravity field, as well as the components of the water cycle. In the 20th century, geophysical methods were developed for remote exploration of the solid Earth and the ocean, and geophysics played an essential role in the development of the theory of plate tectonics.
 Geophysics is applied to societal needs, such as mineral resources, mitigation of natural hazards and environmental protection.[4]  In Exploration geophysics, geophysical survey data are used to analyze potential petroleum reservoirs and mineral deposits, locate groundwater, find archaeological relics, determine the thickness of glaciers and soils, and assess sites for environmental remediation.
"
Gluon,Physics,2,"A gluon (/ˈɡluːɒn/) is an elementary particle that acts as the exchange particle (or gauge boson) for the strong force between quarks. It is analogous to the exchange of photons in the electromagnetic force between two charged particles.[6] In layman's terms, they ""glue"" quarks together, forming hadrons such as protons and neutrons.
 and  In technical terms, gluons are vector gauge bosons that mediate strong interactions of quarks in quantum chromodynamics (QCD). Gluons themselves carry the color charge of the strong interaction. This is unlike the photon, which mediates the electromagnetic interaction but lacks an electric charge. Gluons therefore participate in the strong interaction in addition to mediating it, making QCD significantly harder to analyze than quantum electrodynamics (QED).
"
Graham%27s_law_of_diffusion,Physics,2,"Graham's law of effusion (also called Graham's law of diffusion) was formulated by Scottish physical chemist Thomas Graham in 1848.[1] Graham found experimentally that the rate of effusion of a gas is inversely proportional to the square root of the mass of its particles.[1] This formula can be written as:
 where:
 Graham's law states that the rate of diffusion or of effusion of a gas is inversely proportional to the square root of its molecular weight. Thus, if the molecular weight of one gas is four times that of another, it would diffuse through a porous plug or escape through a small pinhole in a vessel at half the rate of the other (heavier gases diffuse more slowly). A complete theoretical explanation of Graham's law was provided years later by the kinetic theory of gases. Graham's law provides a basis for separating isotopes by diffusion—a method that came to play a crucial role in the development of the atomic bomb.[2] Graham's law is most accurate for molecular effusion which involves the movement of one gas at a time through a hole. It is only approximate for diffusion of one gas in another or in air, as these processes involve the movement of more than one gas.[2] In the same conditions of temperature and pressure, the molar mass is proportional to the mass density. Therefore, the rates of diffusion of different gases are inversely proportional to the square roots of their mass densities.
"
Gravitation,Physics,2,"
 Gravity (from Latin  gravitas 'weight'[1]), or gravitation, is a natural phenomenon by which all things with mass or energy—including planets, stars, galaxies, and even light[2]—are brought toward (or gravitate toward) one another.  On Earth, gravity gives weight to physical objects, and the Moon's gravity causes the ocean tides. The gravitational attraction of the original gaseous matter present in the Universe caused it to begin coalescing and forming stars and caused the stars to group together into galaxies, so gravity is responsible for many of the large-scale structures in the Universe. Gravity has an infinite range, although its effects become increasingly weaker as objects get further away.
 Gravity is most accurately described by the general theory of relativity (proposed by Albert Einstein in 1915), which describes gravity not as a force, but as a consequence of masses moving along geodesic lines in a curved spacetime caused by the uneven distribution of mass. The most extreme example of this curvature of spacetime is a black hole, from which nothing—not even light—can escape once past the black hole's event horizon.[3] However, for most applications, gravity is well approximated by Newton's law of universal gravitation, which describes gravity as a force causing any two bodies to be attracted toward each other, with magnitude proportional to the product of their masses and inversely proportional to the square of the distance between them.
 Gravity is the weakest of the four fundamental interactions of physics, approximately 1038 times weaker than the strong interaction, 1036 times weaker than the electromagnetic force and 1029 times weaker than the weak interaction. As a consequence, it has no significant influence at the level of subatomic particles.[4] In contrast, it is the dominant interaction at the macroscopic scale, and is the cause of the formation, shape and trajectory (orbit) of astronomical bodies.
 Current models of particle physics imply that the earliest instance of gravity in the Universe, possibly in the form of quantum gravity, supergravity or a gravitational singularity, along with ordinary space and time, developed during the Planck epoch (up to 10−43 seconds after the birth of the Universe), possibly from a primeval state, such as a false vacuum, quantum vacuum or virtual particle, in a currently unknown manner.[5] Attempts to develop a theory of gravity consistent with quantum mechanics, a quantum gravity theory, which would allow gravity to be united in a common mathematical framework (a theory of everything) with the other three fundamental interactions of physics, are a current area of research.
"
Gravitational_constant,Physics,2,"
 The gravitational constant (also known as the universal gravitational constant, the Newtonian constant of gravitation, or the Cavendish gravitational constant),[a] denoted by the letter G, is an empirical physical constant involved in the calculation of gravitational effects in Sir Isaac Newton's law of universal gravitation and in Albert Einstein's general theory of relativity.
 In Newton's law, it is the proportionality constant connecting the gravitational force between two bodies with the product of their masses and the inverse square of their distance. In the Einstein field equations, it quantifies the relation between the geometry of spacetime and the energy–momentum tensor (also referred to as the stress–energy tensor).
 The measured value of the constant is known with some certainty to four significant digits. In SI units, its value is approximately 6.674×10−11 m3⋅kg−1⋅s−2.[1] The modern notation of Newton's law involving G was introduced in the 1890s by C. V. Boys. The first implicit measurement with an accuracy within about 1% is attributed to Henry Cavendish in a 1798 experiment.[b]"
Gravitational_energy,Physics,2,"Gravitational energy or gravitational potential energy is the potential energy a massive object has in relation to another massive object due to gravity. It is the potential energy associated with the gravitational field, which is released (converted into kinetic energy) when the objects fall towards each other. Gravitational potential energy increases when two objects are brought further apart.
 For two pairwise interacting point particles, the gravitational potential energy 



U


{  U}
 is given by 
 where 



M


{  M}
 and 



m


{  m}
 are the masses of the two particles, 



R


{  R}
 is the distance between them, and 



G


{  G}
 is the gravitational constant.[1] Close to the Earth's surface, the gravitational field is approximately constant, and the gravitational potential energy of an object reduces to
 where 



m


{  m}
 is the object's mass, 



g
=
G

M

E



/


R

E


2




{  g=GM_{E}/R_{E}^{2}}
 is the gravity of Earth, and 



h


{  h}
 is the height of the object's center of mass above a chosen reference level.[1]"
Gravitational_field,Physics,2,"In physics, a gravitational field is a model used to explain the influence that a massive body extends into the space around itself, producing a force on another massive body.[1] Thus, a gravitational field is used to explain gravitational phenomena, and is measured in newtons per kilogram (N/kg). In its original concept, gravity was a force between point masses. Following Isaac Newton, Pierre-Simon Laplace attempted to model gravity as some kind of radiation field or fluid, and since the 19th century explanations for gravity have usually been taught in terms of a field model, rather than a point attraction.
 In a field model, rather than two particles attracting each other, the particles distort spacetime via their mass, and this distortion is what is perceived and measured as a ""force"".[citation needed]    In such a model one states that matter moves in certain ways in response to the curvature of spacetime,[2] and that there is either no gravitational force,[3] or that gravity is a fictitious force.[4] Gravity is distinguished from other forces by its obedience to the equivalence principle.
"
Gravitational_potential,Physics,2,"In classical mechanics, the gravitational potential at a location is equal to the work (energy transferred) per unit mass that would be needed to move an object to that location from a fixed reference location. It is analogous to the electric potential with mass playing the role of charge. The reference location, where the potential is zero, is by convention infinitely far away from any mass, resulting in a negative potential at any finite distance.
 In mathematics, the gravitational potential is also known as the Newtonian potential and is fundamental in the study of potential theory. It may also be used for solving the electrostatic and magnetostatic fields generated by uniformly charged or polarized ellipsoidal bodies.[1]"
Gravitational_wave,Physics,2,"Gravitational waves are disturbances in the curvature of spacetime, generated by accelerated masses, that propagate as waves outward from their source at the speed of light. They were proposed by Henri Poincaré in 1905[1] and subsequently predicted in 1916[2][3] by Albert Einstein on the basis of his general theory of relativity.[4][5] Gravitational waves transport energy as gravitational radiation, a form of radiant energy similar to electromagnetic radiation.[6] Newton's law of universal gravitation, part of classical mechanics, does not provide for their existence, since that law is predicated on the assumption that physical interactions propagate instantaneously (at infinite speed) –  showing one of the ways the methods of classical physics are unable to explain phenomena associated with relativity.
 Gravitational-wave astronomy is a branch of observational astronomy that uses gravitational waves to collect observational data about sources of detectable gravitational waves such as binary star systems composed of white dwarfs, neutron stars, and black holes; and events such as supernovae, and the formation of the early universe shortly after the Big Bang.
 In 1993, Russell A. Hulse and Joseph Hooton Taylor Jr. received the Nobel Prize in Physics for the discovery and observation of the Hulse–Taylor binary pulsar, which offered the first indirect evidence of the existence of gravitational waves.[7] On 11 February 2016, the LIGO-Virgo collaborations announced the first observation of gravitational waves, from a signal detected at 09:50:45 GMT on 14 September 2015[8] of two black holes with masses of 29 and 36 solar masses merging about 1.3 billion light-years away. During the final fraction of a second of the merger, it released more than 50 times the power of all the stars in the observable universe combined.[9] The signal increased in frequency from 35 to 250 Hz over 10 cycles (5 orbits) as it rose in strength for a period of 0.2 second.[10]  The mass of the new merged black hole was 62 solar masses. Energy equivalent to three solar masses was emitted as gravitational waves.[11] The signal was seen by both LIGO detectors in Livingston and Hanford, with a time difference of 7 milliseconds due to the angle between the two detectors and the source. The signal came from the Southern Celestial Hemisphere, in the rough direction of (but much farther away than) the Magellanic Clouds.[12] The confidence level of this being an observation of gravitational waves was 99.99994%.[11] In 2017, the Nobel Prize in Physics was awarded to Rainer Weiss, Kip Thorne and Barry Barish for their role in the direct detection of gravitational waves.[13][14][15]"
Graviton,Physics,2,"In theories of quantum gravity, the graviton is the hypothetical quantum of gravity, an elementary particle that mediates the force of gravity. There is no complete quantum field theory of gravitons due to an outstanding mathematical problem with renormalization in general relativity. In string theory, believed to be a consistent theory of quantum gravity, the graviton is a massless state of a fundamental string.
 If it exists, the graviton is expected to be massless because the gravitational force is very long range and appears to propagate at the speed of light. The graviton must be a spin-2 boson because the source of gravitation is the stress–energy tensor, a second-order tensor (compared with electromagnetism's spin-1 photon, the source of which is the four-current, a first-order tensor). Additionally, it can be shown that any massless spin-2 field would give rise to a force indistinguishable from gravitation, because a massless spin-2 field would couple to the stress–energy tensor in the same way that gravitational interactions do. This result suggests that, if a massless spin-2 particle is discovered, it must be the graviton.[5]"
Gravity,Physics,2,"
 Gravity (from Latin  gravitas 'weight'[1]), or gravitation, is a natural phenomenon by which all things with mass or energy—including planets, stars, galaxies, and even light[2]—are brought toward (or gravitate toward) one another.  On Earth, gravity gives weight to physical objects, and the Moon's gravity causes the ocean tides. The gravitational attraction of the original gaseous matter present in the Universe caused it to begin coalescing and forming stars and caused the stars to group together into galaxies, so gravity is responsible for many of the large-scale structures in the Universe. Gravity has an infinite range, although its effects become increasingly weaker as objects get further away.
 Gravity is most accurately described by the general theory of relativity (proposed by Albert Einstein in 1915), which describes gravity not as a force, but as a consequence of masses moving along geodesic lines in a curved spacetime caused by the uneven distribution of mass. The most extreme example of this curvature of spacetime is a black hole, from which nothing—not even light—can escape once past the black hole's event horizon.[3] However, for most applications, gravity is well approximated by Newton's law of universal gravitation, which describes gravity as a force causing any two bodies to be attracted toward each other, with magnitude proportional to the product of their masses and inversely proportional to the square of the distance between them.
 Gravity is the weakest of the four fundamental interactions of physics, approximately 1038 times weaker than the strong interaction, 1036 times weaker than the electromagnetic force and 1029 times weaker than the weak interaction. As a consequence, it has no significant influence at the level of subatomic particles.[4] In contrast, it is the dominant interaction at the macroscopic scale, and is the cause of the formation, shape and trajectory (orbit) of astronomical bodies.
 Current models of particle physics imply that the earliest instance of gravity in the Universe, possibly in the form of quantum gravity, supergravity or a gravitational singularity, along with ordinary space and time, developed during the Planck epoch (up to 10−43 seconds after the birth of the Universe), possibly from a primeval state, such as a false vacuum, quantum vacuum or virtual particle, in a currently unknown manner.[5] Attempts to develop a theory of gravity consistent with quantum mechanics, a quantum gravity theory, which would allow gravity to be united in a common mathematical framework (a theory of everything) with the other three fundamental interactions of physics, are a current area of research.
"
Ground_(electricity),Physics,2,"
 In electrical engineering, ground or earth is the reference point in an electrical circuit from which voltages are measured, a common return path for electric current, or a direct physical connection to the earth.
 Electrical circuits may be connected to ground (earth) for several reasons. Exposed metal parts of electrical equipment are connected to ground, so that failures of internal insulation will trigger protective mechanisms such as fuses or circuit breakers in the circuit to remove power from the device. This ensures that exposed parts can never have a dangerous voltage with respect to ground for more than the amount of time required for the fuse or circuit breaker to open the circuit; otherwise a grounded person who touched the parts could receive an electric shock. It is crucial to minimize the impedance of the equipment ground conductor so that fault current is maximized in a fault condition. This is because the larger the amount of fault current, the quicker the fault will be cleared by an inverse-time over-current device. In electric power distribution systems, a protective earth (PE) conductor is an essential part of the safety provided by the earthing system.
 Connection to ground also limits the build-up of static electricity when handling flammable products or electrostatic-sensitive devices. In some telegraph and power transmission circuits, the earth itself can be used as one conductor of the circuit, saving the cost of installing a separate return conductor (see single-wire earth return).
 For measurement purposes, the Earth serves as a (reasonably) constant potential reference against which other potentials can be measured. An electrical ground system should have an appropriate current-carrying capability to serve as an adequate zero-voltage reference level. In electronic circuit theory, a ""ground"" is usually idealized as an infinite source or sink for charge, which can absorb an unlimited amount of current without changing its potential. Where a real ground connection has a significant resistance, the approximation of zero potential is no longer valid. Stray voltages or earth potential rise effects will occur, which may create noise in signals or produce an electric shock hazard if large enough.
 The use of the term ground (or earth) is so common in electrical and electronics applications that circuits in portable electronic devices such as cell phones and media players as well as circuits in vehicles may be spoken of as having a ""ground"" connection without any actual connection to the Earth, despite ""common"" being a more appropriate term for such a connection. This is usually a large conductor attached to one side of the power supply (such as the ""ground plane"" on a printed circuit board) which serves as the common return path for current from many different components in the circuit.
"
Ground_reaction_force,Physics,2,"In physics, and in particular in biomechanics, the ground reaction force (GRF) is the force exerted by the ground on a body in contact with it.[1]
For example, a person standing motionless on the ground exerts a contact force on it (equal to the person's weight) and at the same time an equal and opposite ground reaction force is exerted by the ground on the person.
 In the above example, the ground reaction force coincides with the notion of a normal force. However, in a more general case, the GRF will also have a component parallel to the ground, for example when the person is walking – a motion that requires the exchange of horizontal (frictional) forces with the ground.[2] The use of the word reaction derives from Newton's third law, which essentially states that if a force, called action, acts upon a body, then an equal and opposite force, called reaction, must act upon another body. The force exerted by the ground is conventionally referred to as the reaction, although, since the distinction between action and reaction is completely arbitrary, the expression ground action would be, in principle, equally acceptable.
 The component of the GRF parallel to the surface is the frictional force. When slippage occurs the ratio of the magnitude of the frictional force to the normal force yields the coefficient of static friction.[3] GRF is often observed to evaluate force production in various groups within the community. One of these groups studied often are athletes to help evaluate a subject's ability to exert force and power. This can help create baseline parameters when creating strength and conditioning regimens from a rehabilitation and coaching standpoint. Plyometric jumps such as a drop-jump is an activity often used to build greater power and force which can lead to overall better ability on the playing field. When landing from a safe height in a bilateral comparisons on GRF in relation to landing with the dominant foot first followed by the non-dominant limb, literature has shown there were no significances in bilateral components with landing with the dominant foot first faster than the non-dominant foot on the GRF of the drop-jump or landing on vertical GRF output.[4]"
Ground_state,Physics,2,"The ground state of a quantum-mechanical system is its lowest-energy state; the energy of the ground state is known as the zero-point energy of the system. An excited state is any state with energy greater than the ground state. In quantum field theory, the ground state is usually called the vacuum state or the vacuum.
 If more than one ground state exists, they are said to be degenerate. Many systems have degenerate ground states. Degeneracy occurs whenever there exists a unitary operator that acts non-trivially on a ground state and commutes with the Hamiltonian of the system.
 According to the third law of thermodynamics, a system at absolute zero temperature exists in its ground state; thus, its entropy is determined by the degeneracy of the ground state. Many systems, such as a perfect crystal lattice, have a unique ground state and therefore have zero entropy at absolute zero. It is also possible for the highest excited state to have absolute zero temperature for systems that exhibit negative temperature.
"
Group_velocity,Physics,2,"The group velocity of a wave is the velocity with which the overall envelope shape of the wave's amplitudes—known as the modulation or envelope of the wave—propagates through space.
 For example, if a stone is thrown into the middle of a very still pond, a circular pattern of waves with a quiescent center appears in the water, also known as a capillary wave. The expanding ring of waves is the wave group, within which one can discern individual wavelets of differing wavelengths traveling at different speeds. The shorter waves travel faster than the group as a whole, but their amplitudes diminish as they approach the leading edge of the group. The longer waves travel more slowly, and their amplitudes diminish as they emerge from the trailing boundary of the group.
"
Hadron,Physics,2,"In particle physics, a hadron /ˈhædrɒn/ (listen) (Greek: ἁδρός, hadrós; ""stout, thick"") is a subatomic composite particle made of two or more quarks held together by the strong force in a similar way as molecules are held together by the electromagnetic force. Most of the mass of ordinary matter comes from two hadrons: the proton and the neutron.
 Hadrons are categorized into two families: baryons, made of an odd number of quarks – usually three quarks – and mesons, made of an even number of quarks—usually one quark and one antiquark.[1] Protons and neutrons (which make the majority of the mass of an atom) are examples of baryons; pions are an example of a meson. ""Exotic"" hadrons, containing more than three valence quarks, have been discovered in recent years. A tetraquark state (an exotic meson), named the Z(4430)−, was discovered in 2007 by the Belle Collaboration[2] and confirmed as a resonance in 2014 by the LHCb collaboration.[3] Two pentaquark states (exotic baryons), named P+c(4380) and P+c(4450), were discovered in 2015 by the LHCb collaboration.[4] There are several more exotic hadron candidates, and other colour-singlet quark combinations that may also exist.
 Almost all ""free"" hadrons and antihadrons (meaning, in isolation and not bound within an atomic nucleus) are believed to be unstable and eventually decay (break down) into other particles. The only known exception relates to free protons, which are possibly stable, or at least, take immense amounts of time to decay (order of 1034+ years). Free neutrons are unstable and decay with a half-life of about 611 seconds. Their respective antiparticles are expected to follow the same pattern, but they are difficult to capture and study, because they immediately annihilate on contact with ordinary matter. ""Bound"" protons and neutrons, contained within an atomic nucleus, are generally considered stable. Experimentally, hadron physics is studied by colliding protons or nuclei of heavy elements such as lead or gold, and detecting the debris in the produced particle showers. In the natural environment, mesons such as pions are produced by the collisions of cosmic rays with the atmosphere.
"
Half-life,Physics,2,"Half-life (symbol t1⁄2) is the time required for a quantity to reduce to half of its initial value. The term is commonly used in nuclear physics to describe how quickly unstable atoms undergo, or how long stable atoms survive, radioactive decay. The term is also used more generally to characterize any type of exponential or non-exponential decay. For example, the medical sciences refer to the biological half-life of drugs and other chemicals in the human body. The converse of half-life is doubling time.
 The original term, half-life period, dating to Ernest Rutherford's discovery of the principle in 1907, was shortened to half-life in the early 1950s.[1] Rutherford applied the principle of a radioactive element's half-life to studies of age determination of rocks by measuring the decay period of radium to lead-206.
 Half-life is constant over the lifetime of an exponentially decaying quantity, and it is a characteristic unit for the exponential decay equation. The accompanying table shows the reduction of a quantity as a function of the number of half-lives elapsed.
"
Hamilton%27s_principle,Physics,2,"In physics, Hamilton's principle is William Rowan Hamilton's formulation of the principle of stationary action. (See that article for historical formulations.) It states that the dynamics of a physical system are determined by a variational problem for a functional based on a single function, the Lagrangian, which contains all physical information concerning the system and the forces acting on it. The variational problem is equivalent to and allows for the derivation of the differential equations of motion of the physical system.  Although formulated originally for classical mechanics, Hamilton's principle also applies to classical fields such as the electromagnetic and gravitational fields, and plays an important role in quantum mechanics, quantum field theory and criticality theories.
"
Hamiltonian_mechanics,Physics,2,"Hamiltonian mechanics is a mathematically sophisticated formulation of classical mechanics. Historically, it contributed to the formulation of statistical mechanics and quantum mechanics. Hamiltonian mechanics was first formulated by William Rowan Hamilton in 1833, starting from Lagrangian mechanics, a previous reformulation of classical mechanics introduced by Joseph Louis Lagrange in 1788. Like Lagrangian mechanics, Hamiltonian mechanics is equivalent to Newton's laws of motion in the framework of classical mechanics.
"
Harmonic_mean,Physics,2,"In mathematics, the harmonic mean (sometimes called the subcontrary mean) is one of several kinds of average, and in particular, one of the Pythagorean means. Typically, it is appropriate for situations when the average of rates is desired.
 The harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals of the given set of observations. As a simple example, the harmonic mean of 1, 4, and 4 is
"
Heat,Physics,2,"
 In thermodynamics, heat is energy in transfer to or from a thermodynamic system, by mechanisms other than thermodynamic work or transfer of matter.[1][2][3][4][5][6][7] The various mechanisms of energy transfer that define heat are stated in the next section of this article.
 Like thermodynamic  work, heat transfer is a process involving more than one system, not a property of any one system. In thermodynamics, energy transferred as heat contributes to change in the system's cardinal energy variable of state, for example its internal energy, or for example its enthalpy. This is to be distinguished from the ordinary language conception of heat as a property of an isolated system.
 The quantity of energy transferred as heat in a process is the amount of transferred energy excluding any thermodynamic work that was done and any energy contained in matter transferred. For the precise definition of heat, it is necessary that it occur by a path that does not include transfer of matter.[8] Though not immediately by the definition, but in special kinds of process, quantity of energy transferred as heat can be measured by its effect on the states of interacting bodies. For example, respectively in special circumstances, heat transfer can be measured by the amount of ice melted, or by change in temperature of a body in the surroundings of the system.[9] Such methods are called calorimetry.
 The conventional symbol used to represent the amount of heat transferred in a thermodynamic process is Q. As an amount of energy (being transferred),  the SI unit of heat is the joule (J). 
"
Heat_transfer,Physics,2,"
 Heat transfer is a discipline of thermal engineering that concerns the generation, use, conversion, and exchange of thermal energy (heat) between physical systems. Heat transfer is classified into various mechanisms, such as thermal conduction, thermal convection, thermal radiation, and transfer of energy by phase changes. Engineers also consider the transfer of mass of differing chemical species, either cold or hot, to achieve heat transfer. While these mechanisms have distinct characteristics, they often occur simultaneously in the same system.
 Heat conduction, also called diffusion, is the direct microscopic exchange of kinetic energy of particles through the boundary between two systems. When an object is at a different temperature from another body or its surroundings, heat flows so that the body and the surroundings reach the same temperature, at which point they are in thermal equilibrium. Such spontaneous heat transfer always occurs from a region of high temperature to another region of lower temperature, as described in the second law of thermodynamics.
 Heat convection occurs when bulk flow of a fluid (gas or liquid) carries heat along with the flow of matter in the fluid. The flow of fluid may be forced by external processes, or sometimes (in gravitational fields) by buoyancy forces caused when thermal energy expands the fluid (for example in a fire plume), thus influencing its own transfer. The latter process is often called ""natural convection"". All convective processes also move heat partly by diffusion, as well. Another form of convection is forced convection. In this case the fluid is forced to flow by use of a pump, fan or other mechanical means.
 Thermal radiation occurs through a vacuum or any transparent medium (solid or fluid or gas). It is the transfer of energy by means of photons in electromagnetic waves governed by the same laws.[1]"
Helmholtz_free_energy,Physics,2,"In thermodynamics, the Helmholtz free energy is a thermodynamic potential that measures the useful work obtainable from a closed thermodynamic system at a constant temperature and volume (isothermal, isochoric). The negative of the change in the Helmholtz energy during a process is equal to the maximum amount of work that the system can perform in a thermodynamic process in which volume is held constant. If the volume were not held constant, part of this work would be performed as boundary work. This makes the Helmholtz energy useful for systems held at constant volume. Furthermore, at constant temperature, the Helmholtz free energy is minimized at equilibrium.
 In contrast, the Gibbs free energy or free enthalpy is most commonly used as a measure of thermodynamic potential (especially in chemistry) when it is convenient for applications that occur at constant pressure.  For example, in explosives research Helmholtz free energy is often used, since explosive reactions by their nature induce pressure changes. It is also frequently used to define fundamental equations of state of pure substances.
 The concept of free energy was developed by Hermann von Helmholtz, a German physicist, and first presented in 1882 in a lecture called ""On the thermodynamics of chemical processes"".[1] From the German word Arbeit (work), the International Union of Pure and Applied Chemistry (IUPAC) recommends the symbol A and the name Helmholtz energy.[2] In physics, the symbol F is also used in reference to free energy or Helmholtz function.
"
Henderson%E2%80%93Hasselbalch_equation,Physics,2,"In chemistry and biochemistry, the Henderson–Hasselbalch equation
 can be used to estimate the pH of a buffer solution. The numerical value of the acid dissociation constant, Ka, of the acid is known or assumed. The pH is calculated for given values of the concentrations of the acid, HA and of a salt, MA, of its conjugate base, A−; for example, the solution may contain acetic acid and sodium acetate.
"
Henry%27s_law,Physics,2,"In physical chemistry, Henry's law is a gas law that states that the amount of dissolved gas in a liquid is proportional to its partial pressure above the liquid. The proportionality factor is called Henry's law constant. It was formulated by the English chemist William Henry, who studied the topic in the early 19th century. In his publication about the quantity of gases absorbed by water,[1] he described the results of his experiments:
 … water takes up, of gas condensed by one, two, or more additional atmospheres, a quantity which, ordinarily compressed, would be equal to twice, thrice, &c. the volume absorbed under the common pressure of the atmosphere. An example where Henry's law is at play is in the depth-dependent dissolution of oxygen and nitrogen in the blood of underwater divers that changes during decompression, leading to decompression sickness. An everyday example is given by one's experience with carbonated soft drinks, which contain dissolved carbon dioxide. Before opening, the gas above the drink in its container is almost pure carbon dioxide, at a pressure higher than atmospheric pressure. After the bottle is opened, this gas escapes, moving the partial pressure of carbon dioxide above the liquid to be much lower, resulting in degassing as the dissolved carbon dioxide comes out of solution.
"
Hertz,Physics,2,"
 The hertz (symbol: Hz) is the derived unit of frequency in the International System of Units (SI) and is defined as one cycle per second.[1] It is named after Heinrich Rudolf Hertz, the first person to provide conclusive proof of the existence of electromagnetic waves. Hertz are commonly expressed in multiples: kilohertz (103 Hz, kHz), megahertz (106 Hz, MHz), gigahertz (109 Hz, GHz), terahertz (1012 Hz, THz), petahertz (1015 Hz, PHz),  exahertz (1018 Hz, EHz),
and zettahertz (1021 Hz, ZHz).
 Some of the unit's most common uses are in the description of sine waves and musical tones, particularly those used in radio- and audio-related applications. It is also used to describe the clock speeds at which computers and other electronics are driven. The units are sometimes also used as a representation of energy, via the photon energy equation (E=hν), with one hertz equivalent to h joules.
"
Higgs_boson,Physics,2,"
 The Higgs boson is an elementary particle in the Standard Model of particle physics, produced by the quantum excitation of the Higgs field,[8][9] one of the fields in particle physics theory.[9] It is named after physicist Peter Higgs, who in 1964, along with five other scientists, proposed the Higgs mechanism to explain why particles have mass. This mechanism implies the existence of the Higgs boson. The Higgs boson was initially discovered as a new particle in 2012 by the ATLAS and CMS collaborations based on collisions in the LHC at CERN, and the new particle was subsequently confirmed to match the expected properties of a Higgs boson over the following years.
 
 On 10 December 2013, two of the physicists, Peter Higgs and François Englert, were awarded the Nobel Prize in Physics for their theoretical predictions. Although Higgs's name has come to be associated with this theory (the Higgs mechanism), several researchers between about 1960 and 1972 independently developed different parts of it.
 In mainstream media the Higgs boson has often been called the ""God particle"", from a 1993 book on the topic,[10] although the nickname is strongly disliked by many physicists, including Higgs himself, who regard it as sensationalism.[11][12]"
Homeokinetics,Physics,2,"Homeokinetics is the study of self-organizing, complex systems.[1][2][3] Standard physics studies systems at separate levels, such as atomic physics, nuclear physics, biophysics, social physics, and galactic physics. Homeokinetic physics studies the up-down processes that bind these levels. Tools such as mechanics, quantum field theory, and the laws of thermodynamics provide the key relationships. The subject, described as the physics and thermodynamics associated with the up down movement between levels of systems, originated in the late 1970s work of American physicists Harry Soodak and Arthur Iberall. Complex systems are universes, galaxies, social systems, people, or even those that seem as simple as gases. The basic premise is that the entire universe consists of atomistic-like units bound in interactive ensembles to form systems, level by level, in a nested hierarchy. Homeokinetics treats all complex systems on an equal footing, animate and inanimate, providing them with a common viewpoint. The complexity in studying how they work is reduced by the emergence of common languages in all complex systems.[2]"
Horsepower,Physics,2,"Horsepower (hp) is a unit of measurement of power, or the rate at which work is done, usually in reference to the output of engines or motors. There are many different standards and types of horsepower. Two common definitions used today are the mechanical horsepower (or imperial horsepower), which is about 745.7 watts, and the metric horsepower, which is approximately 735.5 watts.
 The term was adopted in the late 18th century by Scottish engineer James Watt to compare the output of steam engines with the power of draft horses. It was later expanded to include the output power of other types of piston engines, as well as turbines, electric motors and other machinery.[1][2] The definition of the unit varied among geographical regions. Most countries now use the SI unit watt for measurement of power. With the implementation of the EU Directive 80/181/EEC on 1 January 2010, the use of horsepower in the EU is permitted only as a supplementary unit.[3]"
Huygens%E2%80%93Fresnel_principle,Physics,2,"The Huygens–Fresnel principle (named after Dutch physicist Christiaan Huygens and French physicist Augustin-Jean Fresnel) is a method of analysis applied to problems of wave propagation both in the far-field limit and in near-field diffraction and also reflection.  It states that every point on a wavefront is itself the source of spherical wavelets, and the secondary wavelets emanating from different points mutually interfere.[1] The sum of these spherical wavelets forms the wavefront.
"
Hydrostatics,Physics,2,"Fluid statics or hydrostatics is the branch of fluid mechanics that studies ""fluids at rest and the pressure in a fluid or exerted by a fluid on an immersed body"".[1] It encompasses the study of the conditions under which fluids are at rest in stable equilibrium as opposed to fluid dynamics, the study of fluids in motion. Hydrostatics are categorized as a part of the fluid statics, which is the study of all fluids, incompressible or not, at rest.
 Hydrostatics is fundamental to hydraulics, the engineering of equipment for storing, transporting and using fluids.  It is also relevant to geophysics and astrophysics (for example, in understanding plate tectonics and the anomalies of the Earth's gravitational field), to meteorology, to medicine (in the context of blood pressure), and many other fields.
 Hydrostatics offers physical explanations for many phenomena of everyday life, such as why atmospheric pressure changes with altitude, why wood and oil float on water, and why the surface of still water is always level.
"
Ice_point,Physics,2,"Melting, or fusion, is a physical process that results in the phase transition of a substance from a solid to a liquid. This occurs when the internal energy of the solid increases, typically by the application of heat or pressure, which increases the substance's temperature to the melting point. At the melting point, the ordering of ions or molecules in the solid breaks down to a less ordered state, and the solid melts to become a liquid.
 Substances in the molten state generally have reduced viscosity as  the temperature increases. An exception to this principle is the element sulfur, whose viscosity increases in the range of 160 °C to 180 °C due to polymerization.[1] Some organic compounds melt through mesophases, states of partial order between solid and liquid.
"
Electrical_impedance,Physics,2,"In electrical engineering, electrical impedance is the measure of the opposition that a circuit presents to a current when a voltage is applied.
 Quantitatively, the impedance of a two-terminal circuit element is the ratio of the complex representation of the sinusoidal voltage between its terminals, to the complex representation of the current flowing through it.[1] In general, it depends upon the frequency of the sinusoidal voltage. 
 Impedance extends the concept of resistance to alternating current (AC) circuits, and possesses both magnitude and phase, unlike resistance, which has only magnitude. When a circuit is driven with direct current (DC), there is no distinction between impedance and resistance; the latter can be thought of as impedance with zero phase angle.
 Impedance is a complex number, with the same units as resistance, for which the SI unit is the ohm (Ω).
Its symbol is usually Z, and it may be represented by writing its magnitude and phase in the polar form |Z|∠θ. However, cartesian complex number representation is often more powerful for circuit analysis purposes.
 The notion of impedance is useful for performing AC analysis of electrical networks, because it allows relating sinusoidal voltages and currents by a simple linear law.  
In multiple port networks, the two-terminal definition of impedance is inadequate, but the complex voltages at the ports and the currents flowing through them are still linearly related by the impedance matrix.[2] The reciprocal of impedance is admittance, whose SI unit is the siemens, formerly called mho.
 Instruments used to measure the electrical impedance are called impedance analyzers.
"
Indefinite_integral,Physics,2,"In calculus, an antiderivative, inverse derivative, primitive function, primitive integral or indefinite integral[Note 1] of a function f is a differentiable function F whose derivative is equal to the original function f. This can be stated symbolically as F'  = f.[1][2] The process of solving for antiderivatives is called antidifferentiation (or indefinite integration), and its opposite operation is called differentiation, which is the process of finding a derivative. Antiderivatives are often denoted by capital Roman letters such as F and G.[3] Antiderivatives are related to definite integrals through the fundamental theorem of calculus: the definite integral of a function over an interval is equal to the difference between the values of an antiderivative evaluated at the endpoints of the interval.
 In physics, antiderivatives arise in the context of rectilinear motion (e.g., in explaining the relationship between position, velocity and acceleration).[4] The discrete equivalent of the notion of antiderivative is antidifference.
"
Inductance,Physics,2,"In electromagnetism and electronics, inductance is the tendency of an electrical conductor to oppose a change in the electric current flowing through it.  The flow of electric current creates a magnetic field around the conductor. The field strength depends on the magnitude of the current, and follows any changes in current. From Faraday's law of induction, any change in magnetic field through a circuit induces an electromotive force (EMF) (voltage) in the conductors, a process known as electromagnetic induction. This induced voltage created by the changing current has the effect of opposing the change in current. This is stated by Lenz's law, and the voltage is called back EMF.
 Inductance is defined as the ratio of the induced voltage to the rate of change of current causing it.  It is a proportionality factor that depends on the geometry of circuit conductors and the magnetic permeability of nearby materials.[1]  An electronic component designed to add inductance to a circuit is called an inductor.  It typically consists of a coil or helix of wire.
 The term inductance was coined by Oliver Heaviside in 1886.[2] It is customary to use the symbol 



L


{  L}
 for inductance, in honour of the physicist Heinrich Lenz.[3][4] In the SI system, the unit of inductance is the henry (H), which is the amount of inductance that causes a voltage of one volt, when the current is changing at a rate of one ampere per second. It is named for Joseph Henry, who discovered inductance independently of Faraday.[5]"
Infrasound,Physics,2,"
 Infrasound, sometimes referred to as low-frequency sound, describes sound waves with a frequency below the lower limit of audibility (generally 20 Hz). Hearing becomes gradually less sensitive as frequency decreases, so for humans to perceive infrasound, the sound pressure must be sufficiently high. The ear is the primary organ for sensing low sound, but at higher intensities it is possible to feel infrasound vibrations in various parts of the body.
 The study of such sound waves is sometimes referred to as infrasonics, covering sounds beneath 20 Hz down to 0.1 Hz. and rarely to 0.001 Hz. People use this frequency range for monitoring earthquakes and volcanoes, charting rock and petroleum formations below the earth, and also in ballistocardiography and seismocardiography to study the mechanics of the heart.
 Infrasound is characterized by an ability to get around obstacles with little dissipation. In music, acoustic waveguide methods, such as a large pipe organ or, for reproduction, exotic loudspeaker designs such as transmission line, rotary woofer, or traditional subwoofer designs can produce low-frequency sounds, including near-infrasound. Subwoofers designed to produce infrasound are capable of sound reproduction an octave or more below that of most commercially available subwoofers, and are often about 10 times the size.[citation needed]"
Inertia,Physics,2,"Inertia is the resistance of any physical object to any change in its velocity. This includes changes to the object's speed, or direction of motion. 
An aspect of this property is the tendency of objects to keep moving in a straight line at a constant speed, when no forces act upon them.
 Inertia comes from the Latin word, iners, meaning idle, sluggish. Inertia is one of the primary manifestations of mass, which is a quantitative property of physical systems. Isaac Newton defined inertia as his first law in his Philosophiæ Naturalis Principia Mathematica, which states:
 The vis insita, or innate force of matter, is a power of resisting by which every body, as much as in it lies, endeavours to preserve its present state, whether it be of rest or of moving uniformly forward in a straight line.[1] In common usage, the term ""inertia"" may refer to an object's ""amount of resistance to change in velocity"" or for simpler terms, ""resistance to a change in motion"" (which is quantified by its mass), or sometimes to its momentum, depending on the context.  The term ""inertia"" is more properly understood as shorthand for ""the principle of inertia"" as described by Newton in his first law of motion: an object not subject to any net external force moves at a constant velocity. Thus, an object will continue moving at its current velocity until some force causes its speed or direction to change.
 On the surface of the Earth, inertia is often masked by gravity and the effects of friction and air resistance, both of which tend to decrease the speed of moving objects (commonly to the point of rest). This misled the philosopher Aristotle to believe that objects would move only as long as force was applied to them.[2][3] The principle of inertia is one of the fundamental principles in classical physics that are still used today to describe the motion of objects and how they are affected by the applied forces on them.
"
Electrical_reactance#Inductive_reactance,Physics,2,"In electric and electronic systems, reactance is the opposition of a circuit element to the flow of current due to that element's inductance or capacitance. Greater reactance leads to smaller currents for the same voltage applied. Reactance is similar to electric resistance in this respect, but differs in that reactance does not lead to dissipation of electrical energy as heat.  Instead, energy is stored in the reactance, and later returned to the circuit whereas a resistance continuously loses energy.
 Reactance is used to compute amplitude and phase changes of sinusoidal alternating current (AC) going through a circuit element. It is denoted by the symbol 





X




{  \scriptstyle {X}}
. An ideal resistor has zero reactance, whereas ideal inductors and capacitors have zero resistance – that is, respond to current only by reactance. As frequency increases, inductive reactance also increases and capacitive reactance decreases.
"
Integral,Physics,2,"In mathematics, an integral assigns numbers to functions in a way that can describe displacement, area, volume, and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus; its inverse operation, differentiation, is the other. Given a function f of a real variable x and an interval [a, b] of the real line, the definite integral of f from a to b can be interpreted informally as the signed area of the region in the xy-plane that is bounded by the graph of f, the x-axis and the vertical lines x = a and x = b. It is denoted
 The operation of integration, up to an additive constant, is the inverse of the operation of differentiation. For this reason, the term integral may also refer to the related notion of the antiderivative, called an indefinite integral, a function F whose derivative is the given function f. In this case, it is written:
 The integrals discussed in this article are those termed definite integrals. It is the fundamental theorem of calculus that connects differentiation with the definite integral: if f is a continuous real-valued function defined on a closed interval [a, b], then once an antiderivative F of f is known, the definite integral of f over that interval is given by
 The principles of integration were formulated independently by Isaac Newton and Gottfried Wilhelm Leibniz in the late 17th century, who thought of the integral as an infinite sum of rectangles of infinitesimal width. Bernhard Riemann later gave a rigorous mathematical definition of integrals, which is based on a limiting procedure that approximates the area of a curvilinear region by breaking the region into thin vertical slabs. Beginning in the 19th century, more sophisticated notions of integrals began to appear, where the type of the function as well as the domain over which the integration is performed has been generalized. A line integral is defined for functions of two or more variables, and the interval of integration [a, b] is replaced by a curve connecting the two endpoints. In a surface integral, the curve is replaced by a piece of a surface in three-dimensional space.
"
Integral_transform,Physics,2,"In mathematics, an integral transform maps a function from its original function space into another function space via integration, where some of the properties of the original function might be more easily characterized and manipulated than in the original function space. The transformed function can generally be mapped back to the original function space using the inverse transform.
"
International_System_of_Units,Physics,2,"
 The  International System of Units (SI, abbreviated from the French Système international (d'unités)) is the modern form of the metric system. It is the only system of measurement with an official status in nearly every country in the world. It comprises a coherent system of units of measurement starting with seven base units, which are the second (the unit of time with the symbol s), metre (length, m), kilogram (mass, kg), ampere (electric current, A), kelvin (thermodynamic temperature, K), mole (amount of substance, mol), and candela (luminous intensity, cd). The system allows for an unlimited number of additional units, called derived units, which can always be represented as products of powers of the base units.[Note 1] Twenty-two derived units have been provided with special names and symbols.[Note 2] The seven base units and the 22 derived units with special names and symbols may be used in combination to express other derived units,[Note 3] which are adopted to facilitate measurement of diverse quantities. The SI system also provides twenty prefixes to the unit names and unit symbols that may be used when specifying power-of-ten (i.e. decimal) multiples and sub-multiples of SI units. The SI is intended to be an evolving system; units and prefixes are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves.
 Since 2019, the magnitudes of all SI units have been defined by declaring exact numerical values for seven defining constants when expressed in terms of their SI units. These defining constants are the speed of light in vacuum, c, the hyperfine transition frequency of caesium ΔνCs, the Planck constant h, the elementary charge e, the Boltzmann constant k, the Avogadro constant NA, and the luminous efficacy Kcd. The nature of the defining constants ranges from fundamental constants of nature such as c to the purely technical constant Kcd. Prior to 2019, h, e, k, and NA were not defined a priori but were rather very precisely measured quantities. In 2019, their values were fixed by definition to their best estimates at the time, ensuring continuity with previous definitions of the base units. One consequence of the redefinition of the SI is that the distinction between the base units and derived units is in principle not needed, since any unit can be constructed directly from the seven defining constants.[2]:129 The current way of defining the SI system is a result of a decades-long move towards increasingly abstract and idealised formulation in which the realisations of the units are separated conceptually from the definitions. A consequence is that as science and technologies develop, new and superior realisations may be introduced without the need to redefine the unit.  One problem with artefacts is that they can be lost, damaged, or changed; another is that they introduce uncertainties that cannot be reduced by advancements in science and technology. The last artefact used by the SI was the International Prototype of the Kilogram, a cylinder of platinum-iridium.
 The original motivation for the development of the SI was the diversity of units that had sprung up within the centimetre–gram–second (CGS) systems (specifically the inconsistency between the systems of electrostatic units and electromagnetic units) and the lack of coordination between the various disciplines that used them. The General Conference on Weights and Measures (French: Conférence générale des poids et mesures – CGPM), which was established by the Metre Convention of 1875, brought together many international organisations to establish the definitions and standards of a new system and to standardise the rules for writing and presenting measurements. The system was published in 1960 as a result of an initiative that began in 1948,so is based on the metre–kilogram–second system of units (MKS) rather than any variant of the CGS.
"
Invariant_mass,Physics,2,"The invariant mass, rest mass, intrinsic mass, proper mass, or in the case of bound systems simply mass, is the portion of the total mass of an object or system of objects that is independent of the overall motion of the system.  More precisely, it is a characteristic of the system's total energy and momentum that is the same in all frames of reference related by Lorentz transformations.[1] If a center-of-momentum frame exists for the system, then the invariant mass of a system is equal to its total mass in that ""rest frame"". In other reference frames, where the system's momentum is nonzero, the total mass (a.k.a. relativistic mass) of the system is greater than the invariant mass, but the invariant mass remains unchanged.
 Due to mass–energy equivalence, the rest energy of the system is simply the invariant mass times the speed of light squared. Similarly, the total energy of the system is its total (relativistic) mass times the speed of light squared.
 Systems whose four-momentum is a null vector (for example a single photon or many photons moving in exactly the same direction) have zero invariant mass, and are referred to as massless. A physical object or particle moving faster than the speed of light would have space-like four-momenta (such as the hypothesized tachyon), and these do not appear to exist. Any time-like four-momentum possesses a reference frame where the momentum (3-dimensional) is zero, which is a center of momentum frame. In this case, invariant mass is positive and is referred to as the rest mass.
 If objects within a system are in relative motion, then the invariant mass of the whole system will differ from the sum of the objects' rest masses. This is also equal to the total energy of the system divided by c2. See mass–energy equivalence for a discussion of definitions of mass. Since the mass of systems must be measured with a weight or mass scale in a center of momentum frame in which the entire system has zero momentum, such a scale always measures the system's invariant mass. For example, a scale would measure the kinetic energy of the molecules in a bottle of gas to be part of invariant mass of the bottle, and thus also its rest mass. The same is true for massless particles in such system, which add invariant mass and also rest mass to systems, according to their energy.
 For an isolated massive system, the center of mass of the system moves in a straight line with a steady sub-luminal velocity (with a velocity depending on the reference frame used to view it). Thus, an observer can always be placed to move along with it. In this frame, which is the center-of-momentum frame, the total momentum is zero, and the system as a whole may be thought of as being ""at rest"" if it is a bound system (like a bottle of gas). In this frame, which exists under these assumptions, the invariant mass of the system is equal to the total system energy (in the zero-momentum frame) divided by c2. This total energy in the center of momentum frame, is the minimum energy which the system may be observed to have, when seen by various observers from various inertial frames.
 Note that for reasons above, such a rest frame does not exist for single photons, or rays of light moving in one direction. When two or more photons move in different directions, however, a center of mass frame (or ""rest frame"" if the system is bound) exists. Thus, the mass of a system of several photons moving in different directions is positive, which means that an invariant mass exists for this system even though it does not exist for each photon.
"
Ion,Physics,2,"
 An ion (/ˈaɪɒn, -ən/)[1] is an particle,atom or molecule with a net electrical charge. 
 The charge of the electron is considered negative by convention.  The negative charge of an ion is equal and opposite to charged proton(s) considered positive by convention.  The net charge of an ion is non-zero due to its total number of electrons being unequal to its total number of protons. 
 A cation is a positively charged ion, with fewer electrons than protons, while an anion is negatively charged, with more electrons than protons. Because of their opposite electric charges, cations and anions attract each other and readily form ionic compounds.
 Ions consisting of only a single atom are termed atomic or monatomic ions, while two or more atoms form molecular ions or polyatomic ions. In the case of physical ionization in a fluid (gas or liquid), ""ion pairs"" are created by spontaneous molecule collisions, where each generated pair consists of a free electron and a positive ion.[2] Ions are also created by chemical interactions, such as the dissolution of a salt in liquids, or by other means, such as passing a direct current through a conducting solution, dissolving an anode via ionization.
"
Ionic_bond,Physics,2,"
 Ionic bonding is a type of chemical bonding that involves the electrostatic attraction between oppositely charged ions[citation needed], or between two atoms with sharply different electronegativities,[1] and is the primary interaction occurring in ionic compounds. It is one of the main types of bonding along with covalent bonding and metallic bonding. Ions are atoms (or groups of atoms) with an electrostatic charge. Atoms that gain electrons make negatively charged ions (called anions). Atoms that lose electrons make positively charged ions (called cations). This transfer of electrons is known as electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complex nature, e.g. molecular ions like NH+4 or SO2−4. In simpler words, an ionic bond results from the transfer of electrons from a metal to a non-metal in order to obtain a full valence shell for both atoms.
 It is important to recognize that clean ionic bonding — in which one atom or molecule completely transfers an electron to another — cannot exist: all ionic compounds have some degree of covalent bonding, or electron sharing. Thus, the term ""ionic bonding"" is given when the ionic character is greater than the covalent character – that is, a bond in which a large electronegativity difference exists between the two atoms, causing the bonding to be more polar (ionic) than in covalent bonding where electrons are shared more equally. Bonds with partially ionic and partially covalent character are called polar covalent bonds. 
 Ionic compounds conduct electricity when molten or in solution, typically not when solid. Ionic compounds generally have a high melting point, depending on the charge of the ions they consist of. The higher the charges the stronger the cohesive forces and the higher the melting point. They also tend to be soluble in water; the stronger the cohesive forces, the lower the solubility.[2]"
Ionization,Physics,2,"Ionization or ionisation is the process by which an atom or a molecule acquires a negative or positive charge by gaining or losing electrons, often in conjunction with other chemical changes. The resulting electrically charged atom or molecule is called an ion. Ionization can result from the loss of an electron after collisions with subatomic particles, collisions with other atoms, molecules and ions, or through the interaction with electromagnetic radiation. Heterolytic bond cleavage and heterolytic substitution reactions can result in the formation of ion pairs. Ionization can occur through radioactive decay by the internal conversion process, in which an excited nucleus transfers its energy to one of the inner-shell electrons causing it to be ejected.
"
Ionization_chamber,Physics,2,"The ionization chamber is the simplest of all gas-filled radiation detectors, and is widely used for the detection and measurement of certain types of ionizing radiation; X-rays, gamma rays, and beta particles. Conventionally, the term ""ionization chamber"" is used exclusively to describe those detectors which collect all the charges created by direct ionization within the gas through the application of an electric field.[1] It only uses the discrete charges created by each interaction between the incident radiation and the gas, and does not involve the gas multiplication mechanisms used by other radiation instruments, such as the Geiger counter or the proportional counter.
 Ion chambers have a good uniform response to radiation over a wide range of energies and are the preferred means of measuring high levels of gamma radiation. They are widely used in the nuclear power industry, research labs, radiography, radiobiology, and environmental monitoring.
"
Ionizing_radiation,Physics,2,"Ionizing radiation (ionising radiation) is radiation, traveling as a particle or electromagnetic wave, that carries sufficient energy to detach electrons from atoms or molecules, thereby ionizing an atom or a molecule.[1] Ionizing radiation is made up of energetic subatomic particles, ions or atoms moving at high speeds (usually greater than 1% of the speed of light), and electromagnetic waves on the high-energy end of the electromagnetic spectrum.[citation needed] Gamma rays, X-rays, and the higher ultraviolet part of the electromagnetic spectrum are ionizing, whereas the lower ultraviolet part of the electromagnetic spectrum and all the spectrum below UV, including visible light, nearly all types of laser light, infrared, microwaves, and radio waves are considered non-ionizing radiation. The boundary between ionizing and non-ionizing electromagnetic radiation that occurs in the ultraviolet is not sharply defined, since different molecules and atoms ionize at different energies. Conventional definition places the boundary at a photon energy between 10 eV and 33 eV in the ultraviolet (see definition boundary section below).
 Typical ionizing subatomic particles found in radioactive decay include alpha particles, beta particles and neutrons. Almost all products of radioactive decay are ionizing because the energy of radioactive decay is typically far higher than that required to ionize. Other subatomic ionizing particles which occur naturally are muons, mesons, positrons, and other particles that constitute the secondary cosmic particles that are produced after primary cosmic rays interact with Earth's atmosphere.[2][3] Cosmic rays are generated by stars and certain celestial events such as supernova explosions. Cosmic rays may also produce radioisotopes on Earth (for example, carbon-14), which in turn decay and produce ionizing radiation. Cosmic rays and the decay of radioactive isotopes are the primary sources of natural ionizing radiation on Earth referred to as background radiation. Ionizing radiation can also be generated artificially by X-ray tubes, particle accelerators, and any of the various methods that produce radioisotopes artificially.
 Ionizing radiation is not detectable by human senses, so radiation detection instruments such as Geiger counters must be used to indicate its presence and measure it. However, high intensities can cause emission of visible light upon interaction with matter, such as in Cherenkov radiation and radioluminescence. Ionizing radiation is used in a wide variety of fields such as medicine, nuclear power, research, manufacturing, construction, and many other areas, but presents a health hazard if proper measures against undesired exposure are not followed. Exposure to ionizing radiation causes damage to living tissue and can result in radiation burns, cell damage, radiation sickness, cancer, and death.[4]"
Isotope,Physics,2,"
 Isotopes are variants of a particular chemical element which differ in neutron number, and consequently in nucleon number. All isotopes of a given element have the same number of protons but different numbers of neutrons in each atom.[1] The term isotope is formed from the Greek roots isos (ἴσος ""equal"") and topos (τόπος ""place""), meaning ""the same place""; thus, the meaning behind the name is that different isotopes of a single element occupy the same position on the periodic table.[2] It was coined by Scottish doctor and writer Margaret Todd in 1913 in a suggestion to chemist Frederick Soddy.
 The number of protons within the atom's nucleus is called atomic number and is equal to the number of electrons in the neutral (non-ionized) atom. Each atomic number identifies a specific element, but not the isotope; an atom of a given element may have a wide range in its number of neutrons. The number of nucleons (both protons and neutrons) in the nucleus is the atom's mass number, and each isotope of a given element has a different mass number.
 For example, carbon-12, carbon-13, and carbon-14 are three isotopes of the element carbon with mass numbers 12, 13, and 14, respectively. The atomic number of carbon is 6, which means that every carbon atom has 6 protons, so that the neutron numbers of these isotopes are 6, 7, and 8 respectively.
"
Josephson_effect,Physics,2,"The Josephson effect is the phenomenon of supercurrent, a current that flows indefinitely long without any voltage applied, across a device known as a Josephson junction (JJ), which consists of two or more superconductors coupled by a weak link. The weak link can consist of a thin insulating barrier (known as a superconductor–insulator–superconductor junction, or S-I-S), a short section of non-superconducting metal (S-N-S), or a physical constriction that weakens the superconductivity at the point of contact (S-s-S).
 The Josephson effect is an example of a macroscopic quantum phenomenon. It is named after the British physicist Brian David Josephson, who predicted in 1962 the mathematical relationships for the current and voltage across the weak link.[1][2] The DC Josephson effect had been seen in experiments prior to 1962,[3] but had been attributed to ""super-shorts"" or breaches in the insulating barrier leading to the direct conduction of electrons between the superconductors. The first paper to claim the discovery of Josephson's effect, and to make the requisite experimental checks, was that of Philip Anderson and John Rowell.[4] These authors were awarded  patents on the effects that were never enforced, but never challenged.
 Before Josephson's prediction, it was only known that normal (i.e. non-superconducting) electrons can flow through an insulating barrier, by means of quantum tunneling. Josephson was the first to predict the tunneling of superconducting Cooper pairs. For this work, Josephson received the Nobel Prize in Physics in 1973.[5] Josephson junctions have important applications in quantum-mechanical circuits, such as SQUIDs, superconducting qubits, and RSFQ digital electronics. The NIST standard for one volt is achieved by an array of 20,208 Josephson junctions in series.[6]"
Joule,Physics,2,"The joule (/dʒaʊl, dʒuːl/ jowl, jool;[1][2][3] symbol: J) is a derived unit of energy in the International System of Units.[4] It is equal to the energy transferred to (or work done on) an object when a force of one newton acts on that object in the direction of the force's motion through a distance of one metre (1 newton metre or N⋅m). It is also the energy dissipated as heat when an electric current of one ampere passes through a resistance of one ohm for one second. It is named after the English physicist James Prescott Joule (1818–1889).[5][6][7]"
Kelvin,Physics,2,"
 The kelvin is the base unit of temperature in the International System of Units (SI), having the unit symbol K. It is named after the Belfast-born Glasgow University engineer and physicist William Thomson, 1st Baron Kelvin (1824–1907).
 The kelvin is now defined by fixing the numerical value of the Boltzmann constant k to 1.380 649×10−23 J⋅K−1. This unit is equal to kg⋅m2⋅s−2⋅K−1, where the kilogram, metre and second are defined in terms of the Planck constant, the speed of light, and the duration of the caesium-133 ground-state hyperfine transition respectively.[1] Thus, this definition depends only on universal constants, and not on any physical artifacts as practiced previously, such as the International Prototype of the Kilogram, whose mass diverged over time from the original value.
 One kelvin is equal to a change in the thermodynamic temperature T that results in a change of thermal energy kT by 1.380 649×10−23 J.[2] The Kelvin scale fulfills Thomson's requirements as an absolute thermodynamic temperature scale. It uses absolute zero as its null point.
 Unlike the degree Fahrenheit and degree Celsius, the kelvin is not referred to or written as a degree. The kelvin is the primary unit of temperature measurement in the physical sciences, but is often used in conjunction with the degree Celsius, which has the same magnitude.
"
Kinematics,Physics,2,"Kinematics is a subfield of physics, developed in classical mechanics, that describes the motion of points, bodies (objects), and systems of bodies (groups of objects) without considering the forces that cause them to move.[1][2][3] Kinematics, as a field of study, is often referred to as the ""geometry of motion"" and is occasionally seen as a branch of mathematics.[4][5][6] A kinematics problem begins by describing the geometry of the system and declaring the initial conditions of any known values of position, velocity and/or acceleration of points within the system. Then, using arguments from geometry, the position, velocity and acceleration of any unknown parts of the system can be determined.  The study of how forces act on bodies falls within kinetics, not kinematics. For further details, see analytical dynamics.
 Kinematics is used in astrophysics to describe the motion of celestial bodies and collections of such bodies. In mechanical engineering, robotics, and biomechanics[7] kinematics is used to describe the motion of systems composed of joined parts (multi-link systems) such as an engine, a robotic arm or the human skeleton.
 Geometric transformations, also called rigid transformations, are used to describe the movement of components in a mechanical system, simplifying the derivation of the equations of motion. They are also central to dynamic analysis.
 Kinematic analysis is the process of measuring the kinematic quantities used to describe motion.  In engineering, for instance, kinematic analysis may be used to find the range of movement for a given mechanism and working in reverse, using kinematic synthesis to design a mechanism for a desired range of motion.[8]  In addition, kinematics applies algebraic geometry to the study of the mechanical advantage of a mechanical system or mechanism.
"
Kinetic_energy,Physics,2,"
 In physics, the kinetic energy (KE) of an object is the energy that it possesses due to its motion.[1]
It is defined as the work needed to accelerate a body of a given mass from rest to its stated velocity. Having gained this energy during its acceleration, the body maintains this kinetic energy unless its speed changes. The same amount of work is done by the body when decelerating from its current speed to a state of rest.
 In classical mechanics, the kinetic energy of a non-rotating object of mass m traveling at a speed v is 










1
2


m

v

2









{  {\begin{smallmatrix}{\frac {1}{2}}mv^{2}\end{smallmatrix}}}
. In relativistic mechanics, this is a good approximation only when v is much less than the speed of light.
 The standard unit of kinetic energy is the joule, while the imperial unit of kinetic energy is the foot-pound.
"
Kirchhoff%27s_circuit_laws,Physics,2,"Kirchhoff's circuit laws are two equalities that deal with the current and potential difference (commonly known as voltage) in the lumped element model of electrical circuits. They were first described in 1845 by German physicist Gustav Kirchhoff.[1] This generalized the work of Georg Ohm and preceded the work of James Clerk Maxwell. Widely used in electrical engineering, they are also called Kirchhoff's rules or simply Kirchhoff's laws. These laws can be applied in time and frequency domains and form the basis for network analysis.
 Both of Kirchhoff's laws can be understood as corollaries of Maxwell's equations in the low-frequency limit. They are accurate for DC circuits, and for AC circuits at frequencies where the wavelengths of electromagnetic radiation are very large compared to the circuits.
"
Kirchhoff%27s_equations,Physics,2,"In fluid dynamics, the Kirchhoff equations, named after Gustav Kirchhoff, describe the motion of a rigid body in an ideal fluid.
 where 






ω
→





{  {\vec {\omega }}}
 and 






v
→





{  {\vec {v}}}
 are the angular and linear velocity vectors at the point 






x
→





{  {\vec {x}}}
, respectively; 






I
~





{  {\tilde {I}}}
 is the moment of inertia tensor, 



m


{  m}
 is the body's mass; 






n
^





{  {\hat {n}}}
 is
a unit normal to the surface of the body at the point 






x
→





{  {\vec {x}}}
;




p


{  p}
 is a pressure at this point; 







Q
→




h




{  {\vec {Q}}_{h}}
 and 







F
→




h




{  {\vec {F}}_{h}}
 are the hydrodynamic
torque and force acting on the body, respectively;







Q
→





{  {\vec {Q}}}
 and 






F
→





{  {\vec {F}}}
 likewise denote all other torques and forces acting on the
body. The integration is performed over the fluid-exposed portion of the
body's surface.
 If the body is completely submerged body in an infinitely large volume of irrotational, incompressible, inviscid fluid, that is at rest at infinity, then the vectors 







Q
→




h




{  {\vec {Q}}_{h}}
 and 







F
→




h




{  {\vec {F}}_{h}}
 can be found via explicit integration, and the dynamics of the body is described by the Kirchhoff – Clebsch equations:
 Their first integrals read
 Further integration produces explicit expressions for position and velocities. 
"
Lagrangian_mechanics,Physics,2,"
 Lagrangian mechanics is a reformulation of classical mechanics, introduced by the Italian-French mathematician and astronomer Joseph-Louis Lagrange in 1788.
 In Lagrangian mechanics, the trajectory of a system of particles is derived by solving the Lagrange equations in one of two forms: either the Lagrange equations of the first kind,[1] which treat constraints explicitly as extra equations, often using Lagrange multipliers;[2][3] or the Lagrange equations of the second kind, which incorporate the constraints directly by judicious choice of generalized coordinates.[1][4] In each case, a mathematical function called the Lagrangian is a function of the generalized coordinates, their time derivatives, and time, and contains the information about the dynamics of the system.
 No new physics is necessarily introduced in applying Lagrangian mechanics compared to Newtonian mechanics. It is, however, more mathematically sophisticated and systematic. Newton's laws can include non-conservative forces like friction; however, they must include constraint forces explicitly and are best suited to Cartesian coordinates. Lagrangian mechanics is ideal for systems with conservative forces and for bypassing constraint forces in any coordinate system. Dissipative and driven forces can be accounted for by splitting the external forces into a sum of potential and non-potential forces, leading to a set of modified Euler–Lagrange (EL) equations.[5] Generalized coordinates can be chosen for convenience, to exploit symmetries in the system or the geometry of the constraints, which may simplify solving for the motion of the system. Lagrangian mechanics also reveals conserved quantities and their symmetries in a direct way, as a special case of Noether's theorem.
 Lagrangian mechanics is important not just for its broad applications, but also for its role in advancing deep understanding of physics. Although Lagrange only sought to describe classical mechanics in his treatise Mécanique analytique,[6][7] William Rowan Hamilton later developed Hamilton's principle that can be used to derive the Lagrange equation and was later recognized to be applicable to much of fundamental theoretical physics as well, particularly quantum mechanics and the theory of relativity. It can also be applied to other systems by analogy, for instance to coupled electric circuits with inductances and capacitances.[8] Lagrangian mechanics is widely used to solve mechanical problems in physics and when Newton's formulation of classical mechanics is not convenient. Lagrangian mechanics applies to the dynamics of particles, while fields are described using a Lagrangian density. Lagrange's equations are also used in optimization problems of dynamic systems. In mechanics, Lagrange's equations of the second kind are used much more than those of the first kind.
"
Laminar_flow,Physics,2,"In fluid dynamics, laminar flow is characterized by fluid particles following smooth paths in layers, with each layer moving smoothly past the adjacent layers with little or no mixing.[1] At low velocities, the fluid tends to flow without lateral mixing, and adjacent layers slide past one another like playing cards. There are no cross-currents perpendicular to the direction of flow, nor eddies or swirls of fluids.[2] In laminar flow, the motion of the particles of the fluid is very orderly with particles close to a solid surface moving in straight lines parallel to that surface.[3]
Laminar flow is a flow regime characterized by high momentum diffusion and low momentum convection.
 When a fluid is flowing through a closed channel such as a pipe or between two flat plates, either of two types of flow may occur depending on the velocity and viscosity of the fluid: laminar flow or turbulent flow. Laminar flow occurs at lower velocities, below a threshold at which the flow becomes turbulent.  The velocity is determined by a dimensionless parameter characterizing the flow called the Reynolds number, which also depends on the viscosity and density of the fluid and dimensions of the channel.  Turbulent flow is a less orderly flow regime that is characterized by eddies or small packets of fluid particles, which result in lateral mixing.[2] In non-scientific terms, laminar flow is smooth, while turbulent flow is rough.
"
Laplace_transform,Physics,2,"In mathematics, the Laplace transform, named after its inventor Pierre-Simon Laplace (/ləˈplɑːs/), is an integral transform that converts a function of a real variable 



t


{  t}
 (often time) to a function of a complex variable 



s


{  s}
 (complex frequency). The transform has many applications in science and engineering because it is a tool for solving differential equations. In particular, it transforms differential equations into algebraic equations and convolution into multiplication.[1][2][3]"
Laplace%E2%80%93Runge%E2%80%93Lenz_vector,Physics,2,"In classical mechanics, the Laplace–Runge–Lenz (LRL) vector is a vector used chiefly to describe the shape and orientation of the orbit of one astronomical body around another, such as a planet revolving around a star.  For two bodies interacting by Newtonian gravity, the LRL vector is a constant of motion, meaning that it is the same no matter where it is calculated on the orbit;[1] equivalently, the LRL vector is said to be conserved.   More generally, the LRL vector is conserved in all problems in which two bodies interact by a central force that varies as the inverse square of the distance between them; such problems are called Kepler problems.[2] The hydrogen atom is a Kepler problem, since it comprises two charged particles interacting by Coulomb's law of electrostatics, another inverse square central force.  The LRL vector was essential in the first quantum mechanical derivation of the spectrum of the hydrogen atom,[3] before the development of the Schrödinger equation.  However, this approach is rarely used today.
 In classical and quantum mechanics, conserved quantities generally correspond to a symmetry of the system.  The conservation of the LRL vector corresponds to an unusual symmetry; the Kepler problem is mathematically equivalent to a particle moving freely on the surface of a four-dimensional (hyper-)sphere,[4] so that the whole problem is symmetric under certain rotations of the four-dimensional space.[5]  This higher symmetry results from two properties of the Kepler problem: the velocity vector always moves in a perfect circle and, for a given total energy, all such velocity circles intersect each other in the same two points.[6] The Laplace–Runge–Lenz vector is named after Pierre-Simon de Laplace, Carl Runge and Wilhelm Lenz. It is also known as the Laplace vector, the Runge–Lenz vector and the Lenz vector. Ironically, none of those scientists discovered it.  The LRL vector has been re-discovered several times[7] and is also equivalent to the dimensionless eccentricity vector of celestial mechanics.[8]  Various generalizations of the LRL vector have been defined, which incorporate the effects of special relativity, electromagnetic fields and even different types of central forces.
"
Laser,Physics,2,"
 
 A laser is a device that emits light through a process of optical amplification based on the stimulated emission of electromagnetic radiation. The term ""laser"" originated as an acronym for ""light amplification by stimulated emission of radiation"".[1][2][3] The first laser was built in 1960 by Theodore H. Maiman at Hughes Research Laboratories, based on theoretical work by Charles Hard Townes and Arthur Leonard Schawlow.
 A laser differs from other sources of light in that it emits light which is coherent. Spatial coherence allows a laser to be focused to a tight spot, enabling applications such as laser cutting and lithography. Spatial coherence also allows a laser beam to stay narrow over great distances (collimation), enabling applications such as laser pointers and lidar. Lasers can also have high temporal coherence, which allows them to emit light with a very narrow spectrum, i.e., they can emit a single color of light. Alternatively, temporal coherence can be used to produce pulses of light with a broad spectrum but durations as short as a femtosecond (""ultrashort pulses"").
 Lasers are used in optical disk drives, laser printers, barcode scanners, DNA sequencing instruments, fiber-optic, semiconducting chip manufacturing (photolithography), and free-space optical communication, laser surgery and skin treatments, cutting and welding materials, military and law enforcement devices for marking targets and measuring range and speed, and in laser lighting displays for entertainment. They have been used for car headlamps on luxury cars, by using a blue laser and a phosphor to produce highly directional white light.[4][5][6][7]"
Newton%27s_law_of_universal_gravitation,Physics,2,"Newton's law of universal gravitation is usually stated as that every particle attracts every other particle in the universe with a force that is directly proportional to the product of their masses and inversely proportional to the square of the distance between their centers.[note 1] The publication of the theory has become known as the ""first great unification"", as it marked the unification of the previously described phenomena of gravity on Earth with known astronomical behaviors.[1][2][3] This is a general physical law derived from empirical observations by what Isaac Newton called inductive reasoning.[4] It is a part of classical mechanics and was formulated in Newton's work Philosophiæ Naturalis Principia Mathematica (""the Principia""), first published on 5 July 1687. When Newton presented Book 1 of the unpublished text in April 1686 to the Royal Society, Robert Hooke made a claim that Newton had obtained the inverse square law from him.
 In today's language, the law states that every point mass attracts every other point mass by a force acting along the line intersecting the two points. The force is proportional to the product of the two masses, and inversely proportional to the square of the distance between them.[5] The equation for universal gravitation thus takes the form:
 where F is the gravitational force acting between two objects, m1 and m2 are the masses of the objects, r is the distance between the centers of their masses, and G is the gravitational constant.
 The first test of Newton's theory of gravitation between masses in the laboratory was the Cavendish experiment conducted by the British scientist Henry Cavendish in 1798.[6] It took place 111 years after the publication of Newton's Principia and approximately 71 years after his death.
 Newton's law of gravitation resembles Coulomb's law of electrical forces, which is used to calculate the magnitude of the electrical force arising between two charged bodies. Both are inverse-square laws, where force is inversely proportional to the square of the distance between the bodies. Coulomb's law has the product of two charges in place of the product of the masses, and the Coulomb constant in place of the gravitational constant.
 Newton's law has since been superseded by Albert Einstein's theory of general relativity, but it continues to be used as an excellent approximation of the effects of gravity in most applications. Relativity is required only when there is a need for extreme accuracy, or when dealing with very strong gravitational fields, such as those found near extremely massive and dense objects, or at small distances (such as Mercury's orbit around the Sun).
"
LC_circuit,Physics,2,"An LC circuit, also called a resonant circuit, tank circuit, or tuned circuit, is an electric circuit consisting of an inductor, represented by the letter L, and a capacitor, represented by the letter C, connected together. The circuit can act as an electrical resonator, an electrical analogue of a tuning fork, storing energy oscillating at the circuit's resonant frequency.
 LC circuits are used either for generating signals at a particular frequency, or picking out a signal at a particular frequency from a more complex signal; this function is called a bandpass filter. They are key components in many electronic devices, particularly radio equipment, used in circuits such as oscillators, filters, tuners and frequency mixers.
 An LC circuit is an idealized model since it assumes there is no dissipation of energy due to resistance. Any practical implementation of an LC circuit will always include loss resulting from small but non-zero resistance within the components and connecting wires. The purpose of an LC circuit is usually to oscillate with minimal damping, so the resistance is made as low as possible. While no practical circuit is without losses, it is nonetheless instructive to study this ideal form of the circuit to gain understanding and physical intuition. For a circuit model incorporating resistance, see RLC circuit.
"
Lenz%27s_law,Physics,2,"Lenz's law, named after the physicist Emil Lenz  (pronounced /ˈlɛnts/) who formulated it in 1834,[1] states that the direction of the electric current which is induced in a conductor by a changing magnetic field is such that the magnetic field created by the induced current opposes the initial changing magnetic field.
 It is a qualitative law that specifies the direction of induced current, but states nothing about its magnitude. Lenz's law explains the direction of many effects in electromagnetism, such as the direction of voltage induced in an inductor or wire loop by a changing current, or the drag force of eddy currents exerted on moving objects in a magnetic field.
 Lenz's law may be seen as analogous to Newton's third law in classical mechanics.[2]"
Lepton,Physics,2,"In particle physics, a lepton is an elementary particle of half-integer spin (spin ​1⁄2) that does not undergo strong interactions.[1] Two main classes of leptons exist: charged leptons (also known as the electron-like leptons), and neutral leptons (better known as neutrinos). Charged leptons can combine with other particles to form various composite particles such as atoms and positronium, while neutrinos rarely interact with anything, and are consequently rarely observed.  The best known of all leptons is the electron.
 There are six types of leptons, known as flavours, grouped in three generations.[2]  The first-generation leptons, also called electronic leptons, comprise the electron (e−) and the electron neutrino (νe); the second are the muonic leptons, comprising the muon (μ−) and the muon neutrino (νμ); and the third are the tauonic leptons, comprising the tau (τ−) and the tau neutrino (ντ). Electrons have the least mass of all the charged leptons. The heavier muons and taus will rapidly change into electrons and neutrinos through a process of particle decay: the transformation from a higher mass state to a lower mass state. Thus electrons are stable and the most common charged lepton in the universe, whereas muons and taus can only be produced in high energy collisions (such as those involving cosmic rays and those carried out in particle accelerators).
 Leptons have various intrinsic properties, including electric charge, spin, and mass. Unlike quarks, however, leptons are not subject to the strong interaction, but they are subject to the other three fundamental interactions: gravitation, the weak interaction, and to electromagnetism, of which the latter is proportional to charge, and is thus zero for the electrically neutral neutrinos.
 For every lepton flavor, there is a corresponding type of antiparticle, known as an antilepton, that differs from the lepton only in that some of its properties have equal magnitude but opposite sign. According to certain theories, neutrinos may be their own antiparticle. It is not currently known whether this is the case.
 The first charged lepton, the electron, was theorized in the mid-19th century by several scientists[3][4][5] and was discovered in 1897 by J. J. Thomson.[6] The next lepton to be observed was the muon, discovered by Carl D. Anderson in 1936, which was classified as a meson at the time.[7] After investigation, it was realized that the muon did not have the expected properties of a meson, but rather behaved like an electron, only with higher mass. It took until 1947 for the concept of ""leptons"" as a family of particles to be proposed.[8] The first neutrino, the electron neutrino, was proposed by Wolfgang Pauli in 1930 to explain certain characteristics of beta decay.[8] It was first observed in the Cowan–Reines neutrino experiment conducted by Clyde Cowan and Frederick Reines in 1956.[8][9] The muon neutrino was discovered in 1962 by Leon M. Lederman, Melvin Schwartz, and Jack Steinberger,[10] and the tau discovered between 1974 and 1977 by Martin Lewis Perl and his colleagues from the Stanford Linear Accelerator Center and Lawrence Berkeley National Laboratory.[11] The tau neutrino remained elusive until July 2000, when the DONUT collaboration from Fermilab announced its discovery.[12][13] Leptons are an important part of the Standard Model. Electrons are one of the components of atoms, alongside protons and neutrons. Exotic atoms with muons and taus instead of electrons can also be synthesized, as well as lepton–antilepton particles such as positronium.
"
Lever,Physics,2,"A lever (/ˈliːvər/ or US: /ˈlɛvər/) is a simple machine consisting of a beam or rigid rod pivoted at a fixed hinge, or fulcrum. A lever is a rigid body capable of rotating on a point on itself. On the basis of the locations of fulcrum, load and effort, the lever is divided into three types. Also a leverage is a mechanical advantage gained in a mechanical system. It is one of the six simple machines identified by Renaissance scientists. A lever amplifies an input force to provide a greater output force, which is said to provide leverage.  The ratio of the output force to the input force is the mechanical advantage of the lever. As such, the lever is a mechanical advantage device, trading off force against movement.
"
Light,Physics,2,"
 Light or visible light is electromagnetic radiation within the portion of the electromagnetic spectrum that can be perceived by the human eye.[1] Visible light is usually defined as having wavelengths in the range of 400–700 nanometers (nm), or 4.00 × 10−7 to 7.00 × 10−7 m, between the infrared (with longer wavelengths) and the ultraviolet (with shorter wavelengths).[2][3] This wavelength means a frequency range of roughly 430–750 terahertz (THz).
 The main source of light on Earth is the Sun. Sunlight provides the energy that green plants use to create sugars mostly in the form of starches, which release energy into the living things that digest them. This process of photosynthesis provides virtually all the energy used by living things. Historically, another important source of light for humans has been fire, from ancient campfires to modern kerosene lamps. With the development of electric lights and power systems, electric lighting has effectively replaced firelight. Some species of animals generate their own light, a process called bioluminescence. For example, fireflies use light to locate mates, and vampire squids use it to hide themselves from prey.
 The primary properties of visible light are intensity, propagation direction, frequency or wavelength spectrum, and polarization, while its speed in a vacuum, 299,792,458 meters per second, is one of the fundamental constants of nature. Visible light, as with all types of electromagnetic radiation (EMR), is experimentally found to always move at this speed in a vacuum.[4] In physics, the term light sometimes refers to electromagnetic radiation of any wavelength, whether visible or not.[5][6] In this sense, gamma rays, X-rays, microwaves and radio waves are also light. Like all types of EM radiation, visible light propagates as waves. However, the energy imparted by the waves is absorbed at single locations the way particles are absorbed. The absorbed energy of the EM waves is called a photon, and represents the quanta of light. When a wave of light is transformed and absorbed as a photon, the energy of the wave instantly collapses to a single location, and this location is where the photon ""arrives."" This is what is called the wave function collapse. This dual wave-like and particle-like nature of light is known as the wave–particle duality. The study of light, known as optics, is an important research area in modern physics.
"
Linear_actuator,Physics,2,"A linear actuator is an actuator that creates motion in a straight line, in contrast to the circular motion of a conventional electric motor. Linear actuators are used in machine tools and industrial machinery, in computer peripherals such as disk drives and printers, in valves and dampers, and in many other places where linear  motion is required. Hydraulic or pneumatic cylinders inherently produce linear motion. Many other mechanisms are used to generate linear motion from a rotating motor.  
"
Linear_algebra,Physics,2,"Linear algebra is the branch of mathematics concerning linear equations such as: 
 linear maps such as:
 and their representations in vector spaces and through matrices.[1][2][3] Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as basically the application of linear algebra to spaces of functions.
 Linear algebra is also used in most sciences and fields of engineering, because it allows modeling many natural phenomena, and computing efficiently with such models. For nonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing with first-order approximations, using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point.
"
Line_of_force,Physics,2,"A line of force in Faraday's extended sense is synonymous with Maxwell's line of induction.[1] According to J.J. Thomson, Faraday usually discusses lines of force as chains of polarized particles in a dielectric, yet sometimes Faraday discusses them as having an existence all their own as in stretching across a vacuum.[2] In addition to lines of force, J.J. Thomson—similar to Maxwell—also calls them tubes of electrostatic inductance, or simply Faraday tubes.[2]  From the 20th century perspective, lines of force are energy linkages embedded in a 19th-century unified field theory that led to more mathematically and experimentally sophisticated concepts and theories, including Maxwell's equations, electromagnetic waves, and Einstein's relativity.
 Lines of force originated with Michael Faraday, whose theory holds that all of reality is made up of force itself.  His theory predicts that electricity, light, and gravity have finite propagation delays.  The theories and experimental data of later scientific figures such as Maxwell, Hertz, Einstein, and others are in agreement with the ramifications of Faraday's theory.  Nevertheless, Faraday's theory remains distinct. Unlike Faraday, Maxwell and others (e.g., J.J. Thomson) thought that light and electricity must propagate through an ether.  In Einstein's relativity, there is no ether, yet the physical reality of force is much weaker than in the theories of Faraday.[3][4] Historian Nancy J. Nersessian in her paper ""Faraday's Field Concept"" distinguishes between the ideas of Maxwell and Faraday:[5] The specific features of Faraday's field concept, in its 'favourite' and most complete form, are that force is a substance, that it is the only substance and that all forces are interconvertible through various motions of the lines of force. These features of Faraday's 'favourite notion' were not carried on. Maxwell, in his approach to the problem of finding a mathematical representation for the continuous transmission of electric and magnetic forces, considered these to be states of stress and strain in a mechanical aether. This was part of the quite different network of beliefs and problems with which Maxwell was working.
"
Linear_elasticity,Physics,2,"Linear elasticity is a mathematical model of how solid objects deform and become internally stressed due to prescribed loading conditions. It is a simplification of the more general nonlinear theory of elasticity and a branch of continuum mechanics.
 The fundamental ""linearizing"" assumptions of linear elasticity are: infinitesimal strains or ""small"" deformations (or strains) and linear relationships between the components of stress and strain. In addition linear elasticity is valid only for stress states that do not produce yielding.
 These assumptions are reasonable for many engineering materials and engineering design scenarios. Linear elasticity is therefore used extensively in structural analysis and engineering design, often with the aid of finite element analysis.
"
Liouville%27s_theorem_(conformal_mappings),Physics,2,"In mathematics, Liouville's theorem, proved by Joseph Liouville in 1850, is a rigidity theorem about conformal mappings in Euclidean space. It states that any smooth conformal mapping on a domain of Rn, where n > 2, can be expressed as a composition of translations, similarities, orthogonal transformations and inversions: they are Möbius transformations (in n dimensions).[1][2] This theorem severely limits the variety of possible conformal mappings in R3 and higher-dimensional spaces. By contrast, conformal mappings in R2 can be much more complicated – for example, all simply connected planar domains are conformally equivalent, by the Riemann mapping theorem.
 Generalizations of the theorem hold for transformations that are only weakly differentiable (Iwaniec & Martin 2001, Chapter 5). The focus of such a study is the non-linear Cauchy–Riemann system that is a necessary and sufficient condition for a smooth mapping ƒ → Ω → Rn to be conformal:
 where Df is the Jacobian derivative, T is the matrix transpose, and I is the identity matrix.  A weak solution of this system is defined to be an element ƒ of the Sobolev space W1,nloc(Ω,Rn) with non-negative Jacobian determinant almost everywhere, such that the Cauchy–Riemann system holds at almost every point of Ω.  Liouville's theorem is then that every weak solution (in this sense) is a Möbius transformation, meaning that it has the form
 where a,b are vectors in Rn, α is a scalar, A is a rotation matrix, and ε = 0 or 2.  Equivalently stated, any quasiconformal map of a domain in Euclidean space that is also conformal is a Möbius transformation. This equivalent statement justifies using the Sobolev space W1,n, since ƒ ∈ W1,nloc(Ω,Rn) then follows from the geometrical condition of conformality and the ACL characterization of Sobolev space.  The result is not optimal however: in even dimensions n = 2k, the theorem also holds for solutions that are only assumed to be in the space W1,kloc, and this result is sharp in the sense that there are weak solutions of the Cauchy–Riemann system in W1,p for any p < k which are not Möbius transformations.  In odd dimensions, it is known that W1,n is not optimal, but a sharp result is not known.
 Similar rigidity results (in the smooth case) hold on any conformal manifold.  The group of conformal isometries of an n-dimensional conformal Riemannian manifold always has dimension that cannot exceed that of the full conformal group SO(n+1,1).  Equality of the two dimensions holds exactly when the conformal manifold is isometric with the n-sphere or projective space.  Local versions of the result also hold: The Lie algebra of conformal Killing fields in an open set has dimension less than or equal to that of the conformal group, with equality holding if and only if the open set is locally conformally flat.
"
Liquid,Physics,2,"A liquid is a nearly incompressible fluid that conforms to the shape of its container but retains a (nearly) constant volume independent of pressure. As such, it is one of the four fundamental states of matter (the others being solid, gas, and plasma), and is the only state with a definite volume but no fixed shape. A liquid is made up of tiny vibrating particles of matter, such as atoms, held together by intermolecular bonds. Like a gas, a liquid is able to flow and take the shape of a container. Most liquids resist compression, although others can be compressed. Unlike a gas, a liquid does not disperse to fill every space of a container, and maintains a fairly constant density. A distinctive property of the liquid state is surface tension, leading to wetting phenomena. Water is, by far, the most common liquid on Earth.
 The density of a liquid is usually close to that of a solid, and much higher than in a gas. Therefore, liquid and solid are both termed condensed matter. On the other hand, as liquids and gases share the ability to flow, they are both called fluids. Although liquid water is abundant on Earth, this state of matter is actually the least common in the known universe, because liquids require a relatively narrow temperature/pressure range to exist. Most known matter in the universe is in gaseous form (with traces of detectable solid matter) as interstellar clouds or in plasma from within stars.
"
Liquid_crystal,Physics,2,"
 Liquid crystals (LCs) are a  state of matter which has properties between those of conventional liquids and those of solid crystals. For instance, a liquid crystal may flow like a liquid, but its molecules may be oriented in a crystal-like way. There are many different types of liquid-crystal phases, which can be distinguished by their different optical properties (such as textures). The contrasting areas in the textures correspond to domains where the liquid-crystal molecules are oriented in different directions. Within a domain, however, the molecules are well ordered. LC materials may not always be in a liquid-crystal state of matter (just as water may turn into ice or water vapor).
 Liquid crystals can be divided into thermotropic, lyotropic and metallotropic phases. Thermotropic and lyotropic liquid crystals consist mostly of organic molecules, although a few minerals are also known. Thermotropic LCs exhibit a phase transition into the liquid-crystal phase as temperature is changed. Lyotropic LCs exhibit phase transitions as a function of both temperature and concentration of the liquid-crystal molecules in a solvent (typically water). Metallotropic LCs are composed of both organic and inorganic molecules; their liquid-crystal transition depends not only on temperature and concentration, but also on the inorganic-organic composition ratio.
 Examples of liquid crystals can be found both in the natural world and in technological applications. Widespread Liquid-crystal displays use liquid crystals. Lyotropic liquid-crystalline phases are abundant in living systems but can also be found in the mineral world. For example, many proteins and cell membranes are liquid crystals. Other well-known examples of liquid crystals are solutions of soap and various related detergents, as well as the tobacco mosaic virus, and some clays.
"
Longitudinal_wave,Physics,2,"Longitudinal waves are waves in which the displacement of the medium is in the same direction as, or the opposite direction to, the direction of propagation of the wave. Mechanical longitudinal waves are also called compressional or compression waves, because they produce compression and rarefaction when traveling through a medium, and pressure waves, because they produce increases and decreases in pressure.
 The other main type of wave is the transverse wave, in which the displacements of the medium are at right angles to the direction of propagation. Transverse waves, for instance, describe some bulk sound waves in solid materials (but not in fluids); these are also called ""shear waves"" to differentiate them from the (longitudinal) pressure waves that these materials also support.
 Longitudinal waves include sound waves (vibrations in pressure, a particle of displacement, and particle velocity propagated in an elastic medium) and seismic P-waves (created by earthquakes and explosions).
In longitudinal waves, the displacement of the medium is parallel to the propagation of the wave. A wave along the length of a stretched Slinky toy, where the distance between coils increases and decreases, is a good visualization and contrasts with the standing wave along an oscillating guitar string which is transverse.
"
M-theory,Physics,2,"
 M-theory is a theory in physics that unifies all consistent versions of superstring theory. Edward Witten first conjectured the existence of such a theory at a  string-theory conference at the University of Southern California in the spring of 1995. Witten's announcement initiated a flurry of research activity known as the second superstring revolution.
 Prior to Witten's announcement, string theorists had identified five versions of superstring theory. Although these theories appeared, at first, to be very different, work by several physicists showed that the theories were related in intricate and nontrivial ways. Physicists found that apparently distinct theories could be unified by mathematical transformations called S-duality and T-duality. Witten's conjecture was based in part on the existence of these dualities and in part on the relationship of the string theories to a field theory called eleven-dimensional supergravity.
 Although a complete formulation of M-theory is not known, such a formulation should describe two- and five-dimensional objects called branes and should be approximated by eleven-dimensional supergravity at low  energies. Modern attempts to formulate M-theory are typically based on matrix theory or the AdS/CFT correspondence.
 According to Witten, M should stand for ""magic"", ""mystery"" or ""membrane"" according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.[1] Investigations of the mathematical structure of M-theory have spawned important theoretical results in physics and mathematics. More speculatively, M-theory may provide a framework for developing a unified theory of all of the fundamental forces of nature. Attempts to connect M-theory to experiment typically focus on  compactifying its extra dimensions to construct candidate models of the four-dimensional world, although so far none has been verified to give rise to physics as observed in high-energy physics experiments.
"
Mach_number,Physics,2,"
 Mach number (M or Ma) (/mɑːk/; German: [max]) is a dimensionless quantity in fluid dynamics representing the ratio of flow velocity past a boundary to the local speed of sound.[1][2] where:
 By definition, at Mach 1, the local flow velocity u is equal to the speed of sound. At Mach 0.65, u is 65% of the speed of sound (subsonic), and, at Mach 1.35, u is 35% faster than the speed of sound (supersonic). Pilots of high-altitude aerospace vehicles use flight Mach number to express a vehicle's true airspeed, but the flow field around a vehicle varies in three dimensions, with corresponding variations in local Mach number.
 The local speed of sound, and hence the Mach number, depends on the temperature of the surrounding gas. The Mach number is primarily used to determine the approximation with which a flow can be treated as an incompressible flow. The medium can be a gas or a liquid. The boundary can be traveling in the medium, or it can be stationary while the medium flows along it, or they can both be moving, with different velocities: what matters is their relative velocity with respect to each other. The boundary can be the boundary of an object immersed in the medium, or of a channel such as a nozzle, diffuser or wind tunnel channeling the medium. As the Mach number is defined as the ratio of two speeds, it is a dimensionless number. If M < 0.2–0.3 and the flow is quasi-steady and isothermal, compressibility effects will be small and simplified incompressible flow equations can be used.[1][2] The Mach number is named after Austrian physicist and philosopher Ernst Mach,[3] and is a designation proposed by aeronautical engineer Jakob Ackeret in 1929.[4] As the Mach number is a dimensionless quantity rather than a unit of measure, the number comes after the unit; the second Mach number is Mach 2 instead of 2 Mach (or Machs). This is somewhat reminiscent of the early modern ocean sounding unit mark (a synonym for fathom), which was also unit-first, and may have influenced the use of the term Mach. In the decade preceding faster-than-sound human flight, aeronautical engineers referred to the speed of sound as Mach's number, never Mach 1.[5]"
Machine,Physics,2,
Machine_element,Physics,2,"Machine element refers to an elementary component of a machine.  These elements consist of three basic types:
 While generally not considered to be a machine element, the shape, texture and color of covers are an important part of a machine that provide a styling and operational interface between the mechanical components of a machine and its users.
 Machine elements are basic mechanical parts and features used as the building blocks of most machines.[2] Most are standardized to common sizes, but customs are also common for specialized applications.[3] Machine elements may be features of a part (such as screw threads or integral plain bearings) or they may be discrete parts in and of themselves such as wheels, axles, pulleys, rolling-element bearings, or gears. All of the simple machines may be described as machine elements, and many machine elements incorporate concepts of one or more simple machines.  For example, a leadscrew incorporates a screw thread, which is an inclined plane wrapped around a cylinder. 
 Many mechanical design, invention, and engineering tasks involve a knowledge of various machine elements and an intelligent and creative combining of these elements into a component or assembly that fills a need (serves an application).
"
Maclaurin_series,Physics,2,"
 In mathematics, the Taylor series of a function is an infinite sum of terms that are expressed in terms of the function's derivatives at a single point. For most common functions, the function and the sum of its Taylor series are equal near this point. Taylor's series are named after Brook Taylor who introduced them in 1715.
 If zero is the point where the derivatives are considered, a Taylor series is also called a Maclaurin series, after Colin Maclaurin, who made extensive use of this special case of Taylor series in the 18th century.
 The partial sum formed by the n first terms of a Taylor series is a polynomial of degree n that is called the nth Taylor polynomial of the function. Taylor polynomials are approximations of a function, which become generally better as n increases. Taylor's theorem gives quantitative estimates on the error introduced by the use of such approximations. If the Taylor series of a function is convergent, its sum is the limit of the infinite sequence of the Taylor polynomials. A function may differ from the sum of its Taylor series, even if its Taylor series is convergent. A function is analytic at a point x if it is equal to the sum of its Taylor series in some open interval (or open disk in the complex plane) containing x. This implies that the function is analytic at every point of the interval (or disk).
"
Magnetic_field,Physics,2,"
 A magnetic field is a vector field that describes the magnetic influence on moving electric charges, electric currents,[1]:ch1[2] and magnetized materials. A charge that is moving in a magnetic field experiences a force perpendicular to its own velocity and to the magnetic field.[1]:ch13[3]  The effects of magnetic fields are commonly seen in permanent magnets, which pull on magnetic materials such as iron, and attract or repel other magnets.  In addition, a magnetic field that varies with location will exert a force on a range of non-magnetic materials by affecting the motion of their outer atomic electrons. Magnetic fields surround magnetized materials, and are created by electric currents such as those used in electromagnets, and by electric fields varying in time.  Since both strength and direction of a magnetic field may vary with location, they are described as a map assigning a vector to each point of space or, more precisely—because of the way the magnetic field transforms under mirror reflection—as a field of pseudovectors.
 In electromagnetics, the term ""magnetic field"" is used for two distinct but closely related vector fields denoted by the symbols B and H. In the International System of Units, H, magnetic field strength, is measured in the SI base units of ampere per meter (A/m).[4] B, magnetic flux density, is measured in tesla (in SI base units: kilogram per second2 per ampere),[5] which is equivalent to newton per meter per ampere. H and B differ in how they account for magnetization. In a vacuum, the two fields are related through the vacuum permeability, 




B


/


μ

0


=

H



{  \mathbf {B} /\mu _{0}=\mathbf {H} }
; but in a magnetized material, the terms differ by the material's magnetization at each point.
 Magnetic fields are produced by moving electric charges and the intrinsic magnetic moments of elementary particles associated with a fundamental quantum property, their spin.[6][1]:ch1 Magnetic fields and electric fields are interrelated and are both components of the electromagnetic force, one of the four fundamental forces of nature.
 Magnetic fields are used throughout modern technology, particularly in electrical engineering and electromechanics. Rotating magnetic fields are used in both electric motors and generators. The interaction of magnetic fields in electric devices such as transformers is conceptualized and investigated as magnetic circuits. Magnetic forces give information about the charge carriers in a material through the Hall effect. The Earth produces its own magnetic field, which shields the Earth's ozone layer from the solar wind and is important in navigation using a compass.
"
Magnetism,Physics,2,"Magnetism is a class of physical phenomena that are mediated by magnetic fields. Electric currents and the magnetic moments of elementary particles give rise to a magnetic field, which acts on other currents and magnetic moments. Magnetism is one aspect of the combined phenomenon of electromagnetism. The most familiar effects occur in ferromagnetic materials, which are strongly attracted by magnetic fields and can be magnetized to become permanent magnets, producing magnetic fields themselves.  Demagnetizing a magnet is also possible.  Only a few substances are ferromagnetic; the most common ones are iron, cobalt and nickel and their alloys.  The prefix ferro- refers to iron, because permanent magnetism was first observed in lodestone, a form of natural iron ore called magnetite, Fe3O4.
 All substances exhibit some type of magnetism. Magnetic materials are classified according to their bulk susceptibility[1]. Ferromagnetism is responsible for most of the effects of magnetism encountered in everyday life, but there are actually several types of magnetism.  Paramagnetic substances, such as aluminum and oxygen, are weakly attracted to an applied magnetic field; diamagnetic substances, such as copper and carbon, are weakly repelled; while antiferromagnetic materials, such as chromium and spin glasses, have a more complex relationship with a magnetic field.  The force of a magnet on paramagnetic, diamagnetic, and antiferromagnetic materials is usually too weak to be felt and can be detected only by laboratory instruments, so in everyday life, these substances are often described as non-magnetic.
 The magnetic state (or magnetic phase) of a material depends on temperature, pressure, and the applied magnetic field. A material may exhibit more than one form of magnetism as these variables change.
 The strength of a magnetic field almost always decreases with distance, though the exact mathematical relationship between strength and distance varies. Different configurations of magnetic moments and electric currents can result in complicated magnetic fields.
 Only magnetic dipoles have been observed, although some theories predict the existence of magnetic monopoles.
"
Magnetostatics,Physics,2,"
 Magnetostatics is the study of magnetic fields in systems where the currents are steady (not changing with time). It is the magnetic analogue of electrostatics, where the charges are stationary. The magnetization need not be static; the equations of magnetostatics can be used to predict fast magnetic switching events that occur on time scales of nanoseconds or less.[1] Magnetostatics is even a good approximation when the currents are not static — as long as the currents do not alternate rapidly. Magnetostatics is widely used in applications of micromagnetics such as models of magnetic storage devices as in computer memory. Magnetostatic focussing can be achieved either by a permanent magnet or by passing current through a coil of wire whose axis coincides with the beam axis.
"
Mass,Physics,2,"
 Mass is both a property of a physical body and a measure of its resistance to acceleration (a change in its state of motion) when a net force is applied.[1] An object's mass also determines the strength of its gravitational attraction to other bodies.
 The basic SI unit of mass is the kilogram (kg). In physics, mass is not the same as weight, even though mass is often determined by measuring the object's weight using a spring scale, rather than balance scale comparing it directly with known masses. An object on the Moon would weigh less than it does on Earth because of the lower gravity, but it would still have the same mass. This is because weight is a force, while mass is the property that (along with gravity) determines the strength of this force.
"
Mass_balance,Physics,2,"A mass balance, also called a material balance, is an application of conservation of mass to the analysis of physical systems. By accounting for material entering and leaving a system, mass flows can be identified which might have been unknown, or difficult to measure without this technique.  The exact conservation law used in the analysis of the system depends on the context of the problem, but all revolve around mass conservation, i.e., that matter cannot disappear or be created spontaneously.[1]:59–62 Therefore, mass balances are used widely in engineering and environmental analyses. For example, mass balance theory is used to design chemical reactors, to analyse alternative processes to produce chemicals, as well as to model pollution dispersion and other processes of physical systems. Closely related and complementary analysis techniques include the population balance, energy balance and the somewhat more complex entropy balance. These techniques are required for thorough design and analysis of systems such as the refrigeration cycle.
 In environmental monitoring the term budget calculations is used to describe mass balance equations where they are used to evaluate the monitoring data (comparing input and output, etc.). In biology the dynamic energy budget theory for metabolic organisation makes explicit use of mass and energy balance.
"
Mass_density,Physics,2,"
 The density (more precisely, the volumetric mass density; also known as specific mass), of a substance is its mass per unit volume. The symbol most often used for density is ρ (the lower case Greek letter rho), although the Latin letter D can also be used. Mathematically, density is defined as mass divided by volume:[1] where ρ is the density, m is the mass, and V is the volume. In some cases (for instance, in the United States oil and gas industry), density is loosely defined as its weight per unit volume,[2] although this is scientifically inaccurate – this quantity is more specifically called specific weight.
 For a pure substance the density has the same numerical value as its mass concentration.
Different materials usually have different densities, and density may be relevant to buoyancy, purity and packaging. Osmium and iridium are the densest known elements at standard conditions for temperature and pressure.
 To simplify comparisons of density across different systems of units, it is sometimes replaced by the dimensionless quantity ""relative density"" or ""specific gravity"", i.e. the ratio of the density of the material to that of a standard material, usually water. Thus a relative density less than one means that the substance floats in water.
 The density of a material varies with temperature and pressure. This variation is typically small for solids and liquids but much greater for gases. Increasing the pressure on an object decreases the volume of the object and thus increases its density. Increasing the temperature of a substance (with a few exceptions) decreases its density by increasing its volume. In most materials, heating the bottom of a fluid results in convection of the heat from the bottom to the top, due to the decrease in the density of the heated fluid. This causes it to rise relative to more dense unheated material.
 The reciprocal of the density of a substance is occasionally called its specific volume, a term sometimes used in thermodynamics. Density is an intensive property in that increasing the amount of a substance does not increase its density; rather it increases its mass.
"
Mass_flux,Physics,2,"In physics and engineering, mass flux is the rate of mass flow per unit area, perfectly overlapping with the momentum density, the momentum per unit volume. The common symbols are j, J, q, Q, φ, or Φ (Greek lower or capital Phi), sometimes with subscript m to indicate mass is the flowing quantity. Its SI units are kg s−1 m−2. Mass flux can also refer to an alternate form of flux in Fick's law that includes the molecular mass, or in Darcy's law that includes the mass density.[1] Unfortunately, sometimes the defining equation for mass flux in this article is used interchangeably with the defining equation in mass flow rate. For example, Fluid Mechanics, Schaum's et al [2] uses the definition of mass flux as the equation in the mass flow rate article.
"
Mass_moment_of_inertia,Physics,2,"The moment of inertia, otherwise known as the mass moment of inertia, angular mass or rotational inertia, of a rigid body is a quantity that determines the torque needed for a desired angular acceleration about a rotational axis; similar to how mass determines the force needed for a desired acceleration. It depends on the body's mass distribution and the axis chosen, with larger moments requiring more torque to change the body's rate of rotation. 
 It is an extensive (additive) property: for a point mass the moment of inertia is simply the mass times the square of the perpendicular distance to the axis of rotation. The moment of inertia of a rigid composite system is the sum of the moments of inertia of its component subsystems (all taken about the same axis). Its simplest definition is the second moment of mass with respect to distance from an axis. 
 For bodies constrained to rotate in a plane, only their moment of inertia about an axis perpendicular to the plane, a scalar value, matters. For bodies free to rotate in three dimensions, their moments can be described by a symmetric 3 × 3 matrix, with a set of mutually perpendicular principal axes for which this matrix is diagonal and torques around the axes act independently of each other.
"
Mass_number,Physics,2,"The mass number (symbol A, from the German word Atomgewicht [atomic weight]),[1] also called  atomic mass number or nucleon number, is the total number of protons and neutrons (together known as nucleons) in an atomic nucleus. It is approximately equal to the atomic (also known as isotopic) mass of the atom expressed in atomic mass units. Since protons and neutrons are both baryons, the mass number A is identical with the baryon number B as of the nucleus as of the whole atom or ion. The mass number is different for each different isotope of a chemical element. Hence, the difference between the mass number and the atomic number Z gives the number of neutrons (N) in a given nucleus: N = A − Z.[2] The mass number is written either after the element name or as a superscript to the left of an element's symbol. For example, the most common isotope of carbon is carbon-12, or 12C, which has 6 protons and 6 neutrons. The full isotope symbol would also have the atomic number (Z) as a subscript to the left of the element symbol directly below the mass number: 126C.[3]"
Mass_spectrometry,Physics,2,"Mass spectrometry (MS) is an analytical technique that measures the mass-to-charge ratio of  ions. The results are typically presented as a mass spectrum, a plot of intensity as a function of the mass-to-charge ratio. Mass spectrometry is used in many different fields and is applied to pure samples as well as complex mixtures.
 A mass spectrum is a plot of the ion signal as a function of the mass-to-charge ratio. These spectra are used to determine the elemental or isotopic signature of a sample, the masses of particles and of molecules, and to elucidate the chemical identity or structure of molecules and other chemical compounds.
 In a typical MS procedure, a sample, which may be solid, liquid, or gaseous, is ionized, for example by bombarding it with electrons. This may cause some of the sample's molecules to break into charged fragments or simply become charged without fragmenting. These ions are then separated according to their mass-to-charge ratio, for example by accelerating them and subjecting them to an electric or magnetic field: ions of the same mass-to-charge ratio will undergo the same amount of deflection.[1] The ions are detected by a mechanism capable of detecting charged particles, such as an electron multiplier. Results are displayed as spectra of the signal intensity of detected ions as a function of the mass-to-charge ratio. The atoms or molecules in the sample can be identified by correlating known masses (e.g. an entire molecule) to the identified masses or through a characteristic fragmentation pattern.
"
Material_properties,Physics,2,"A material's property (or material property) is an intensive property of some material, i.e. a physical property that does not depend on the amount of the material. These quantitative properties may be used as a metric by which the benefits of one material versus another can be compared, thereby aiding in materials selection.
 A property may be a constant or may be a function of one or more independent variables, such as temperature. Materials properties often vary to some degree according to the direction in the material in which they are measured, a condition referred to as anisotropy. Materials properties that relate to different physical phenomena often behave linearly (or approximately so) in a given operating range[further explanation needed]. Modeling them as linear can significantly simplify the differential constitutive equations that the property describes.
 Some materials are used in relevant equations to predict the attributes of a system a priori. 
 The properties are measured by standardized test methods. Many such methods have been documented by their respective user communities and published through the Internet; see ASTM International.
"
Materials_science,Physics,2,"
 The interdisciplinary field of materials science, also commonly termed materials science and engineering, is the design and discovery of new materials, particularly solids. The intellectual origins of materials science stem from the Enlightenment, when researchers began to use analytical thinking from chemistry, physics, and engineering to understand ancient, phenomenological observations in metallurgy and mineralogy.[1][2] Materials science still incorporates elements of physics, chemistry, and engineering. As such, the field was long considered by academic institutions as a sub-field of these related fields. Beginning in the 1940s, materials science began to be more widely recognized as a specific and distinct field of science and engineering, and major technical universities around the world created dedicated schools for its study. 
 Many of the most pressing scientific problems humans currently face are due to the limits of available materials and how they are used. Thus, breakthroughs in materials science are likely to affect the future of technology significantly.[4][5] Materials scientists emphasize understanding how the history of a material (its processing) influences its structure, and thus the material's properties and performance. The understanding of processing-structure-properties relationships is called the § materials paradigm. This paradigm is used to advance understanding in a variety of research areas, including nanotechnology, biomaterials, and metallurgy. Materials science is also an important part of forensic engineering and failure analysis –  investigating materials, products, structures or components which fail or do not function as intended, causing personal injury or damage to property. Such investigations are key to understanding, for example, the causes of various aviation accidents and incidents.
"
Mathematical_physics,Physics,2,"Mathematical physics refers to the development of mathematical methods for application to problems in physics. The Journal of Mathematical Physics defines the field as ""the application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories"".[1]"
Mathematics,Physics,2,"
 
 Mathematics (from Greek: μάθημα, máthēma, 'knowledge, study, learning') includes the study of such topics as quantity (number theory),[1] structure (algebra),[2] space (geometry),[1] and change (mathematical analysis).[3][4][5] It has no generally accepted definition.[6][7] Mathematicians seek and use patterns[8][9] to formulate new conjectures; they resolve the truth or falsity of such by mathematical proof. When mathematical structures are good models of real phenomena, mathematical reasoning can be used to provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.
 Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements.[10] Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.[11] Mathematics is essential in many fields, including natural science, engineering, medicine, finance, and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians engage in pure mathematics (mathematics for its own sake) without having any application in mind, but practical applications for what began as pure mathematics are often discovered later.[12][13]"
Matrix_(mathematics),Physics,2,"In mathematics, a matrix (plural matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns.[1][2] For example, the dimension of the matrix below is 2 × 3 (read ""two by three""), because there are two rows and three columns:
 Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for an (m×n)-matrix times an (n×p)-matrix, resulting in an (m×p)-matrix). There is no product the other way round—a first hint that matrix multiplication is not commutative. Any matrix can be multiplied element-wise by a scalar from its associated field. Matrices are often denoted using capital roman letters such as 



A


{  A}
, 



B


{  B}
 and 



C


{  C}
.[3] The individual items in an m×n matrix A, often denoted by ai,j, where i and j usually vary from 1 to m and n, respectively, are called its elements or entries.[4][5] For conveniently expressing an element of the results of matrix operations, the indices of the element are often attached to the parenthesized or bracketed matrix expression (e.g., (AB)i,j refers to an element of a matrix product). In the context of abstract index notation, this ambiguously refers also to the whole matrix product.
 A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. 
 If the matrix is square, then it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.
 Applications of matrices are found in most scientific fields.[6] In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. 
 In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[7] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.
 A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.
"
Matter,Physics,2,"
 In classical physics and general chemistry, matter is any substance that has mass and takes up space by having volume.[1]:21 All everyday objects that can be touched are ultimately composed of atoms, which are made up of interacting subatomic particles, and in everyday as well as scientific usage, ""matter"" generally includes atoms and anything made up of them, and any particles (or combination of particles) that act as if they have both rest mass and volume. However it does not include massless particles such as photons, or other energy phenomena or waves such as light.[1]:21[2] Matter exists in various states (also known as phases). These include classical everyday phases such as solid, liquid, and gas – for example water exists as ice, liquid water, and gaseous steam – but other states are possible, including plasma, Bose–Einstein condensates, fermionic condensates, and quark–gluon plasma.[3] Usually atoms can be imagined as a nucleus of protons and neutrons, and a surrounding ""cloud"" of orbiting electrons which ""take up space"".[4][5] However this is only somewhat correct, because subatomic particles and their properties are governed by their quantum nature, which means they do not act as everyday objects appear to act – they can act like waves as well as particles and they do not have well-defined sizes or positions. In the Standard Model of particle physics, matter is not a fundamental concept because the elementary constituents of atoms are quantum entities which do not have an inherent ""size"" or ""volume"" in any everyday sense of the word. Due to the exclusion principle and other fundamental interactions, some ""point particles"" known as fermions (quarks, leptons), and many composites and atoms, are effectively forced to keep a distance from other particles under everyday conditions; this creates the property of matter which appears to us as matter taking up space.
 For much of the history of the natural sciences people have contemplated the exact nature of matter. The idea that matter was built of discrete building blocks, the so-called particulate theory of matter, independently appeared in ancient Greece and ancient India among Buddhists, Hindus and Jains in 1st-millennium BC.[6] Ancient philosophers who proposed the particulate theory of matter include Kanada (c. 6th–century BC or after),[7] Leucippus (~490 BC) and Democritus (~470–380 BC).[8]"
Maxwell%27s_equations,Physics,2,"Maxwell's equations are a set of coupled partial differential equations that, together with the Lorentz force law, form the foundation of classical electromagnetism, classical optics, and electric circuits. 
The equations provide a mathematical model for electric, optical, and radio technologies, such as power generation, electric motors, wireless communication, lenses, radar etc. They describe how electric and magnetic fields are generated by charges, currents, and changes of the fields.[note 1] The equations are named after the physicist and mathematician James Clerk Maxwell, who, in 1861 and 1862, published an early form of the equations that included the Lorentz force law. Maxwell first used the equations to propose that light is an electromagnetic phenomenon.
 An important consequence of Maxwell's equations is that they demonstrate how fluctuating electric and magnetic fields propagate at a constant speed (c) in a vacuum. Known as electromagnetic radiation, these waves may occur at various wavelengths to produce a spectrum of light from radio waves to gamma rays.
 The equations have two major variants. The microscopic Maxwell equations have universal applicability but are unwieldy for common calculations. They relate the electric and magnetic fields to total charge and total current, including the complicated charges and currents in materials at the atomic scale. The ""macroscopic"" Maxwell equations define two new auxiliary fields that describe the large-scale behaviour of matter without having to consider atomic scale charges and quantum phenomena like spins. However, their use requires experimentally determined parameters for a phenomenological description of the electromagnetic response of materials.
 The term ""Maxwell's equations"" is often also used for equivalent alternative formulations. Versions of Maxwell's equations based on the electric and magnetic scalar potentials are preferred for explicitly solving the equations as a boundary value problem, analytical mechanics, or for use in quantum mechanics. The covariant formulation (on spacetime rather than space and time separately) makes the compatibility of Maxwell's equations with special relativity manifest. Maxwell's equations in curved spacetime, commonly used in high energy and gravitational physics, are compatible with general relativity.[note 2] In fact, Albert Einstein developed special and general relativity to accommodate the invariant speed of light, a consequence of  Maxwell's equations, with the principle that only relative movement has physical consequences.
 The publication of the equations marked the unification of a theory for previously separately described phenomena: magnetism, electricity, light and associated radiation.
Since the mid-20th century, it has been understood that Maxwell's equations do not give an exact description of electromagnetic phenomena, but are instead a classical limit of the more precise theory of quantum electrodynamics.
"
Measure_of_central_tendency,Physics,2,"In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution.[1] It may also be called a center or location of the distribution. Colloquially, measures of central tendency are often called averages. The term central tendency dates from the late 1920s.[2] The most common measures of central tendency are the arithmetic mean, the median, and the mode.  A middle tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. Occasionally authors use central tendency to denote ""the tendency of quantitative data to cluster around some central value.""[2][3] The central tendency of a distribution is typically contrasted with its dispersion or variability; dispersion and central tendency are the often characterized properties of distributions. Analysis may judge whether data has a strong or a weak central tendency based on its dispersion.
"
Mechanical_energy,Physics,2,"In  physical sciences, mechanical energy is the sum of potential energy and kinetic energy. It is the macroscopic energy associated with a system. The principle of conservation of mechanical energy states that if an isolated system is subject only to conservative forces, then the mechanical energy is constant. If an object moves in the opposite direction of a conservative net force, the potential energy will increase; and if the speed (not the velocity) of the object changes, the kinetic energy of the object also changes. In all real systems, however, nonconservative forces, such as frictional forces, will be present, but if they are of negligible magnitude, the mechanical energy changes little and its conservation is a useful approximation. In elastic collisions, the kinetic energy is conserved, but in inelastic collisions some mechanical energy may be converted into thermal energy. The equivalence between lost mechanical energy (dissipation) and an increase in temperature was discovered by James Prescott Joule.
 Many devices are used to convert mechanical energy to or from other forms of energy, e.g. an electric motor converts electrical energy to mechanical energy, an electric generator converts mechanical energy into electrical energy and a heat engine converts heat energy to mechanical energy.
"
Mechanical_filter,Physics,2,"A mechanical filter is a signal processing filter usually used in place of an electronic filter at radio frequencies.  Its purpose is the same as that of a normal electronic filter: to pass a range of signal frequencies, but to block others.  The filter acts on mechanical vibrations which are the analogue of the electrical signal.  At the input and output of the filter, transducers convert the electrical signal into, and then back from, these mechanical vibrations.
 The components of a mechanical filter are all directly analogous to the various elements found in electrical circuits.  The mechanical elements obey mathematical functions which are identical to their corresponding electrical elements.  This makes it possible to apply electrical network analysis and filter design methods to mechanical filters.  Electrical theory has developed a large library of mathematical forms that produce useful filter frequency responses and the mechanical filter designer is able to make direct use of these.  It is only necessary to set the mechanical components to appropriate values to produce a filter with an identical response to the electrical counterpart.
 Steel alloys and iron–nickel alloys are common materials for mechanical filter components; nickel is sometimes used for the input and output couplings.  Resonators in the filter made from these materials need to be machined to precisely adjust their resonance frequency before final assembly.
 While the meaning of mechanical filter in this article is one that is used in an electromechanical role, it is possible to use a mechanical design to filter mechanical vibrations or sound waves (which are also essentially mechanical) directly. For example, filtering of audio frequency response in the design of loudspeaker cabinets can be achieved with mechanical components.  In the electrical application, in addition to mechanical components which correspond to their electrical counterparts, transducers are needed to convert between the mechanical and electrical domains.  A representative selection of the wide variety of component forms and topologies for mechanical filters are presented in this article.
 The theory of mechanical filters was first applied to improving the mechanical parts of phonographs in the 1920s.  By the 1950s mechanical filters were being manufactured as self-contained components for applications in radio transmitters and high-end receivers.  The high ""quality factor"", Q, that mechanical resonators can attain, far higher than that of an all-electrical LC circuit, made possible the construction of mechanical filters with excellent selectivity. Good selectivity, being important in radio receivers, made such filters highly attractive.  Contemporary researchers are working on microelectromechanical filters, the mechanical devices corresponding to electronic integrated circuits.
"
Mechanical_equilibrium,Physics,2,"In classical mechanics, a particle is in mechanical equilibrium if the net force on that particle is zero.[1]:39 By extension, a physical system made up of many parts is in mechanical equilibrium if the net force on each of its individual parts is zero.[1]:45–46[2] In addition to defining mechanical equilibrium in terms of force, there are many alternative definitions for mechanical equilibrium which are all mathematically equivalent. In terms of momentum, a system is in equilibrium if the momentum of its parts is all constant. In terms of velocity, the system is in equilibrium if velocity is constant. In a rotational mechanical equilibrium the angular momentum of the object is conserved and the net torque is zero.[2] More generally in conservative systems, equilibrium is established at a point in configuration space where the gradient of the potential energy with respect to the generalized coordinates is zero.
 If a particle in equilibrium has zero velocity, that particle is in static equilibrium.[3][4] Since all particles in equilibrium have constant velocity, it is always possible to find an inertial reference frame in which the particle is stationary with respect to the frame.
"
Mechanical_wave,Physics,2,"A mechanical wave is a wave that is an oscillation of matter, and therefore transfers energy through a medium.[1] While waves can move over long distances, the movement of the medium of transmission—the material—is limited. Therefore, the oscillating material does not move far from its initial equilibrium position. Mechanical waves transport energy. This energy propagates in the same direction as the wave. Any kind of wave (mechanical or electromagnetic) has a certain energy. Mechanical waves can be produced only in media which possess elasticity and inertia.
 A mechanical wave requires an initial energy input. Once this initial energy is added, the wave travels through the medium until all its energy is transferred. In contrast, electromagnetic waves require no medium, but can still travel through one.
 One important property of mechanical waves is that their amplitudes are measured in an unusual way, displacement divided by (reduced) wavelength. When this gets comparable to unity, significant nonlinear effects such as harmonic generation may occur, and, if large enough, may result in chaotic effects. For example, waves on the surface of a body of water break when this dimensionless amplitude exceeds 1, resulting in a foam on the surface and turbulent mixing.
 There are three types of mechanical waves: transverse waves, longitudinal waves, and surface waves, etc. Some of the most common examples of mechanical waves are water waves, sound waves, and seismic waves.
"
Mechanics,Physics,2,"Mechanics (Greek: μηχανική) is the area of physics concerned with the motions of physical objects. Forces applied to objects result in displacements, or changes of an object's position relative to its environment.
This branch of physics has its origins in Ancient Greece with the writings of Aristotle and Archimedes[1][2][3] (see History of classical mechanics and Timeline of classical mechanics). During the early modern period, scientists such as Galileo, Kepler, and Newton laid the foundation for what is now known as classical mechanics.
It is a branch of classical physics that deals with particles that are either at rest or are moving with velocities significantly less than the speed of light. 
It can also be defined as a branch of science which deals with the motion of and forces on bodies not in the quantum realm. The field is today less widely understood in terms of quantum theory.
"
Melting,Physics,2,"Melting, or fusion, is a physical process that results in the phase transition of a substance from a solid to a liquid. This occurs when the internal energy of the solid increases, typically by the application of heat or pressure, which increases the substance's temperature to the melting point. At the melting point, the ordering of ions or molecules in the solid breaks down to a less ordered state, and the solid melts to become a liquid.
 Substances in the molten state generally have reduced viscosity as  the temperature increases. An exception to this principle is the element sulfur, whose viscosity increases in the range of 160 °C to 180 °C due to polymerization.[1] Some organic compounds melt through mesophases, states of partial order between solid and liquid.
"
Meson,Physics,2,"In particle physics, mesons (/ˈmiːzɒnz/ or /ˈmɛzɒnz/) are hadronic subatomic particles composed of one quark and one antiquark, bound together by strong interactions. Because mesons are composed of quark subparticles, they have a meaningful physical size, a diameter of roughly one femtometer (1×10−15 m),[1] which is about 1.2 times the size of a proton or neutron. All mesons are unstable, with the longest-lived lasting for only a few hundredths of a microsecond. Charged mesons decay (sometimes through mediating particles) to form electrons and neutrinos. Uncharged mesons may decay to photons. Both of these decays imply that color is no longer a property of the byproducts.
 Outside the nucleus, mesons appear in nature only as short-lived products of very high-energy collisions between particles made of quarks, such as cosmic rays (high-energy protons and neutrons) and baryonic matter. Mesons are often produced artificially in a cyclotron in the collisions of protons, antiprotons, or other particles.
 Higher-energy (more massive) mesons were created momentarily in the Big Bang, but are not thought to play a role in nature today. However, such heavy mesons are regularly created in particle accelerator experiments, in order to understand the nature of the heavier types of quark that compose the heavier mesons.
 Mesons are part of the hadron particle family, which are defined simply as particles composed of two or more quarks. The other members of the hadron family are the baryons: subatomic particles composed of odd numbers of valence quarks (at least 3), and some experiments show evidence of exotic mesons, which do not have the conventional valence quark content of two quarks (one quark and one antiquark), but 4 or more.
 Because quarks have a spin 1/2, the difference in quark number between mesons and baryons results in conventional two-quark mesons being bosons, whereas baryons are fermions.
 Each type of meson has a corresponding antiparticle (antimeson) in which quarks are replaced by their corresponding antiquarks and vice versa. For example, a positive pion (π+) is made of one up quark and one down antiquark; and its corresponding antiparticle, the negative pion (π−), is made of one up antiquark and one down quark.
 Because mesons are composed of quarks, they participate in both the weak and strong interactions. Mesons with net electric charge also participate in the electromagnetic interaction. Mesons are classified according to their quark content, total angular momentum, parity and various other properties, such as C-parity and G-parity. Although no meson is stable, those of lower mass are nonetheless more stable than the more massive, and hence are easier to observe and study in particle accelerators or in cosmic ray experiments. Mesons are also typically less massive than baryons, meaning that they are more easily produced in experiments, and thus exhibit certain higher-energy phenomena more readily than do baryons. For example, the charm quark was first seen in the J/Psi meson (J/ψ) in 1974,[2][3] and the bottom quark in the upsilon meson (ϒ) in 1977.[4]"
Modulus_of_elasticity,Physics,2,"An elastic modulus (also known as modulus of elasticity) is a quantity that measures an object or substance's resistance to being deformed elastically (i.e., non-permanently) when a stress is applied to it. The elastic modulus of an object is defined as the slope of its stress–strain curve in the elastic deformation region:[1] A stiffer material will have a higher elastic modulus. An elastic modulus has the form:
 where stress is the force causing the deformation divided by the area to which the force is applied and strain is the ratio of the change in some parameter caused by the deformation to the original value of the parameter. Since strain is a dimensionless quantity, the units of 



δ


{  \delta }
 will be the same as the units of stress.[2] Specifying how stress and strain are to be measured, including directions, allows for many types of elastic moduli to be defined. The three primary ones are:
 Two other elastic moduli are Lamé's first parameter, λ, and P-wave modulus, M, as used in table of modulus comparisons given below references.
 Homogeneous and isotropic (similar in all directions) materials (solids) have their (linear) elastic properties fully described by two elastic moduli, and one may choose any pair.  Given a pair of elastic moduli, all other elastic moduli can be calculated according to formulas in the table below at the end of page.
 Inviscid fluids are special in that they cannot support shear stress, meaning that the shear modulus is always zero. This also implies that Young's modulus for this group is always zero.
 In some texts, the modulus of elasticity is referred to as the elastic constant, while the inverse quantity is referred to as elastic modulus.
"
Molar_concentration,Physics,2,"Molar concentration (also called molarity, amount concentration or substance concentration) is a measure of the concentration of a chemical species, in particular of a solute in a solution, in terms of amount of substance per unit volume of solution. In chemistry, the most commonly used unit for molarity is the number of moles per liter, having the unit symbol mol/L or mol⋅dm−3 in SI unit. A solution with a concentration of 1 mol/L is said to be 1 molar, commonly designated as 1 M.  To avoid confusion with SI prefix mega, which has the same abbreviation, small caps ᴍ or italicized M are also used in journals and textbooks.[1]"
Molar_mass,Physics,2,"In chemistry, the molar mass of a chemical compound is defined as the mass of a sample of that compound divided by the amount of substance in that sample, measured in moles.[1] The molar mass is a bulk, not molecular, property of a substance. The molar mass is an average of many instances of the compound, which often vary in mass due to the presence of isotopes. Most commonly, the molar mass is computed from the standard atomic weights and is thus a terrestrial average and a function of the relative abundance of the isotopes of the constituent atoms on Earth. The molar mass is appropriate for converting between the mass of a substance and the amount of a substance for bulk quantities.
 The molecular weight is very commonly used as a synonym of molar mass, particularly for molecular compounds; however, the most authoritative sources define it differently (see molecular mass).
 The formula weight is a synonym of molar mass that is frequently used for non-molecular compounds, such as ionic salts.
 The molar mass is an intensive property of the substance, that does not depend on the size of the sample. In the International System of Units (SI), the base unit of molar mass is kg/mol. However, for historical reasons, molar masses are almost always expressed in g/mol.
 The mole was defined in such as way that the molar mass of a compound, in g/mol, is numerically equal (for all practical purposes) to the average mass of one molecule, in daltons. Thus, for example, the average mass of a molecule of water is about 18.0153 daltons, and the molar mass of water is about 18.0153 g/mol.
 For chemical elements without isolated molecules, such as carbon and metals, the molar mass is computed dividing by the number of moles of atoms instead. Thus, for example, the molar mass of iron is about 55.845 g/mol.
 Between 1971 and 2019, SI defined the ""amount of substance"" as a separate dimension of measurement, and the mole was defined as the amount of substance that has as many constituent particles as there are atoms in 12 grams of carbon-12. In that period, the molar mass of carbon-12 was thus exactly 12 g/mol, by definition. Since 2019, a mole of any substance has been redefined in the SI as the amount of that substance containing an exactly defined number of particles, N = 6.02214076×1023. Therefore, the molar mass of a compound now is simply the mass of this number of molecules of the compound.
"
Molecule,Physics,2,"
 A molecule is an electrically neutral group of two or more atoms held together by chemical bonds.[4][5][6][7][8] Molecules are distinguished from ions by their lack of electrical charge. 
 In quantum physics, organic chemistry, and biochemistry, the distinction from ions is dropped and molecule is often used when referring to polyatomic ions.
 In the kinetic theory of gases, the term molecule is often used for any gaseous particle regardless of its composition. This violates the definition that a molecule contain two or more atoms, since the noble gases are individual atoms.[9] A molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with two atoms in the oxygen molecule (O2); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (two hydrogen atoms and one oxygen atom; H2O). 
 Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are typically not considered single molecules.[10] Molecules as components of matter are common.  They also make up most of the oceans and atmosphere. Most organic substances are molecules.  The substances of life are molecules, e.g. proteins, the amino acids they are made of, the nucleic acids (DNA & RNA), sugars, carbohydrates, fats, and vitamins. The nutrient minerals ordinarily are not molecules, e.g. iron sulfate.
 However, the majority of familiar solid substances on Earth are not made of molecules. These include all of the minerals that make up the substance of the Earth, soil, dirt, sand, clay, pebbles, rocks, boulders, bedrock, the molten interior, and the core of the Earth. All of these contain many chemical bonds, but are not made of identifiable molecules. 
 No typical molecule can be defined for salts nor for covalent crystals, although these are often composed of repeating unit cells that extend either in a plane, e.g. graphene; or three-dimensionally e.g. diamond, quartz, sodium chloride. The theme of repeated unit-cellular-structure also holds for most metals which are condensed phases with metallic bonding. Thus solid metals are not made of molecules. 
 In glasses, which are solids that exist in a vitreous disordered state, the atoms are held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating unit-cellular-structure that characterizes salts, covalent crystals, and metals.
"
Molecular_physics,Physics,2,"Molecular physics is the study of the physical properties of molecules, the chemical bonds between atoms as well as the molecular dynamics.  Its most important experimental techniques are the various types of spectroscopy; scattering is also used. The field is closely related to atomic physics and overlaps greatly with theoretical chemistry, physical chemistry and chemical physics.[1] In addition to the electronic excitation states which are known from atoms, molecules exhibit rotational and vibrational modes whose energy levels are quantized. The smallest energy differences exist between different rotational states: pure rotational spectra are in the far infrared region (about 30 - 150 μm wavelength) of the electromagnetic spectrum. Vibrational spectra are in the near infrared (about 1 - 5 μm) and spectra resulting from electronic transitions are mostly in the visible and ultraviolet regions. From measuring rotational and vibrational spectra properties of molecules like the distance between the nuclei can be specifically calculated.
 One important aspect of molecular physics is that the essential atomic orbital theory in the field of atomic physics expands to the molecular orbital theory.
"
Moment_(physics),Physics,2,"In physics, a moment is an expression involving the product of a distance and physical quantity, and in this way it accounts for how the physical quantity is located or arranged.  
 Moments are usually defined with respect to a fixed reference point; they deal with physical quantities located at some distance relative to that reference point. For example, the moment of force, often called torque, is the product of a force on an object and the distance from the reference point to the object. In principle, any physical quantity can be multiplied by a distance to produce a moment. Commonly used quantities include forces, masses, and electric charge distributions.
"
Moment_of_inertia,Physics,2,"The moment of inertia, otherwise known as the mass moment of inertia, angular mass or rotational inertia, of a rigid body is a quantity that determines the torque needed for a desired angular acceleration about a rotational axis; similar to how mass determines the force needed for a desired acceleration. It depends on the body's mass distribution and the axis chosen, with larger moments requiring more torque to change the body's rate of rotation. 
 It is an extensive (additive) property: for a point mass the moment of inertia is simply the mass times the square of the perpendicular distance to the axis of rotation. The moment of inertia of a rigid composite system is the sum of the moments of inertia of its component subsystems (all taken about the same axis). Its simplest definition is the second moment of mass with respect to distance from an axis. 
 For bodies constrained to rotate in a plane, only their moment of inertia about an axis perpendicular to the plane, a scalar value, matters. For bodies free to rotate in three dimensions, their moments can be described by a symmetric 3 × 3 matrix, with a set of mutually perpendicular principal axes for which this matrix is diagonal and torques around the axes act independently of each other.
"
Spectral_color,Physics,2,"A spectral color is a color that is evoked in a typical human by a single wavelength of light in the visible spectrum, or by a relatively narrow band of wavelengths, also known as monochromatic light.  Every wavelength of visible light is perceived as a spectral color, in a continuous spectrum; the colors of sufficiently close wavelengths are indistinguishable for the human eye.
 The spectrum is often divided into named colors, though any division is somewhat arbitrary; the spectrum is continuous. Traditional colors in English include: red, orange, yellow, green, blue, and violet. In some other languages the ranges corresponding to color names do not necessarily agree with those in English.
 The division used by Isaac Newton, in his color wheel, was: red, orange, yellow, green, blue, indigo and violet; a mnemonic for this order is ""Roy G. Biv"". Less commonly, ""VIBGYOR"" is also used for the reverse order. In modern divisions of the spectrum, indigo is often omitted.
 One needs at least trichromatic color vision for there to be a distinction between spectral and non-spectral colours: trichromacy gives a possibility to perceive both hue and saturation in the chroma. In color models capable of representing spectral colors,[note 1][1] such as CIELUV, a spectral color has the maximal saturation.
"
Motion_(physics),Physics,2,"In physics, motion is the phenomenon in which an object changes its position over time. Motion is mathematically described in terms of displacement, distance, velocity, acceleration, speed, and time. The motion of a body is observed by attaching a frame of reference to an observer and measuring the change in position of the body relative to that frame with change in time. The branch of physics describing the motion of objects without reference to its cause is kinematics; the branch studying forces and their effect on motion is dynamics.
 If an object is not changing relatively to a given frame of reference, the object is said to be at rest, motionless, immobile, stationary, or to have a constant or time-invariant position with reference to its surroundings. As there is no absolute frame of reference, absolute motion cannot be determined.[1] Thus, everything in the universe can be considered to be in motion.[2]:20–21 Motion applies to various physical systems: to objects, bodies, matter particles, matter fields, radiation, radiation fields, radiation particles, curvature and space-time. One can also speak of motion of images, shapes and boundaries. So, the term motion, in general, signifies a continuous change in the positions or configuration of a physical system in space. For example, one can talk about motion of a wave or about motion of a quantum particle, where the configuration consists of probabilities of occupying specific positions.
 The main quantity that measures the motion of a body is momentum. An object's momentum increases with the object's mass and with its velocity. The total momentum of all objects in an isolated system (one not affected by external forces) does not change with time, as described by the law of conservation of momentum. An object's motion, and thus its momentum, cannot change unless a force acts on the body.
"
Muon,Physics,2,"
 The muon (/ˈmjuːɒn/; from the Greek letter mu (μ) used to represent it) is an elementary particle similar to the electron, with an electric charge of −1 e and a spin of 1/2, but with a much greater mass. It is classified as a lepton. As with other leptons, the muon is not known to have any sub-structure – that is, it is not thought to be composed of any simpler particles.
 105.6583755(23) MeV/c2[1] The muon is an unstable subatomic particle with a mean lifetime of 2.2 μs, much longer than many other subatomic particles. As with the decay of the non-elementary neutron (with a lifetime around 15 minutes), muon decay is slow (by subatomic standards) because the decay is mediated only by the weak interaction (rather than the more powerful strong interaction or electromagnetic interaction), and because the mass difference between the muon and the set of its decay products is small, providing few kinetic degrees of freedom for decay. Muon decay almost always produces at least three particles, which must include an electron of the same charge as the muon and two types of neutrinos.
 Like all elementary particles, the muon has a corresponding antiparticle of opposite charge (+1 e) but equal mass and spin: the antimuon (also called a positive muon). Muons are denoted by μ− and antimuons by μ+. Muons were formerly called ""mu mesons"", but are not classified as mesons by modern particle physicists (see § History), and that name is no longer used by the physics community.
 Muons have a mass of 105.66 MeV/c2, which is about 207 times that of the electron, 




m

e




{  m_{e}}
. More precisely, it is 206.768 2830(46) 




m

e




{  m_{e}}
.[1] Due to their greater mass, muons accelerate more slowly than electrons in electromagnetic fields, and emit less bremsstrahlung (deceleration radiation). This allows muons of a given energy to penetrate far deeper into matter because the deceleration of electrons and muons is primarily due to energy loss by the bremsstrahlung mechanism. For example, so-called ""secondary muons"", created by cosmic rays hitting the atmosphere, can penetrate the atmosphere and reach Earth's land surface and even into deep mines.
 Because muons have a greater mass and energy than the decay energy of radioactivity, they are not produced by radioactive decay. However they are produced in great amounts in high-energy interactions in normal matter, in certain particle accelerator experiments with hadrons, and in cosmic ray interactions with matter. These interactions usually produce pi mesons initially, which almost always decay to muons.
 As with the other charged leptons, the muon has an associated muon neutrino, denoted by νμ, which differs from the electron neutrino and participates in different nuclear reactions.
"
Nanoengineering,Physics,2,"Nanoengineering is the practice of engineering on the nanoscale. It derives its name from the nanometre, a unit of measurement equalling one billionth of a meter.
 Nanoengineering is largely a synonym for nanotechnology, but emphasizes the engineering rather than the pure science aspects of the field.
"
Nanotechnology,Physics,2,"
 Nanotechnology (or ""nanotech"") is the use of matter on an atomic, molecular, and supramolecular scale for industrial purposes. The earliest, widespread description of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabrication of macroscale products, also now referred to as molecular nanotechnology.[1][2] A more generalized description of nanotechnology was subsequently established by the National Nanotechnology Initiative, which defined nanotechnology as the manipulation of matter with at least one dimension sized from 1 to 100 nanometers. This definition reflects the fact that quantum mechanical effects are important at this quantum-realm scale, and so the definition shifted from a particular technological goal to a research category inclusive of all types of research and technologies that deal with the special properties of matter which occur below the given size threshold. It is therefore common to see the plural form ""nanotechnologies"" as well as ""nanoscale technologies"" to refer to the broad range of research and applications whose common trait is size.
 Nanotechnology as defined by size is naturally broad, including fields of science as diverse as surface science, organic chemistry, molecular biology, semiconductor physics, energy storage,[3][4] engineering,[5] microfabrication,[6] and molecular engineering.[7]  The associated research and applications are equally diverse, ranging from extensions of conventional device physics to completely new approaches based upon molecular self-assembly,[8] from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale.
 Scientists currently debate the future implications of nanotechnology. Nanotechnology may be able to create many new materials and devices with a vast range of applications, such as in nanomedicine, nanoelectronics, biomaterials energy production, and consumer products. On the other hand, nanotechnology raises many of the same issues as any new technology, including concerns about the toxicity and environmental impact of nanomaterials,[9] and their potential effects on global economics, as well as speculation about various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted.
"
Navier%E2%80%93Stokes_equations,Physics,2,"In physics, the Navier–Stokes equations (/nævˈjeɪ stoʊks/) are a set of partial differential equations which describe the motion of viscous fluid substances, named after French engineer and physicist Claude-Louis Navier and Anglo-Irish physicist and mathematician George Gabriel Stokes.
 The Navier–Stokes equations mathematically express conservation of momentum and conservation of mass for Newtonian fluids. They are sometimes accompanied by an equation of state relating pressure, temperature  and density.[1] They  arise from applying Isaac Newton's second law to fluid motion, together with the assumption that the stress in the fluid is the sum of a diffusing viscous term (proportional to the gradient of velocity) and a pressure term—hence describing viscous flow. The difference between them and the closely related Euler equations is that Navier–Stokes equations take viscosity into account while the Euler equations model only inviscid flow. As a result, the Navier–Stokes are a parabolic equation and therefore have better analytic properties, at the expense of having less mathematical structure (e.g. they are never completely integrable).
 The Navier–Stokes equations are useful because they describe the physics of many phenomena of scientific and engineering interest. They may be used to model the weather, ocean currents, water flow in a pipe and air flow around a wing. The Navier–Stokes equations, in their full and simplified forms, help with the design of aircraft and cars, the study of blood flow, the design of power stations, the analysis of pollution, and many other things. Coupled with Maxwell's equations, they can be used to model and study magnetohydrodynamics.
 The Navier–Stokes equations are also of great interest in a purely mathematical sense. Despite their wide range of practical uses, it has not yet been proven whether smooth solutions always exist in three dimensions – i.e. they are infinitely differentiable (or even just bounded) at all points in the domain. This is called the Navier–Stokes existence and smoothness problem. The Clay Mathematics Institute has called this one of the seven most important open problems in mathematics and has offered a US$1 million prize for a solution or a counterexample.[2][3]"
Neurophysics,Physics,2,"Neurophysics (or neurobiophysics) is the branch of biophysics dealing with the development and use of physical techniques to gain information about the nervous system on a molecular level.[1] Neurophysics is an interdisciplinary science which applies the approaches and methods of experimental biophysics to study the nervous system. Neurophysics aims to describe various cerebral activities using physics with the purpose of studying them and combining them with other neurosciences to better understand neural processes.  The properties studied are mostly electrical, mechanical or fluidic. The term ""neurophysics"" is a portmanteau of ""neuron"" and ""physics"".
 Among other examples, the theorisation of ectopic action potentials in neurons using a Kramers-Moyal expansion[2] and the description of physical phenomena measured during an EEG using a dipole approximation[3] use neurophysics to better understand neural activity.
"
Neutrino,Physics,2,"A neutrino (/nuːˈtriːnoʊ/ or /njuːˈtriːnoʊ/) (denoted by the Greek letter ν) is a fermion (an elementary particle with spin of 1/2) that interacts only via the weak subatomic force and gravity.[2][3] The neutrino is so named because it is electrically neutral and because its rest mass is so small (-ino) that it was long thought to be zero. The mass of the neutrino is much smaller than that of the other known elementary particles.[1] The weak force has a very short range, the gravitational interaction is extremely weak, and neutrinos do not participate in the strong interaction.[4] Thus, neutrinos typically pass through normal matter unimpeded and undetected.[2][3] Weak interactions create neutrinos in one of three leptonic flavors: electron neutrinos (νe), muon neutrinos (νμ), or tau neutrinos (ντ), in association with the corresponding charged lepton.[5] Although neutrinos were long believed to be massless, it is now known that there are three discrete neutrino masses with different tiny values, but they do not correspond uniquely to the three flavors. A neutrino created with a specific flavor has an associated specific quantum superposition of all three mass states. As a result, neutrinos oscillate between different flavors in flight. For example, an electron neutrino produced in a beta decay reaction may interact in a distant detector as a muon or tau neutrino.[6][7] Although only differences between squares of the three mass values are known as of 2019,[8] cosmological observations imply that the sum of the three masses must be less than one millionth that of the electron.[1][9] For each neutrino, there also exists a corresponding antiparticle, called an antineutrino, which also has spin of 1/2 and no electric charge. Antineutrinos are distinguished from the neutrinos by having opposite signs of lepton number and right-handed instead of left-handed chirality. To conserve total lepton number (in nuclear beta decay), electron neutrinos only appear together with positrons (anti-electrons) or electron-antineutrinos, whereas electron antineutrinos only appear with electrons or electron neutrinos.[10][11] Neutrinos are created by various radioactive decays; the following list is not exhaustive, but includes some of those processes:
 The majority of neutrinos which are detected about the Earth are from nuclear reactions inside the Sun. At the surface of the Earth, the flux is about 65 billion (6.5×1010) solar neutrinos, per second per square centimeter.[12][13] Neutrinos can be used for tomography of the interior of the earth.[14][15] Research is intense in the hunt to elucidate the essential nature of neutrinos, with aspirations of finding:
"
Neutron,Physics,2,"The neutron is a subatomic particle, symbol n or n0, which has a neutral (not positive or negative) charge and a mass slightly greater than that of a proton. Protons and neutrons constitute the nuclei of atoms. Since protons and neutrons behave similarly within the nucleus, and each has a mass of approximately one atomic mass unit, they are both referred to as nucleons.[6] Their properties and interactions are described by nuclear physics.
 The chemical properties of an atom are mostly determined by the configuration of electrons that orbit the atom's heavy nucleus. The electron configuration is determined by the charge of the nucleus, set by the number of protons, or atomic number. Neutrons do not affect the electron configuration, but the sum of atomic number and the number of neutrons, or neutron number, is the mass of the nucleus.
 Atoms of a chemical element that differ only in neutron number are called isotopes. For example, carbon, with atomic number 6, has an abundant isotope carbon-12 with 6 neutrons and a rare isotope carbon-13 with 7 neutrons.  Some elements occur in nature with only one stable isotope, such as fluorine. Other elements occur with many stable isotopes, such as tin with ten stable isotopes.
 The properties of an atomic nucleus are dependent on both atomic and neutron numbers. With their positive charge, the protons within the nucleus are repelled by the long-range electromagnetic force, but the much stronger, but short-range, nuclear force binds the nucleons closely together. Neutrons are required for the stability of nuclei, with the exception of the single-proton hydrogen nucleus. Neutrons are produced copiously in nuclear fission and fusion. They are a primary contributor to the nucleosynthesis of chemical elements within stars through fission, fusion, and neutron capture processes.
 The neutron is essential to the production of nuclear power. In the decade after the neutron was discovered by James Chadwick in 1932,[7] neutrons were used to induce many different types of nuclear transmutations. With the discovery of nuclear fission in 1938,[8] it was quickly realized that, if a fission event produced neutrons, each of these neutrons might cause further fission events, in a cascade known as a nuclear chain reaction.[9] These events and findings led to the first self-sustaining nuclear reactor (Chicago Pile-1, 1942) and the first nuclear weapon (Trinity, 1945).
 Free neutrons, while not directly ionizing atoms, cause ionizing radiation. As such they can be a biological hazard, depending upon dose.[9] A small natural ""neutron background"" flux of free neutrons exists on Earth, caused by cosmic ray showers, and by the natural radioactivity of spontaneously fissionable elements in the Earth's crust.[10] Dedicated neutron sources like neutron generators, research reactors and spallation sources produce free neutrons for use in irradiation and in neutron scattering experiments.
"
Neutron_cross-section,Physics,2,"In nuclear and particle physics, the concept of a neutron cross section is used to express the likelihood of interaction between an incident neutron and a target nucleus. In conjunction with the neutron flux, it enables the calculation of the reaction rate, for example to derive the thermal power of a nuclear power plant. The standard unit for measuring the cross section is the barn, which is equal to 10−28 m2 or 10−24 cm2. The larger the neutron cross section, the more likely a neutron will react with the nucleus.
 An isotope (or nuclide) can be classified according to its neutron cross section and how it reacts to an incident neutron. Nuclides that tend to absorb a neutron and either decay or keep the neutron in its nucleus are neutron absorbers and will have a capture cross section for that reaction. Isotopes that fission are fissionable fuels and have a corresponding fission cross section. The remaining isotopes will simply scatter the neutron, and have a scatter cross section. Some isotopes, like uranium-238, have nonzero cross sections of all three.
 Isotopes which have a large scatter cross section and a low mass are good neutron moderators (see chart below). Nuclides which have a large absorption cross section are neutron poisons if they are neither fissile nor undergo decay. A poison that is purposely inserted into a nuclear reactor for controlling its reactivity in the long term and improve its shutdown margin is called a burnable poison.
"
Newton_(unit),Physics,2,"
 The newton (symbol: N) is the International System of Units (SI) derived unit of force. It is named after Isaac Newton in recognition of his work on classical mechanics, specifically Newton's second law of motion. One newton is the force needed to accelerate one kilogram of mass at the rate of one metre per second squared in the direction of the applied force.
 See below for the conversion factors. 
"
Newton%27s_laws_of_motion,Physics,2,"
 Newton's laws of motion are three physical laws that, together, laid the foundation for classical mechanics. They describe the relationship between a body and the forces acting upon it, and its motion in response to those forces. More precisely, the first law defines the force qualitatively, the second law offers a quantitative measure of the force, and the third asserts that a single isolated force does not exist. These three laws have been expressed in several ways, over nearly three centuries,[a] and can be summarised as follows:
 The three laws of motion were first compiled by Isaac Newton in his Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), first published in 1687.[4] Newton used them to explain and investigate the motion of many physical objects and systems.[5] For example, in the third volume of the text, Newton showed that these laws of motion, combined with his law of universal gravitation, explained Kepler's laws of planetary motion.
 Some also describe a fourth law which states that forces add up like vectors, that is, that forces obey the principle of superposition.[6][7][8]"
Newton%27s_law_of_universal_gravitation,Physics,2,"Newton's law of universal gravitation is usually stated as that every particle attracts every other particle in the universe with a force that is directly proportional to the product of their masses and inversely proportional to the square of the distance between their centers.[note 1] The publication of the theory has become known as the ""first great unification"", as it marked the unification of the previously described phenomena of gravity on Earth with known astronomical behaviors.[1][2][3] This is a general physical law derived from empirical observations by what Isaac Newton called inductive reasoning.[4] It is a part of classical mechanics and was formulated in Newton's work Philosophiæ Naturalis Principia Mathematica (""the Principia""), first published on 5 July 1687. When Newton presented Book 1 of the unpublished text in April 1686 to the Royal Society, Robert Hooke made a claim that Newton had obtained the inverse square law from him.
 In today's language, the law states that every point mass attracts every other point mass by a force acting along the line intersecting the two points. The force is proportional to the product of the two masses, and inversely proportional to the square of the distance between them.[5] The equation for universal gravitation thus takes the form:
 where F is the gravitational force acting between two objects, m1 and m2 are the masses of the objects, r is the distance between the centers of their masses, and G is the gravitational constant.
 The first test of Newton's theory of gravitation between masses in the laboratory was the Cavendish experiment conducted by the British scientist Henry Cavendish in 1798.[6] It took place 111 years after the publication of Newton's Principia and approximately 71 years after his death.
 Newton's law of gravitation resembles Coulomb's law of electrical forces, which is used to calculate the magnitude of the electrical force arising between two charged bodies. Both are inverse-square laws, where force is inversely proportional to the square of the distance between the bodies. Coulomb's law has the product of two charges in place of the product of the masses, and the Coulomb constant in place of the gravitational constant.
 Newton's law has since been superseded by Albert Einstein's theory of general relativity, but it continues to be used as an excellent approximation of the effects of gravity in most applications. Relativity is required only when there is a need for extreme accuracy, or when dealing with very strong gravitational fields, such as those found near extremely massive and dense objects, or at small distances (such as Mercury's orbit around the Sun).
"
Newtonian_fluid,Physics,2,"A Newtonian fluid is a fluid in which the viscous stresses arising from its flow, at every point, are linearly[1] correlated to the local strain rate—the rate of change of its deformation over time.[2][3][4] That is equivalent to saying those forces are proportional to the rates of change of the fluid's velocity vector as one moves away from the point in question in various directions.
 More precisely, a fluid is Newtonian only if the tensors that describe the viscous stress and the strain rate are related by a constant viscosity tensor that does not depend on the stress state and velocity of the flow. If the fluid is also isotropic (that is, its mechanical properties are the same along any direction), the viscosity tensor reduces to two real coefficients, describing the fluid's resistance to continuous shear deformation and continuous compression or expansion, respectively.
 Newtonian fluids are the simplest mathematical models of fluids that account for viscosity. While no real fluid fits the definition perfectly, many common liquids and gases, such as water and air, can be assumed to be Newtonian for practical calculations under ordinary conditions. However, non-Newtonian fluids are relatively common, and include oobleck (which becomes stiffer when vigorously sheared), or non-drip paint (which becomes thinner when sheared). Other examples include many polymer solutions (which exhibit the Weissenberg effect), molten polymers, many solid suspensions, blood, and most highly viscous fluids.
 Newtonian fluids are named after Isaac Newton, who first used the differential equation to postulate the relation between the shear strain rate and shear stress for such fluids.
"
Newtonian_mechanics,Physics,2,"
 Classical[note 1] mechanics is a physical theory describing the motion of macroscopic objects, from projectiles to parts of machinery, and astronomical objects, such as spacecraft, planets, stars and galaxies.  For objects governed by classical mechanics, if the present state is known, it is possible to predict how it will move in the future (determinism) and how it has moved in the past (reversibility).
 The earliest development of classical mechanics is often referred to as Newtonian mechanics. It consists of the physical concepts employed and the mathematical methods invented by Isaac Newton, Gottfried Wilhelm Leibniz and others in the 17th century to describe the motion of bodies under the influence of a system of forces.  Later, more abstract methods were developed, leading to the reformulations of classical mechanics known as Lagrangian mechanics and Hamiltonian mechanics. These advances, made predominantly in the 18th and 19th centuries, extend substantially beyond Newton's work, particularly through their use of analytical mechanics. They are, with some modification, also used in all areas of modern physics.
 Classical mechanics provides extremely accurate results when studying large objects that are not extremely massive and speeds not approaching the speed of light. When the objects being examined have about the size of an atom diameter, it becomes necessary to introduce the other major sub-field of mechanics: quantum mechanics. To describe velocities that are not small compared to the speed of light, special relativity is needed. In cases where objects become extremely massive,  general relativity becomes applicable. However, a number of modern sources do include relativistic mechanics in classical physics, which in their view represents classical mechanics in its most developed and accurate form.
"
Normal_force,Physics,2,"In mechanics, the normal force 




F

n


 


{  F_{n}\ }
 is the component of a contact force that is perpendicular to the surface that an object contacts.[1] For example, the surface of a floor or table that prevents an object from falling. In this instance normal is used in the geometric sense and means perpendicular, as opposed to the common language use of normal meaning common or expected. For example, a person standing still on flat ground is supported by a ground reaction force that consists of only a normal force. If the person stands on a slope and does not slide down it, then the total ground reaction force can be divided into two components: a normal force perpendicular to the ground and a frictional force parallel to the ground. In another common situation, if an object hits a surface with some speed, and the surface can withstand it, the normal force provides for a rapid deceleration, which will depend on the flexibility of the surface and the object.
"
Nth_root,Physics,2,"In mathematics, an nth root of a number x is a number r which, when raised to the power n, yields x:
 where n is a positive integer, sometimes called the degree of the root.  A root of degree 2 is called a square root and a root of degree 3, a cube root. Roots of higher degree are referred by using ordinal numbers, as in fourth root, twentieth root, etc.  The computation of an nth root is a root extraction.
 For example, 3 is a square root of 9, since 32 = 9, and −3 is also a square root of 9, since (−3)2 = 9.
 Any non-zero number considered as a complex number has n different complex nth roots, including the real ones (at most two). The nth root of 0 is zero for all positive integers n, since 0n = 0. In particular, if n is even and x is a positive real number, one of its nth roots is real and positive, one is negative, and the others (when n > 2) are non-real complex numbers; if n is even and x is a negative real number, none of the nth roots is real. If n is odd and x is real, one nth root is real and has the same sign as x, while the other (n – 1) roots are not real. Finally, if x is not real, then none of its nth roots are real.
 Roots of real numbers are usually written using the radical symbol or radix with 





x




{  {\sqrt {x}}}
 denoting the positive square root of x if x is positive, and 





x

n





{  {\sqrt[{n}]{x}}}
 denoting the real nth root, if n is odd, and the positive square root if n is even and x is nonnegative. In the other cases, the symbol is not commonly used as being ambiguous. In the expression 





x

n





{  {\sqrt[{n}]{x}}}
, the integer n is called the index, 








 

 




 








{  {\sqrt {{~^{~}}^{~}\!\!}}}
 is the radical sign or radix, and x is called the radicand. 
 When complex nth roots are considered, it is often useful to choose one of the roots as a principal value. The common choice is the one that makes the nth root a continuous function that is real and positive for x real and positive. More precisely, the principal nth root of x is the nth root, with the greatest real part, and, when there are two (for x real and negative), the one with a positive imaginary part. 
 A difficulty with this choice is that, for a negative real number and an odd index, the principal nth root is not the real one. For example, 



−
8


{  -8}
 has three cube roots, 



−
2


{  -2}
, 



1
+
i


3




{  1+i{\sqrt {3}}}
 and 



1
−
i


3


.


{  1-i{\sqrt {3}}.}
 The real cube root is 



−
2


{  -2}
 and the principal cube root is 



1
+
i


3


.


{  1+i{\sqrt {3}}.}

 An unresolved root, especially one using the radical symbol, is sometimes referred to as a surd[1] or a radical.[2] Any expression containing a radical, whether it is a square root, a cube root, or a higher root, is called a radical expression, and if it contains no transcendental functions or transcendental numbers it is called an algebraic expression.
 Roots can also be defined as special cases of exponentiation, where the exponent is a fraction:
 Roots are used for determining the radius of convergence of a power series with the root test. The nth roots of 1 are called roots of unity and play a fundamental role in various areas of mathematics, such as number theory, theory of equations, and Fourier transform.
"
Nuclear_force,Physics,2,"The nuclear force (or nucleon–nucleon interaction or residual strong force) is a force that acts between the protons and neutrons of atoms. Neutrons and protons, both nucleons, are affected by the nuclear force almost identically. Since protons have charge +1 e, they experience an electric force that tends to push them apart, but at short range the attractive nuclear force is strong enough to overcome the electromagnetic force.  The nuclear force binds nucleons into atomic nuclei.
 The nuclear force is powerfully attractive between nucleons at distances of about 1 femtometre (fm, or 1.0 × 10−15 metres), but it rapidly decreases to insignificance at distances beyond about 2.5 fm. At distances less than 0.7 fm, the nuclear force becomes repulsive. This repulsive component is responsible for the physical size of nuclei, since the nucleons can come no closer than the force allows. By comparison, the size of an atom, measured in angstroms (Å, or 1.0 × 10−10 m), is five orders of magnitude larger. The nuclear force is not simple, however, since it depends on the nucleon spins, has a tensor component, and may depend on the relative momentum of the nucleons.[2] The nuclear force plays an essential role in storing energy that is used in nuclear power and nuclear weapons.  Work (energy) is required to bring charged protons together against their electric repulsion.  This energy is stored when the protons and neutrons are bound together by the nuclear force to form a nucleus.  The mass of a nucleus is less than the sum total of the individual masses of the protons and neutrons.  The difference in masses is known as the mass defect, which can be expressed as an energy equivalent. Energy is released when a heavy nucleus breaks apart into two or more lighter nuclei.  This energy is the electromagnetic potential energy that is released when the nuclear force no longer holds the charged nuclear fragments together.[3][4] A quantitative description of the nuclear force relies on equations that are partly empirical.  These equations model the internucleon potential energies, or potentials. (Generally, forces within a system of particles can be more simply modeled by describing the system's potential energy; the negative gradient of a potential is equal to the vector force.) The constants for the equations are phenomenological, that is, determined by fitting the equations to experimental data. The internucleon potentials attempt to describe the properties of nucleon–nucleon interaction. Once determined, any given potential can be used in, e.g., the Schrödinger equation to determine the quantum mechanical properties of the nucleon system.
 The discovery of the neutron in 1932 revealed that atomic nuclei were made of protons and neutrons, held together by an attractive force. By 1935 the nuclear force was conceived to be transmitted by particles called mesons. This theoretical development included a description of the Yukawa potential, an early example of a nuclear potential. Pions, fulfilling the prediction, were discovered experimentally in 1947. By the 1970s, the quark model had been developed, by which the mesons and nucleons were viewed as composed of quarks and gluons. By this new model, the nuclear force, resulting from the exchange of mesons between neighboring nucleons, is a residual effect of the strong force.
"
Nuclear_physics,Physics,2,"Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions. Other forms of nuclear matter are also studied.[1] Nuclear physics should not be confused with atomic physics, which studies the atom as a whole, including its electrons.
 Discoveries in nuclear physics have led to applications in many fields. This includes nuclear power, nuclear weapons, nuclear medicine and magnetic resonance imaging, industrial and agricultural isotopes, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology. Such applications are studied in the field of nuclear engineering.
 Particle physics evolved out of nuclear physics and the two fields are typically taught in close association. Nuclear astrophysics, the application of nuclear physics to astrophysics, is crucial in explaining the inner workings of stars and the origin of the chemical elements.
"
Nuclear_reaction,Physics,2,"In nuclear physics and nuclear chemistry, a nuclear reaction is semantically considered to be the process in which two nuclei, or a nucleus and an external subatomic particle, collide to produce one or more new nuclides. Thus, a nuclear reaction must cause a transformation of at least one nuclide to another. If a nucleus interacts with another nucleus or particle and they then separate without changing the nature of any nuclide, the process is simply referred to as a type of nuclear scattering, rather than a nuclear reaction.
 In principle, a reaction can involve more than two particles colliding, but because the probability of three or more nuclei to meet at the same time at the same place is much less than for two nuclei, such an event is exceptionally rare (see triple alpha process for an example very close to a three-body nuclear reaction). The term ""nuclear reaction"" may refer either to a change in a nuclide induced by collision with another particle, or to a spontaneous change of a nuclide without collision.
 Natural nuclear reactions occur in the interaction between cosmic rays and matter, and nuclear reactions can be employed artificially to obtain nuclear energy, at an adjustable rate, on demand. Perhaps the most notable nuclear reactions are the nuclear chain reactions in fissionable materials that produce induced nuclear fission, and the various nuclear fusion reactions of light elements that power the energy production of the Sun and stars.
"
Nuclear_transmutation,Physics,2,"Nuclear transmutation is the conversion of one chemical element or an isotope into another chemical element.[1] Because any element (or isotope of one) is defined by its number of protons (and neutrons) in its atoms, i.e. in the atomic nucleus, nuclear transmutation occurs in any process where the number of protons or neutrons in the nucleus is changed.
 A transmutation can be achieved either by nuclear reactions (in which an outside particle reacts with a nucleus) or by radioactive decay, where no outside cause is needed.
 Natural transmutation by stellar nucleosynthesis in the past created most of the heavier chemical elements in the known existing universe, and continues to take place to this day, creating the vast majority of the most common elements in the universe, including helium, oxygen and carbon. Most stars carry out transmutation through fusion reactions involving hydrogen and helium, while much larger stars are also capable of fusing heavier elements up to iron late in their evolution.
 Elements heavier than iron, such as gold or lead, are created through elemental transmutations that can only naturally occur in supernovae.  As stars begin to fuse heavier elements, substantially less energy is released from each fusion reaction. Reactions that produce elements heavier than iron are endothermic  and unable to generate the energy required to maintain stable fusion inside the star.
 One type of natural transmutation observable in the present occurs when certain radioactive elements present in nature spontaneously decay by a process that causes transmutation, such as alpha or beta decay. An example is the natural decay of potassium-40 to argon-40, which forms most of the argon in the air. Also on Earth, natural transmutations from the different mechanisms of natural nuclear reactions occur, due to cosmic ray bombardment of elements (for example, to form carbon-14), and also occasionally from natural neutron bombardment (for example, see natural nuclear fission reactor).
 Artificial transmutation may occur in machinery that has enough energy to cause changes in the nuclear structure of the elements. Such machines include particle accelerators and tokamak reactors. Conventional fission power reactors also cause artificial transmutation, not from the power of the machine, but by exposing elements to neutrons produced by fission from an artificially produced nuclear chain reaction. For instance, when a uranium atom is bombarded with slow neutrons, fission takes place. This releases, on average, 3 neutrons and a large amount of energy. The released neutrons then cause fission of other uranium atoms, until all of the available uranium is exhausted. This is called a chain reaction.
 Artificial nuclear transmutation has been considered as a possible mechanism for reducing the volume and hazard of radioactive waste.[2]"
Nucleon,Physics,2,"In chemistry and physics, a nucleon is either a proton or a neutron, considered in its role as a component of an atomic nucleus. The number of nucleons in a nucleus defines an isotope's mass number (nucleon number).
 Until the 1960s, nucleons were thought to be elementary particles, not made up of smaller parts. Now they are known to be composite particles, made of three quarks bound together by the so-called strong interaction. The interaction between two or more nucleons is called internucleon interaction or nuclear force, which is also ultimately caused by the strong interaction. (Before the discovery of quarks, the term ""strong interaction"" referred to just internucleon interactions.)
 Nucleons sit at the boundary where particle physics and nuclear physics overlap. Particle physics, particularly quantum chromodynamics, provides the fundamental equations that explain the properties of quarks and of the strong interaction. These equations explain quantitatively how quarks can bind together into protons and neutrons (and all the other hadrons). However, when multiple nucleons are assembled into an atomic nucleus (nuclide), these fundamental equations become too difficult to solve directly (see lattice QCD). Instead, nuclides are studied within nuclear physics, which studies nucleons and their interactions by approximations and models, such as the nuclear shell model. These models can successfully explain nuclide properties, as for example, whether or not a particular nuclide undergoes radioactive decay.
 The proton and neutron are in a scheme of categories being at once fermions, hadrons and baryons. The proton carries a positive net charge and the neutron carries a zero net charge; the proton's mass is only about 0.13% less than the neutron's. Thus, they can be viewed as two states of the same nucleon, and together form an isospin doublet (I = ​1⁄2). In isospin space, neutrons can be transformed into protons via SU(2) symmetries, and vice versa. These nucleons are acted upon equally by the strong interaction, which is invariant under rotation in isospin space. According to the Noether theorem, isospin is conserved with respect to the strong interaction.[1]:129–130"
Atomic_nucleus,Physics,2,"
 The atomic nucleus is the small, dense region consisting of protons and neutrons at the center of an atom, discovered in 1911 by Ernest Rutherford based on the 1909 Geiger–Marsden gold foil experiment.  After the discovery of the neutron in 1932, models for a nucleus composed of protons and neutrons were quickly developed by Dmitri Ivanenko[1] and Werner Heisenberg.[2][3][4][5][6]  An atom is composed of a positively-charged nucleus, with a cloud of negatively-charged electrons surrounding it, bound together by electrostatic force.  Almost all of the mass of an atom is located in the nucleus, with a very small contribution from the electron cloud.  Protons and neutrons are bound together to form a nucleus by the nuclear force.
 The diameter of the nucleus is in the range of 1.7566 fm (1.7566×10−15 m) for hydrogen (the diameter of a single proton) to about 11.7142 fm for uranium.[7] These dimensions are much smaller than the diameter of the atom itself (nucleus + electron cloud), by a factor of about 26,634 (uranium atomic radius is about 156 pm (156×10−12 m))[8] to about 60,250 (hydrogen atomic radius is about 52.92 pm).[a] The branch of physics concerned with the study and understanding of the atomic nucleus, including its composition and the forces which bind it together, is called nuclear physics.
"
Nuclide,Physics,2,"A nuclide (or nucleide, from nucleus, also known as nuclear species) is an atom characterized by its number of protons, Z, its number of neutrons, N, and its nuclear energy state.[1] The word nuclide was coined by Truman P. Kohman in 1947.[2][3] Kohman defined nuclide as a ""species of atom characterized by the constitution of its nucleus"" containing a certain number of neutrons and protons. The term thus originally focused on the nucleus.
"
Ohm,Physics,2,"
 The ohm (symbol: Ω) is the SI derived unit of electrical resistance, named after German physicist Georg Ohm. Various empirically derived standard units for electrical resistance were developed in connection with early telegraphy practice, and the British Association for the Advancement of Science proposed a unit derived from existing units of mass, length and time, and of a convenient scale for practical work as early as 1861. As of 2020, the definition of the ohm is expressed in terms of the quantum Hall effect.
"
Ohm%27s_law,Physics,2,"
 Ohm's law states that the current through a conductor between two points is directly proportional to the voltage across the two points. Introducing the constant of proportionality, the resistance,[1] one arrives at the usual mathematical equation that describes this relationship:[2] where I is the current through the conductor in units of amperes, V is the voltage measured across the conductor in units of volts,  and R is the resistance of the conductor in units of ohms.  More specifically, Ohm's law states that the R in this relation is constant, independent of the current.[3]  Ohm's law is an empirical relation which accurately describes the conductivity of the vast majority of electrically conductive materials over many orders of magnitude of current.  However some materials do not obey Ohm's law, these are called non-ohmic.
 The law was named after the German physicist Georg Ohm, who, in a treatise published in 1827, described measurements of applied voltage and current through simple electrical circuits containing various lengths of wire. Ohm explained his experimental results by a slightly more complex equation than the modern form above (see § History below).
 In physics, the term Ohm's law is also used to refer to various generalizations of the law; for example the vector form of the law used in electromagnetics and material science:
 where J is the current density at a given location in a resistive material, E is the electric field at that location, and σ (sigma) is a material-dependent parameter called the conductivity.  This reformulation of Ohm's law is due to Gustav Kirchhoff.[4]"
Optical_tweezers,Physics,2,"Optical tweezers (originally called single-beam gradient force trap) are scientific instruments that use a highly focused laser beam to hold and move microscopic objects like atoms, nanoparticles and droplets, in a manner similar to tweezers. If the object is held in air or vacuum without additional support, it can be called optical levitation.
 The laser light provides an attractive or repulsive force (typically on the order of piconewtons), depending on the relative refractive index between particle and surrounding medium. Levitation is possible if the force of the light counters the force of gravity. The trapped particles are usually micron-sized, or smaller. Dielectric and absorbing particles can be trapped, too. 
 Optical tweezers are used in biology and medicine (for example to grab and hold a single bacterium or cell like a sperm cell, blood cell or DNA), nanoengineering and nanochemistry (to study and build materials from single molecules), quantum optics and quantum optomechanics (to study the interaction of single particles with light). The development of optical tweezing by Arthur Ashkin was lauded with the 2018 Nobel Prize in Physics.
"
Optically_detected_magnetic_resonance,Physics,2,"In physics, Optically Detected Magnetic Resonance (ODMR) is a double resonance technique by which the electron spin state of a crystal defect may be optically pumped for spin initialisation and readout.[1] Like electron paramagnetic resonance (EPR), ODMR makes use of the Zeeman effect in unpaired electrons. The negatively charged nitrogen vacancy centre (NV−) has been the target of considerable interest with regards to performing experiments using ODMR.[2] ODMR of NV−s in diamond has applications in magnetometry [3] and sensing, biomedical imaging, quantum information and the exploration of fundamental physics.
"
Optics,Physics,2,"Optics is the branch of physics that studies the behaviour and properties of light, including its interactions with matter and the construction of instruments that use or detect it.[1] Optics usually describes the behaviour of visible, ultraviolet, and infrared light. Because light is an electromagnetic wave, other forms of electromagnetic radiation such as X-rays, microwaves, and radio waves exhibit similar properties.[1] Most optical phenomena can be accounted for by using the classical electromagnetic description of light. Complete electromagnetic descriptions of light are, however, often difficult to apply in practice. Practical optics is usually done using simplified models. The most common of these, geometric optics, treats light as a collection of rays that travel in straight lines and bend when they pass through or reflect from surfaces. Physical optics is a more comprehensive model of light, which includes wave effects such as diffraction and interference that cannot be accounted for in geometric optics. Historically, the ray-based model of light was developed first, followed by the wave model of light. Progress in electromagnetic theory in the 19th century led to the discovery that light waves were in fact electromagnetic radiation.
 Some phenomena depend on the fact that light has both wave-like and particle-like properties. Explanation of these effects requires quantum mechanics. When considering light's particle-like properties, the light is modelled as a collection of particles called ""photons"". Quantum optics deals with the application of quantum mechanics to optical systems.
 Optical science is relevant to and studied in many related disciplines including astronomy, various engineering fields, photography, and medicine (particularly ophthalmology and optometry). Practical applications of optics are found in a variety of technologies and everyday objects, including mirrors, lenses, telescopes, microscopes, lasers, and fibre optics.
"
Paraffin_wax,Physics,2,"
 Paraffin wax (or petroleum wax) is a soft colorless solid derived from petroleum, coal or shale oil that consists of a mixture of hydrocarbon molecules containing between twenty and forty carbon atoms. It is solid at room temperature and begins to melt above approximately 37 °C (99 °F),[2] and its boiling point is above 370 °C (698 °F).[3] Common applications for paraffin wax include lubrication, electrical insulation, and candles;[4] dyed paraffin wax can be made into crayons. It is distinct from kerosene and other petroleum products that are sometimes called paraffin.[5] Un-dyed, unscented paraffin candles are odorless and bluish-white. Paraffin wax was first created by Carl Reichenbach in Germany in 1830 and marked a major advancement in candlemaking technology, as it burned more cleanly and reliably than tallow candles and was cheaper to produce.[6] In chemistry, paraffin is used synonymously with alkane, indicating hydrocarbons with the general formula CnH2n+2. The name is derived from Latin parum (""barely"") + affinis, meaning ""lacking affinity"" or ""lacking reactivity"", referring to paraffin's unreactive nature.[7]"
Series_and_parallel_circuits,Physics,2,"
 Components of an electrical circuit or electronic circuit can be connected in series, parallel, or series-parallel.  The two simplest of these are called series and parallel and occur frequently. Components connected in series are connected along a single conductive path, so the same current flows through all of the components but voltage is dropped (lost) across each of the resistances. In a series circuit, the sum of the voltages consumed by each individual resistance is equal to the source voltage.[1][2] Components connected in parallel are connected along multiple paths so that the current can split up; the same voltage is applied to each component.[1] A circuit composed solely of components connected in series is known as a series circuit; likewise, one connected completely in parallel is known as a parallel circuit.
 In a series circuit, the current that flows through each of the components is the same, and the voltage across the circuit is the sum of the individual voltage drops across each component.[1] In a parallel circuit, the voltage across each of the components is the same, and the total current is the sum of the currents flowing through each component.[1] Consider a very simple circuit consisting of four light bulbs and a 12-volt automotive battery. If a wire joins the battery to one bulb, to the next bulb, to the next bulb, to the next bulb, then back to the battery in one continuous loop, the bulbs are said to be in series. If each bulb is wired to the battery in a separate loop, the bulbs are said to be in parallel. If the four light bulbs are connected in series, the same current flows through all of them and the voltage drop is 3-volts across each bulb, which may not be sufficient to make them glow.  If the light bulbs are connected in parallel, the currents through the light bulbs combine to form the current in the battery, while the voltage drop is 12-volts across each bulb and they all glow.
 In a series circuit, every device must function for the circuit to be complete. If one bulb burns out in a series circuit, the entire circuit is broken. In parallel circuits, each light bulb has its own circuit, so all but one light could be burned out, and the last one will still function.
"
Parity_(physics),Physics,2,"In quantum mechanics, a parity transformation (also called parity inversion) is the flip in the sign of one spatial coordinate. In three dimensions, it can also refer to the simultaneous flip in the sign of all three spatial coordinates (a point reflection):
 It can also be thought of as a test for chirality of a physical phenomenon, in that a parity inversion transforms a phenomenon into its mirror image. All fundamental interactions of elementary particles, with the exception of the weak interaction, are symmetric under parity. The weak interaction is chiral and thus provides a means for probing chirality in physics. In interactions that are symmetric under parity, such as electromagnetism in atomic and molecular physics, parity serves as a powerful controlling principle underlying quantum transitions.
 A matrix representation of P (in any number of dimensions) has determinant equal to −1, and hence is distinct from a rotation, which has a determinant equal to 1. In a two-dimensional plane, a simultaneous flip of all coordinates in sign is not a parity transformation; it is the same as a 180°-rotation.
 In quantum mechanics, wave functions that are unchanged by a parity transformation are described as even functions, while those that change sign under a parity transformation are odd functions.
"
Particle,Physics,2,"In the physical sciences, a particle (or corpuscule in older texts) is a small localized object to which can be ascribed several physical or chemical properties such as volume, density or mass.[1][2] They vary greatly in size or quantity, from subatomic particles like the electron, to microscopic particles like atoms and molecules, to macroscopic particles like powders and other granular materials. Particles can also be used to create scientific models of even larger objects depending on their density, such as humans moving in a crowd or celestial bodies in motion.
 The term 'particle' is rather general in meaning, and is refined as needed by various scientific fields. Anything that is composed of particles may be referred to as being particulate.[3] However, the noun 'particulate' is most frequently used to refer to pollutants in the Earth's atmosphere, which are a suspension of unconnected particles, rather than a connected particle aggregation.
"
Particle_accelerator,Physics,2,"A particle accelerator is a machine that uses electromagnetic fields to propel charged particles to very high speeds and energies, and to contain them in well-defined beams.[1] Large accelerators are used for basic research in particle physics. The largest accelerator currently operating is the Large Hadron Collider (LHC) near Geneva, Switzerland, operated by the CERN.  It is a collider accelerator, which can accelerate two beams of protons to an energy of 6.5 TeV and cause them to collide head-on, creating center-of-mass energies of 13 TeV.  Other powerful accelerators are, RHIC at Brookhaven National Laboratory in New York and, formerly, the Tevatron at Fermilab, Batavia, Illinois. Accelerators are also used as synchrotron light sources for the study of condensed matter physics. Smaller particle accelerators are used in a wide variety of applications, including particle therapy for oncological purposes, radioisotope production for medical diagnostics, ion implanters for manufacture of semiconductors, and accelerator mass spectrometers for measurements of rare isotopes such as radiocarbon. There are currently more than 30,000 accelerators in operation around the world.[2] There are two basic classes of accelerators: electrostatic and electrodynamic (or electromagnetic) accelerators.[3] Electrostatic accelerators use static electric fields to accelerate particles. The most common types are the Cockcroft–Walton generator and the Van de Graaff generator. A small-scale example of this class is the cathode ray tube in an ordinary old television set. The achievable kinetic energy for particles in these devices is determined by the accelerating voltage, which is limited by electrical breakdown. Electrodynamic or electromagnetic accelerators, on the other hand, use changing electromagnetic fields (either magnetic induction or oscillating radio frequency fields) to accelerate particles. Since in these types the particles can pass through the same accelerating field multiple times, the output energy is not limited by the strength of the accelerating field. This class, which was first developed in the 1920s, is the basis for most modern large-scale accelerators.
 Rolf Widerøe, Gustav Ising, Leó Szilárd, Max Steenbeck, and Ernest Lawrence are considered pioneers of this field, conceiving and building the first operational linear particle accelerator,[4] the betatron, and the cyclotron.
 Because the target of the particle beams of early accelerators was usually the atoms of a piece of matter, with the goal being to create collisions with their nuclei in order to investigate nuclear structure, accelerators were commonly referred to as atom smashers in the 20th century.[5] The term persists despite the fact that many modern accelerators create collisions between two subatomic particles, rather than a particle and an atomic nucleus.[6][7][8]"
Particle_displacement,Physics,2,"Particle displacement or displacement amplitude is a measurement of distance of the movement of a sound particle from its equilibrium position in a medium as it transmits a sound wave.[1]  
The SI unit of particle displacement is the metre (m). In most cases this is a longitudinal wave of pressure (such as sound), but it can also be a transverse wave, such as the vibration of a taut string.  In the case of a sound wave travelling through air, the particle displacement is evident in the oscillations of air molecules with, and against, the direction in which the sound wave is travelling.[2] A particle of the medium undergoes displacement according to the particle velocity of the sound wave traveling through the medium, while the sound wave itself moves at the speed of sound, equal to 343 m/s in air at 20 °C.
"
Particle_physics,Physics,2,"
 Particle physics (also known as high energy physics) is a branch of physics that studies the nature of the particles that constitute matter and radiation. Although the word particle can refer to various types of very small objects (e.g. protons, gas particles, or even household dust), particle physics usually investigates the irreducibly smallest detectable particles and the fundamental interactions necessary to explain their behaviour. By our current understanding, these elementary particles are excitations of the quantum fields that also govern their interactions. The currently dominant theory explaining these fundamental particles and fields, along with their dynamics, is called the Standard Model. Thus, modern particle physics generally investigates the Standard Model and its various possible extensions, e.g. to the newest ""known"" particle, the Higgs boson, or even to the oldest known force field, gravity.[1][2]"
Pascal%27s_law,Physics,2,Pascal's law (also Pascal's principle[1][2][3] or the principle of transmission of fluid-pressure) is a principle in fluid mechanics given by Blaise Pascal that states that a pressure change at any point in a confined incompressible fluid is transmitted throughout the fluid such that the same change occurs everywhere.[4] The law was established by French mathematician Blaise Pascal in 1653 and published in 1663.[5][6]
Pauli_exclusion_principle,Physics,2,"The Pauli exclusion principle is the quantum mechanical principle which states that two or more identical fermions (particles with half-integer spin) cannot occupy the same quantum state within a quantum system simultaneously. This principle was formulated by Austrian physicist Wolfgang Pauli in 1925 for electrons, and later extended to all fermions with his spin–statistics theorem of 1940.
 In the case of electrons in atoms, it can be stated as follows: it is impossible for two electrons of a poly-electron atom to have the same values of the four quantum numbers: n, the principal quantum number, ℓ, the azimuthal quantum number, mℓ, the magnetic quantum number, and ms, the spin quantum number. For example, if two electrons reside in the same orbital, then their n, ℓ, and mℓ values are the same, therefore their ms must be different, and thus the electrons must have opposite half-integer spin projections of 1/2 and −1/2.
 Particles with an integer spin, or bosons, are not subject to the Pauli exclusion principle: any number of identical bosons can occupy the same quantum state, as with, for instance, photons produced by a laser or atoms in a Bose–Einstein condensate.
 A more rigorous statement is that concerning the exchange of two identical particles: the total (many-particle) wave function is antisymmetric for fermions, and symmetric for bosons. This means that if the space and spin coordinates of two identical particles are interchanged, then the total wave function changes its sign for fermions and does not change for bosons.
 If two fermions were in the same state (for example the same orbital with the same spin in the same atom), interchanging them would change nothing and the total wave function would be unchanged. The only way the total wave function can both change sign as required for fermions and also remain unchanged is that this function must be zero everywhere, which means that the state cannot exist. This reasoning does not apply to bosons because the sign does not change.
"
Pendulum,Physics,2,"A pendulum is a weight suspended from a pivot so that it can swing freely.[1] When a pendulum is displaced sideways from its resting, equilibrium position, it is subject to a restoring force due to gravity that will accelerate it back toward the equilibrium position. When released, the restoring force acting on the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth. The time for one complete cycle, a left swing and a right swing, is called the period. The period depends on the length of the pendulum and also to a slight degree on the amplitude, the width of the pendulum's swing.
 From the first scientific investigations of the pendulum around 1602 by Galileo Galilei, the regular motion of pendulums was used for timekeeping, and was the world's most accurate timekeeping technology until the 1930s.[2] The pendulum clock invented by Christian Huygens in 1658 became the world's standard timekeeper, used in homes and offices for 270 years, and achieved accuracy of about one second per year before it was superseded as a time standard by the quartz clock in the 1930s.  Pendulums are also used in scientific instruments such as accelerometers and seismometers. Historically they were used as gravimeters to measure the acceleration of gravity in geo-physical surveys, and even as a standard of length. The word ""pendulum"" is new Latin, from the Latin pendulus, meaning 'hanging'.[3]"
Periodic_table_of_the_elements,Physics,2,"
 The periodic table, also known as the periodic table of elements, arranges the chemical elements such as  hydrogen, silicon, iron, and uranium according to their recurring properties. The number of each element corresponds to the number of protons in its nucleus (which is the same as the number of electrons orbiting that nucleus). The modern periodic table provides a useful framework for analyzing chemical reactions, and is widely used in chemistry, physics and other sciences.
 
 The seven rows of the table, called periods, generally have metals on the left and nonmetals on the right. The columns, called groups, contain elements with similar chemical behaviours. Six groups have accepted names as well as assigned numbers: for example, group 17 elements are the halogens; and group 18 are the noble gases. Also displayed are four simple rectangular areas or blocks associated with the filling of different atomic orbitals. The organization of the periodic table can be used to derive relationships between the properties of the various elements, and to predict chemical properties and behaviours of undiscovered or newly synthesized elements.
 Russian chemist Dmitri Mendeleev published a periodic table in 1869, which he developed mainly to illustrate recurring trends among the properties of the then-known elements. He was the first to predict some properties of unidentified elements that were expected to fill gaps within the table. Most of his forecasts proved to be correct, culminating with the discovery of gallium and germanium in 1875 and 1886 respectively, which corroborated his predictions.[1] Mendeleev's idea has been slowly expanded and refined with the discovery or synthesis of further new elements and the development of new theoretical models to explain chemical behaviour.
 The table here shows a widely used layout. Other forms (discussed below) show different structures in detail. Some questions remain as to the placement and categorisation of specific elements, future extensions and limits of the table, and whether there is an optimal form of table.
"
Phase_(matter),Physics,2,"In the physical sciences, a phase is a region of space (a thermodynamic system), throughout which all physical properties of a material are essentially uniform.[1][2]:86[3]:3 Examples of physical properties include density, index of refraction, magnetization and chemical composition. A simple description is that a phase is a region of material that is chemically uniform, physically distinct, and (often) mechanically separable. In a system consisting of ice and water in a glass jar, the ice cubes are one phase, the water is a second phase, and the humid air is a third phase over the ice and water. The glass of the jar is another separate phase. (See state of matter § Glass)
 The term phase is sometimes used as a synonym for state of matter, but there can be several immiscible phases of the same state of matter. Also, the term phase is sometimes used to refer to a set of equilibrium states demarcated in terms of state variables such as pressure and temperature by a phase boundary on a phase diagram. Because phase boundaries relate to changes in the organization of matter, such as a change from liquid to solid or a more subtle change from one crystal structure to another, this latter usage is similar to the use of ""phase"" as a synonym for state of matter. However, the state of matter and phase diagram usages are not commensurate with the formal definition given above and the intended meaning must be determined in part from the context in which the term is used.
"
Phase_(waves),Physics,2,"In physics and mathematics, the phase of a periodic function 



F


{  F}
 of some real variable 



t


{  t}
 (such as time) is an angle representing the number of periods spanned by that variable.  It is denoted 



ϕ
(
t
)


{  \phi (t)}
 and expressed in such a scale that it varies by one full turn as the variable 



t


{  t}
 goes through each period (and 



F
(
t
)


{  F(t)}
 goes through each complete cycle).  It may be measured in any angular unit such as degrees or radians, thus increasing by 360° or 



2
π


{  2\pi }
 as the variable 



t


{  t}
 completes a full period.[1] This convention is especially appropriate for a sinusoidal function, since its value at any argument 



t


{  t}
 then can be expressed as the sine of the phase 



ϕ
(
t
)


{  \phi (t)}
, multiplied by some factor (the amplitude of the sinusoid). (The cosine may be used instead of sine, depending on where one considers each period to start.)
 Usually, whole turns are ignored when expressing the phase; so that 



ϕ
(
t
)


{  \phi (t)}
 is also a periodic function, with the same period as 



F


{  F}
, that repeatedly scans the same range of angles as 



t


{  t}
 goes through each period.  Then, 



F


{  F}
 is said to be ""at the same phase"" at two argument values 




t

1




{  t_{1}}
 and 




t

2




{  t_{2}}
 (that is, 



ϕ
(

t

1


)
=
ϕ
(

t

2


)


{  \phi (t_{1})=\phi (t_{2})}
) if the difference between them is a whole number of periods.
 The numeric value of the phase 



ϕ
(
t
)


{  \phi (t)}
 depends on the arbitrary choice of the start of each period, and on the interval of angles that each period is to be mapped to. 
 The term ""phase"" is also used when comparing a periodic function 



F


{  F}
 with a shifted version 



G


{  G}
 of it.  If the shift in 



t


{  t}
 is expressed as a fraction of the period, and then scaled to an angle 



φ


{  \varphi }
 spanning a whole turn, one gets the phase shift, phase offset, or phase difference of 



G


{  G}
 relative to 



F


{  F}
.  If 



F


{  F}
 is a ""canonical"" function for a class of signals, like 



sin
⁡
(
t
)


{  \sin(t)}
 is for all sinusoidal signals, then 



φ


{  \varphi }
 is called the initial phase of 



G


{  G}
.
"
Phase_equilibrium,Physics,2,"The phase rule is a general principle governing ""pVT systems"" in thermodynamic equilibrium, whose states are completely described by the variables pressure (p), volume (V) and temperature (T). If F is the number of degrees of freedom, C is the number of components and P is the number of phases, then[1][2] It was derived by Josiah Willard Gibbs in his landmark paper titled On the Equilibrium of Heterogeneous Substances, published in parts between 1875 and 1878.[3]
The rule assumes the components do not react with each other.
 The number of degrees of freedom is the number of independent intensive variables, i.e. the largest number of thermodynamic parameters such as temperature or pressure that can be varied simultaneously and arbitrarily without determining one another. An example of one-component system is a system involving one pure chemical, while two-component systems, such as mixtures of water and ethanol, have two chemically independent components, and so on. Typical phases are solids, liquids and gases.
"
Phenomenology_(physics),Physics,2,"In physics, phenomenology is the application of theoretical physics to experimental data by making quantitative predictions based upon known theories. It is in contrast to experimentation in the scientific method, in which the goal of the experiment is to test a scientific hypothesis instead of making predictions. Phenomenology is related to the philosophical notion in that these predictions describe anticipated behaviors for the phenomena in reality. 
 Phenomenology is commonly applied to the field of particle physics, where it forms a bridge between the mathematical models of theoretical physics (such as quantum field theories and theories of the structure of space-time) and the results of the high-energy particle experiments. It is sometimes used in other fields such as in condensed matter physics[1][2] and plasma physics,[3][4] when there are no existing theories for the observed experimental data.
"
Phosphorescence,Physics,2,"Phosphorescence is a type of photoluminescence related to fluorescence. Unlike fluorescence, a phosphorescent material does not immediately re-emit the radiation it absorbs.  The slower time scales of the re-emission are associated with ""forbidden"" energy state transitions in quantum mechanics. As these transitions occur very slowly in certain materials, absorbed radiation is re-emitted at a lower intensity for up to several hours after the original excitation.
 Everyday examples of phosphorescent materials are the glow-in-the-dark toys, stickers, paint, wristwatch and clock dials that glow after being charged with a bright light such as in any normal reading or room light.  Typically, the glow slowly fades out, sometimes within a few minutes or up to a few hours in a dark room.[1] Around 1604, Vincenzo Casciarolo discovered a ""lapis solaris"" near Bologna, Italy. Once heated in an oxygen-rich furnace, it thereafter absorbed sunlight and glowed in the dark. The study of phosphorescent materials led to the discovery of radioactive decay.
"
Photoelectric_effect,Physics,2,"The photoelectric effect is the emission of electrons when electromagnetic radiation, such as light, hits a material. Electrons emitted in this manner are called photoelectrons. The phenomenon is studied in condensed matter physics, and solid state and quantum chemistry to draw inferences about the properties of atoms, molecules and solids. The effect has found use in electronic devices specialized for light detection and precisely timed electron emission.
 The experimental results disagree with classical electromagnetism, which predicts that continuous light waves transfer energy to electrons, which would then be emitted when they accumulate enough energy. An alteration in the intensity of light would theoretically change the kinetic energy of the emitted electrons, with sufficiently dim light resulting in a delayed emission. The experimental results instead show that electrons are dislodged only when the light exceeds a certain frequency—regardless of the light's intensity or duration of exposure. Because a low-frequency beam at a high intensity could not build up the energy required to produce photoelectrons like it would have if light's energy was coming from a continuous wave, Albert Einstein proposed that a beam of light is not a wave propagating through space, but a collection of discrete wave packets, known as photons.
 Emission of conduction electrons from typical metals requires a few electron-volt (eV) light quanta, corresponding to short-wavelength visible or ultraviolet light. In extreme cases, emissions are induced with photons approaching zero energy, like in systems with negative electron affinity and the emission from excited states, or a few hundred keV photons for core electrons in elements with a high atomic number.[1] Study of the photoelectric effect led to important steps in understanding the quantum nature of light and electrons and influenced the formation of the concept of wave–particle duality.[2] Other phenomena where light affects the movement of electric charges include the photoconductive effect, the photovoltaic effect, and the photoelectrochemical effect.
"
Photon,Physics,2,"The photon is a type of elementary particle. It is the quantum of the electromagnetic field including electromagnetic radiation such as light and radio waves, and the force carrier for the electromagnetic force. Photons are massless,[a] so they always move at the speed of light in vacuum, 299792458 m/s.
 Like all elementary particles, photons are currently best explained by quantum mechanics and exhibit wave–particle duality, their behavior featuring properties of both waves and particles.[2] The modern photon concept originated during the first two decades of the 20th century with the work of Albert Einstein, who built upon the research of Max Planck. While trying to explain how matter and electromagnetic radiation could be in thermal equilibrium with one another, Planck proposed that the energy stored within a material object should be regarded as composed of an integer number of discrete, equal-sized parts. To explain the photoelectric effect, Einstein introduced the idea that light itself is made of discrete units of energy. In 1926, Gilbert N. Lewis popularized the term photon for these energy units.[3][4][5] Subsequently, many other experiments validated Einstein's approach.[6][7][8] In the Standard Model of particle physics, photons and other elementary particles are described as a necessary consequence of physical laws having a certain symmetry at every point in spacetime. The intrinsic properties of particles, such as charge, mass, and spin, are determined by this gauge symmetry.  The photon concept has led to momentous advances in experimental and theoretical physics, including lasers, Bose–Einstein condensation, quantum field theory, and the probabilistic interpretation of quantum mechanics. It has been applied to photochemistry, high-resolution microscopy, and measurements of molecular distances. Recently, photons have been studied as elements of quantum computers, and for applications in optical imaging and optical communication such as quantum cryptography.
"
Photonics,Physics,2,"Photonics is the physical science of light (photon) generation, detection, and manipulation through emission, transmission, modulation, signal processing, switching, amplification, and sensing.[1][2] Though covering all light's technical applications over the whole spectrum, most photonic applications are in the range of visible and near-infrared light. The term photonics developed as an outgrowth of the first practical semiconductor light emitters invented in the early 1960s and optical fibers developed in the 1970s.
"
Physical_chemistry,Physics,2,"Physical chemistry is the study of macroscopic, and particulate phenomena in chemical systems in terms of the principles, practices, and concepts of physics such as motion, energy, force, time, thermodynamics, quantum chemistry, statistical mechanics, analytical dynamics and chemical equilibrium.
 Physical chemistry, in contrast to chemical physics, is predominantly (but not always) a macroscopic or supra-molecular science, as the majority of the principles on which it was founded relate to the bulk rather than the molecular/atomic structure alone (for example, chemical equilibrium and colloids).
 Some of the relationships that physical chemistry strives to resolve include the effects of:
"
Physical_constant,Physics,2,"A physical constant, sometimes fundamental physical constant or universal constant, is a physical quantity that is generally believed to be both universal in nature and have constant value in time. It is contrasted with a mathematical constant, which has a fixed numerical value, but does not directly involve any physical measurement.
 There are many physical constants in science, some of the most widely recognized being the speed of light in vacuum c, the gravitational constant G, the Planck constant h, the electric constant ε0, and the elementary charge e. Physical constants can take many dimensional forms: the speed of light signifies a maximum speed for any object and its dimension is length divided by time; while the fine-structure constant α, which characterizes the strength of the electromagnetic interaction, is dimensionless.
 The term fundamental physical constant is sometimes used to refer to universal-but-dimensioned physical constants such as those mentioned above.[1] Increasingly, however, physicists only use fundamental physical constant for dimensionless physical constants, such as the fine-structure constant α.
 Physical constant, as discussed here, should not be confused with other quantities called ""constants"", which are assumed to be constant in a given context without being fundamental, such as the ""time constant"" characteristic of a given system, or material constants (e.g., Madelung constant, electrical resistivity, and heat capacity).
 Since May 2019, all of the SI base units have been defined in terms of physical constants. As a result, five constants: the speed of light in vacuum, c; the Planck constant, h; the elementary charge, e; the Avogadro constant, NA; and the Boltzmann constant, kB, have known exact numerical values when expressed in SI units.   The first three of these constants are fundamental constants, whereas NA and kB are of a technical nature only: they do not describe any property of the universe, but instead only give a proportionality factor for defining the units used with large numbers of atomic-scale entities.
"
Physical_quantity,Physics,2,"A physical quantity is a property of a material or system that can be quantified by measurement. A physical quantity can be expressed as the combination of a numerical value and a unit. For example, the physical quantity mass can be quantified as n kg, where n is the numerical value and kg is the unit. A physical quantity possesses at least two characteristics in common, one is numerical magnitude and other is the unit in which it is measured. 
"
Physics,Physics,2,"
 
 Physics (from Ancient Greek: φυσική (ἐπιστήμη), romanized: physikḗ (epistḗmē), lit. 'knowledge of nature', from φύσις phýsis 'nature')[1][2][3] is the natural science that studies matter,[a] its motion and behavior through space and time, and the related entities of energy and force.[5] Physics is one of the most fundamental scientific disciplines, and its main goal is to understand how the universe behaves.[b][6][7][8] Physics is one of the oldest academic disciplines and, through its inclusion of astronomy, perhaps the oldest.[9] Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century these natural sciences emerged as unique research endeavors in their own right.[c] Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences[6] and suggest new avenues of research in academic disciplines such as mathematics and philosophy.
 Advances in physics often enable advances in new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons;[6] advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.
"
Piezoelectricity,Physics,2,"Piezoelectricity is the electric charge that accumulates in certain solid materials (such as crystals, certain ceramics, and biological matter such as bone, DNA and various proteins)[1] in response to applied mechanical stress. The word piezoelectricity means electricity resulting from pressure and latent heat. It is derived from the Greek word πιέζειν; piezein, which means to squeeze or press, and ἤλεκτρον ēlektron, which means amber, an ancient source of electric charge.[2][3] French physicists Jacques and Pierre Curie discovered piezoelectricity in 1880.[4] The piezoelectric effect results from the linear electromechanical interaction between the mechanical and electrical states in crystalline materials with no inversion symmetry.[5] The piezoelectric effect is a reversible process: materials exhibiting the piezoelectric effect (the internal generation of electrical charge resulting from an applied mechanical force) also exhibit the reverse piezoelectric effect, the internal generation of a mechanical strain resulting from an applied electrical field. For example, lead zirconate titanate crystals will generate measurable piezoelectricity when their static structure is deformed by about 0.1% of the original dimension. Conversely, those same crystals will change about 0.1% of their static dimension when an external electric field is applied to the material. The inverse piezoelectric effect is used in the production of ultrasonic sound waves.[6] Piezoelectricity is exploited in a number of useful applications, such as the production and detection of sound, piezoelectric inkjet printing, generation of high voltages, clock generator in electronics, microbalances, to drive an ultrasonic nozzle, and ultrafine focusing of optical assemblies. It forms the basis for a number of scientific instrumental techniques with atomic resolution, the scanning probe microscopies, such as STM, AFM, MTA, and SNOM. It also finds everyday uses such as acting as the ignition source for cigarette lighters, push-start propane barbecues, used as the time reference source in quartz watches, as well as in amplification pickups for some guitars and triggers in most modern electronic drums.[7][8]"
Pion,Physics,2,"In particle physics, a pion (or a pi meson, denoted with the Greek letter pi: π) is any of three subatomic particles: π0, π+, and π−. Each pion consists of a quark and an antiquark and is therefore a meson. Pions are the lightest mesons and, more generally, the lightest hadrons. They are unstable, with the charged pions π+ and π− decaying after a mean lifetime of 26.033 nanoseconds (2.6033×10−8 seconds), and the neutral pion π0 decaying after a much shorter lifetime of 84 attoseconds (8.4×10−17 seconds). Charged pions most often decay into muons and muon neutrinos, while neutral pions generally decay into gamma rays.
 The exchange of virtual pions, along with vector, rho and omega mesons, provides an explanation for the residual strong force between nucleons. Pions are not produced in radioactive decay, but commonly are in high energy collisions between hadrons. Pions also result from some matter-antimatter annihilation events. All types of pions are also produced in natural processes when high energy cosmic ray protons and other hadronic cosmic ray components interact with matter in Earth's atmosphere. In 2013, the detection of characteristic gamma rays originating from the decay of neutral pions in two supernova remnants has shown that pions are produced copiously after supernovas, most probably in conjunction with production of high energy protons that are detected on Earth as cosmic rays.[1] The concept of mesons as the carrier particles of the nuclear force was first proposed in 1935 by Hideki Yukawa. While the muon was first proposed to be this particle after its discovery in 1936, later work found that it did not participate in the strong nuclear interaction. The pions, which turned out to be examples of Yukawa's proposed mesons, were discovered later: the charged pions in 1947, and the neutral pion in 1950.
"
Planck_constant,Physics,2,"The Planck constant, or Planck's constant, is the quantum of electromagnetic action that relates a photon's energy to its frequency. The Planck constant multiplied by a photon's frequency is equal to a photon's energy. The Planck constant is a fundamental physical constant denoted as 



h


{  h}
, and of fundamental importance in quantum mechanics. In metrology it is used to define the kilogram in SI units.[2] The Planck constant is defined to have the exact value 



h
=


{  h=}
 6.62607015×10−34 J⋅s in SI units.[3][4] At the end of the 19th century, accurate measurements of the spectrum of black body radiation existed, but predictions of the frequency distribution of the radiation by then-existing theories diverged significantly at higher frequencies. In 1900, Max Planck empirically derived a formula for the observed spectrum. He assumed a hypothetical electrically charged oscillator in a cavity that contained black-body radiation could only change its energy in a minimal increment, 



E


{  E}
, that was proportional to the frequency of its associated electromagnetic wave.[5] He was able to calculate the proportionality constant, 



h


{  h}
, from the experimental measurements, and that constant is named in his honor. In 1905, the value 



E


{  E}
 was associated by Albert Einstein with a ""quantum"" or minimal element of the energy of the electromagnetic wave itself. The light quantum behaved in some respects as an electrically neutral particle. It was eventually called a photon. Max Planck received the 1918 Nobel Prize in Physics ""in recognition of the services he rendered to the advancement of Physics by his discovery of energy quanta"".
 Confusion can arise when dealing with frequency or the Planck constant because the units of angular measure (cycle or radian) are omitted in SI.[6][7][8][9][10] 
In the language of quantity calculus,[11] the expression for the value of the Planck constant, or a frequency, is the product of a numerical value and a unit of measurement. The symbol f  (or ν), when used for the value of a frequency, implies cycles per second or hertz as the unit. When the symbol ω is used for the frequency's value it implies radians per second as the unit. The numerical values of these two ways of expressing the frequency have a ratio of 2π. Omitting the units of angular measure ""cycle"" and ""radian"" can lead to an error of 2π. A similar state of affairs occurs for the Planck constant.  The symbol h is used to express the value of the Planck constant in J⋅s/cycle, and the symbol ħ (""h-bar"") is used to express its value in J⋅s/radian.  Both represent the value of the Planck constant, but, as discussed below, their numerical values have a ratio of 2π. In this Wikipedia article the word ""value"" as used in the tables means ""numerical value"", and the equations involving the Planck constant and/or frequency actually involve their numerical values using the appropriate implied units. 
 Since energy and mass are equivalent, the Planck constant also relates mass to frequency.
"
Planck_units,Physics,2,"
 In particle physics and physical cosmology, Planck units are a set of units of measurement defined exclusively in terms of four universal physical constants, in such a manner that these physical constants take on the numerical value of 1 when expressed in terms of these units.
 Originally proposed in 1899 by German physicist Max Planck, these units are a system of natural units because the origin of their definition comes only from properties of nature and not from any human construct. Planck units are only one of several systems of natural units, but Planck units are not based on properties of any prototype object or particle (the choice of which is inherently arbitrary), but rather on only the properties of free space. They are relevant in research on unified theories such as quantum gravity.
 The term Planck scale refers to quantities of space, time, energy and other units that are similar in magnitude to corresponding Planck units.  This region may be characterized by energies of around 1019 GeV, time intervals of around 10−43 s and lengths of around 10−35 m (approximately respectively the energy-equivalent of the Planck mass, the Planck time and the Planck length).  At the Planck scale, the predictions of the Standard Model, quantum field theory and general relativity are not expected to apply, and quantum effects of gravity are expected to dominate.  The best known example is represented by the conditions in the first 10−43 seconds of our universe after the Big Bang, approximately 13.8 billion years ago.
 The four universal constants that, by definition, have a numeric value 1 when expressed in these units are:
 Planck units do not incorporate an electromagnetic dimension.  Some authors choose to extend the system to electromagnetism by, for example, defining the electric constant ε0 as having the numeric value 1 or 1 / 4π in this system.  Similarly, authors choose to use variants of the system that give other numeric values to one or more of the four constants above.
"
Planck%27s_law,Physics,2,"
 Planck's law describes the spectral density of electromagnetic radiation emitted by a black body in thermal equilibrium at a given temperature T, when there is no net flow of matter or energy between the body and its environment.[1] At the end of the 19th century, physicists were unable to explain why the observed spectrum of black-body radiation, which by then had been accurately measured, diverged significantly at higher frequencies from that predicted by existing theories. In 1900, Max Planck heuristically derived a formula for the observed spectrum by assuming that a hypothetical electrically charged oscillator in a cavity that contained black-body radiation could only change its energy in a minimal increment, E, that was proportional to the frequency of its associated electromagnetic wave. This resolved the problem of the ultraviolet catastrophe predicted by classical physics.  This discovery was a pioneering insight of modern physics and is of fundamental importance to quantum theory.
"
Plasma_(physics),Physics,2,"
 
 Plasma (from Ancient Greek  πλάσμα​ 'moldable substance'[1]) is one of the four fundamental states of matter, and was first described by chemist Irving Langmuir[2] in the 1920s.[3] It consists of a gas of ions – atoms which have some of their orbital electrons removed – and free electrons.  Plasma can be artificially generated by heating a neutral gas or subjecting it to a strong electromagnetic field to the point where an ionized gaseous substance becomes increasingly electrically conductive. The resulting charged ions and electrons become influenced by long-range electromagnetic fields, making the plasma dynamics more sensitive to these fields than a neutral gas.[4] Plasma and ionized gases have properties and behaviours unlike those of the other states, and the transition between them is mostly a matter of nomenclature[2] and subject to interpretation.[5] Based on the temperature and density of the environment that contains a plasma, partially ionized or fully ionized forms of plasma may be produced. Neon signs and lightning are examples of partially ionized plasmas.[6] The Earth's ionosphere is a plasma and the magnetosphere contains plasma in the Earth's surrounding space environment. The interior of the Sun is an example of fully ionized plasma,[7] along with the solar corona[8] and stars.[9] Positive charges in ions are achieved by stripping away electrons orbiting the atomic nuclei, where the total number of electrons removed is related to either increasing temperature or the local density of other ionized matter. This also can be accompanied by the dissociation of molecular bonds,[10] though this process is distinctly different from chemical processes of ion interactions in liquids or the behaviour of shared ions in metals. The response of plasma to electromagnetic fields is used in many modern technological devices, such as plasma televisions or plasma etching.[11] Plasma may be the most abundant form of ordinary matter in the universe,[12] although this hypothesis is currently tentative based on the existence and unknown properties of dark matter. Plasma is mostly associated with stars, extending to the rarefied intracluster medium and possibly the intergalactic regions.[13]"
Plasma_physics,Physics,2,"
 
 Plasma (from Ancient Greek  πλάσμα​ 'moldable substance'[1]) is one of the four fundamental states of matter, and was first described by chemist Irving Langmuir[2] in the 1920s.[3] It consists of a gas of ions – atoms which have some of their orbital electrons removed – and free electrons.  Plasma can be artificially generated by heating a neutral gas or subjecting it to a strong electromagnetic field to the point where an ionized gaseous substance becomes increasingly electrically conductive. The resulting charged ions and electrons become influenced by long-range electromagnetic fields, making the plasma dynamics more sensitive to these fields than a neutral gas.[4] Plasma and ionized gases have properties and behaviours unlike those of the other states, and the transition between them is mostly a matter of nomenclature[2] and subject to interpretation.[5] Based on the temperature and density of the environment that contains a plasma, partially ionized or fully ionized forms of plasma may be produced. Neon signs and lightning are examples of partially ionized plasmas.[6] The Earth's ionosphere is a plasma and the magnetosphere contains plasma in the Earth's surrounding space environment. The interior of the Sun is an example of fully ionized plasma,[7] along with the solar corona[8] and stars.[9] Positive charges in ions are achieved by stripping away electrons orbiting the atomic nuclei, where the total number of electrons removed is related to either increasing temperature or the local density of other ionized matter. This also can be accompanied by the dissociation of molecular bonds,[10] though this process is distinctly different from chemical processes of ion interactions in liquids or the behaviour of shared ions in metals. The response of plasma to electromagnetic fields is used in many modern technological devices, such as plasma televisions or plasma etching.[11] Plasma may be the most abundant form of ordinary matter in the universe,[12] although this hypothesis is currently tentative based on the existence and unknown properties of dark matter. Plasma is mostly associated with stars, extending to the rarefied intracluster medium and possibly the intergalactic regions.[13]"
Plasticity_(physics),Physics,2,"In physics and materials science, plasticity, also known as plastic deformation, is the ability of a solid material to undergo permanent deformation, a non-reversible change of shape in response to applied forces.[1][2]  For example, a solid piece of metal being bent or pounded into a new shape displays plasticity as permanent changes occur within the material itself. In engineering, the transition from elastic behavior to plastic behavior is known as yielding.
 Plastic deformation is observed in most materials, particularly metals, soils, rocks, concrete, foams.[3][4][5][6] However, the physical mechanisms that cause plastic deformation can vary widely.  At a crystalline scale, plasticity in metals is usually a consequence of dislocations. Such defects are relatively rare in most crystalline materials, but are numerous in some and part of their crystal structure; in such cases, plastic crystallinity can result. In brittle materials such as rock, concrete and bone, plasticity is caused predominantly by slip at microcracks. In cellular materials such as liquid foams or biological tissues, plasticity is mainly a consequence of bubble or cell rearrangements, notably T1 processes.
 For many ductile metals, tensile loading applied to a sample will cause it to behave in an elastic manner.  Each increment of load is accompanied by a proportional increment in extension. When the load is removed, the piece returns to its original size.  However, once the load exceeds a threshold – the yield strength – the extension increases more rapidly than in the elastic region; now when the load is removed, some degree of extension will remain.
 Elastic deformation, however, is an approximation and its quality depends on the time frame considered and loading speed. If, as indicated in the graph opposite, the deformation includes elastic deformation, it is also often referred to as ""elasto-plastic deformation"" or ""elastic-plastic deformation"".
 Perfect plasticity is a property of materials to undergo irreversible deformation without any increase in stresses or loads.  Plastic materials that have been hardened by prior deformation, such as cold forming, may need increasingly higher stresses to deform further.  Generally, plastic deformation is also dependent on the deformation speed, i.e. higher stresses usually have to be applied to increase the rate of deformation. Such materials are said to deform visco-plastically.
"
Pneumatics,Physics,2,"Pneumatics (From Greek: πνεύμα pneuma, meaning breath of life) is a branch of engineering that makes use of gas or pressurized air.
 Pneumatic systems used in industry are commonly powered by compressed air or compressed inert gases. A centrally located and electrically-powered compressor powers cylinders, air motors, pneumatic actuators,  and other pneumatic devices.  A pneumatic system controlled through manual or automatic solenoid valves is selected when it provides a lower cost, more flexible, or safer alternative to electric motors and hydraulic actuators.
 Pneumatics also has applications in dentistry, construction, mining, and other areas.
"
Positron,Physics,2,"
 The positron or antielectron is the antiparticle or the antimatter counterpart of the electron. The positron has an electric charge of +1 e, a spin of 1/2 (the same as the electron), and has the same mass as an electron. When a positron collides with an electron, annihilation occurs. If this collision occurs at low energies, it results in the production of two or more photons.
 9.10938356(11)×10−31 kg[1]5.485799090(16)×10−4 u[1] Positrons can be created by positron emission radioactive decay (through weak interactions), or by pair production from a sufficiently energetic photon which is interacting with an atom in a material.
"
Potential_energy,Physics,2,"
 
 In physics, potential energy is the energy held by an object because of its position relative to other objects, stresses within itself, its electric charge, or other factors.[1][2] U            =  ½ · k · x2(elastic)U            = ½ · C · V2 (electric)U            = -m · B (magnetic)
 Common types of potential energy include the gravitational potential energy of an object that depends on its mass and its distance from the center of mass of another object, the elastic potential energy of an extended spring, and the electric potential energy of an electric charge in an electric field. The unit for energy in the International System of Units (SI) is the joule, which has the symbol J.
 The term potential energy was introduced by the 19th-century Scottish engineer and physicist William Rankine,[3][4] although it has links to Greek philosopher Aristotle's concept of potentiality.
Potential energy is associated with forces that act on a body in a way that the total work done by these forces on the body depends only on the initial and final positions of the body in space. These forces, that are called conservative forces, can be represented at every point in space by vectors expressed as gradients of a certain scalar function called potential.
 Since the work of potential forces acting on a body that moves from a start to an end position is determined only by these two positions, and does not depend on the trajectory of the body, there is a function known as potential that can be evaluated at the two positions to determine this work.
"
Power_(physics),Physics,2,"
 In physics, power is the amount of energy transferred or converted per unit time. In the International System of Units, the unit of power is the watt, equal to one joule per second. In older works, power is sometimes called activity.[1][2][3] Power is a scalar quantity.
 The output power of a motor is the product of the torque that the motor generates and the angular velocity of its output shaft. The power involved in moving a ground vehicle is the product of the traction force on the wheels and the velocity of the vehicle. The power of a jet-propelled vehicle is the product of the engine thrust and the velocity of the vehicle. The rate at which a light bulb converts electrical energy into light and heat is measured in watts – the electrical energy used per unit of time.[4][5]"
Pressure,Physics,2,"Pressure (symbol: p or P) is the force applied perpendicular to the surface of an object per unit area over which that force is distributed.:445[1] Gauge pressure (also spelled gage pressure)[a] is the pressure relative to the ambient pressure.
 Various units are used to express pressure. Some of these derive from a unit of force divided by a unit of area; the SI unit of pressure, the pascal (Pa), for example, is one newton per square metre (N/m2); similarly, the pound-force per square inch (psi) is the traditional unit of pressure in the imperial and U.S. customary systems. Pressure may also be expressed in terms of standard atmospheric pressure; the atmosphere (atm) is equal to this pressure, and the torr is defined as ​1⁄760 of this. Manometric units such as the centimetre of water, millimetre of mercury, and inch of mercury are used to express pressures in terms of the height of column of a particular fluid in a manometer.
"
Probability,Physics,2,"
 Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true.  The probability of an event is a number between 0 and 1, where, roughly speaking, 0 indicates impossibility of the event and 1 indicates certainty.[note 1][1][2] The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (""heads"" and ""tails"") are both equally probable; the probability of ""heads"" equals the probability of ""tails""; and since no other outcomes are possible, the probability of either ""heads"" or ""tails"" is 1/2 (which could also be written as 0.5 or 50%).
 These concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in areas of study such as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence, machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.[3]"
Probability_distribution,Physics,2,"In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment.[1][2] It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).[3] For instance, if X is used to denote the outcome of a coin toss (""the experiment""), then the probability distribution of X would take the value 0.5 for X = heads, and 0.5 for X = tails (assuming that the coin is fair). Examples of random phenomena include the weather condition in a future date, the height of a person, the fraction of male students in a school, the results of a survey, etc.[4]"
Probability_theory,Physics,2,"Probability theory is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of these outcomes is called an event.
Central subjects in probability theory include discrete and continuous random variables, probability distributions, and stochastic processes, which provide mathematical abstractions of non-deterministic or uncertain processes or measured quantities that may either be single occurrences or evolve over time in a random fashion.
Although it is not possible to perfectly predict random events, much can be said about their behavior. Two major results in probability theory describing such behaviour are the law of large numbers and the central limit theorem.
 As a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of data.[1] Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics. A great discovery of twentieth-century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics.[2]"
Proton,Physics,2,"A proton is a subatomic particle, symbol p or p+, with a positive electric charge of +1e elementary charge and a mass slightly less than that of a neutron. Protons and neutrons, each with masses of approximately one atomic mass unit, are collectively referred to as ""nucleons"" (particles present in atomic nuclei).
 938.27208816(29) MeV/c2[2] 1.52103220230(46)×10−3 μB[2] One or more protons are present in the nucleus of every atom; they are a necessary part of the nucleus. The number of protons in the nucleus is the defining property of an element, and is referred to as the atomic number (represented by the symbol Z). Since each element has a unique number of protons, each element has its own unique atomic number.
 The word proton is Greek for ""first"", and this name was given to the hydrogen nucleus by Ernest Rutherford in 1920. In previous years, Rutherford had discovered that the hydrogen nucleus (known to be the lightest nucleus) could be extracted from the nuclei of nitrogen by atomic collisions.[3] Protons were therefore a candidate to be a fundamental particle, and hence a building block of nitrogen and all other heavier atomic nuclei.
 Although protons were originally considered fundamental or elementary particles, in the modern Standard Model of particle physics, protons are classified as hadrons, like neutrons, the other nucleon. Protons are composite particles composed of three valence quarks: two up quarks of charge +2/3e and one down quark of charge −1/3e. The rest masses of quarks contribute only about 1% of a proton's mass.[4] The remainder of a proton's mass is due to quantum chromodynamics binding energy, which includes the kinetic energy of the quarks and the energy of the gluon fields that bind the quarks together.  Because protons are not fundamental particles, they possess a measurable size; the root mean square charge radius of a proton is about 0.84–0.87 fm (or 0.84×10−15 to 0.87×10−15 m).[5][6] In 2019, two different studies, using different techniques, have found the radius of the proton to be 0.833 fm, with an uncertainty of ±0.010 fm.[7][8] At sufficiently low temperatures, free protons will bind to electrons. However, the character of such bound protons does not change, and they remain protons. A fast proton moving through matter will slow by interactions with electrons and nuclei, until it is captured by the electron cloud of an atom. The result is a protonated atom, which is a chemical compound of hydrogen. In vacuum, when free electrons are present, a sufficiently slow proton may pick up a single free electron, becoming a neutral hydrogen atom, which is chemically a free radical. Such ""free hydrogen atoms"" tend to react chemically with many other types of atoms at sufficiently low energies. When free hydrogen atoms react with each other, they form neutral hydrogen molecules (H2), which are the most common molecular component of molecular clouds in interstellar space.
"
Psi_particle,Physics,2,"The J/ψ (J/psi) meson /ˈdʒeɪ ˈsaɪ ˈmiːzɒn/ or psion[1] is a subatomic particle, a flavor-neutral meson consisting of a charm quark and a charm antiquark.  Mesons formed by a bound state of a charm quark and a charm anti-quark are generally known as ""charmonium"".  The J/ψ is the most common form of charmonium, due to its spin of 1 and its low rest mass.  The J/ψ has a rest mass of 3.0969 GeV/c2, just above that of the ηc (2.9836 GeV/c2), and a mean lifetime of 7.2×10−21 s. This lifetime was about a thousand times longer than expected.[2] Its discovery was made independently by two research groups, one at the Stanford Linear Accelerator Center, headed by Burton Richter, and one at the Brookhaven National Laboratory, headed by Samuel Ting of MIT. They discovered they had actually found the same particle, and both announced their discoveries on 11 November 1974. The importance of this discovery is highlighted by the fact that the subsequent, rapid changes in high-energy physics at the time have become collectively known as the ""November Revolution"". Richter and Ting were awarded the 1976 Nobel Prize in Physics.
"
Pulley,Physics,2,"A pulley is a wheel on an axle or shaft that is designed to support movement and change of direction of a taut cable or belt, or transfer of power between the shaft and cable or belt. In the case of a pulley supported by a frame or shell that does not transfer power to a shaft, but is used to guide the cable or exert a force, the supporting shell is called a block, and the pulley may be called a sheave.
 A pulley may have a groove or grooves between flanges around its circumference to locate the cable or belt. The drive element of a pulley system can be a rope, cable, belt, or chain.
 The earliest evidence of pulleys dates back to Ancient Egypt in the Twelfth Dynasty (1991-1802 BCE)[1] and Mesopotamia in the early 2nd millennium BCE.[2] In Roman Egypt, Hero of Alexandria (c. 10-70 CE) identified the pulley as one of six simple machines used to lift weights.[3] Pulleys are assembled to form a block and tackle in order to provide mechanical advantage to apply large forces. Pulleys are also assembled as part of belt and chain drives in order to transmit power from one rotating shaft to another.[4][5] Plutarch's Lives recounts a scene where Archimedes proved the effectiveness of compound pulleys and the block-and-tackle system by using one to pull a fully laden ship towards him as if it was gliding through water. [6]"
Pulse_(physics),Physics,2,"In physics, a pulse is a generic term describing a single disturbance that moves through a transmission medium. This medium may be vacuum (in the case of electromagnetic radiation) or matter, and may be indefinitely large or finite.
"
Pulse_wave,Physics,2,"A pulse wave or pulse train is a kind of non-sinusoidal waveform that includes square waves (duty cycle of 50%) and similarly periodic but asymmetrical waves (duty cycles other than 50%). It is a term common to synthesizer programming, and is a typical waveform available on many synthesizers. The exact shape of the wave is determined by the duty cycle of the oscillator. In many synthesizers, the duty cycle can be modulated (sometimes called pulse-width modulation) for a more dynamic timbre.[1]
The pulse wave is also known as the rectangular wave, the periodic version of the rectangular function.
 The average level of a rectangular wave is also given by the duty cycle, therefore by varying the on and off periods and then averaging these said periods, it is possible to represent any value between the two limiting levels. This is the basis of pulse width modulation.
 The Fourier series expansion for a rectangular pulse wave with period T and pulse time τ is
 Note that, for symmetry, the starting time (t = 0) in this expansion is halfway through the first pulse. The phase can be offset to match the accompanying graph by replacing t with t - τ/2.
 A pulse wave can be created by subtracting a sawtooth wave from a phase-shifted version of itself. If the sawtooth waves are bandlimited, the resulting pulse wave is bandlimited, too. Another way to create one is with a single ramp wave (sawtooth or triangle) and a comparator, with the ramp wave on one input, and a variable DC[clarification needed] threshold on the other. The result will be a precisely controlled pulse width, but it will not be bandlimited.
"
Quantization_(physics),Physics,2,"In physics, quantization (in British English quantisation) is the process of transition from a classical understanding of physical phenomena to a newer understanding known as quantum mechanics.  It is a procedure for constructing a quantum field theory starting from a classical field theory. This is a generalization of the procedure for building quantum mechanics from classical mechanics. Also related is field quantization, as in the ""quantization of the electromagnetic field"", referring to photons as field ""quanta"" (for instance as light quanta). This procedure is basic to theories of particle physics, nuclear physics, condensed matter physics, and quantum optics.
"
Quantum,Physics,2,"In physics, a quantum (plural quanta) is the minimum amount of any physical entity (physical property) involved in an interaction. The fundamental notion that a physical property can be ""quantized"" is referred to as ""the hypothesis of quantization"".[1] This means that the magnitude of the physical property can take on only discrete values consisting of integer multiples of one quantum.
 For example, a photon is a single quantum of light (or of any other form of electromagnetic radiation). Similarly, the energy of an electron bound within an atom is quantized and can exist only in certain discrete values. (Atoms and matter in general are stable because electrons can exist only at discrete energy levels within an atom.) Quantization is one of the foundations of the much broader physics of quantum mechanics. Quantization of energy and its influence on how energy and matter interact (quantum electrodynamics) is part of the fundamental framework for understanding and describing nature.
"
Quantum_chromodynamics,Physics,2,"In theoretical physics, quantum chromodynamics (QCD) is the theory of the strong interaction between quarks and gluons, the fundamental particles that make up composite hadrons such as the proton, neutron and pion.  QCD is a type of quantum field theory called a non-abelian gauge theory, with symmetry group SU(3). The QCD analog of electric charge is a property called color. Gluons are the force carrier of the theory, like photons are for the electromagnetic force in quantum electrodynamics. The theory is an important part of the Standard Model of particle physics. A large body of experimental evidence for QCD has been gathered over the years.
 QCD exhibits two main properties:
"
Quantum_electrodynamics,Physics,2,"In particle physics, quantum electrodynamics (QED) is the relativistic quantum field theory of electrodynamics. In essence, it describes how light and matter interact and is the first theory where full agreement between quantum mechanics and special relativity is achieved. QED mathematically describes all phenomena involving electrically charged particles interacting by means of exchange of photons and represents the quantum counterpart of classical electromagnetism giving a complete account of matter and light interaction.
 In technical terms, QED can be described as a perturbation theory of the electromagnetic quantum vacuum.  Richard Feynman called it ""the jewel of physics"" for its extremely accurate predictions of quantities like the anomalous magnetic moment of the electron and the Lamb shift of the energy levels of hydrogen.[1]:Ch1"
Quantum_field_theory,Physics,2,"In theoretical physics, quantum field theory (QFT) is a theoretical framework that combines classical field theory, special relativity and quantum mechanics,[1]:xi but not general relativity's description of gravity. QFT is used in particle physics to construct physical models of subatomic particles and in condensed matter physics to construct models of quasiparticles.
 QFT treats particles as excited states (also called quanta) of their underlying quantum fields, which are more fundamental than the particles. Interactions between particles are described by interaction terms in the Lagrangian involving their corresponding quantum fields. Each interaction can be visually represented by Feynman diagrams according to perturbation theory in quantum mechanics.
"
Quantum_gravity,Physics,2,"Quantum gravity (QG) is a field of theoretical physics that seeks to describe gravity according to the principles of quantum mechanics, and where quantum effects cannot be ignored,[1] such as in the vicinity of black holes or similar compact astrophysical objects where the effects of gravity are strong, such as neutron stars.
 Three of the four fundamental forces of physics are described within the framework of quantum mechanics and quantum field theory. The current understanding of the fourth force, gravity, is based on Albert Einstein's general theory of relativity, which is formulated within the entirely different framework of classical physics. However, that description is incomplete: describing the gravitational field of a black hole in the general theory of relativity, physical quantities such as the spacetime curvature diverge at the center of the black hole.
 This signals the breakdown of the general theory of relativity and the need for a theory that goes beyond general relativity into the quantum. At distances very close to the center of the black hole (closer than the Planck length), quantum fluctuations of spacetime are expected to play an important role.[2] To describe these quantum effects a theory of quantum gravity is needed. Such a theory should allow the description to be extended closer to the center and might even allow an  understanding of physics at the center of a black hole. On more formal grounds, one can argue that a classical system cannot consistently be coupled to a quantum one.[3][4]:11–12 The field of quantum gravity is actively developing and theorists are exploring a variety of approaches to the problem of quantum gravity, the most popular approaches being M-theory and loop quantum gravity.[5] 
All of these approaches aim to describe the quantum behavior of the gravitational field. This does not necessarily include unifying all fundamental interactions into a single mathematical framework. However, many approaches to quantum gravity, such as string theory, try to develop a framework that describes all fundamental forces. Such theories are often referred to as a theory of everything. Others, such as loop quantum gravity, make no such attempt; instead, they make an effort to quantize the gravitational field while it is kept separate from the other forces.
 One of the difficulties of formulating a quantum gravity theory is that quantum gravitational effects only appear at length scales near the Planck scale, around 10−35 meters, a scale far smaller, and hence only accessible with far higher energies, than those currently available in high energy particle accelerators.  Therefore, physicists lack experimental data which could distinguish between the competing theories which have been proposed[n.b. 1][n.b. 2] and thus thought experiment approaches are suggested as a testing tool for these theories.[6][7][8]"
Quantum_mechanics,Physics,2,"
 Quantum mechanics is a fundamental theory in physics that provides a description of the physical properties of nature at the scale of atoms and subatomic particles.[2] It is the foundation of all quantum physics including quantum chemistry, quantum field theory, quantum technology, and quantum information science.
 Classical physics, the description of physics that existed before the theory of relativity and quantum mechanics, describes many aspects of nature at an ordinary (macroscopic) scale, while quantum mechanics explains the aspects of nature at small (atomic and subatomic) scales, for which classical mechanics is insufficient. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale.[3] Quantum mechanics differs from classical physics in that energy, momentum, angular momentum, and other quantities of a bound system are restricted to discrete values (quantization), objects have characteristics of both particles and waves (wave-particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle).[note 1] Quantum mechanics arose gradually, from theories to explain observations which could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper which explained the photoelectric effect. Early quantum theory was profoundly re-conceived in the mid-1920s by Niels Bohr, Erwin Schrödinger, Werner Heisenberg, Max Born and others. The original interpretation of quantum mechanics is the Copenhagen interpretation, developed by Niels Bohr and Werner Heisenberg in Copenhagen during the 1920s. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of energy, momentum, and other physical properties of a particle.
"
Quantum_number,Physics,2,"In chemistry and quantum physics, quantum numbers describe values of conserved quantities in the dynamics of a quantum system.  Quantum numbers correspond to eigenvalues of operators that commute with the Hamiltonian—quantities that can be known with precision at the same time as the system's energy[note 1]—and their corresponding eigenspaces. Together, a specification of all of the quantum numbers of a quantum system fully characterize a basis state of the system, and can in principle be measured together. 
 An important aspect of quantum mechanics is the quantization of many observable quantities of interest.[note 2]  In particular, this leads to quantum numbers that take values in discrete sets of integers or half-integers; although they could approach infinity in some cases. This distinguishes quantum mechanics from classical mechanics where the values that characterize the system such as mass, charge, or momentum, all range continuously. Quantum numbers often describe specifically the energy levels of electrons in atoms, but other possibilities include angular momentum, spin, etc. An important family is flavour quantum numbers – internal quantum numbers which determine the type of a particle and its interactions with other particles through the fundamental forces.  Any quantum system can have one or more quantum numbers; it is thus difficult to list all possible quantum numbers.
"
Quantum_physics,Physics,2,"Quantum physics is a branch of modern physics in which energy and matter are described at their most fundamental level, that of energy quanta, elementary particles, and quantum fields. Quantum physics encompasses any discipline concerned with systems that exhibit notable quantum-mechanical effects, where waves have properties of particles, and particles behave like waves.[1] When particles are bound in quantum-mechanical systems, their energy can only change in discrete steps, energy quanta. Even though our description of systems in quantum physics starts at the scale of atoms and subatomic particles, the effects can extend to collective behavior and emergent phenomena at macroscopic scales.
"
Quantum_state,Physics,2,"In quantum physics, a quantum state is a mathematical entity that provides a probability distribution for the outcomes of each possible measurement on a system. Knowledge of the quantum state together with the rules for the system's evolution in time exhausts all that can be predicted about the system's behavior. A mixture of quantum states is again a quantum state. Quantum states that cannot be written as a mixture of other states are called pure quantum states, while all other states are called mixed quantum states. A pure quantum state can be represented by a ray in a Hilbert space over the complex numbers,[1][2] while mixed states are represented by density matrices, which are positive semidefinite operators that act on Hilbert spaces.[3][4] Pure states are also known as state vectors or wave functions, the latter term applying particularly when they are represented as functions of position or momentum. For example, when dealing with the energy spectrum of the electron in a hydrogen atom, the relevant state vectors are identified by the principal quantum number n, the angular momentum quantum number l, the magnetic quantum number m, and the spin z-component sz. For another example, if the spin of an electron is measured in any direction, e.g. with a Stern–Gerlach experiment, there are two possible results: up or down.  The Hilbert space for the electron's spin is therefore two-dimensional, constituting a qubit. A pure state here is represented by a two-dimensional complex vector 



(
α
,
β
)


{  (\alpha ,\beta )}
, with a length of one; that is, with
 where 




|

α

|



{  |\alpha |}
 and 




|

β

|



{  |\beta |}
 are the absolute values of 



α


{  \alpha }
 and 



β


{  \beta }
. A mixed state, in this case, has the structure of a 



2
×
2


{  2\times 2}
 matrix that is Hermitian and positive semi-definite, and has trace 1.[5] A more complicated case is given (in bra–ket notation) by the singlet state, which exemplifies quantum entanglement:
 which involves superposition of joint spin states for two particles with spin ​1⁄2. The singlet state satisfies the property that if the particles' spins are measured along the same direction then either the spin of the first particle is observed up and the spin of the second particle is observed down, or the first one is observed down and the second one is observed up, both possibilities occurring with equal probability.
 A mixed quantum state corresponds to a probabilistic mixture of pure states; however, different distributions of pure states can generate equivalent (i.e., physically indistinguishable) mixed states. The Schrödinger–HJW theorem classifies the multitude of ways to write a given mixed state as a convex combination of pure states.[6] Before a particular measurement is performed on a quantum system, the theory gives only a probability distribution for the outcome, and the form that this distribution takes is completely determined by the quantum state and the linear operators describing the measurement. Probability distributions for different measurements exhibit tradeoffs exemplified by the uncertainty principle: a state that implies a narrow spread of possible outcomes for one experiment necessarily implies a wide spread of possible outcomes for another.
"
Quark,Physics,2,"
 A quark (/kwɔːrk, kwɑːrk/) is a type of elementary particle and a fundamental constituent of matter. Quarks combine to form composite particles called hadrons, the most stable of which are protons and neutrons, the components of atomic nuclei.[1] All commonly observable matter is composed of up quarks, down quarks and electrons. Due to a phenomenon known as color confinement, quarks are never found in isolation; they can be found only within hadrons, which include baryons (such as protons and neutrons) and mesons, or in quark–gluon plasmas.[2][3][nb 1] For this reason, much of what is known about quarks has been drawn from observations of hadrons.
 Quarks have various intrinsic properties, including electric charge, mass, color charge, and spin. They are the only elementary particles in the Standard Model of particle physics to experience all four fundamental interactions, also known as fundamental forces (electromagnetism, gravitation, strong interaction, and weak interaction), as well as the only known particles whose electric charges are not integer multiples of the elementary charge.
 There are six types, known as flavors, of quarks: up, down, charm, strange, top, and bottom.[4] Up and down quarks have the lowest masses of all quarks. The heavier quarks rapidly change into up and down quarks through a process of particle decay: the transformation from a higher mass state to a lower mass state. Because of this, up and down quarks are generally stable and the most common in the universe, whereas strange, charm, bottom, and top quarks can only be produced in high energy collisions (such as those involving cosmic rays and in particle accelerators). For every quark flavor there is a corresponding type of antiparticle, known as an antiquark, that differs from the quark only in that some of its properties (such as the electric charge) have equal magnitude but opposite sign.
 The quark model was independently proposed by physicists Murray Gell-Mann and George Zweig in 1964.[5] Quarks were introduced as parts of an ordering scheme for hadrons, and there was little evidence for their physical existence until deep inelastic scattering experiments at the Stanford Linear Accelerator Center in 1968.[6][7] Accelerator experiments have provided evidence for all six flavors. The top quark, first observed at Fermilab in 1995, was the last to be discovered.[5]"
Quasiparticle,Physics,2,"
In physics, quasiparticles and collective excitations (which are closely related) are emergent phenomena that occur when a microscopically complicated system such as a solid behaves as if it contained different weakly interacting particles in vacuum. For example, as an electron travels through a semiconductor, its motion is disturbed in a complex way by its interactions with other electrons and with atomic nuclei. The electron behaves as though it has a different effective mass travelling unperturbed in vacuum. Such an electron is called an electron quasiparticle.[1] In another example, the aggregate motion of electrons in the valence band of a semiconductor or a hole band in a metal[2] behave as though the material instead contained positively charged quasiparticles called electron holes. Other quasiparticles or collective excitations include the phonon (a particle derived from the vibrations of atoms in a solid), the plasmons (a particle derived from plasma oscillation), and many others.
 These particles are typically called quasiparticles if they are related to fermions, and called collective excitations if they are related to bosons,[1] although the precise distinction is not universally agreed upon.[3] Thus, electrons and electron holes (fermions) are typically called quasiparticles, while phonons and plasmons (baryons) are typically called collective excitations.
 The quasiparticle concept is important in condensed matter physics because it can simplify the many-body problem in quantum mechanics.
"
Radiant_energy,Physics,2,"In physics, and in particular as measured by radiometry, radiant energy is the energy of electromagnetic and gravitational radiation.[1] As energy, its SI unit is the joule (J). The quantity of radiant energy may be calculated by integrating radiant flux (or power) with respect to time. The symbol Qe is often used throughout literature to denote radiant energy (""e"" for ""energetic"", to avoid confusion with photometric quantities). In branches of physics other than radiometry, electromagnetic energy is referred to using E or W. The term is used particularly when electromagnetic radiation is emitted by a source into the surrounding environment. This radiation may be visible or invisible to the human eye.[2][3]"
Radiation,Physics,2,"
 In physics, radiation is the emission or transmission of energy in the form of waves or particles through space or through a material medium.[1][2] This includes:
 Radiation is often categorized as either ionizing or non-ionizing depending on the energy of the radiated particles. Ionizing radiation carries more than 10 eV, which is enough to ionize atoms and molecules and break chemical bonds. This is an important distinction due to the large difference in harmfulness to living organisms. A common source of ionizing radiation is radioactive materials that emit α, β, or γ radiation, consisting of helium nuclei, electrons or positrons, and photons, respectively. Other sources include X-rays from medical radiography examinations and muons, mesons, positrons, neutrons and other particles that constitute the secondary cosmic rays that are produced after primary cosmic rays interact with Earth's atmosphere.
 Gamma rays, X-rays and the higher energy range of ultraviolet light constitute the ionizing part of the electromagnetic spectrum. The word ""ionize"" refers to the breaking of one or more electrons away from an atom, an action that requires the relatively high energies that these electromagnetic waves supply. Further down the spectrum, the non-ionizing lower energies of the lower ultraviolet spectrum cannot ionize atoms, but can disrupt the inter-atomic bonds which form molecules, thereby breaking down molecules rather than atoms; a good example of this is sunburn caused by long-wavelength solar ultraviolet. The waves of longer wavelength than UV in visible light, infrared and microwave frequencies cannot break bonds but can cause vibrations in the bonds which are sensed as heat. Radio wavelengths and below generally are not regarded as harmful to biological systems. These are not sharp delineations of the energies; there is some overlap in the effects of specific frequencies.[3] The word radiation arises from the phenomenon of waves radiating (i.e., traveling outward in all directions) from a source. This aspect leads to a system of measurements and physical units that are applicable to all types of radiation. Because such radiation expands as it passes through space, and as its energy is conserved (in vacuum), the intensity of all types of radiation from a point source follows an inverse-square law in relation to the distance from its source. Like any ideal law, the inverse-square law approximates a measured radiation intensity to the extent that the source approximates a geometric point.
"
Radioactive_decay,Physics,2,"
 Radioactive decay (also known as nuclear decay, radioactivity, radioactive disintegration or nuclear disintegration) is the process by which an unstable atomic nucleus loses energy by radiation. A material containing unstable nuclei is considered radioactive. Three of the most common types of decay are alpha decay, beta decay, and gamma decay, all of which involve emitting one or more particles or photons. The weak force is the mechanism that is responsible for beta decay.[1] Radioactive decay is a stochastic (i.e. random) process at the level of single atoms. According to quantum theory, it is impossible to predict when a particular atom will decay, regardless of how long the atom has existed.[2][3][4]  However, for a significant number of identical atoms, the overall decay rate can be expressed as a decay constant or as half-life. The half-lives of radioactive atoms have a huge range; from nearly instantaneous to far longer than the age of the universe.
 The decaying nucleus is called the parent radionuclide (or parent radioisotope[note 1]), and the process produces at least one daughter nuclide. Except for gamma decay or internal conversion from a nuclear excited state, the decay is a nuclear transmutation resulting in a daughter containing a different number of protons or neutrons (or both). When the number of protons changes, an atom of a different chemical element is created.
 By contrast, there are radioactive decay processes that do not result in a nuclear transmutation. The energy of an excited nucleus may be emitted as a gamma ray in a process called gamma decay, or that energy may be lost when the nucleus interacts with an orbital electron causing its ejection from the atom, in a process called internal conversion. Another type of radioactive decay results in products that vary, appearing as two or more ""fragments"" of the original nucleus with a range of possible masses. This decay, called spontaneous fission, happens when a large unstable nucleus spontaneously splits into two (or occasionally three) smaller daughter nuclei, and generally leads to the emission of gamma rays, neutrons, or other particles from those products.
In contrast, decay products from a nucleus with spin may be distributed non-isotropically with respect to that spin direction. Either because of an external influence such as an electromagnetic field, or because the nucleus was produced in a dynamic process that constrained the direction of its spin, the anisotropy may be detectable. Such a parent process could be a previous decay, or a nuclear reaction.[5][6][7][note 2] For a summary table showing the number of stable and radioactive nuclides in each category, see radionuclide. There are 28 naturally occurring chemical elements on Earth that are radioactive, consisting of 34 radionuclides (6 elements have 2 different radionuclides) that date before the time of formation of the Solar System. These 34 are known as primordial nuclides. Well-known examples are uranium and thorium, but also included are naturally occurring long-lived radioisotopes, such as potassium-40.
 Another 50 or so shorter-lived radionuclides, such as radium-226 and radon-222, found on Earth, are the products of decay chains that began with the primordial nuclides, or are the product of ongoing cosmogenic processes, such as the production of carbon-14 from nitrogen-14 in the atmosphere by cosmic rays. Radionuclides may also be produced artificially in particle accelerators or nuclear reactors, resulting in 650 of these with half-lives of over an hour, and several thousand more with even shorter half-lives. (See List of nuclides for a list of these sorted by half-life.)
"
Radionuclide,Physics,2,"A radionuclide (radioactive nuclide, radioisotope or radioactive isotope) is an atom that has excess nuclear energy, making it unstable. This excess energy can be used in one of three ways: emitted from the nucleus as gamma radiation; transferred to one of its electrons to release it as a conversion electron; or used to create and emit a new particle (alpha particle or beta particle) from the nucleus. During those processes, the radionuclide is said to undergo radioactive decay.[1] These emissions are considered ionizing radiation because they are powerful enough to liberate an electron from another atom. The radioactive decay can produce a stable nuclide or will sometimes produce a new unstable radionuclide which may undergo further decay. Radioactive decay is a random process at the level of single atoms: it is impossible to predict when one particular atom will decay.[2][3][4][5] However, for a collection of atoms of a single element the decay rate, and thus the half-life (t1/2) for that collection, can be calculated from their measured decay constants. The range of the half-lives of radioactive atoms has no known limits and spans a time range of over 55 orders of magnitude.
 Radionuclides occur naturally or are artificially produced in nuclear reactors, cyclotrons, particle accelerators or radionuclide generators. There are about 730 radionuclides with half-lives longer than 60 minutes (see list of nuclides). Thirty-two of those are primordial radionuclides that were created before the earth was formed. At least another 60 radionuclides are detectable in nature, either as daughters of primordial radionuclides or as radionuclides produced through natural production on Earth by cosmic radiation. More than 2400 radionuclides have half-lives less than 60 minutes. Most of those are only produced artificially, and have very short half-lives. For comparison, there are about 252 stable nuclides. (In theory, only 146 of them are stable, and the other 106 are believed to decay via alpha decay, beta decay, double beta decay, electron capture, or double electron capture.)
 All chemical elements can exist as radionuclides. Even the lightest element, hydrogen, has a well-known radionuclide, tritium. Elements heavier than lead, and the elements technetium and promethium, exist only as radionuclides. (In theory, elements heavier than dysprosium exist only as radionuclides, but some such elements, like gold and platinum, are observationally stable and their half-lives have not been determined).
 Unplanned exposure to radionuclides generally has a harmful effect on living organisms including humans, although low levels of exposure occur naturally without harm. The degree of harm will depend on the nature and extent of the radiation produced, the amount and nature of exposure (close contact, inhalation or ingestion), and the biochemical properties of the element; with increased risk of cancer the most usual consequence. However, radionuclides with suitable properties are used in nuclear medicine for both diagnosis and treatment.  An imaging tracer made with radionuclides is called a radioactive tracer. A pharmaceutical drug made with radionuclides is called a radiopharmaceutical.
"
Radius_of_curvature_(optics),Physics,2,"Radius of curvature (ROC) has specific meaning and sign convention in optical design.  A spherical lens or mirror surface has a center of curvature located either along or decentered from the system local optical axis.  The vertex of the lens surface is located on the local optical axis. The distance from the vertex to the center of curvature is the radius of curvature of the surface.[1][2] The sign convention for the optical radius of curvature is as follows:
 Thus when viewing a biconvex lens from the side, the left surface radius of curvature is positive, and the right radius of curvature is negative.
 Note however that in areas of optics other than design, other sign conventions are sometimes used. In particular, many undergraduate physics textbooks use the Gaussian sign convention in which convex surfaces of lenses are always positive.[3] Care should be taken when using formulas taken from different sources.
"
Redshift,Physics,2,"In physics, redshift is a phenomenon where electromagnetic radiation (such as light) from an object undergoes an increase in wavelength. Whether or not the radiation is visible, ""redshift"" means an increase in wavelength, equivalent to a decrease in wave frequency and photon energy, in accordance with, respectively, the wave and quantum theories of light.
 Neither the emitted nor perceived light is necessarily red; instead, the term refers to the human perception of longer wavelengths as red, which is at the section of the visible spectrum with the longest wavelengths. Examples of redshifting are a gamma ray perceived as an X-ray, or initially visible light perceived as radio waves. The opposite of a redshift is a blueshift, where wavelengths shorten and energy increases. However, redshift is a more common term and sometimes blueshift is referred to as negative redshift.
 There are three main causes of redshifts in astronomy and cosmology:
 Knowledge of redshifts and blueshifts has been used to develop several terrestrial technologies such as Doppler radar and radar guns.[1] Redshifts are also seen in the spectroscopic observations of astronomical objects.[2] Its value is represented by the letter z.
 A special relativistic redshift formula (and its classical approximation) can be used to calculate the redshift of a nearby object when spacetime is flat. However, in many contexts, such as black holes and Big Bang cosmology, redshifts must be calculated using general relativity.[3] Special relativistic, gravitational, and cosmological redshifts can be understood under the umbrella of frame transformation laws. There exist other physical processes that can lead to a shift in the frequency of electromagnetic radiation, including scattering and optical effects; however, the resulting changes are distinguishable from true redshift and are not generally referred to as such (see section on physical optics and radiative transfer).
"
Refraction,Physics,2,"
 In physics, refraction is the change in direction of a wave passing from one medium to another or from a gradual change in the medium.[1] Refraction of light is the most commonly observed phenomenon, but other waves such as sound waves and water waves also experience refraction. How much a wave is refracted is determined by the change in wave speed and the initial direction of wave propagation relative to the direction of change in speed.
 For light, refraction follows Snell's law, which states that, for a given pair of media, the ratio of the sines of the angle of incidence θ1 and angle of refraction θ2 is equal to the ratio of phase velocities (v1 / v2) in the two media, or equivalently, to the indices of refraction (n2 / n1) of the two media.[2] Optical prisms and lenses use refraction to redirect light, as does the human eye. The refractive index of materials varies with the wavelength of light,[3] and thus the angle of the refraction also varies correspondingly. This is called dispersion and causes prisms and rainbows to divide white light into its constituent spectral colors.[4]"
Refractive_index,Physics,2,"In optics, the refractive index (also known as refraction index or index of refraction) of a material is a dimensionless number that describes how fast light  travels through the material. It is defined as
 where c is the speed of light in vacuum and v is the phase velocity of light in the medium. For example, the refractive index of water is 1.333, meaning that light travels 1.333 times slower in water than in a vacuum. Increasing the refractive index corresponds to decreasing the speed of light in the material.
 The refractive index determines how much the path of light is bent, or refracted, when entering a material. This is described by Snell's law of refraction, n1 sinθ1 = n2 sinθ2,
where θ1 and θ2 are the angles of incidence and refraction, respectively, of a ray crossing the interface between two media with refractive indices n1 and n2. The refractive indices also determine the amount of light that is reflected when reaching the interface, as well as the critical angle for total internal reflection, their intensity (Fresnel's equations) and Brewster's angle.[1] The refractive index can be seen as the factor by which the speed and the wavelength of the radiation are reduced with respect to their vacuum values: the speed of light in a medium is v = c/n, and similarly the wavelength in that medium is λ = λ0/n, where λ0 is the wavelength of that light in vacuum. This implies that vacuum has a refractive index of 1, and that the frequency (f = v/λ) of the wave is not affected by the refractive index. As a result, the perceived color of the refracted light to a human eye which depends on the frequency is not affected by the refraction or the refractive index of the medium.
 The refractive index varies with wavelength, this causes white light to split into constituent colors when refracted. This is called dispersion. It can be observed in prisms and rainbows, and as chromatic aberration in lenses. Light propagation in absorbing materials can be described using a complex-valued refractive index.[2] The imaginary part then handles the attenuation, while the real part accounts for refraction. For most materials the refractive index changes with wavelength by several percent across the visible spectrum. Nevertheless, refractive indices for materials are commonly reported using a single value for n, typically measured at 633 nm.
 The concept of refractive index applies within the full electromagnetic spectrum, from X-rays to radio waves. It can also be applied to wave phenomena such as sound. In this case the speed of sound is used instead of that of light, and a reference medium other than vacuum must be chosen.[3]"
Relative_atomic_mass,Physics,2,"Relative atomic mass (symbol: Ar) or atomic weight is a dimensionless physical quantity defined as the ratio of the average mass of atoms of a chemical element in a given sample to the atomic mass constant. The atomic mass constant (symbol: mu) is defined as being 1/12 of the mass of a carbon-12 atom.[1][2] Since both quantities in the ratio are masses, the resulting value is dimensionless; hence the value is said to be relative.
 For a single given sample, the relative atomic mass of a given element is the weighted arithmetic mean of the masses of the individual atoms (including their isotopes) that are present in the sample. This quantity can vary substantially between samples because the sample's origin (and therefore its radioactive history or diffusion history) may have produced unique combinations of isotopic abundances. For example, due to a different mixture of stable carbon-12 and carbon-13 isotopes, a sample of elemental carbon from volcanic methane will have a different relative atomic mass than one collected from plant or animal tissues.
 The more common, and more specific quantity known as standard atomic weight (Ar, standard) is an application of the relative atomic mass values obtained from multiple different samples. It is sometimes interpreted as the expected range of the relative atomic mass values for the atoms of a given element from all terrestrial sources, with the various sources being taken from Earth.[3] ""Atomic weight"" is often loosely and incorrectly used as a synonym for standard atomic weight (incorrectly because standard atomic weights are not from a single sample). Standard atomic weight is nevertheless the most widely published variant of relative atomic mass.
 Additionally, the continued use of the term ""atomic weight"" (for any element) as opposed to ""relative atomic mass"" has attracted considerable controversy since at least the 1960s, mainly due to the technical difference between weight and mass in physics.[4] Still, both terms are officially sanctioned by the IUPAC. The term ""relative atomic mass"" now seems to be replacing ""atomic weight"" as the preferred term, although the term ""standard atomic weight"" (as opposed to the more correct ""standard relative atomic mass"") continues to be used.
"
Relativistic_mechanics,Physics,2,"In physics, relativistic mechanics refers to mechanics compatible with special relativity (SR) and general relativity (GR). It provides a non-quantum mechanical description of a system of particles, or of a fluid, in cases where the velocities of moving objects are comparable to the speed of light c. As a result, classical mechanics is extended correctly to particles traveling at high velocities and energies, and provides a consistent inclusion of electromagnetism with the mechanics of particles. This was not possible in Galilean relativity, where it would be permitted for particles and light to travel at any speed, including faster than light. The foundations of relativistic mechanics are the postulates of special relativity and general relativity. The unification of SR with quantum mechanics is relativistic quantum mechanics, while attempts for that of GR is quantum gravity, an unsolved problem in physics.
 As with classical mechanics, the subject can be divided into ""kinematics""; the description of motion by specifying positions, velocities and accelerations, and ""dynamics""; a full description by considering energies, momenta, and angular momenta and their conservation laws, and forces acting on particles or exerted by particles. There is however a subtlety; what appears to be ""moving"" and what is ""at rest""—which is termed by ""statics"" in classical mechanics—depends on the relative motion of observers who measure in frames of reference.
 Although some definitions and concepts from classical mechanics do carry over to SR, such as force as the time derivative of momentum (Newton's second law), the work done by a particle as the line integral of force exerted on the particle along a path, and power as the time derivative of work done, there are a number of significant modifications to the remaining definitions and formulae. SR states that motion is relative and the laws of physics are the same for all experimenters irrespective of their inertial reference frames. In addition to modifying notions of space and time, SR forces one to reconsider the concepts of mass, momentum, and energy all of which are important constructs in Newtonian mechanics. SR shows that these concepts are all different aspects of the same physical quantity in much the same way that it shows space and time to be interrelated. Consequently, another modification is the concept of the center of mass of a system, which is straightforward to define in classical mechanics but much less obvious in relativity – see relativistic center of mass for details.
 The equations become more complicated in the more familiar three-dimensional vector calculus formalism, due to the nonlinearity in the Lorentz factor, which accurately accounts for relativistic velocity dependence and the speed limit of all particles and fields. However, they have a simpler and elegant form in four-dimensional spacetime, which includes flat Minkowski space (SR) and curved spacetime (GR), because three-dimensional vectors derived from space and scalars derived from time can be collected into four vectors, or four-dimensional tensors. However, the six component angular momentum tensor is sometimes called a bivector because in the 3D viewpoint it is two vectors (one of these, the conventional angular momentum, being an axial vector).
"
Rigid_body,Physics,2,"In physics, a rigid body (also known as a rigid object [2]) is a solid body in which deformation is zero or so small it can be neglected. The distance between any two given points on a rigid body remains constant in time regardless of external forces exerted on it. A rigid body is usually considered as a continuous distribution of mass.
 In the study of special relativity, a perfectly rigid body does not exist; and objects can only be assumed to be rigid if they are not moving near the speed of light. In quantum mechanics a rigid body is usually thought of as a collection of point masses. For instance, in quantum mechanics molecules (consisting of the point masses: electrons and nuclei) are often seen as rigid bodies (see classification of molecules as rigid rotors).
"
Rotational_energy,Physics,2,"Rotational energy or angular kinetic energy is kinetic energy due to the rotation of an object and is part of its total kinetic energy. Looking at rotational energy separately around an object's axis of rotation, the following dependence on the object's moment of inertia is observed:
 where
 The mechanical work required for or applied during rotation is the torque times the rotation angle. The instantaneous power of an angularly accelerating body is the torque times the angular velocity. For free-floating (unattached) objects, the axis of rotation is commonly around its center of mass.
 Note the close relationship between the result for rotational energy and the energy held by linear (or translational) motion:
 In the rotating system, the moment of inertia, I, takes the role of the mass, m, and the angular velocity, 



ω


{  \omega }
, takes the role of the linear velocity, v. The rotational energy of a rolling cylinder varies from one half of the translational energy (if it is massive) to the same as the translational energy (if it is hollow).
 An example is the calculation of the rotational kinetic energy of the Earth. As the Earth has a period of about 23.93 hours, it has an angular velocity of 7.29×10−5 rad/s. The Earth has a moment of inertia, I = 8.04×1037 kg·m2.[1] Therefore, it has a rotational kinetic energy of 2.138×1029 J.
 A good example of actually using earth's rotational energy is the location of the European spaceport in French Guiana. This is within about 5 degrees of the equator, so space rocket launches (for primarily geo-stationary satellites) from here to the east obtain nearly all of the full rotational speed of the earth at the equator (about 1,000 mph, sort of a ""sling-shot"" benefit). This saves significant rocket fuel per launch compared with rocket launches easterly from Kennedy Space Center (USA), which obtain only about 900 mph added benefit due to the lower relative rotational speed of the earth at that northerly latitude of 28 degrees. 
 Part of the earth's rotational energy can also be tapped using tidal power. Additional friction of the two global tidal waves creates energy in a physical manner, infinitesimally slowing down Earth's angular velocity ω. Due to the conservation of angular momentum, this process transfers angular momentum to the Moon's orbital motion, increasing its distance from Earth and its orbital period (see tidal locking for a more detailed explanation of this process).
"
Rotational_speed,Physics,2,"Rotational speed (or speed of revolution) of an object rotating around an axis is the number of turns of the object divided by time, specified as revolutions per minute (rpm), cycles per second (cps), radians per second (rad/s), etc.[1] The symbol for rotational speed is 




ω

cyc




{  \omega _{\text{cyc}}}
[citation needed](the Greek lowercase letter ""omega"").
 Tangential speed v, rotational speed 




ω

cyc




{  \omega _{\text{cyc}}}
, and radial distance r, are related by the following equation:[2] An algebraic rearrangement of this equation allows us to solve for rotational speed:
 Thus, the tangential speed will be directly proportional to r when all parts of a system simultaneously have the same ω, as for a wheel, disk, or rigid wand. The direct proportionality of v to r is not valid for the planets, because the planets have different rotational speeds (ω).
 Rotational speed can measure, for example, how fast a motor is running. Rotational speed and angular speed are sometimes used as synonyms, but typically they are measured with a different unit. Angular speed, however, tells the change in angle per time unit, which is measured in radians per second in the SI system. Since there are 2π radians per cycle, or 360 degrees per cycle, we can convert angular speed to rotational speed by
 and
 where
 For example, a stepper motor might turn exactly one complete revolution each second.
Its angular speed is 360 degrees per second (360°/s), or 2π radians per second (2π rad/s), while the rotational speed is 60 rpm.
 Rotational speed is not to be confused with tangential speed, despite some relation between the two concepts. Imagine a rotating merry-go-round.  No matter how close or far you stand from the axis of rotation, your rotational speed will remain constant. However, your tangential speed does not remain constant.  If you stand two meters from the axis of rotation, your tangential speed will be double the amount if you were standing only one meter from the axis of rotation.
"
Rydberg_formula,Physics,2,"
 In atomic physics, the Rydberg formula calculates the wavelengths of a spectral line in many chemical elements. The formula was primarily presented as a generalization of the Balmer series for all atomic electron transitions of hydrogen. It was first empirically stated in 1888 by the Swedish physicist Johannes Rydberg,[1] then theoretically by Niels Bohr in 1913, who used a primitive form of quantum mechanics. The formula directly generalizes the equations used to calculate the wavelengths of the hydrogen spectral series.
"
Scalar_(physics),Physics,2,"A scalar or scalar quantity  in physics is one that can be described by a single element of a number field such as a real number, often accompanied by units of measurement (e.g. cm). A scalar is usually said to be a physical quantity that only has magnitude, possibly a sign, and no other characteristics. This is in contrast to vectors, tensors, etc. which are described by several numbers that characterize their magnitude, direction, and so on.
 The concept of a scalar in physics is essentially the same as a scalar in mathematics. Formally, a scalar is unchanged by coordinate system transformations. In classical theories, like Newtonian mechanics, this means that rotations or reflections preserve scalars, while in relativistic theories, Lorentz transformations or space-time translations preserve scalars.
"
Scattering,Physics,2,"Scattering is a term used in physics to describe a wide range of physical processes where moving particles or radiation of some form, such as light or sound, is forced to deviate from a straight trajectory by localized non-uniformities (including particles and radiation) in the medium through which they pass. In conventional use, this also includes deviation of reflected radiation from the angle predicted by the law of reflection. Reflections of radiation that undergo scattering are often called diffuse reflections and unscattered reflections are called specular (mirror-like) reflections. Originally, the term was confined to light scattering (going back at least as far as Isaac Newton in the 17th century[1]). As more ""ray""-like phenomena were discovered, the idea of scattering was extended to them, so that William Herschel could refer to the scattering of ""heat rays"" (not then recognized as electromagnetic in nature) in 1800.[2] John Tyndall, a pioneer in light scattering research, noted the connection between light scattering and acoustic scattering in the 1870s.[3] Near the end of the 19th century, the scattering of cathode rays (electron beams)[4] and X-rays[5] was observed and discussed. With the discovery of subatomic particles (e.g. Ernest Rutherford in 1911[6]) and the development of quantum theory in the 20th century, the sense of the term became broader as it was recognized that the same mathematical frameworks used in light scattering could be applied to many other phenomena.
 Scattering thus refers to particle-particle collisions between molecules, atoms, electrons, photons and other particles. Examples include: cosmic ray scattering in the Earth's upper atmosphere; particle collisions inside particle accelerators; electron scattering by gas atoms in fluorescent lamps; and neutron scattering inside nuclear reactors. 
 The types of non-uniformities which can cause scattering, sometimes known as scatterers or scattering centers, are too numerous to list, but a small sample includes particles, bubbles, droplets, density fluctuations in fluids, crystallites in polycrystalline solids, defects in monocrystalline solids, surface roughness, cells in organisms, and textile fibers in clothing. The effects of such features on the path of almost any type of propagating wave or moving particle can be described in the framework of scattering theory.
 Some areas where scattering and scattering theory are significant include radar sensing, medical ultrasound, semiconductor wafer inspection, polymerization process monitoring, acoustic tiling, free-space communications and computer-generated imagery. Particle-particle scattering theory is important in areas such as particle physics, atomic, molecular, and optical physics, nuclear physics and astrophysics. In Particle Physics the quantum interaction and scattering of fundamental particles is described by the Scattering Matrix or S-Matrix, introduced and developed by John Archibald Wheeler and Werner Heisenberg.[7] Scattering is quantified using many different concepts, including scattering cross section (σ), attenuation coefficients, the bidirectional scattering distribution function (BSDF), S-matrices, and mean free path.
 "
Science,Physics,2,"
 Science (from the Latin word scientia, meaning ""knowledge"")[1] is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.[2][3][4] The earliest roots of science can be traced to Ancient Egypt and Mesopotamia in around 3500 to 3000 BCE.[5][6] Their contributions to mathematics, astronomy, and medicine entered and shaped Greek natural philosophy of classical antiquity, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes.[5][6] After the fall of the Western Roman Empire, knowledge of Greek conceptions of the world deteriorated in Western Europe during the early centuries (400 to 1000 CE) of the Middle Ages[7] but was preserved in the Muslim world during the Islamic Golden Age.[8] The recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th century revived ""natural philosophy"",[7][9] which was later transformed by the Scientific Revolution that began in the 16th century[10] as new ideas and discoveries departed from previous Greek conceptions and traditions.[11][12][13][14] The scientific method soon played a greater role in knowledge creation and it was not until the 19th century that many of the institutional and professional features of science began to take shape;[15][16][17] along with the changing of ""natural philosophy"" to ""natural science.""[18] Modern science is typically divided into three major branches that consist of the natural sciences (e.g., biology, chemistry, and physics), which study nature in the broadest sense; the social sciences (e.g., economics, psychology, and sociology), which study individuals and societies; and the formal sciences (e.g., logic, mathematics, and theoretical computer science), which study abstract concepts. There is disagreement,[19][20][21] however, on whether the formal sciences actually constitute a science as they do not rely on empirical evidence.[22][20] Disciplines that use existing scientific knowledge for practical purposes, such as engineering and medicine, are described as applied sciences.[23][24][25][26] Science is based on research, which is commonly conducted in academic and research institutions as well as in government agencies and companies. The practical impact of scientific research has led to the emergence of science policies that seek to influence the scientific enterprise by prioritizing the development of commercial products, armaments, health care, and environmental protection.
"
Screw_(simple_machine),Physics,2,"A screw is a mechanism that converts rotational motion to linear motion, and a torque (rotational force) to a linear force.[1] It is one of the six classical simple machines. The most common form consists of a cylindrical shaft with helical grooves or ridges called threads around the outside.[2][3] The screw passes through a hole in another object or medium, with threads on the inside of the hole that mesh with the screw's threads. When the shaft of the screw is rotated relative to the stationary threads, the screw moves along its axis relative to the medium surrounding it; for example rotating a wood screw forces it into wood. In screw mechanisms, either the screw shaft can rotate through a threaded hole in a stationary object, or a threaded collar such as a nut can rotate around a stationary screw shaft.[4][5]  Geometrically, a screw can be viewed as a narrow inclined plane wrapped around a cylinder.[1] Like the other simple machines a screw can amplify force; a small rotational force (torque) on the shaft can exert a large axial force on a load. The smaller the pitch (the distance between the screw's threads), the greater the mechanical advantage (the ratio of output to input force). Screws are widely used in threaded fasteners to hold objects together, and in devices such as screw tops for containers, vises, screw jacks and screw presses.
 Other mechanisms that use the same principle, also called screws, don't necessarily have a shaft or threads. For example, a corkscrew is a helix-shaped rod with a sharp point, and an Archimedes' screw is a water pump that uses a rotating helical chamber to move water uphill. The common principle of all screws is that a rotating helix can cause linear motion.
"
Second_law_of_thermodynamics,Physics,2,"The second law of thermodynamics states that the total entropy of an isolated system can never decrease over time, and is constant if and only if all processes are reversible.[1] Isolated systems spontaneously evolve towards thermodynamic equilibrium, the state with maximum entropy. 
 The total entropy of a system and its surroundings can remain constant in ideal cases where the system is in thermodynamic equilibrium, or is undergoing a (fictive) reversible process. In all processes that occur, including spontaneous processes,[2] the total entropy of the system and its surroundings increases and the process is irreversible in the thermodynamic sense. The increase in entropy accounts for the irreversibility of natural processes, and the asymmetry between future and past.[3] Historically, the second law was an empirical finding that was accepted as an axiom of thermodynamic theory. Statistical mechanics, classical or quantum, explains the microscopic origin of the law.
 The second law has been expressed in many ways. Its first formulation is credited to the French scientist Sadi Carnot, who in 1824 showed that there is an upper limit to the efficiency of conversion of heat to work in a heat engine. This aspect of the second law is often named after Carnot.[4]"
Seebeck_effect,Physics,2,"The thermoelectric effect is the direct conversion of temperature differences to electric voltage and vice versa via a thermocouple.[1] A thermoelectric device creates a voltage when there is a different temperature on each side. Conversely, when a voltage is applied to it, heat is transferred from one side to the other, creating a temperature difference. At the atomic scale, an applied temperature gradient causes charge carriers in the material to diffuse from the hot side to the cold side.
 This effect can be used to generate electricity, measure temperature or change the temperature of objects. Because the direction of heating and cooling is affected by the applied voltage, thermoelectric devices can be used as temperature controllers.
 The term ""thermoelectric effect"" encompasses three separately identified effects: the Seebeck effect, Peltier effect, and Thomson effect. The Seebeck and Peltier effects are different manifestations of the same physical process; textbooks may refer to this process as the Peltier–Seebeck effect (the separation derives from the independent discoveries by French physicist Jean Charles Athanase Peltier and Baltic German physicist Thomas Johann Seebeck). The Thomson effect is an extension of the Peltier–Seebeck model and is credited to Lord Kelvin.
 Joule heating, the heat that is generated whenever a current is passed through a conductive material, is not generally termed a thermoelectric effect. The Peltier–Seebeck and Thomson effects are thermodynamically reversible,[2] whereas Joule heating is not.
"
Series_and_parallel_circuit,Physics,2,"
 Components of an electrical circuit or electronic circuit can be connected in series, parallel, or series-parallel.  The two simplest of these are called series and parallel and occur frequently. Components connected in series are connected along a single conductive path, so the same current flows through all of the components but voltage is dropped (lost) across each of the resistances. In a series circuit, the sum of the voltages consumed by each individual resistance is equal to the source voltage.[1][2] Components connected in parallel are connected along multiple paths so that the current can split up; the same voltage is applied to each component.[1] A circuit composed solely of components connected in series is known as a series circuit; likewise, one connected completely in parallel is known as a parallel circuit.
 In a series circuit, the current that flows through each of the components is the same, and the voltage across the circuit is the sum of the individual voltage drops across each component.[1] In a parallel circuit, the voltage across each of the components is the same, and the total current is the sum of the currents flowing through each component.[1] Consider a very simple circuit consisting of four light bulbs and a 12-volt automotive battery. If a wire joins the battery to one bulb, to the next bulb, to the next bulb, to the next bulb, then back to the battery in one continuous loop, the bulbs are said to be in series. If each bulb is wired to the battery in a separate loop, the bulbs are said to be in parallel. If the four light bulbs are connected in series, the same current flows through all of them and the voltage drop is 3-volts across each bulb, which may not be sufficient to make them glow.  If the light bulbs are connected in parallel, the currents through the light bulbs combine to form the current in the battery, while the voltage drop is 12-volts across each bulb and they all glow.
 In a series circuit, every device must function for the circuit to be complete. If one bulb burns out in a series circuit, the entire circuit is broken. In parallel circuits, each light bulb has its own circuit, so all but one light could be burned out, and the last one will still function.
"
Shadow_matter,Physics,2,"In physics, mirror matter, also called shadow matter or Alice matter, is a hypothetical counterpart to ordinary matter.[1]"
Shear_modulus,Physics,2,"In materials science, shear modulus or modulus of rigidity, denoted by G, or sometimes S or μ, is defined as the ratio of shear stress to the shear strain:[1] where 
 The derived SI unit of shear modulus is the pascal (Pa), although it is usually expressed in gigapascals (GPa) or in thousand pounds per square inch (ksi). Its dimensional form is M1L−1T−2, replacing force by mass times acceleration.
 "
Shear_strength,Physics,2,"In engineering, shear strength is the strength of a material or component against the type of yield or structural failure when the material or component fails in shear. A shear load is a force that tends to produce a sliding failure on a material along a plane that is parallel to the direction of the force. When a paper is cut with scissors, the paper fails in shear.
 In structural and  mechanical engineering, the shear strength of a component is important for designing the dimensions and materials to be used for the manufacture or construction of the component (e.g. beams, plates, or bolts). In a reinforced concrete beam, the main purpose of reinforcing bar (rebar) stirrups is to increase the shear strength.
 "
Shear_stress,Physics,2,"Shear stress, often denoted by τ (Greek: tau), is the component of stress coplanar with a material cross section. It arises from the shear force, the component of force vector parallel to the material cross section.  Normal stress, on the other hand, arises from the force vector component perpendicular to the material cross section on which it acts.
"
Shortwave_radiation,Physics,2,"Shortwave radiation (SW) is radiant energy with wavelengths in the visible (VIS), near-ultraviolet (UV), and near-infrared (NIR) spectra.
 There is no standard cut-off for the near-infrared range; therefore, the shortwave radiation range is also variously defined. It may be broadly defined to include all radiation with a wavelength of 0.1μm and 5.0μm or narrowly defined so as to include only radiation between 0.2μm and 3.0μm.
 There is little radiation flux (in terms of W/m²) to the Earth's surface below 0.2μm or above 3.0μm, although photon flux remains significant as far as 6.0μm, compared to shorter wavelength fluxes.  UV-C radiation spans from 0.1μm to .28μm, UV-B from 0.28μm to 0.315μm, UV-A from 0.315μm to 0.4μm, the visible spectrum from 0.4μm to 0.7μm, and NIR arguably from 0.7μm to 5.0μm, beyond which the infrared is thermal.[1] Shortwave radiation is distinguished from longwave radiation. Downward shortwave radiation is sensitive to solar zenith angle, cloud cover.[2]"
Schr%C3%B6dinger_equation,Physics,2,"
 The Schrödinger equation is a linear partial differential equation that describes the wave function or state function of a quantum-mechanical system.[1]:1–2 It is a key result in quantum mechanics, and its discovery was a significant landmark in the development of the subject. The equation is named after Erwin Schrödinger, who postulated the equation in 1925, and published it in 1926, forming the basis for the work that resulted in his Nobel Prize in Physics in 1933.[2][3] In classical mechanics, Newton's second law (F = ma)[note 1] is used to make a mathematical prediction as to what path a given physical system will take over time following a set of known initial conditions. Solving this equation gives the position and the momentum of the physical system as a function of the external force 




F



{  \mathbf {F} }
 on the system. Those two parameters are sufficient to describe its state at each time instant. In quantum mechanics, the analogue of Newton's law is Schrödinger's equation.
 The concept of a wave function is a fundamental postulate of quantum mechanics; the wave function defines the state of the system at each spatial position and time. Using these postulates, Schrödinger's equation can be derived from the fact that the time-evolution operator must be unitary, and must therefore be generated by the exponential of a self-adjoint operator, which is the quantum Hamiltonian. This derivation is explained below.
 In the Copenhagen interpretation of quantum mechanics, the wave function is the most complete description that can be given of a physical system. Solutions to Schrödinger's equation describe not only molecular, atomic, and subatomic systems, but also macroscopic systems, possibly even the whole universe.[4]:292ff The Schrödinger equation is not the only way to study quantum mechanical systems and make predictions. The other formulations of quantum mechanics include matrix mechanics, introduced by Werner Heisenberg, and the path integral formulation, developed chiefly by Richard Feynman. Paul Dirac incorporated matrix mechanics and the Schrödinger equation into a single formulation.
"
Simple_harmonic_motion,Physics,2,"
 In mechanics and physics, simple harmonic motion is a special type of periodic motion where the restoring force on the moving object is directly proportional to the object's displacement magnitude and acts towards the object's equilibrium position. It results in an oscillation which, if uninhibited by friction or any other dissipation of energy, continues indefinitely.
 Simple harmonic motion can serve as a mathematical model for a variety of motions, but is typified by the oscillation of a mass on a spring when it is subject to the linear elastic restoring force given by Hooke's law. The motion is sinusoidal in time and demonstrates a single resonant frequency. Other phenomena can be modeled by simple harmonic motion, including the motion of a simple pendulum, although for it to be an accurate model, the net force on the object at the end of the pendulum must be proportional to the displacement (and even so, it is only a good approximation when the angle of the swing is small; see small-angle approximation). Simple harmonic motion can also be used to model molecular vibration as well.
 Simple harmonic motion provides a basis for the characterization of more complicated periodic motion through the techniques of Fourier analysis.
"
Simple_machine,Physics,2,"
 A  simple machine is a  mechanical device that changes the direction or magnitude of a force.[2]  In general, they can be defined as the simplest mechanisms that use mechanical advantage (also called leverage) to multiply force.[3] Usually the term refers to the six classical simple machines that were defined by Renaissance scientists:[4][5][6] A simple machine uses a single applied force to do work against a single load force.   Ignoring friction losses, the work done on the load is equal to the work done by the applied force.  The machine can increase the amount of the output force, at the cost of a proportional decrease in the distance moved by the load.  The ratio of the output to the applied force is called the mechanical advantage.
 Simple machines can be regarded as the elementary ""building blocks"" of which all more complicated machines (sometimes called ""compound machines""[7][8]) are composed.[3][9]  For example, wheels, levers, and pulleys are all used in the mechanism of a bicycle.[10][11]  The mechanical advantage of a compound machine is just the product of the mechanical advantages of the simple machines of which it is composed.
 Although they continue to be of great importance in mechanics and applied science, modern mechanics has moved beyond the view of the simple machines as the ultimate building blocks of which all machines are composed, which arose in the Renaissance as a neoclassical amplification of ancient Greek texts.  The great variety and sophistication of modern machine linkages, which arose during the Industrial Revolution, is inadequately described by these six simple categories.   Various post-Renaissance authors have compiled expanded lists of ""simple machines"", often using terms like basic machines,[10] compound machines,[7] or machine elements to distinguish them from the classical simple machines above.  By the late 1800s, Franz Reuleaux[12] had identified hundreds of machine elements, calling them simple machines.[13]   Modern machine theory analyzes machines as kinematic chains composed of elementary linkages called kinematic pairs.
"
Siphon,Physics,2,"A siphon (from Ancient Greek: σίφων, ""pipe, tube"", also spelled nonetymologically syphon) is any of a wide variety of devices that involve the flow of liquids through tubes. In a narrower sense, the word refers particularly to a tube in an inverted ""U"" shape, which causes a liquid to flow upward, above the surface of a reservoir, with no pump, but powered by the fall of the liquid as it flows down the tube under the pull of gravity, then discharging at a level lower than the surface of the reservoir from which it came.
 There are two leading theories about how siphons cause liquid to flow uphill, against gravity, without being pumped, and powered only by gravity. The traditional theory for centuries was that gravity pulling the liquid down on the exit side of the siphon resulted in reduced pressure at the top of the siphon. Then atmospheric pressure was able to push the liquid from the upper reservoir, up into the reduced pressure at the top of the siphon, like in a barometer or drinking straw, and then over.[1][2][3][4] However, it has been demonstrated that siphons can operate in a vacuum[4][5][6][7] and to heights exceeding the barometric height of the liquid.[4][5][8] Consequently, the cohesion tension theory of siphon operation has been advocated, where the liquid is pulled over the siphon in a way similar to the chain model.[9] It need not be one theory or the other that is correct, but rather both theories may be correct in different circumstances of ambient pressure. The atmospheric pressure with gravity theory obviously cannot explain siphons in vacuum, where there is no significant atmospheric pressure. But the cohesion tension with gravity theory cannot explain CO2 gas siphons,[10] siphons working despite bubbles, and the flying droplet siphon, where gases do not exert significant pulling forces, and liquids not in contact cannot exert a cohesive tension force.
 All known published theories in modern times recognize Bernoulli’s equation as a decent approximation to idealized, friction-free siphon operation.
"
Snell%27s_law,Physics,2,"Snell's law (also known as Snell–Descartes law  and the law of refraction) is a formula used to describe the relationship between the angles of incidence and refraction, when referring to light or other waves passing through a boundary between two different isotropic media, such as water, glass, or air.
 In optics, the law is used in ray tracing to compute the angles of incidence or refraction, and in experimental optics to find the refractive index of a material. The law is also satisfied in metamaterials, which allow light to be bent ""backward"" at a negative angle of refraction with a negative refractive index.
 Snell's law states that the ratio of the sines of the angles of incidence and refraction is equivalent to the ratio of phase velocities in the two media, or equivalent to the reciprocal of the ratio of the indices of refraction:
 with each 



θ


{  \theta }
 as the angle measured from the normal of the boundary, 



v


{  v}
 as the velocity of light in the respective medium (SI units are meters per second, or m/s), and 



n


{  n}
 as the refractive index (which is unitless) of the respective medium.
 The law follows from Fermat's principle of least time, which in turn follows from the propagation of light as waves.
"
Solar_cell,Physics,2,"
 A solar cell, or photovoltaic cell, is an electrical device that converts the energy of light directly into electricity by the photovoltaic effect, which is a physical and chemical phenomenon.[1] It is a form of photoelectric cell, defined as a device whose electrical characteristics, such as current, voltage, or resistance, vary when exposed to light. Individual solar cell devices are often the electical building blocks of photovoltaic modules, known colloquially as solar panels. The common single junction silicon solar cell can produce a maximum open-circuit voltage of approximately 0.5 to 0.6 volts.[2] Solar cells are described as being photovoltaic, irrespective of whether the source is sunlight or an artificial light. In addition to producing energy, they can be used as a photodetector (for example infrared detectors), detecting light or other electromagnetic radiation near the visible range, or measuring light intensity.
 The operation of a photovoltaic (PV) cell requires three basic attributes:
 In contrast, a solar thermal collector supplies heat by absorbing sunlight, for the purpose of either direct heating or indirect electrical power generation from heat. A ""photoelectrolytic cell"" (photoelectrochemical cell), on the other hand, refers either to a type of photovoltaic cell (like that developed by Edmond Becquerel and modern dye-sensitized solar cells), or to a device that splits water directly into hydrogen and oxygen using only solar illumination.
"
Solid,Physics,2,"Solid is one of the four fundamental states of matter (the others being liquid, gas and plasma). The molecules in a solid are closely packed together and contain the least amount of kinetic energy. A solid is characterized by structural rigidity and resistance to a force applied to the surface. Unlike a liquid, a solid object does not flow to take on the shape of its container, nor does it expand to fill the entire available volume like a gas. The atoms in a solid are bound to each other, either in a regular geometric lattice (crystalline solids, which include metals and ordinary ice), or irregularly (an amorphous solid such as common window glass). Solids cannot be compressed with little pressure whereas gases can be compressed with little pressure because the molecules in a gas are loosely packed.
 The branch of physics that deals with solids is called solid-state physics, and is the main branch of condensed matter physics (which also includes liquids). Materials science is primarily concerned with the physical and chemical properties of solids. Solid-state chemistry is especially concerned with the synthesis of novel materials, as well as the science of identification and chemical composition.
"
Solid_mechanics,Physics,2,"Solid mechanics, also known as mechanics of solids, is the branch of continuum mechanics that studies the behavior of solid materials, especially their motion and deformation under the action of forces, temperature changes, phase changes, and other external or internal agents.
 Solid mechanics is fundamental for civil, aerospace, nuclear, biomedical and mechanical engineering, for geology, and for many branches of physics such as materials science.[1] It has specific applications in many other areas, such as understanding the anatomy of living beings, and the design of dental prostheses and surgical implants. One of the most common practical applications of solid mechanics is the Euler-Bernoulli beam equation. Solid mechanics extensively uses tensors to describe stresses, strains, and the relationship between them.
 Solid mechanics is a vast subject because of the wide range of solid materials available, such as steel, wood, concrete, biological materials, textiles, geological materials, and plastics.
"
Solid-state_physics,Physics,2,"Solid-state physics is the study of rigid matter, or solids, through methods such as quantum mechanics, crystallography, electromagnetism, and metallurgy.  It is the largest branch of condensed matter physics. Solid-state physics studies how the large-scale properties of solid materials result from their atomic-scale properties.   Thus,  solid-state physics forms a theoretical basis of materials science.  It also has direct applications, for example in the technology of transistors and semiconductors.
"
Solubility,Physics,2,"
 Solubility is the property of a solid, liquid or gaseous chemical substance called solute to dissolve in a solid, liquid or gaseous solvent. The solubility of a substance fundamentally depends on the physical and chemical properties of the solute and solvent as well as on temperature, pressure and presence of other chemicals (including changes to the pH) of the solution. The extent of the solubility of a substance in a specific solvent is measured as the saturation concentration, where adding more solute does not increase the concentration of the solution and begins to precipitate the excess amount of solute.
 Insolubility is the inability to dissolve in a solid, liquid or gaseous solvent.
 Most often, the solvent is a liquid, which can be a pure substance or a mixture. One may also speak of solid solution, but rarely of solution in a gas (see vapor–liquid equilibrium instead).
 Under certain conditions, the equilibrium solubility can be exceeded to give a so-called supersaturated solution, which is metastable.[1] Metastability of crystals can also lead to apparent differences in the amount of a chemical that dissolves depending on its crystalline form or particle size. A supersaturated solution generally crystallises when 'seed' crystals are introduced and rapid equilibration occurs. Phenylsalicylate is one such simple observable substance when fully melted and then cooled below its fusion point.
 Solubility is not to be confused with the ability to dissolve a substance, because the solution might also occur because of a chemical reaction. For example, zinc dissolves (with effervescence) in hydrochloric acid as a result of a chemical reaction releasing hydrogen gas in a displacement reaction. The zinc ions are soluble in the acid.
 The solubility of a substance is an entirely different property from the rate of solution, which is how fast it dissolves. The smaller a particle is, the faster it dissolves although there are many factors to add to this generalization.
 Crucially, solubility applies to all areas of chemistry, geochemistry, inorganic, physical, organic and biochemistry. In all cases it will depend on the physical conditions (temperature, pressure and concentration) and the enthalpy and entropy directly relating to the solvents and solutes concerned.
By far the most common solvent in chemistry is water which is a solvent for most ionic compounds as well as a wide range of organic substances. This is a crucial factor in acidity and alkalinity and much environmental and geochemical work.
"
Sound,Physics,2,"
 In physics, sound is a vibration that propagates as an acoustic wave, through a transmission medium such as a gas, liquid or solid.
 In human physiology and psychology, sound is the reception of such waves and their perception by the brain.[1] Only acoustic waves that have  frequencies lying between about 20 Hz and 20 kHz, the audio frequency range, elicit an auditory percept in humans. In air at atmospheric pressure, these represent sound waves with wavelengths of 17 meters (56 ft) to 1.7 centimetres (0.67 in).   Sound waves above 20 kHz are known as ultrasound and are not audible to humans. Sound waves below 20 Hz are known as infrasound. Different animal species have varying hearing ranges.
"
Special_relativity,Physics,2,"In physics, the special theory of relativity, or special relativity for short, is a scientific theory regarding the relationship between space and time. In Albert Einstein's original treatment, the theory is based on two postulates:[1][2]"
Specific_activity,Physics,2,"Specific activity is the activity per quantity of a radionuclide and is a physical property of that radionuclide.[1][2] Activity is a quantity related to radioactivity, for which the SI unit is the becquerel (Bq), equal to one reciprocal second.[3] The becquerel is defined as the number of radioactive transformations per second that occur in a particular radionuclide. The older, non-SI unit of activity is the curie (Ci), which is 3.7×1010 transformations per second.
 Since the probability of radioactive decay for a given radionuclide is a fixed physical quantity (with some slight exceptions, see changing decay rates), the number of decays that occur in a given time of a specific number of atoms of that radionuclide is also a fixed physical quantity (if there are large enough numbers of atoms to ignore statistical fluctuations).
 Thus, specific activity is defined as the activity per quantity of atoms of a particular radionuclide. It is usually given in units of Bq/Kg, but another commonly used unit of activity is the curie (Ci) allowing the definition of specific activity in Ci/g. The amount of specific activity should not be confused with level of exposure to ionizing radiation and thus the exposure or absorbed dose. The absorbed dose is the quantity important in assessing the effects of ionizing radiation on humans.
"
Speed,Physics,2,"
 In everyday use and in kinematics, the speed of an object is the magnitude of the change of its position; it is thus a scalar quantity.[1] The average speed of an object in an interval of time is the distance travelled by the object divided by the duration of the interval;[2] the instantaneous speed is the limit of the average speed as the duration of the time interval approaches zero.
 Speed has the dimensions of distance divided by time. The SI unit of speed is the metre per second, but the most common unit of speed in everyday usage is the kilometre per hour or, in the US and the UK, miles per hour. For air and marine travel the knot is commonly used.
 The fastest possible speed at which energy or information can travel, according to special relativity, is the speed of light in a vacuum c = 299792458 metres per second (approximately 1079000000 km/h or 671000000 mph). Matter cannot quite reach the speed of light, as this would require an infinite amount of energy. In relativity physics, the concept of rapidity replaces the classical idea of speed.
"
Speed_of_light,Physics,2,"
 The speed of light in vacuum, commonly denoted c, is a universal physical constant important in many areas of physics. Its exact value is defined as 299792458 metres per second (approximately 300000 km/s, or 186000 mi/s[Note 3]). It is exact because, by international agreement, a metre is defined as the length of the path travelled by light in vacuum during a time interval of ​1⁄299792458 second.[Note 4][3] According to special relativity, c is the upper limit for the speed at which conventional matter, energy or any information can travel through coordinate space. Though this speed is most commonly associated with light, it is also the speed at which all massless particles and field perturbations travel in vacuum, including electromagnetic radiation (of which light is a small range in the frequency spectrum) and gravitational waves. Such particles and waves travel at c regardless of the motion of the source or the inertial reference frame of the observer. Particles with nonzero rest mass can approach c, but can never actually reach it, regardless of the frame of reference in which their speed is measured. In the special and general theories of relativity, c interrelates space and time, and also appears in the famous equation of mass–energy equivalence E = mc2.[4] In some cases objects or waves may appear to travel faster than light even though they don't actually do so, e.g., with optical illusions, phase velocities, certain high-speed astronomical objects, particular quantum effects, and in the case of the expansion of space itself.
 The speed at which light propagates through transparent materials, such as glass or air, is less than c; similarly, the speed of electromagnetic waves in wire cables is slower than c. The ratio between c and the speed v at which light travels in a material is called the refractive index n of the material (n = c / v). For example, for visible light, the refractive index of glass is typically around 1.5, meaning that light in glass travels at c / 1.5 ≈ 200000 km/s (124000 mi/s); the refractive index of air for visible light is about 1.0003, so the speed of light in air is about 90 km/s (56 mi/s) slower than c.
 For many practical purposes, light and other electromagnetic waves will appear to propagate instantaneously, but for long distances and very sensitive measurements, their finite speed has noticeable effects. In communicating with distant space probes, it can take minutes to hours for a message to get from Earth to the spacecraft, or vice versa. The light seen from stars left them many years ago, allowing the study of the history of the universe by looking at distant objects. The finite speed of light also limits the data transfer between the CPU and memory chips in computers. The speed of light can be used with time of flight measurements to measure large distances to high precision.
 Ole Rømer first demonstrated in 1676 that light travels at a finite speed (non-instantaneously) by studying the apparent motion of Jupiter's moon Io. In 1865, James Clerk Maxwell proposed that light was an electromagnetic wave, and therefore travelled at the speed c appearing in his theory of electromagnetism.[5] In 1905, Albert Einstein postulated that the speed of light c with respect to any inertial frame is a constant and is independent of the motion of the light source.[6] He explored the consequences of that postulate by deriving the theory of relativity and in doing so showed that the parameter c had relevance outside of the context of light and electromagnetism.
 After centuries of increasingly precise measurements, in 1975 the speed of light was known to be 299792458 m/s (983571056 ft/s; 186282.397 mi/s) with a measurement uncertainty of 4 parts per billion. In 1983, the metre was redefined in the International System of Units (SI) as the distance travelled by light in vacuum in 1 / 299792458 of a second.
"
Speed_of_sound,Physics,2,"
 The speed of sound is the distance travelled per unit of time by a sound wave as it propagates through an elastic medium. At 20 °C (68 °F), the speed of sound in air is about 343 metres per second (1,235 km/h; 1,125 ft/s; 767 mph; 667 kn), or a kilometre in 2.9 s or a mile in 4.7 s. It depends strongly on temperature as well as the medium through which a sound wave is propagating.
 The speed of sound in an ideal gas depends only on its temperature and composition. The speed has a weak dependence on frequency and pressure in ordinary air, deviating slightly from ideal behavior.
 In colloquial speech speed of sound refers to the speed of sound waves in air. However, the speed of sound varies from substance to substance: typically sound travels most slowly in gases, faster in liquids, and faster still in solids. For example, while as noted above sound travels at 343 m/s in air, it travels at 1,481 m/s in water (almost 4.3 times faster) and at 5,120 m/s in iron (almost 15 times faster). In an exceptionally stiff material such as diamond, sound travels at 12,000 metres per second (39,000 ft/s),[1]— about 35 times its speed in air and about the fastest it can travel under normal conditions.
 Sound waves in solids are composed of compression waves (just as in gases and liquids), and a different type of sound wave called a shear wave, which occurs only in solids. Shear waves in solids usually travel at different speeds, as exhibited in seismology. The speed of compression waves in solids is determined by the medium's compressibility, shear modulus and density. The speed of shear waves is determined only by the solid material's shear modulus and density.
 In fluid dynamics, the speed of sound in a fluid medium (gas or liquid) is used as a relative measure for the speed of an object moving through the medium. The ratio of the speed of an object to the speed of sound in the fluid is called the object's Mach number. Objects moving at speeds greater than Mach1 are said to be traveling at supersonic speeds.
"
Spherical_aberration,Physics,2,"Spherical aberration is a type of aberration found in optical systems that use elements with spherical surfaces. Lenses and curved mirrors are most often made with surfaces that are spherical, because this shape is easier to form than non-spherical curved surfaces. Light rays that strike a spherical surface off-centre are refracted or reflected more or less than those that strike close to the centre. This deviation reduces the quality of images produced by optical systems.
  Tilt Spherical aberration Astigmatism Coma  Distortion Petzval field curvature Chromatic aberration
"
Spin_quantum_number,Physics,2,"In atomic physics, the spin quantum number is a quantum number that describes the intrinsic angular momentum (or spin angular momentum, or simply spin) of a given particle. The spin quantum number is designated by the letter s, and is the fourth of a set of quantum numbers (the principal quantum number, the azimuthal quantum number, the magnetic quantum number, and the spin quantum number), which completely describe the quantum state of an electron. The name comes from a physical spinning of the electron about an axis that was proposed by Uhlenbeck and Goudsmit. However this simplistic picture was quickly realized to be physically impossible,[1] and replaced by a more abstract quantum-mechanical description.
"
Stable_isotope_ratio,Physics,2,"The term stable isotope has a meaning similar to stable nuclide, but is preferably used when speaking of nuclides of a specific element. Hence, the plural form stable isotopes usually refers to isotopes of the same element. The relative abundance of such stable isotopes can be measured experimentally (isotope analysis), yielding an isotope ratio that can be used as a research tool. Theoretically, such stable isotopes could include the radiogenic daughter products of radioactive decay, used in radiometric dating. However, the expression stable-isotope ratio is preferably used to refer to isotopes whose relative abundances are affected by isotope fractionation in nature. This field is termed stable isotope geochemistry.
"
Stable_nuclide,Physics,2,"Stable nuclides are nuclides that are not radioactive and so (unlike radionuclides) do not spontaneously undergo radioactive decay. When such nuclides are referred to in relation to specific elements, they are usually termed stable isotopes.
 The 80 elements with one or more stable isotopes comprise a total of 252 nuclides that have not been known to decay using current equipment (see list at the end of this article). Of these elements, 26 have only one stable isotope; they are thus termed monoisotopic. The rest have more than one stable isotope. Tin has ten stable isotopes, the largest number of stable isotopes known for an element.
"
Standard_atomic_weight,Physics,2,"The standard atomic weight (Ar, standard(E)) of a chemical element is the weighted arithmetic mean of the relative isotopic masses of all isotopes of that element weighted by each isotope's abundance on Earth.  For example, isotope 63Cu (Ar = 62.929) constitutes 69% of the copper on Earth, the rest being 65Cu (Ar = 64.927), so
 Because relative isotopic masses are dimensionless quantities, this weighted mean is also dimensionless. It can be converted into a measure of mass (with dimension M) by multiplying it with the dalton, also known as the atomic mass constant.
 Among various variants of the notion of atomic weight (Ar, also known as relative atomic mass) used by scientists, the standard atomic weight (Ar, standard) is the most common and practical. The standard atomic weight of each chemical element is determined and published by the Commission on Isotopic Abundances and Atomic Weights (CIAAW) of the International Union of Pure and Applied Chemistry (IUPAC) based on natural, stable, terrestrial sources of the element. The definition specifies the use of samples from many representative sources from the Earth, so that the value can widely be used as 'the' atomic weight for substances as they are encountered in reality—for example, in pharmaceuticals and scientific research. Non-standardized atomic weights of an element are specific to sources and samples, such as the atomic weight of carbon in a particular bone from a particular archeological site. Standard atomic weight averages such values to the range of atomic weights that a chemist might expect to derive from many random samples from Earth. This range is the rationale for the interval notation given for some standard atomic weight values.
 Of the 118 known chemical elements, 80 have stable isotopes and 84 have this Earth-environment based value. Typically, such a value is, for example helium: Ar, standard(He) = 4.002602(2). The ""(2)"" indicates the uncertainty in the last digit shown, to read 4.002602±0.000002. IUPAC also publishes abridged values, rounded to five significant figures. For helium, Ar, abridged(He) = 4.0026.
 For thirteen elements the samples diverge on this value, because their sample sources have had a different decay history. For example, thallium (Tl) in sedimentary rocks has a different isotopic composition than in igneous rocks and volcanic gases. For these elements, the standard atomic weight is noted as an interval: Ar, standard(Tl) = [204.38, 204.39]. With such an interval, for less demanding situations, IUPAC also publishes a conventional value. For thallium, Ar, conventional(Tl) = 204.38.
"
Standard_Model,Physics,2,"
 The Standard Model of particle physics is the theory describing three of the four known fundamental forces (the electromagnetic, weak, and strong interactions, and not including the gravitational force) in the universe, as well as classifying all known elementary particles. It was developed in stages throughout the latter half of the 20th century, through the work of many scientists around the world,[1] with the current formulation being finalized in the mid-1970s upon experimental confirmation of the existence of quarks.  Since then, confirmation of the top quark (1995), the tau neutrino (2000), and the Higgs boson (2012) have added further credence to the Standard Model. In addition, the Standard Model has predicted various properties of weak neutral currents and the W and Z bosons with great accuracy.
 Although the Standard Model is believed to be theoretically self-consistent[2] and has demonstrated huge successes in providing experimental predictions, it leaves some phenomena unexplained and falls short of being a complete theory of fundamental interactions. It does not fully explain baryon asymmetry, incorporate the full theory of gravitation[3] as described by general relativity, or account for the accelerating expansion of the Universe as possibly described by dark energy.  The model does not contain any viable dark matter particle that possesses all of the required properties deduced from observational cosmology. It also does not incorporate neutrino oscillations and their non-zero masses.
 The development of the Standard Model was driven by theoretical and experimental particle physicists alike. For theorists, the Standard Model is a paradigm of a quantum field theory, which exhibits a wide range of phenomena including spontaneous symmetry breaking, anomalies and non-perturbative behavior. It is used as a basis for building more exotic models that incorporate hypothetical particles, extra dimensions, and elaborate symmetries (such as supersymmetry) in an attempt to explain experimental results at variance with the Standard Model, such as the existence of dark matter and neutrino oscillations.
"
Standing_wave,Physics,2,"In physics, a standing wave, also known as a stationary wave, is a wave which oscillates in time but whose peak amplitude profile does not move in space.  The peak amplitude of the wave oscillations at any point in space is constant with time, and the oscillations at different points throughout the wave are in phase.   The locations at which the absolute value of the amplitude is minimum are called nodes, and the locations where the absolute value of the amplitude is maximum are called antinodes.
 Standing waves were first noticed by Michael Faraday in 1831.  Faraday observed standing waves on the surface of a liquid in a vibrating container.[1][2] Franz Melde coined the term ""standing wave"" (German: stehende Welle or Stehwelle) around 1860 and demonstrated the phenomenon in his classic experiment with vibrating strings.[3][4][5][6] This phenomenon can occur because the medium is moving in the opposite direction to the wave, or it can arise in a stationary medium as a result of interference between two waves traveling in opposite directions. The most common cause of standing waves is the phenomenon of resonance, in which standing waves occur inside a resonator due to interference between waves reflected back and forth at the resonator's resonant frequency.
 For waves of equal amplitude traveling in opposing directions, there is on average no net propagation of energy.
"
State_of_matter,Physics,2,"
 In physics, a state of matter is one of the distinct forms in which matter can exist. Four states of matter are observable in everyday life: solid, liquid, gas, and plasma. Many intermediate states are known to exist, such as liquid crystal, and some states only exist under extreme conditions, such as Bose–Einstein condensates, neutron-degenerate matter, and quark–gluon plasma, which only occur, respectively, in situations of extreme cold, extreme density, and extremely high energy. For a complete list of all exotic states of matter, see the list of states of matter.
 Historically, the distinction is made based on qualitative differences in properties. Matter in the solid state maintains a fixed volume and shape, with component particles (atoms, molecules or ions) close together and fixed into place. Matter in the liquid state maintains a fixed volume, but has a variable shape that adapts to fit its container. Its particles are still close together but move freely. Matter in the gaseous state has both variable volume and shape, adapting both to fit its container. Its particles are neither close together nor fixed in place. Matter in the plasma state has variable volume and shape, and contains neutral atoms as well as a significant number of ions and electrons, both of which can move around freely.
 The term phase is sometimes used as a synonym for state of matter, but a system can contain several immiscible phases of the same state of matter.
"
Statics,Physics,2,"Statics is the branch of mechanics that is concerned with the analysis of loads (force and torque, or ""moment"") acting on physical systems that do not experience an acceleration (a=0), but rather, are in static equilibrium with their environment. The application of Newton's second law to a system gives:
 Where bold font indicates a vector that has magnitude and direction. 





F




{  {\textbf {F}}}
 is the total of the forces acting on the system, 



m


{  m}
 is the mass of the system and 





a




{  {\textbf {a}}}
 is the acceleration of the system. The summation of forces will give the direction and the magnitude of the acceleration and will be inversely proportional to the mass. The assumption of static equilibrium of 





a




{  {\textbf {a}}}
 = 0 leads to:
 The summation of forces, one of which might be unknown, allows that unknown to be found. So when in static equilibrium, the acceleration of the system is zero and the system is either at rest, or its center of mass moves at constant velocity. Likewise the application of the assumption of zero acceleration to the summation of moments acting on the system leads to:
 Here, 





M




{  {\textbf {M}}}
 is the summation of all moments acting on the system, 



I


{  I}
 is the moment of inertia of the mass and 



α


{  \alpha }
 = 0 the angular acceleration of the system, which when assumed to be zero leads to:
 The summation of moments, one of which might be unknown, allows that unknown to be found.
These two equations together, can be applied to solve for as many as two loads (forces and moments) acting on the system.
 From Newton's first law, this implies that the net force and net torque on every part of the system is zero. The net forces equaling zero is known as the first condition for equilibrium, and the net torque equaling zero is known as the second condition for equilibrium. See statically indeterminate.
"
Statistical_mechanics,Physics,2,"
 Statistical mechanics, one of the pillars of modern physics, describes how macroscopic observations (such as temperature and pressure) are related to microscopic parameters that fluctuate around an average. It connects thermodynamic quantities (such as heat capacity) to microscopic behavior, whereas, in classical thermodynamics, the only available option would be to measure and tabulate such quantities for various materials.[1] Statistical mechanics is necessary for the fundamental study of any physical system that has many degrees of freedom. The approach is based on statistical methods, probability theory and the microscopic physical laws.[1][2][3][note 1] It can be used to explain the thermodynamic behaviour of large systems. This branch of statistical mechanics, which treats and extends classical thermodynamics, is known as statistical thermodynamics or equilibrium statistical mechanics.
 Statistical mechanics can also be used to study systems that are out of equilibrium. An important sub-branch known as non-equilibrium statistical mechanics (sometimes called statistical dynamics) deals with the issue of microscopically modelling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions or flows of particles and heat. The fluctuation–dissipation theorem is the basic knowledge obtained from applying non-equilibrium statistical mechanics to study the simplest non-equilibrium situation of a steady state current flow in a system of many particles.
"
Stiffness,Physics,2,"Stiffness is the extent to which an object resists deformation in response to an applied force.[1] The complementary concept is flexibility or pliability: the more flexible an object is, the less stiff it is.[2]"
Deformation_(mechanics),Physics,2,"In physics, deformation is the continuum mechanics transformation of a body from a reference configuration to a current configuration.[1]  A configuration is a set containing the positions of all particles of the body.
 A  deformation may be caused by external loads,[2] body forces (such as gravity or electromagnetic forces), or changes in temperature, moisture content, or chemical reactions, etc.
 Strain is a description of deformation in terms of relative displacement of particles in the body that excludes rigid-body motions.  Different equivalent choices may be made for the expression of a strain field depending on whether it is defined with respect to the initial or the final configuration of the body and on whether the metric tensor or its dual is considered.
 In a continuous body, a deformation field results from a stress field induced by applied forces or is due to changes in the temperature field inside the body. The relation between stresses and induced strains is expressed by constitutive equations, e.g., Hooke's law for linear elastic materials. Deformations which are recovered after the stress field has been removed are called elastic deformations. In this case, the continuum completely recovers its original configuration. On the other hand, irreversible deformations remain even after stresses have been removed. One type of irreversible deformation is plastic deformation, which occurs in material bodies after stresses have attained a certain threshold value known as the elastic limit or yield stress, and are the result of slip, or dislocation mechanisms at the atomic level. Another type of irreversible deformation is viscous deformation, which is the irreversible part of viscoelastic deformation.
 In the case of elastic deformations, the response function linking strain to the deforming stress is the compliance tensor of the material.
"
Work_hardening,Physics,2,"
 Work hardening, also known as strain hardening, is the strengthening of a metal or polymer by plastic deformation. Work hardening may be desirable, undesirable, or inconsequential, depending on the context. 
 This strengthening occurs because of dislocation movements and dislocation generation within the crystal structure of the material.[1] Many non-brittle metals with a reasonably high melting point as well as several polymers can be strengthened in this fashion.[2] Alloys not amenable to heat treatment, including low-carbon steel, are often work-hardened. Some materials cannot be work-hardened at low temperatures, such as indium,[3] however others can only be strengthened via work hardening, such as pure copper and aluminum.[4]"
Strength_of_materials,Physics,2,"
Strength of materials, also called mechanics of materials, deals with the behavior of solid objects subject to stresses and strains. The complete theory began with the consideration of the behavior of one and two dimensional members of structures, whose states of stress can be approximated as two dimensional, and was then generalized to three dimensions to develop a more complete theory of the elastic and plastic behavior of materials. An important founding pioneer in mechanics of materials was Stephen Timoshenko.
 The study of strength of materials often refers to various methods of calculating the stresses and strains in structural members, such as beams, columns, and shafts. The methods employed to predict the response of a structure under loading and its susceptibility to various failure modes takes into account the properties of the materials such as its yield strength, ultimate strength, Young's modulus, and Poisson's ratio. In addition, the mechanical element's macroscopic properties (geometric properties) such as its length, width, thickness, boundary constraints and abrupt changes in geometry such as holes are considered.
"
Stress_(mechanics),Physics,2,"In continuum mechanics, stress is a physical quantity that expresses the internal forces that neighbouring particles of a continuous material exert on each other, while strain is the measure of the deformation of the material. For example, when a solid vertical bar is supporting an overhead weight, each particle in the bar pushes on the particles immediately below it. When a liquid is in a closed container under pressure, each particle gets pushed against by all the surrounding particles.  The container walls and the pressure-inducing surface (such as a piston) push against them in (Newtonian) reaction. These macroscopic forces are actually the net result of a very large number of intermolecular forces and collisions between the particles in those molecules. Stress is frequently represented by a lowercase Greek letter sigma (σ).
 Strain inside a material may arise by various mechanisms, such as stress as applied by external forces to the bulk material (like gravity) or to its surface (like contact forces, external pressure, or friction).  Any strain (deformation) of a solid material generates an internal elastic stress, analogous to the reaction force of a spring, that tends to restore the material to its original non-deformed state.  In liquids and gases, only deformations that change the volume generate persistent elastic stress. However, if the deformation changes gradually with time, even in fluids there will usually be some viscous stress, opposing that change.  Elastic and viscous stresses are usually combined under the name mechanical stress.
 Significant stress may exist even when deformation is negligible or non-existent (a common assumption when modeling the flow of water). Stress may exist in the absence of external forces; such built-in stress is important, for example, in prestressed concrete and tempered glass. Stress may also be imposed on a material without the application of net forces, for example by changes in temperature or chemical composition, or by external electromagnetic fields (as in piezoelectric and magnetostrictive materials).
 The relation between mechanical stress, deformation, and the rate of change of deformation can be quite complicated, although a linear approximation may be adequate in practice if the quantities are sufficiently small. Stress that exceeds certain strength limits of the material will result in permanent deformation (such as plastic flow, fracture, cavitation) or even change its crystal structure  and chemical composition.
 In some branches of engineering, the term stress is occasionally used in a looser sense as a synonym of ""internal force"". For example, in the analysis of trusses, it may refer to the total traction or compression force acting on a beam, rather than the force divided by the area of its cross-section.
"
Stress%E2%80%93strain_curve,Physics,2,"In engineering and materials science, a stress–strain curve for a material gives the relationship between stress and strain. It is obtained by gradually applying load to a test coupon and measuring the deformation, from which the stress and strain can be determined (see tensile testing). These curves reveal many of the properties of a material, such as the Young's modulus, the yield strength and the ultimate tensile strength.
"
String_duality,Physics,2,"String duality is a class of symmetries in physics that link different string theories, theories which assume that the fundamental building blocks of the universe are strings instead of point particles. 
"
String_theory,Physics,2,"
 In physics, string theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force. Thus string theory is a theory of quantum gravity.
 String theory is a broad and varied subject that attempts to address a number of deep questions of fundamental physics. String theory has contributed a number of advances to mathematical physics, which have been applied to a variety of problems in black hole physics, early universe cosmology, nuclear physics, and condensed matter physics, and it has stimulated a number of major developments in pure mathematics. Because string theory potentially provides a unified description of gravity and particle physics, it is a candidate for a theory of everything, a self-contained mathematical model that describes all fundamental forces and forms of matter. Despite much work on these problems, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of its details. 
 String theory was first studied in the late 1960s as a theory of the strong nuclear force, before being abandoned in favor of quantum chromodynamics. Subsequently, it was realized that the very properties that made string theory unsuitable as a theory of nuclear physics made it a promising candidate for a quantum theory of gravity. The earliest version of string theory, bosonic string theory, incorporated only the class of particles known as bosons. It later developed into superstring theory, which posits a connection called supersymmetry between bosons and the class of particles called fermions. Five consistent versions of superstring theory were developed before it was conjectured in the mid-1990s that they were all different limiting cases of a single theory in 11 dimensions known as M-theory. In late 1997, theorists discovered an important relationship called the AdS/CFT correspondence, which relates string theory to another type of physical theory called a quantum field theory.
 One of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. Another issue is that the theory is thought to describe an enormous landscape of possible universes, which has complicated efforts to develop theories of particle physics based on string theory. These issues have led some in the community to criticize these approaches to physics, and to question the value of continued research on string theory unification.
"
Structural_load,Physics,2,"Structural loads or actions are forces, deformations, or accelerations applied to structure components.[1][2] Loads cause stresses, deformations, and displacements in structures. Assessment of their effects is carried out by the methods of structural analysis. Excess load or overloading may cause structural failure, and hence such possibility should be either considered in the design or strictly controlled. Mechanical structures, such as aircraft, satellites, rockets, space stations, ships, and submarines, have their own particular structural loads and actions.[3] Engineers often evaluate structural loads based upon published regulations, contracts, or specifications.  Accepted technical standards are used for acceptance testing and inspection.
"
Subatomic_particle,Physics,2,"In the physical sciences, subatomic particles are smaller than atoms.[1] They can be composite particles, such as the neutron and proton; or elementary particles, which according to the standard model are not made of other particles.[2] Particle physics and nuclear physics study these particles and how they interact.[3]
The concept of a subatomic particle was refined when experiments showed that light could behave like a stream of particles (called photons) as well as exhibiting wave-like properties. This led to the concept of wave–particle duality to reflect that quantum-scale particles behave like both particles and waves (they are sometimes described as wavicles to reflect this[citation needed]). Another concept, the uncertainty principle, states that some of their properties taken together, such as their simultaneous position and momentum, cannot be measured exactly.[4] The wave–particle duality has been shown to apply not only to photons but to more massive particles as well.[5] Interactions of particles in the framework of quantum field theory are understood as creation and annihilation of quanta of corresponding fundamental interactions. This blends particle physics with field theory.
 Even among particle physicists, the exact definition of a particle has diverse descriptions.  These professional attempts at the definition of a particle include:  
"
Sublimation_(phase_transition),Physics,2,"
 Sublimation is the transition of a substance directly from the solid to the gas state,[1] without passing through the liquid state.[2] Sublimation is an endothermic process that occurs at temperatures and pressures below a substance's triple point in its phase diagram, which corresponds to the lowest pressure at which the substance can exist as a liquid. The reverse process of sublimation is deposition or desublimation, in which a substance passes directly from a gas to a solid phase.[3] Sublimation has also been used as a generic term to describe a solid-to-gas transition (sublimation) followed by a gas-to-solid transition (deposition).[4] While vaporization from liquid to gas occurs as evaporation from the surface if it occurs below the boiling point of the liquid, and as boiling with formation of bubbles in the interior of the liquid if it occurs at the boiling point, there is no such distinction for the solid-to-gas transition which always occurs as sublimation from the surface.
 At normal pressures, most chemical compounds and elements possess three different states at different temperatures. In these cases, the transition from the solid to the gaseous state requires an intermediate liquid state. The pressure referred to is the partial pressure of the substance, not the total (e.g. atmospheric) pressure of the entire system. So, all solids that possess an appreciable vapour pressure at a certain temperature usually can sublime in air (e.g. water ice just below 0 °C). For some substances, such as carbon and arsenic, sublimation is much easier than evaporation from the melt, because the pressure of their triple point is very high, and it is difficult to obtain them as liquids.
 The term sublimation refers to a physical change of state and is not used to describe the transformation of a solid to a gas in a chemical reaction. For example, the dissociation on heating of solid ammonium chloride into hydrogen chloride and ammonia is not sublimation but a chemical reaction. Similarly the combustion of candles, containing paraffin wax, to carbon dioxide and water vapor is not sublimation but a chemical reaction with oxygen.
 Sublimation is caused by the absorption of heat which provides enough energy for some molecules to overcome the attractive forces of their neighbors and escape into the vapor phase. Since the process requires additional energy, it is an endothermic change. The enthalpy of sublimation (also called heat of sublimation) can be calculated by adding the enthalpy of fusion and the enthalpy of vaporization.
"
Superconductivity,Physics,2,"Superconductivity is a set of physical properties observed in certain materials where electrical resistance vanishes and magnetic flux fields are expelled from the material. Any material exhibiting these properties is a superconductor. Unlike an ordinary metallic conductor, whose resistance decreases gradually as its temperature is lowered even down to near absolute zero, a superconductor has a characteristic critical temperature below which the resistance drops abruptly to zero. An electric current through a loop of superconducting wire can persist indefinitely with no power source.[1][2][3][4] The superconductivity phenomenon was discovered in 1911 by Dutch physicist Heike Kamerlingh Onnes. Like ferromagnetism and atomic spectral lines, superconductivity is a phenomenon which can only be explained by quantum mechanics. It is characterized by the Meissner effect, the complete ejection of magnetic field lines from the interior of the superconductor during its transitions into the superconducting state. The occurrence of the Meissner effect indicates that superconductivity cannot be understood simply as the idealization of perfect conductivity in classical physics.
 In 1986, it was discovered that some cuprate-perovskite ceramic materials have a critical temperature above 90 K (−183 °C).[5] Such a high transition temperature is theoretically impossible for a conventional superconductor, leading the materials to be termed high-temperature superconductors. The cheaply available coolant liquid nitrogen boils at 77 K, and thus the existence of superconductivity at higher temperatures than this facilitates many experiments and applications that are less practical at lower temperatures.
"
Superconductor,Physics,2,"Superconductivity is a set of physical properties observed in certain materials where electrical resistance vanishes and magnetic flux fields are expelled from the material. Any material exhibiting these properties is a superconductor. Unlike an ordinary metallic conductor, whose resistance decreases gradually as its temperature is lowered even down to near absolute zero, a superconductor has a characteristic critical temperature below which the resistance drops abruptly to zero. An electric current through a loop of superconducting wire can persist indefinitely with no power source.[1][2][3][4] The superconductivity phenomenon was discovered in 1911 by Dutch physicist Heike Kamerlingh Onnes. Like ferromagnetism and atomic spectral lines, superconductivity is a phenomenon which can only be explained by quantum mechanics. It is characterized by the Meissner effect, the complete ejection of magnetic field lines from the interior of the superconductor during its transitions into the superconducting state. The occurrence of the Meissner effect indicates that superconductivity cannot be understood simply as the idealization of perfect conductivity in classical physics.
 In 1986, it was discovered that some cuprate-perovskite ceramic materials have a critical temperature above 90 K (−183 °C).[5] Such a high transition temperature is theoretically impossible for a conventional superconductor, leading the materials to be termed high-temperature superconductors. The cheaply available coolant liquid nitrogen boils at 77 K, and thus the existence of superconductivity at higher temperatures than this facilitates many experiments and applications that are less practical at lower temperatures.
"
Superhard_material,Physics,2,"A superhard material is a material with a hardness value exceeding 40 gigapascals (GPa) when measured by the Vickers hardness test.[1][2][3][4] They are virtually incompressible solids with high electron density and high bond covalency. As a result of their unique properties, these materials are of great interest in many industrial areas including, but not limited to, abrasives, polishing and cutting tools, disc brakes, and wear-resistant and protective coatings.
 Diamond is the hardest known material to date, with a Vickers hardness in the range of 70–150 GPa. Diamond demonstrates both high thermal conductivity and electrically insulating properties and much attention has been put into finding practical applications of this material. However, diamond has several limitations for mass industrial application, including its high cost and oxidation at temperatures above 800 °C.[5][6] In addition, diamond dissolves in iron and forms iron carbides at high temperatures and therefore is inefficient in cutting ferrous materials including steel. Therefore, recent research of superhard materials has been focusing on compounds which would be thermally and chemically more stable than pure diamond.
 The search for new superhard materials has generally taken two paths.[7] In the first approach, researchers emulate the short, directional covalent carbon bonds of diamond by combining light elements like boron, carbon, nitrogen, and oxygen. This approach became popular in the late 1980s with the exploration of C3N4 and B-C-N ternary compounds. The second approach towards designing superhard materials incorporates these lighter elements (B, C, N, and O), but also introduces transition metals with high valence electron densities to provide high incompressibility. In this way, metals with high bulk moduli but low hardness are coordinated with small covalent-forming atoms to produce superhard materials. Tungsten carbide is an industrially-relevant manifestation of this approach, although it is not considered superhard. Alternatively, borides combined with transition metals have become a rich area of superhard research and have led to discoveries such as ReB2, OsB2, and WB4.
 Superhard materials can be generally classified into two categories: intrinsic compounds and extrinsic compounds. The intrinsic group includes diamond, cubic boron nitride (c-BN), carbon nitrides and ternary compounds such as B-N-C, which possess an innate hardness. Conversely, extrinsic materials are those that have superhardness and other mechanical properties that are determined by their microstructure rather than composition.[8][9][10] An example of extrinsic superhard material is nanocrystalline diamond known as aggregated diamond nanorods.
"
Superposition_principle,Physics,2,"The superposition principle,[1] also known as superposition property, states that, for all linear systems, the net response caused by two or more stimuli is the sum of the responses that would have been caused by each stimulus individually. So that if input A produces response X and input B produces response Y then input (A + B) produces response (X + Y).
 A function 



F
(
x
)


{  F(x)}
 that satisfies the superposition principle is called a linear function. Superposition can be defined by two simpler properties; additivity and homogeneity
 This principle has many applications in physics and engineering because many physical systems can be modeled as linear systems. For example, a beam can be modeled as a linear system where the input stimulus is the load on the beam and the output response is the deflection of the beam. The importance of linear systems is that they are easier to analyze mathematically; there is a large body of mathematical techniques, frequency domain linear transform methods such as Fourier, Laplace transforms, and linear operator theory, that are applicable. Because physical systems are generally only approximately linear, the superposition principle is only an approximation of the true physical behavior.
 The superposition principle applies to 'any linear system, including algebraic equations, linear differential equations, and systems of equations of those forms. The stimuli and responses could be numbers, functions, vectors, vector fields, time-varying signals, or any other object that satisfies certain axioms. Note that when vectors or vector fields are involved, a superposition is interpreted as a vector sum.
"
Supersymmetry,Physics,2,"In particle physics, supersymmetry (SUSY) is a conjectured relationship between two basic classes of elementary particles: bosons, which have an integer-valued spin, and fermions, which have a half-integer spin.[1][2] A type of spacetime symmetry, supersymmetry is a possible candidate for undiscovered particle physics, and seen by some physicists as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to be incomplete. A supersymmetrical extension to the Standard Model could resolve major hierarchy problems within gauge theory, by guaranteeing that quadratic divergences of all orders will cancel out in perturbation theory.
 In supersymmetry, each particle from one group would have an associated particle in the other, known as its superpartner, the spin of which differs by a half-integer. These superpartners would be new and undiscovered particles; for example, there would be a particle called a ""selectron"" (superpartner electron), a bosonic partner of the electron. In the simplest supersymmetry theories, with perfectly ""unbroken"" supersymmetry, each pair of superpartners would share the same mass and internal quantum numbers besides spin. Since we expect to find these ""superpartners"" using present-day equipment, if supersymmetry exists then it consists of a spontaneously broken symmetry, allowing superpartners to differ in mass.[3][4][5] Spontaneously broken supersymmetry could solve many problems in particle physics, including the hierarchy problem.
 There is no experimental evidence that supersymmetry is correct, or whether or not other extensions to current models might be more accurate. In part, this is because it is only since around 2010 that particle accelerators specifically designed to study physics beyond the Standard Model have become operational (i.e. the LHC), and because it is not yet known where exactly to look, nor the energies required for a successful search.
 The main reasons for supersymmetry being supported by some physicists is that the current theories are known to be incomplete and their limitations are well established, and supersymmetry could be an attractive solution to some of the major concerns.[6][7]"
Surface_tension,Physics,2,"Surface tension is the tendency of liquid surfaces to shrink into the minimum surface area possible. Surface tension allows insects (e.g. water striders), usually denser than water, to float and slide on a water surface.
 At liquid–air interfaces, surface tension results from the greater attraction of liquid molecules to each other (due to cohesion) than to the molecules in the air (due to adhesion).
 There are two primary mechanisms in play. One is an inward force on the surface molecules causing the liquid to contract.[1][2] Second is a tangential force parallel to the surface of the liquid.[2] The net effect is the liquid behaves as if its surface were covered with a stretched elastic membrane.
 Because of the relatively high attraction of water molecules to each other through a web of hydrogen bonds, water has a higher surface tension (72.8 millinewtons (mN) per meter at 20 °C) than most other liquids. Surface tension is an important factor in the phenomenon of capillarity.
 Surface tension has the dimension of force per unit length, or of energy per unit area. The two are equivalent, but when referring to energy per unit of area, it is common to use the term surface energy, which is a more general term in the sense that it applies also to solids.
 In materials science, surface tension is used for either surface stress or surface energy.
"
Temperature,Physics,2,"
 Temperature is a physical quantity that expresses hot and cold. It is the manifestation of thermal energy, present in all matter, which is the source of the occurrence of heat, a flow of energy, when a body is in contact with another that is colder.
 Temperature is measured with a thermometer. Thermometers are calibrated in various temperature scales that historically have used various reference points and thermometric substances for definition. The most common scales are the Celsius scale (formerly called centigrade, denoted °C), the Fahrenheit scale (denoted °F), and the Kelvin scale (denoted K), the last of which is predominantly used for scientific purposes by conventions of the International System of Units (SI). 
 The lowest theoretical temperature is absolute zero, at which no more thermal energy can be extracted from a body. Experimentally, it can only be approached very closely, but not reached, which is recognized in the third law of thermodynamics.
 Temperature is important in all fields of natural science, including physics, chemistry, Earth science, astronomy, medicine, biology, ecology and geography as well as most aspects of daily life.
"
Tensile_Modulus,Physics,2,"
 Young's modulus 



E


{  E}
, the Young modulus or the modulus of elasticity in tension, is a mechanical property that measures the tensile stiffness of a solid material. It quantifies the relationship between tensile stress 



σ


{  \sigma }
 (force per unit area) and axial strain 



ε


{  \varepsilon }
 (proportional deformation) in the linear elastic region of a material and is determined using the formula:[1] Young's moduli are typically so large that they are expressed not in pascals but in gigapascals (GPa).
 Although Young's modulus is named after the 19th-century British scientist Thomas Young, the concept was developed in 1727 by Leonhard Euler. The first experiments that used the concept of Young's modulus in its current form were performed by the Italian scientist Giordano Riccati in 1782, pre-dating Young's work by 25 years.[2]  The term modulus is derived from the Latin root term modus which means measure.
"
Tensile_strength,Physics,2,"
 Ultimate tensile strength (UTS), often shortened to tensile strength (TS), ultimate strength, or 




F

tu




{  F_{\text{tu}}}
 within equations,[1][2][3] is the maximum stress that a material can withstand while being stretched or pulled before breaking. In brittle materials the ultimate tensile strength is close to the yield point, whereas in ductile materials the ultimate tensile strength can be higher.
 The ultimate tensile strength is usually found by performing a tensile test and recording the engineering stress versus strain. The highest point of the stress–strain curve is the ultimate tensile strength and has units of stress.
 Tensile strengths are rarely used in the design of ductile members, but they are important in brittle members. They are tabulated for common materials such as alloys, composite materials, ceramics, plastics, and wood.
"
Tesla_(unit),Physics,2,"The tesla (symbol: T) is a derived unit of the magnetic induction (also, magnetic flux density)  in the International System of Units. 
 One tesla is equal to one weber per square metre.  The unit was announced during the General Conference on Weights and Measures in 1960 and is named[1] in honour of Nikola Tesla, upon the proposal of the Slovenian electrical engineer France Avčin.
 The strongest fields encountered from permanent magnets on Earth are from Halbach spheres and can be over 4.5 T.  The record for the highest sustained pulsed magnetic field has been produced by scientists at the Los Alamos National Laboratory campus of the National High Magnetic Field Laboratory, the world's first 100-tesla non-destructive magnetic field.[2] In September 2018, researchers at the University of Tokyo generated a field of 1200 T which lasted in the order of 100 microseconds using the electromagnetic flux-compression technique.[3]"
Test_particle,Physics,2,"In physical theories, a test particle, or test charge, is an idealized model of an object whose physical properties (usually mass, charge, or size) are assumed to be negligible except for the property being studied, which is  considered to be insufficient to alter the behavior of the rest of the system. The concept of a test particle often simplifies  problems, and can provide a good approximation for physical phenomena. In addition to its uses in the simplification of the dynamics of a system in particular limits, it is also used as a diagnostic in computer simulations of physical processes.
"
Theoretical_physics,Physics,2,"
 Theoretical physics is a branch of physics that employs mathematical models and abstractions of physical objects and systems to rationalize, explain and predict natural phenomena. This is in contrast to experimental physics, which uses experimental tools to probe these phenomena.
 The advancement of science generally depends on the interplay between experimental studies and theory. In some cases, theoretical physics adheres to standards of mathematical rigour while giving little weight to experiments and observations.[a] For example, while developing special relativity, Albert Einstein was concerned with the Lorentz transformation which left Maxwell's equations invariant, but was apparently uninterested in the Michelson–Morley experiment on Earth's drift through a luminiferous aether.[1] Conversely, Einstein was awarded the Nobel Prize for explaining the photoelectric effect, previously an experimental result lacking a theoretical formulation.[2]"
Theory_of_everything,Physics,2,"A theory of everything (TOE[1] or ToE), final theory, ultimate theory, or master theory is a hypothetical single, all-encompassing, coherent theoretical framework of physics that fully explains and links together all physical aspects of the universe.[2]:6 Finding a TOE is one of the major unsolved problems in physics.[3] String theory and M-theory have been proposed as theories of everything. Over the past few centuries, two theoretical frameworks have been developed that, together, most closely resemble a TOE. These two theories upon which all modern physics rests are general relativity and quantum mechanics. General relativity is a theoretical framework that only focuses on gravity for understanding the universe in regions of both large scale and high mass: stars, galaxies, clusters of galaxies, etc. On the other hand, quantum mechanics is a theoretical framework that only focuses on three non-gravitational forces for understanding the universe in regions of both small scale and low mass: sub-atomic particles, atoms, molecules, etc. Quantum mechanics successfully implemented the Standard Model that describes the three non-gravitational forces – strong nuclear, weak nuclear, and electromagnetic force – as well as all observed elementary particles.[4]:122 General relativity and quantum mechanics have been thoroughly proven in their separate fields of relevance. Since the usual domains of applicability of general relativity and quantum mechanics are so different, most situations require that only one of the two theories be used.[5][6]:842–844 However, the two theories are considered incompatible in regions of extremely small scale – the Planck scale – such as those that exist within a black hole or during the beginning stages of the universe (i.e., the moment immediately following the Big Bang). To resolve the incompatibility, a theoretical framework revealing a deeper underlying reality, unifying gravity with the other three interactions, must be discovered to harmoniously integrate the realms of general relativity and quantum mechanics into a seamless whole: the TOE is a single theory that, in principle, is capable of describing all phenomena in the universe. 
 In pursuit of this goal, quantum gravity has become one area of active research. One example is string theory, which evolved into a candidate for the TOE, but not without drawbacks (most notably, its lack of currently testable predictions) and controversy. String theory posits that at the beginning of the universe (up to 10−43 seconds after the Big Bang), the four fundamental forces were once a single fundamental force. According to string theory, every particle in the universe, at its most microscopic level (Planck length), consists of varying combinations of vibrating strings (or strands) with preferred patterns of vibration. String theory further claims that it is through these specific oscillatory patterns of strings that a particle of unique mass and force charge is created (that is to say, the electron is a type of string that vibrates one way, while the up quark is a type of string vibrating another way, and so forth).
"
Theory_of_relativity,Physics,2,"
 The theory of relativity usually encompasses two interrelated theories by Albert Einstein: special relativity and general relativity.[1] Special relativity applies to all physical phenomena in the absence of gravity. General relativity explains the law of gravitation and its relation to other forces of nature.[2] It applies to the cosmological and astrophysical realm, including astronomy.[3] The theory transformed theoretical physics and astronomy during the 20th century, superseding a 200-year-old theory of mechanics created primarily by Isaac Newton.[3][4][5] It introduced concepts including spacetime as a unified entity of space and time, relativity of simultaneity, kinematic and gravitational time dilation, and length contraction. In the field of physics, relativity improved the science of elementary particles and their fundamental interactions, along with ushering in the nuclear age. With relativity, cosmology and astrophysics predicted extraordinary astronomical phenomena such as neutron stars, black holes, and gravitational waves.[3][4][5]"
Thermal_conduction,Physics,2,"Thermal conduction is the transfer of internal energy by microscopic collisions of particles and movement of electrons within a body. The colliding particles, which include molecules, atoms and electrons, transfer disorganized microscopic kinetic and potential energy, jointly known as internal energy. Conduction takes place in all phases: solid, liquid, and gas. The rate at which energy is conducted as the heat between two bodies depends on the temperature difference (and hence temperature gradient) between the two bodies and the properties of the conductive interface through which the heat is transferred. 
 Heat spontaneously flows from a hotter to a colder body. For example, heat is conducted from the hotplate of an electric stove to the bottom of a saucepan in contact with it. In the absence of an opposing external driving energy source, within a body or between bodies, temperature differences decay over time, and thermal equilibrium is approached, temperature becoming more uniform.
 In conduction, the heat flow is within and through the body itself. In contrast, in heat transfer by thermal radiation, the transfer is often between bodies, which may be separated spatially. Also possible is the transfer of heat by a combination of conduction and thermal radiation. In convection, the internal energy is carried between bodies by a moving material carrier. In solids, conduction is mediated by the combination of vibrations and collisions of molecules, of propagation and collisions of phonons, and of diffusion and collisions of free electrons. In gases and liquids, conduction is due to the collisions and diffusion of molecules during their random motion. Photons in this context do not collide with one another, and so heat transport by electromagnetic radiation is conceptually distinct from heat conduction by microscopic diffusion and collisions of material particles and phonons. But the distinction is often not easily observed unless the material is semi-transparent.
 In the engineering sciences, heat transfer includes the processes of thermal radiation, convection, and sometimes mass transfer. Usually, more than one of these processes occurs in a given situation.
 The conventional symbol for thermal conductivity is k.
"
Thermal_equilibrium,Physics,2,"
 Two physical systems are in thermal equilibrium if there is no net flow of thermal energy between them when they are connected by a path permeable to heat.  Thermal equilibrium obeys the zeroth law of thermodynamics.  A system is said to be in thermal equilibrium with itself if the temperature within the system is spatially uniform and temporally constant.
 Systems in thermodynamic equilibrium are always in thermal equilibrium, but the converse is not always true.  If the connection between the systems allows transfer of energy as heat but does not allow transfer of matter or transfer of energy as work, the two systems may reach thermal equilibrium without reaching thermodynamic equilibrium.
"
Thermal_radiation,Physics,2,"
 Thermal radiation is electromagnetic radiation generated by the thermal motion of particles in matter. All matter with a temperature greater than absolute zero emits thermal radiation. Particle motion results in charge-acceleration or dipole oscillation which produces electromagnetic radiation.
 Infrared radiation emitted by animals (detectable with an infrared camera) and cosmic microwave background radiation are examples of thermal radiation.
 If a radiation object meets the physical characteristics of a black body in thermodynamic equilibrium, the radiation is called blackbody radiation.[1] Planck's law describes the spectrum of blackbody radiation, which depends solely on the object's temperature. Wien's displacement law determines the most likely frequency of the emitted radiation, and the Stefan–Boltzmann law gives the radiant intensity.[2] Thermal radiation is also one of the fundamental mechanisms of heat transfer.
"
Thermionic_emission,Physics,2,"Thermionic emission is the liberation of electrons from an electrode by virtue of its temperature (releasing of energy supplied by heat). This occurs because the thermal energy given to the charge carrier overcomes the work function of the material. The charge carriers can be electrons or ions, and in older literature are sometimes referred to as thermions. After emission, a charge that is equal in magnitude and opposite in sign to the total charge emitted is initially left behind in the emitting region. But if the emitter is connected to a battery, the charge left behind is neutralized by charge supplied by the battery as the emitted charge carriers move away from the emitter, and finally the emitter will be in the same state as it was before emission.
 The classical example of thermionic emission is that of electrons from a hot cathode into a vacuum (also known as thermal electron emission or the Edison effect) in a vacuum tube. The hot cathode can be a metal filament, a coated metal filament, or a separate structure of metal or carbides or borides of transition metals. Vacuum emission from metals tends to become significant only for temperatures over 1,000 K (730 °C; 1,340 °F).
 The term 'thermionic emission' is now also used to refer to any thermally-excited charge emission process, even when the charge is emitted from one solid-state region into another. This process is crucially important in the operation of a variety of electronic devices and can be used for electricity generation (such as thermionic converters and electrodynamic tethers) or cooling. The magnitude of the charge flow increases dramatically with increasing temperature.
"
Thermodynamic_equilibrium,Physics,2,"Thermodynamic equilibrium is an axiomatic concept of thermodynamics. It is an internal state of a single thermodynamic system, or a relation between several thermodynamic systems connected by more or less permeable or impermeable walls. In thermodynamic equilibrium there are no net macroscopic flows of matter or of energy, either within a system or between systems. 
 In a system that is in its own state of internal thermodynamic equilibrium, no macroscopic change occurs. 
 Systems in mutual thermodynamic equilibrium are simultaneously in mutual thermal, mechanical, chemical, and radiative equilibria. Systems can be in one kind of mutual equilibrium, though not in others. In thermodynamic equilibrium, all kinds of equilibrium hold at once and indefinitely, until disturbed by a thermodynamic operation. In a macroscopic equilibrium, perfectly or almost perfectly balanced microscopic exchanges occur; this is the physical explanation of the notion of macroscopic equilibrium. 
 A thermodynamic system in a state of internal thermodynamic equilibrium has a spatially uniform temperature. Its intensive properties, other than temperature, may be driven to spatial inhomogeneity by an unchanging long-range force field imposed on it by its surroundings.
 In systems that are at a state of non-equilibrium there are, by contrast, net flows of matter or energy. If such changes can be triggered to occur in a system in which they are not already occurring, the system is said to be in a meta-stable equilibrium.
 Though not a widely named ""law,"" it is an axiom of thermodynamics that there exist states of thermodynamic equilibrium. The second law of thermodynamics states that when a body of material starts from an equilibrium state, in which, portions of it are held at different states by more or less permeable or impermeable partitions, and a thermodynamic operation removes or makes the partitions more permeable and it is isolated, then it spontaneously reaches its own, new state of internal thermodynamic equilibrium, and this is accompanied by an increase in the sum of the entropies of the portions.
"
Thermodynamic_free_energy,Physics,2,"The thermodynamic free energy is a concept useful in the thermodynamics of chemical or thermal processes in engineering and science. The change in the free energy is the maximum amount of work that a thermodynamic system can perform in a process at constant temperature, and its sign indicates whether a process is thermodynamically favorable or forbidden. Since free energy usually contains potential energy, it is not absolute but depends on the choice of a zero point. Therefore, only relative free energy values, or changes in free energy, are physically meaningful.
 The free energy is a thermodynamic state function, like the internal energy, enthalpy, and entropy.
"
Thermodynamics,Physics,2,"
 Thermodynamics is a branch of physics that deals with heat, work, and temperature, and their relation to energy, radiation, and physical properties of matter. The behavior of these quantities is governed by the four laws of thermodynamics which convey a quantitative description using measurable macroscopic physical quantities, but may be explained in terms of microscopic constituents by statistical mechanics. Thermodynamics applies to a wide variety of topics in science and engineering, especially physical chemistry, chemical engineering and mechanical engineering, but also in other complex fields such as meteorology.
 Historically, thermodynamics developed out of a desire to increase the efficiency of early steam engines, particularly through the work of French physicist Nicolas Léonard Sadi Carnot (1824) who believed that engine efficiency was the key that could help France win the Napoleonic Wars.[1] Scots-Irish physicist Lord Kelvin was the first to formulate a concise definition of thermodynamics in 1854[2] which stated, ""Thermo-dynamics is the subject of the relation of heat to forces acting between contiguous parts of bodies, and the relation of heat to electrical agency.""
 The initial application of thermodynamics to mechanical heat engines was quickly extended to the study of chemical compounds and chemical reactions. Chemical thermodynamics studies the nature of the role of entropy in the process of chemical reactions and has provided the bulk of expansion and knowledge of the field.[3][4][5][6][7][8][9][10][11] Other formulations of thermodynamics emerged. Statistical thermodynamics, or statistical mechanics, concerns itself with statistical predictions of the collective motion of particles from their microscopic behavior. In 1909, Constantin Carathéodory presented a purely mathematical approach in an axiomatic formulation, a description often referred to as geometrical thermodynamics.
"
Thermometer,Physics,2,"A thermometer is a device that measures temperature or a temperature gradient (the degree of hotness or coldness of an object). A thermometer has two important elements: (1) a temperature sensor (e.g. the bulb of a mercury-in-glass thermometer or the pyrometric sensor in an infrared thermometer) in which some change occurs with a change in temperature; and (2) some means of converting this change into a numerical value (e.g. the visible scale that is marked on a mercury-in-glass thermometer or the digital readout on an infrared model). Thermometers are widely used in technology and industry to monitor processes, in meteorology, in medicine, and in scientific research.
 Some of the principles of the thermometer were known to Greek philosophers of two thousand years ago. As Henry Carrington Bolton (1900) noted, the thermometer's ""development from a crude toy to an instrument of precision occupied more than a century, and its early history is encumbered with erroneous statements that have been reiterated with such dogmatism that they have received the false stamp of authority.""[2] The Italian physician Santorio Santorio (Sanctorius, 1561-1636)[3] is commonly credited with the invention of the first thermometer, but its standardisation was completed through the 17th and 18th centuries.[4][5][6] In the first decades of the 18th century in the Dutch Republic, Daniel Gabriel Fahrenheit[7] made two revolutionary breakthroughs in the history of thermometry. He invented the mercury-in-glass thermometer (first widely used, accurate, practical thermometer)[2][1] and Fahrenheit scale (first standardized temperature scale to be widely used).[2]"
Third_law_of_thermodynamics,Physics,2,"
The third law of thermodynamics states as follows, regarding the properties of closed systems in thermodynamic equilibrium: .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0} The entropy of a system approaches a constant value as its temperature approaches absolute zero.  This constant value cannot depend on any other parameters characterizing the closed system, such as pressure or applied magnetic field. At absolute zero (zero kelvins) the system must be in a state with the minimum possible energy. Entropy is related to the number of accessible microstates, and there is typically one unique state (called the ground state) with minimum energy.[1] In such a case, the entropy at absolute zero will be exactly zero. If the system does not have a well-defined order (if its order is glassy, for example), then there may remain some finite entropy as the system is brought to very low temperatures, either because the system becomes locked into a configuration with non-minimal energy or because the minimum energy state is non-unique. The constant value is called the residual entropy of the system.[2] The entropy is essentially a state-function meaning the inherent value of different atoms, molecules, and other configurations of particles including subatomic or atomic material is defined by entropy, which can be discovered near 0 K.
The Nernst–Simon statement of the third law of thermodynamics concerns thermodynamic processes at a fixed, low temperature:  The entropy change associated with any condensed system undergoing a reversible isothermal process approaches zero as the temperature at which it is performed approaches 0 K.  Here a condensed system refers to liquids and solids.
A classical formulation by Nernst (actually a consequence of the Third Law) is:  It is impossible for any process, no matter how idealized, to reduce the entropy of a system to its absolute-zero value in a finite number of operations.[3] 
There also exists a formulation of the Third Law which approaches the subject by postulating a specific energy behavior:  If the composite of two thermodynamic systems constitutes an isolated system, then any energy exchange in any form between those two systems is bounded.[4]"
Torque,Physics,2,"In physics and mechanics, torque is the rotational equivalent of linear force.[1] It is also referred to as the moment, moment of force, rotational force or turning effect, depending on the field of study.  The concept originated with the studies by Archimedes of the usage of levers. Just as a linear force is a push or a pull, a torque can be thought of as a twist to an object around a specific axis. Another definition of torque is the product of the magnitude of the force and the perpendicular distance of the line of action of a force from the axis of rotation. The symbol for torque is typically 




τ



{  {\boldsymbol {\tau }}}
, the lowercase Greek letter tau. When being referred to as moment of force, it is commonly denoted by M.
 In three dimensions, the torque is a pseudovector; for point particles, it is given by the cross product of the position vector (distance vector) and the force vector. The magnitude of torque of a rigid body depends on three quantities: the force applied, the lever arm vector[2] connecting the point about which the torque is being measured to the point of force application, and the angle between the force and lever arm vectors. In symbols:
 where
 The SI unit for torque is the Newton-metre (N⋅m). For more on the units of torque, see Units.
"
Total_internal_reflection,Physics,2,"Total internal reflection (TIR) is the optical phenomenon in which the surface of the water in a fish-tank (for example) when viewed from below the water level, reflects the underwater scene like a mirror, with no loss of brightness (Fig. 1). In general, TIR occurs when waves in one medium reach the boundary with another medium at a sufficiently slanting angle, provided that the second (""external"") medium is transparent to the waves and allows them to travel faster than in the first (""internal"") medium. TIR occurs not only with electromagnetic waves such as light and microwaves, but also with other types of waves, including sound and water waves. In the case of a narrow train of waves, such as a laser beam (Fig. 2), we tend to describe the reflection in terms of ""rays"" rather than waves. In a medium whose properties are independent of direction, such as air, water, or glass, each ""ray"" is perpendicular to the associated wavefronts.[importance?] Refraction is generally accompanied by partial reflection. When waves are refracted from a medium of lower propagation speed to a medium of higher propagation speed (e.g., from water to air), the angle of refraction (between the refracted ray and the line perpendicular to the refracting surface) is greater than the angle of incidence (between the incident ray and the perpendicular). As the angle of incidence approaches a certain limit, called the critical angle, the angle of refraction approaches 90°, at which the refracted ray becomes parallel to the surface. As the angle of incidence increases beyond the critical angle, the conditions of refraction can no longer be satisfied; so there is no refracted ray, and the partial reflection becomes total. For visible light, the critical angle is about 49° for incidence at the water-to-air boundary, and about 42° for incidence at the common glass-to-air boundary.
 Details of the mechanism of TIR give rise to more subtle phenomena. While total reflection, by definition, involves no continuing flow of power across the interface between the two media, the external medium carries a so-called evanescent wave, which travels along the interface with an amplitude that falls off exponentially with distance from the interface. The ""total"" reflection is indeed total if the external medium is lossless (perfectly transparent), continuous, and of infinite extent, but can be conspicuously less than total if the evanescent wave is absorbed by a lossy external medium (""attenuated total reflectance""), or diverted by the outer boundary of the external medium or by objects embedded in that medium (""frustrated"" TIR). Unlike partial reflection between transparent media, total internal reflection is accompanied by a non-trivial phase shift (not just zero or 180°) for each component of polarization (perpendicular or parallel to the plane of incidence), and the shifts vary with the angle of incidence. The explanation of this effect by Augustin-Jean Fresnel, in 1823, added to the evidence in favor of the wave theory of light.
 The phase shifts are utilized by Fresnel's invention, the Fresnel rhomb, to modify polarization. The efficiency of the reflection is exploited by optical fibers (used in telecommunications cables and in image-forming fiberscopes), and by reflective prisms, such as erecting prisms for binoculars.
"
Toughness,Physics,2,"
 In materials science and metallurgy, toughness is the ability of a material to absorb energy and plastically deform without fracturing.[1] One definition of material toughness is the amount of energy per unit volume that a material can absorb before rupturing. This measure of toughness is different from that used for fracture toughness, which describes load bearing capabilities of materials with flaws.[2] It is also defined as a material's resistance to fracture when stressed.
 Toughness requires a balance of strength and ductility.[1]"
Trajectory,Physics,2,"A trajectory or flight path is the path that an object with mass in motion follows through space as a function of time. In classical mechanics, a trajectory is defined by Hamiltonian mechanics via canonical coordinates; hence, a complete trajectory is defined by position and momentum, simultaneously.
 The mass might be a projectile or a satellite.[1] For example, it can be an orbit — the path of a planet, asteroid, or comet as it travels around a central mass.
 In control theory, a trajectory is a time-ordered set of states of a dynamical system (see e.g. Poincaré map). In discrete mathematics, a trajectory is a sequence 



(

f

k


(
x
)

)

k
∈

N





{  (f^{k}(x))_{k\in \mathbb {N} }}
 of values calculated by the iterated application of a mapping 



f


{  f}
 to an element 



x


{  x}
 of its source.
"
Transducer,Physics,2,"A transducer is a device that converts energy from one form to another. Usually a transducer converts a signal in one form of energy to a signal in another.[1] Transducers are often employed at the boundaries of automation, measurement, and control systems, where electrical signals are converted to and from other physical quantities (energy, force, torque, light, motion, position, etc.). The process of converting one form of energy to another is known as transduction.[2]"
Transmission_medium,Physics,2,"A transmission medium is something that can mediate the propagation of signals for the purposes of telecommunication.
 Signals are typically imposed on a wave of some kind suitable for the chosen medium. For example, data can modulate sound and a transmission medium for sounds may be air, but solids and liquids may also act as the transmission medium. Vacuum or air constitutes a good transmission medium for electromagnetic waves such as light and radio waves. While material substance is not required for electromagnetic waves to propagate, such waves are usually affected by the transmission media they pass through, for instance by absorption or by reflection or refraction at the interfaces between media. Technical devices can therefore be employed to transmit or guide waves. Thus, an optical fiber or a copper cable are used as transmission media.
 Electromagnetic radiation can be transmitted through an optical medium, such as optical fiber, or through twisted pair wires, coaxial cable, or dielectric-slab waveguides.  It may also pass through any physical material that is transparent to the specific wavelength, such as water, air, glass, or concrete. Sound is, by definition, the vibration of matter, so it requires a physical medium for transmission, as do other kinds of mechanical waves and heat energy. Historically, science incorporated various aether theories to explain the transmission medium. However, it is now known that electromagnetic waves do not require a physical transmission medium, and so can travel through the ""vacuum"" of free space. Regions of the insulative vacuum can become conductive for electrical conduction through the presence of free electrons, holes, or ions.
"
Transverse_wave,Physics,2,"In physics, a transverse wave is a  wave that vibrates perpendicular to the direction of the wave or path of propagation.
 A simple example is given by the waves that can be created on a horizontal length of string by anchoring one end and moving the other end up and down.
 Another example is the waves that are created on the membrane of a drum.  The waves propagate in directions that are parallel to the membrane plane, but the membrane itself gets displaced up and down, perpendicular to that plane.
 Light is another example of a transverse wave, where the oscillations are the electric and magnetic fields, which point at right angles to the ideal light rays that describe the direction of propagation.
 Transverse waves commonly occur in elastic solids; the oscillations in this case are the displacement of the solid particles away from their relaxed position, in directions perpendicular to the propagation of the wave.  Since those displacements correspond to a local shear deformation of the material, a transverse wave of this nature is called a shear wave.  In seismology, shear waves are also called secondary waves or S-waves.
 Transverse waves are contrasted with longitudinal waves, where the oscillations occur in the direction of the wave.  The standard example of a longitudinal wave is a sound wave or ""pressure wave"" in gases, liquids, or solids, whose oscillations cause compression and expansion of the material through which the wave is propagating.  Pressure waves are called ""primary waves"", or ""P-waves"" in geophysics.
"
Trigonometry,Physics,2,"
 Trigonometry (from Greek trigōnon, ""triangle"" and metron, ""measure""[1]) is a branch of mathematics that studies relationships between side lengths and angles of triangles. The field emerged in the Hellenistic world during the 3rd century BC from applications of geometry to astronomical studies.[2] The Greeks focused on the calculation of chords, while mathematicians in India created the earliest-known tables of values for  trigonometric ratios (also called trigonometric functions) such as sine.[3] Throughout history, trigonometry has been applied in areas such as geodesy, surveying, celestial mechanics, and navigation.[4] Trigonometry is known for its many identities,[5][6] which are equations used for rewriting trigonometrical expressions to solve equations, to find a more useful expression, or to discover new relationships.[7]"
Trimean,Physics,2,"In statistics the trimean (TM), or Tukey's trimean, is a measure of a probability distribution's location defined as a weighted average of the distribution's median and its two quartiles:
 This is equivalent to the average of the median and the midhinge:
 The foundations of the trimean were part of Arthur Bowley's teachings, and later popularized by statistician John Tukey in his 1977 book[1] which has given its name to a set of techniques called exploratory data analysis.
 Like the median and the midhinge, but unlike the sample mean, it is a statistically resistant L-estimator with a breakdown point of 25%. This beneficial property has been described as follows:
 An advantage of the trimean as a measure of the center (of a distribution) is that it combines the median's emphasis on center values with the midhinge's attention to the extremes."
Triple_point,Physics,2,"In thermodynamics, the triple point of a substance is the temperature and pressure at which the three phases (gas, liquid, and solid) of that substance coexist in thermodynamic equilibrium.[1] It is that temperature and pressure at which the sublimation curve, fusion curve and the vaporisation curve meet. For example, the triple point of mercury occurs at a temperature of −38.83440 °C (−37.90192 °F) and a pressure of 0.2 mPa.
 In addition to the triple point for solid, liquid, and gas phases, a triple point may involve more than one solid phase, for substances with multiple polymorphs. Helium-4 is a special case that presents a triple point involving two different fluid phases (lambda point).[1] The triple point of water was used to define the kelvin, the base unit of thermodynamic temperature in the International System of Units (SI).[2] The value of the triple point of water was fixed by definition, rather than measured, but that changed with the 2019 redefinition of SI base units. The triple points of several substances are used to define points in the ITS-90 international temperature scale, ranging from the triple point of hydrogen (13.8033 K) to the triple point of water (273.16 K, 0.01 °C, or 32.018 °F).
 The term ""triple point"" was coined in 1873 by James Thomson, brother of Lord Kelvin.[3]"
Truncated_mean,Physics,2,"A truncated mean or trimmed mean is a statistical measure of central tendency, much like the mean and median. It involves the calculation of the mean after discarding given parts of a probability distribution or sample at the high and low end, and typically discarding an equal amount of both. This number of points to be discarded is usually given as a percentage of the total number of points, but may also be given as a fixed number of points.
 For most statistical applications, 5 to 25 percent of the ends are discarded. For example, given a set of 8 points, trimming by 12.5% would discard the minimum and maximum value in the sample: the smallest and largest values, and would compute the mean of the remaining 6 points. The 25% trimmed mean (when the lowest 25% and the highest 25% are discarded) is known as the interquartile mean. 
 The median can be regarded as a fully truncated mean and is most robust.  As with other trimmed estimators, the main advantage of the trimmed mean is robustness and higher efficiency for mixed distributions and heavy-tailed distribution (like the Cauchy distribution), at the cost of lower efficiency for some other less heavily-tailed distributions (such as the normal distribution). For intermediate distributions the differences between the efficiency of the mean and the median are not very big, e.g. for the student-t distribution with 2 degrees of freedom the variances for mean and median are nearly equal.
"
Uncertainty_principle,Physics,2,"
 In quantum mechanics, the uncertainty principle (also known as Heisenberg's uncertainty principle) is any of a variety of mathematical inequalities[1] asserting a fundamental limit to the accuracy with which the values for certain pairs of physical quantities of a particle, such as position, x, and momentum, p, can be predicted from initial conditions. 
 Such variable pairs are known as complementary variables or canonically conjugate variables; and, depending on interpretation, the uncertainty principle limits to what extent such conjugate properties maintain their approximate meaning, as the mathematical framework of quantum physics does not support the notion of simultaneously well-defined conjugate properties expressed by a single value. The uncertainty principle implies that it is in general not possible to predict the value of a quantity with arbitrary certainty, even if all initial conditions are specified.
 Introduced first in 1927 by the German physicist Werner Heisenberg, the uncertainty principle states that the more precisely the position of some particle is determined, the less precisely its momentum can be predicted from initial conditions, and vice versa.[2] The formal inequality relating the standard deviation of position σx and the standard deviation of momentum σp was derived by Earle Hesse Kennard[3] later that year and by Hermann Weyl[4] in 1928:
 




σ

x



σ

p


≥


ℏ
2


 
 


{  \sigma _{x}\sigma _{p}\geq {\frac {\hbar }{2}}~~}

 where ħ is the reduced Planck constant, h/(2π).
 Historically, the uncertainty principle has been confused[5][6] with a related effect in physics, called the observer effect, which notes that measurements of certain systems cannot be made without affecting the system, that is, without changing something in a system. Heisenberg utilized such an observer effect at the quantum level (see below) as a physical ""explanation"" of quantum uncertainty.[7] It has since become clearer, however, that the uncertainty principle is inherent in the properties of all wave-like systems,[8] and that it arises in quantum mechanics simply due to the matter wave nature of all quantum objects. Thus, the uncertainty principle actually states a fundamental property of quantum systems and is not a statement about the observational success of current technology.[9] It must be emphasized that measurement does not mean only a process in which a physicist-observer takes part, but rather any interaction between classical and quantum objects regardless of any observer.[10][note 1] [note 2] Since the uncertainty principle is such a basic result in quantum mechanics, typical experiments in quantum mechanics routinely observe aspects of it. Certain experiments, however, may deliberately test a particular form of the uncertainty principle as part of their main research program. These include, for example, tests of number–phase uncertainty relations in superconducting[12] or quantum optics[13] systems. Applications dependent on the uncertainty principle for their operation include extremely low-noise technology such as that required in gravitational wave interferometers.[14]"
Uniform_motion,Physics,2,"
 Newton's laws of motion are three physical laws that, together, laid the foundation for classical mechanics. They describe the relationship between a body and the forces acting upon it, and its motion in response to those forces. More precisely, the first law defines the force qualitatively, the second law offers a quantitative measure of the force, and the third asserts that a single isolated force does not exist. These three laws have been expressed in several ways, over nearly three centuries,[a] and can be summarised as follows:
 The three laws of motion were first compiled by Isaac Newton in his Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), first published in 1687.[4] Newton used them to explain and investigate the motion of many physical objects and systems.[5] For example, in the third volume of the text, Newton showed that these laws of motion, combined with his law of universal gravitation, explained Kepler's laws of planetary motion.
 Some also describe a fourth law which states that forces add up like vectors, that is, that forces obey the principle of superposition.[6][7][8]"
Uniform_circular_motion,Physics,2,"In physics, circular motion is a movement of an object along the circumference of a circle or rotation along a circular path. It can be uniform, with constant angular rate of rotation and constant speed, or non-uniform with a changing rate of rotation. The rotation around a fixed axis of a three-dimensional body involves circular motion of its parts. The equations of motion describe the movement of the center of mass of a body.
 Examples of circular motion include: an artificial satellite orbiting the Earth at a constant height, a ceiling fan's blades rotating around a hub, a stone which is tied to a rope and is being swung in circles, a car turning through a curve in a race track, an electron moving perpendicular to a uniform magnetic field, and a gear turning inside a mechanism.
 Since the object's velocity vector is constantly changing direction, the moving object is undergoing acceleration by a centripetal force in the direction of the center of rotation.  Without this acceleration, the object would move in a straight line, according to Newton's laws of motion.
"
Unit_vector,Physics,2,"In mathematics, a unit vector in a normed vector space is a vector (often a spatial vector) of length 1. A unit vector is often denoted by a lowercase letter with a circumflex, or ""hat"", as in 







v

^





{  {\hat {\mathbf {v} }}}
 (pronounced ""v-hat"").[1][2] The term direction vector is used to describe a unit vector being used to represent spatial direction, and such quantities are commonly denoted as d; 2D spatial directions represented this way are numerically equivalent to points on the unit circle.
The same construct is used to specify spatial directions in 3D, which are equivalent to a point on the unit sphere.
 The normalized vector û of a non-zero vector u is the unit vector in the direction of u, i.e.,
 where |u| is the norm (or length) of u.[3][4] The term normalized vector is sometimes used as a synonym for unit vector.
 Unit vectors are often chosen to form the basis of a vector space, and every vector in the space may be written as a linear combination of unit vectors.
 By definition, the dot product of two unit vectors in a Euclidean space is a scalar value amounting to the cosine of the smaller subtended angle. In three-dimensional Euclidean space, the cross product of two arbitrary unit vectors is a third vector orthogonal to both of them, whose length is equal to the sine of the smaller subtended angle. The normalized cross product corrects for this varying length, and yields the mutually orthogonal unit vector to the two inputs, applying the right-hand rule to resolve one of two possible directions.
"
Utility_frequency,Physics,2,"The utility frequency, (power) line frequency (American English) or mains frequency (British English) is the nominal frequency of the oscillations of alternating current (AC) in a wide area synchronous grid transmitted from a power station to the end-user. In large parts of the world this is 50 Hz, although in the Americas and parts of Asia it is typically 60 Hz. Current usage by country or region is given in the list of mains electricity by country.
 During the development of commercial electric power systems in the late-19th and early-20th centuries, many different frequencies (and voltages) had been used. Large investment in equipment at one frequency made standardization a slow process. However, as of the turn of the 21st century, places that now use the 50 Hz frequency tend to use 220–240 V, and those that now use 60 Hz tend to use 100–127 V. Both frequencies coexist today (Japan uses both) with no great technical reason to prefer one over the other[1] and no apparent desire for complete worldwide standardization.
 In practice, the exact frequency of the grid varies around the nominal frequency, reducing when the grid is heavily loaded, and speeding up when lightly loaded. However, most utilities will adjust the frequency of the grid over the course of the day to ensure a constant number of cycles occur.[2] This is used by some clocks to accurately maintain their time.
"
Vacuum,Physics,2,"A vacuum is space devoid of matter. The word stems from the Latin adjective vacuus for ""vacant"" or ""void"".  An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure.[1]  Physicists often discuss ideal test results that would occur in a perfect vacuum, which they sometimes simply call ""vacuum"" or free space, and use the term partial vacuum to refer to an actual imperfect vacuum as one might have in a laboratory or in space. In engineering and applied physics on the other hand, vacuum refers to any space in which the pressure is considerably lower than atmospheric pressure.[2] The Latin term in vacuo is used to describe an object that is surrounded by a vacuum.
 The quality of a partial vacuum refers to how closely it approaches a perfect vacuum. Other things equal, lower gas pressure means higher-quality vacuum. For example, a typical vacuum cleaner produces enough suction to reduce air pressure by around 20%.[3] But higher-quality vacuums are possible. Ultra-high vacuum chambers, common in chemistry, physics, and engineering, operate below one trillionth (10−12) of atmospheric pressure (100 nPa), and can reach around 100 particles/cm3.[4] Outer space is an even higher-quality vacuum, with the equivalent of just a few hydrogen atoms per cubic meter on average in intergalactic space.[5] Vacuum has been a frequent topic of philosophical debate since ancient Greek times, but was not studied empirically until the 17th century. Evangelista Torricelli produced the first laboratory vacuum in 1643, and other experimental techniques were developed as a result of his theories of atmospheric pressure. A Torricellian vacuum is created by filling a tall glass container closed at one end with mercury, and then inverting it in a bowl to contain the mercury (see below).[6] Vacuum became a valuable industrial tool in the 20th century with the introduction of incandescent light bulbs and vacuum tubes, and a wide array of vacuum technologies has since become available. The recent development of human spaceflight has raised interest in the impact of vacuum on human health, and on life forms in general.
"
Valence_electron,Physics,2,"In chemistry and physics, a valence electron is an outer shell electron that is associated with an atom, and that can participate in the formation of a chemical bond if the outer shell is not closed; in a single covalent bond, both atoms in the bond contribute one valence electron in order to form a shared pair.
 The presence of valence electrons can determine the element's chemical properties, such as its valence—whether it may bond with other elements and, if so, how readily and with how many. In this way, a given element's reactivity is highly dependent upon its electronic configuration. For a main group element, a valence electron can exist only in the outermost electron shell; for a transition metal, a valence electron can also be in an inner shell. 
 An atom with a closed shell of valence electrons (corresponding to an electron configuration s2p6 for main group elements or d10s2p6 for transition metals) tends to be chemically inert. Atoms with one or two valence electrons more than a closed shell are highly reactive due to the relatively low energy to remove the extra valence electrons to form a positive ion. An atom with one or two electrons less than a closed shell is reactive due to its tendency either to gain the missing valence electrons and form a negative ion, or else to share valence electrons and form a covalent bond.
 Similar to a core electron, a valence electron has the ability to absorb or release energy in the form of a photon. An energy gain can trigger the electron to move (jump) to an outer shell; this is known as atomic excitation. Or the electron can even break free from its associated atom's shell; this is ionization to form a positive ion. When an electron loses energy (thereby causing a photon to be emitted), then it can move to an inner shell which is not fully occupied.
"
Valence_shell,Physics,2,"In chemistry and physics, a valence electron is an outer shell electron that is associated with an atom, and that can participate in the formation of a chemical bond if the outer shell is not closed; in a single covalent bond, both atoms in the bond contribute one valence electron in order to form a shared pair.
 The presence of valence electrons can determine the element's chemical properties, such as its valence—whether it may bond with other elements and, if so, how readily and with how many. In this way, a given element's reactivity is highly dependent upon its electronic configuration. For a main group element, a valence electron can exist only in the outermost electron shell; for a transition metal, a valence electron can also be in an inner shell. 
 An atom with a closed shell of valence electrons (corresponding to an electron configuration s2p6 for main group elements or d10s2p6 for transition metals) tends to be chemically inert. Atoms with one or two valence electrons more than a closed shell are highly reactive due to the relatively low energy to remove the extra valence electrons to form a positive ion. An atom with one or two electrons less than a closed shell is reactive due to its tendency either to gain the missing valence electrons and form a negative ion, or else to share valence electrons and form a covalent bond.
 Similar to a core electron, a valence electron has the ability to absorb or release energy in the form of a photon. An energy gain can trigger the electron to move (jump) to an outer shell; this is known as atomic excitation. Or the electron can even break free from its associated atom's shell; this is ionization to form a positive ion. When an electron loses energy (thereby causing a photon to be emitted), then it can move to an inner shell which is not fully occupied.
"
Valley_of_stability,Physics,2,"In nuclear physics, the valley of stability (also called the belt of stability, nuclear valley, energy valley, or beta stability valley) is a characterization of the stability of nuclides to radioactivity based on their binding energy.[1]   Nuclides are composed of protons and neutrons.  The shape of the valley refers to the profile of binding energy as a function of the numbers of neutrons and protons, with the lowest part of the valley corresponding to the region of most stable nuclei.[2]  The line of stable nuclides down the center of the valley of stability is known as the line of beta stability.  The sides of the valley correspond to increasing instability to beta decay (β− or β+). The decay of a nuclide becomes more energetically favorable the further it is from the line of beta stability.  The boundaries of the valley correspond to the nuclear drip lines, where nuclides become so unstable they emit single protons or single neutrons.  Regions of instability within the valley at high atomic number also include radioactive decay by alpha radiation or spontaneous fission.  The shape of the valley is roughly an elongated paraboloid corresponding to the nuclide binding energies as a function of neutron and atomic numbers.[1] The nuclides within the valley of stability encompass the entire table of nuclides.  The chart of those nuclides is known as a Segrè chart, after the physicist Emilio Segrè.[3]  The Segrè chart may be considered a map of the nuclear valley.  The region of proton and neutron combinations outside of the valley of stability is referred to as the sea of instability.[4][5] Scientists have long searched for long-lived heavy isotopes outside of the valley of stability,[6][7][8] hypothesized by Glenn T. Seaborg in the late 1960s.[9][10]  These relatively stable nuclides are expected to have particular configurations of ""magic"" atomic and neutron numbers, and form a so-called island of stability.
"
Van_de_Graaff_generator,Physics,2,"A Van de Graaff generator is an electrostatic generator which uses a moving belt to accumulate electric charge on a hollow metal globe on the top of an insulated column, creating very high electric potentials. It produces very high voltage direct current (DC) electricity at low current levels. It was invented by American physicist Robert J. Van de Graaff in 1929.[1] 
The potential difference achieved by modern Van de Graaff generators can be as much as 5 megavolts. A tabletop version can produce on the order of 100,000 volts and can store enough energy to produce a visible spark. Small Van de Graaff machines are produced for entertainment, and for physics education to teach electrostatics; larger ones are displayed in some science museums.
 The Van de Graaff generator was developed as a particle accelerator for physics research; its high potential is used to accelerate subatomic particles to great speeds in an evacuated tube. It was the most powerful type of accelerator of the 1930s until the cyclotron was developed. Van de Graaff generators are still used as accelerators to generate energetic particle and X-ray beams for nuclear research and nuclear medicine.  
 Particle-beam Van de Graaff accelerators are often used in a ""tandem"" configuration: first, negatively charged ions are injected at one end towards the high potential terminal, where they are accelerated by attractive force towards the terminal.  When the particles reach the terminal, they are stripped of some electrons to make them positively charged and are subsequently accelerated by repulsive forces away from the terminal. This configuration results in two accelerations for the cost of one Van de Graaff generator, and has the added advantage of leaving the complicated ion source instrumentation accessible near ground potential.  
 The voltage produced by an open-air Van de Graaff machine is limited by arcing and corona discharge to about 5 megavolts. Most modern industrial machines are enclosed in a pressurized tank of insulating gas; these can achieve potentials of as much as about 25 megavolts.
"
Variable_capacitor,Physics,2,"
 A variable capacitor  is a capacitor whose capacitance may be intentionally and repeatedly changed mechanically or electronically.  Variable capacitors are often used in L/C circuits to set the resonance frequency, e.g. to tune a radio (therefore it is sometimes called a tuning capacitor or tuning condenser), or as a variable reactance, e.g. for impedance matching in antenna tuners.
"
Variable_resistor,Physics,2,"A potentiometer is a three-terminal resistor with a sliding or rotating contact that forms an adjustable voltage divider.[1] If only two terminals are used, one end and the wiper, it acts as a variable resistor or rheostat.
 The measuring instrument called a potentiometer is essentially a voltage divider used for measuring electric potential (voltage); the component is an implementation of the same principle, hence its name.
 Potentiometers are commonly used to control electrical devices such as volume controls on audio equipment. Potentiometers operated by a mechanism can be used as position transducers, for example, in a joystick. Potentiometers are rarely used to directly control significant power (more than a watt), since the power dissipated in the potentiometer would be comparable to the power in the controlled load.
"
Vector_(mathematics_and_physics),Physics,2,"In mathematics and physics, a vector is an element of a vector space.
 For many specific vector spaces, the vectors have received specific names, which are listed below.
 Historically, vectors were introduced in geometry and physics (typically in mechanics) before the formalization of the concept of vector space. Therefore, one often talks about vectors without specifying the vector space to which they belong. Specifically, in a Euclidean space, one considers spatial vectors, also called Euclidean vectors which are used to represent quantities that have both magnitude and direction, and may be added, subtracted and scaled (i.e. multiplied by a real number) for forming a vector space.[1]"
Vector_space,Physics,2,"A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied (""scaled"") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms (listed below in § Definition). To specify that the scalars are real or complex numbers, the terms real vector space and complex vector space are often used.
 Certain sets of Euclidean vectors are common examples of a vector space. They represent physical quantities such as forces, where any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein (but in a more geometric sense), vectors representing displacements in the plane or three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.
 Vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis as function spaces, whose vectors are functions. These vector spaces are generally endowed with some additional structure such as a topology, which allows the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used (being equipped with a notion of distance between two vectors). This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.
 Historically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.
 Today, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.
"
Velocity,Physics,2,"
 The velocity of an object is the rate of change of its position with respect to a frame of reference, and is a function of time. Velocity is equivalent to a specification of an object's speed and direction of motion (e.g. 60 km/h to the north). Velocity is a fundamental concept in kinematics, the branch of classical mechanics that describes the motion of bodies.
 Velocity is a physical vector quantity; both magnitude and direction are needed to define it. The scalar absolute value (magnitude) of velocity is called speed, being a coherent derived unit whose quantity is measured in the SI (metric system) as metres per second (m/s) or as the SI base unit of (m⋅s−1). For example, ""5 metres per second"" is a scalar, whereas ""5 metres per second east"" is a vector. If there is a change in speed, direction or both, then the object has a changing velocity and is said to be undergoing an acceleration.
"
Virtual_image,Physics,2,"In optics, an image is defined as the collection of focus points of light rays coming from an object. A real image is the collection of focus points actually made by converging rays, while a virtual image is the collection of focus points made by extensions of diverging rays. In other words, a virtual image is found by tracing real rays that emerge from an optical device (lens, mirror, or some combination) backward to perceived or apparent origins of ray divergences. In diagrams of optical systems, virtual rays are conventionally represented by dotted lines.
 Because the rays never really converge, a virtual image cannot be projected onto a screen. In contrast, a real image can be projected on the screen as it is formed by rays that converge on a real location. A real image can be projected onto a diffusely reflecting screen so people can see the image (the image on the screen plays as an object to be imaged by human eyes)[1]"
Virtual_particle,Physics,2,"
 In physics, a virtual particle is a transient quantum fluctuation that exhibits some of the characteristics of an ordinary particle, while having its existence limited by the uncertainty principle. The concept of virtual particles arises in perturbation theory of quantum field theory where interactions between ordinary particles are described in terms of exchanges of virtual particles.  A process involving virtual particles can be described by a schematic representation known as a Feynman diagram, in which virtual particles are represented by internal lines.[1][2] Virtual particles do not necessarily carry the same mass as the corresponding real particle, although they always conserve energy and momentum. The longer the virtual particle exists, the closer its characteristics come to those of ordinary particles.  They are important in the physics of many processes, including particle scattering and Casimir forces.   In quantum field theory, forces—such as the electromagnetic repulsion or attraction between two charges—can be thought of as due to the exchange of virtual photons between the charges. Virtual photons are the exchange particle for the electromagnetic interaction.
 The term is somewhat loose and vaguely defined, in that it refers to the view that the world is made up of ""real particles"". It is not. ""Real particles"" are better understood to be excitations of the underlying quantum fields. Virtual particles are also excitations of the underlying fields, but are ""temporary"" in the sense that they appear in calculations of interactions, but never as asymptotic states or indices to the scattering matrix. The accuracy and use of virtual particles in calculations is firmly established, but as they cannot be detected in experiments, deciding how to precisely describe them is a topic of debate.[3]"
Viscoelasticity,Physics,2,"Viscoelasticity is the property of materials that exhibit both viscous and elastic characteristics when undergoing deformation. Viscous materials, like water, resist shear flow and strain linearly with time when a stress is applied. Elastic materials strain when stretched and immediately return to their original state once the stress is removed. 
 Viscoelastic materials have elements of both of these properties and, as such, exhibit time-dependent strain. Whereas elasticity is usually the result of bond stretching along crystallographic planes in an ordered solid, viscosity is the result of the diffusion of atoms or molecules inside an amorphous material.[1]"
Viscosity,Physics,2,"The viscosity of a fluid is a measure of its resistance to deformation at a given rate. For liquids, it corresponds to the informal concept of ""thickness"": for example, syrup has a higher viscosity than water.[1] Viscosity can be conceptualized as quantifying the internal frictional force that arises between adjacent layers of fluid that are in relative motion. For instance, when a fluid is forced through a tube, it flows more quickly near the tube's axis than near its walls. In such a case, experiments show that some stress (such as a pressure difference between the two ends of the tube) is needed to sustain the flow through the tube.  This is because a force is required to overcome the friction between the layers of the fluid which are in relative motion: the strength of this force is proportional to the viscosity.
 A fluid that has no resistance to shear stress is known as an ideal or inviscid fluid. Zero viscosity is observed only at very low temperatures in superfluids. Otherwise, the second law of thermodynamics requires all fluids to have positive viscosity;[2][3] such fluids are technically said to be viscous or viscid. A fluid with a high viscosity, such as pitch, may appear to be a solid.
"
Visible_light,Physics,2,"
 Light or visible light is electromagnetic radiation within the portion of the electromagnetic spectrum that can be perceived by the human eye.[1] Visible light is usually defined as having wavelengths in the range of 400–700 nanometers (nm), or 4.00 × 10−7 to 7.00 × 10−7 m, between the infrared (with longer wavelengths) and the ultraviolet (with shorter wavelengths).[2][3] This wavelength means a frequency range of roughly 430–750 terahertz (THz).
 The main source of light on Earth is the Sun. Sunlight provides the energy that green plants use to create sugars mostly in the form of starches, which release energy into the living things that digest them. This process of photosynthesis provides virtually all the energy used by living things. Historically, another important source of light for humans has been fire, from ancient campfires to modern kerosene lamps. With the development of electric lights and power systems, electric lighting has effectively replaced firelight. Some species of animals generate their own light, a process called bioluminescence. For example, fireflies use light to locate mates, and vampire squids use it to hide themselves from prey.
 The primary properties of visible light are intensity, propagation direction, frequency or wavelength spectrum, and polarization, while its speed in a vacuum, 299,792,458 meters per second, is one of the fundamental constants of nature. Visible light, as with all types of electromagnetic radiation (EMR), is experimentally found to always move at this speed in a vacuum.[4] In physics, the term light sometimes refers to electromagnetic radiation of any wavelength, whether visible or not.[5][6] In this sense, gamma rays, X-rays, microwaves and radio waves are also light. Like all types of EM radiation, visible light propagates as waves. However, the energy imparted by the waves is absorbed at single locations the way particles are absorbed. The absorbed energy of the EM waves is called a photon, and represents the quanta of light. When a wave of light is transformed and absorbed as a photon, the energy of the wave instantly collapses to a single location, and this location is where the photon ""arrives."" This is what is called the wave function collapse. This dual wave-like and particle-like nature of light is known as the wave–particle duality. The study of light, known as optics, is an important research area in modern physics.
"
Volt,Physics,2,"The volt (symbol: V) is the derived unit for electric potential, electric potential difference (voltage), and electromotive force.[1] It is named after the Italian physicist Alessandro Volta (1745–1827).
"
Volta_potential,Physics,2,"The Volta potential (also called Volta potential difference, contact potential difference, outer potential difference, Δψ, or ""delta psi"") in electrochemistry, is the electrostatic potential difference between two metals (or one metal and one electrolyte) that are in contact and are in thermodynamic equilibrium. Specifically, it is the potential difference between a point close to the surface of the first metal,  and a point close to the surface of the second metal (or electrolyte).[1] The Volta potential is named after Alessandro Volta.
"
Voltage,Physics,2,"Voltage, electric potential difference, electric pressure or electric tension is the difference in electric potential between two points, which (in a static electric field) is defined as the work needed per unit of charge to move a test charge between the two points. In the International System of Units, the derived unit for voltage (potential difference) is named volt.[1]:166 In SI units, work per unit charge is expressed as joules per coulomb, where 1 volt = 1 joule (of work) per 1 coulomb (of charge). The old SI definition for volt used power and current; starting in 1990, the quantum Hall and Josephson effect were used, and recently (2019) fundamental physical constants have been introduced for the definition of all SI units and derived units.[1]:177f, 197f Voltage or electric potential difference is denoted symbolically by ∆V, simplified V,[2] or U,[3] for instance in the context of Ohm's or Kirchhoff's circuit laws.
 Electric potential differences between points can be caused by electric charge, by electric current through a magnetic field, by time-varying magnetic fields, or some combination of these three.[4][5] A voltmeter can be used to measure the voltage (or potential difference) between two points in a system; often a common reference potential such as the ground of the system is used as one of the points. A voltage may represent either a source of energy (electromotive force) or lost, used, or stored energy (potential drop).
"
Voltmeter,Physics,2,"A voltmeter is an instrument used for measuring electric potential difference between two points in an electric circuit. It is  connected in parallel. It usually has a high resistance so that it takes a negligible current from the circuit.
 Analog voltmeters move a pointer across a scale in proportion to the voltage of the circuit; digital voltmeters give a numerical display of voltage by use of an analog-to-digital converter. 
Voltmeters are made in a wide range of styles. Instruments permanently mounted in a panel are used to monitor generators or other fixed apparatus. Portable instruments, usually equipped to also measure current and resistance in the form of a multimeter, are standard test instruments used in electrical and electronics work. Any measurement that can be converted to a voltage can be displayed on a meter that is suitably calibrated; for example, pressure, temperature, flow or level in a chemical process plant.
 General-purpose analog voltmeters may have an accuracy of a few percent of full scale, and are used with voltages from a fraction of a volt to several thousand volts. Digital meters can be made with high accuracy, typically better than 1%. Specially calibrated test instruments have higher accuracies, with laboratory instruments capable of measuring to accuracies of a few parts per million. Meters using amplifiers can measure tiny voltages of microvolts or less.
 Part of the problem of making an accurate voltmeter is that of calibration to check its accuracy. In laboratories, the Weston cell is used as a standard voltage for precision work. Precision voltage references are available based on electronic circuits.
"
Electric_field,Physics,2,"An electric field (sometimes E-field[1]) is the physical field that surrounds each electric charge and exerts force on all other charges in the field, either attracting or repelling them.[2][3] Electric fields originate from electric charges, or from time-varying magnetic fields. Electric fields and magnetic fields are both manifestations of the electromagnetic force, one of the four fundamental forces (or interactions) of nature.
 Electric fields are important in many areas of physics, and are exploited practically in electrical technology. In atomic physics and chemistry, for instance, the electric field is used to model the attractive force holding the atomic nucleus and electrons together in atoms. It also models the forces in chemical bonding between atoms that result in molecules.
 The electric field is defined mathematically as a vector field that associates to each point in space the (electrostatic or Coulomb) force per unit of charge exerted on an infinitesimal positive test charge at rest at that point.[4][5][6] The derived SI units for the electric field are volts per meter (V/m), exactly equivalent to newtons per coulomb (N/C).[7]"
Volume,Physics,2,"Volume is the quantity of three-dimensional space enclosed by a closed surface, for example, the space that a substance (solid, liquid, gas, or plasma) or shape occupies or contains.[1] Volume is often quantified numerically using the SI derived unit, the cubic metre. The volume of a container is generally understood to be the capacity of the container; i. e., the amount of fluid (gas or liquid) that the container could hold, rather than the amount of space the container itself displaces.
Three dimensional mathematical shapes are also assigned volumes. Volumes of some simple shapes, such as regular, straight-edged, and circular shapes can be easily calculated using arithmetic formulas. Volumes of complicated shapes can be calculated with integral calculus if a formula exists for the shape's boundary. One-dimensional figures (such as lines) and two-dimensional shapes (such as squares) are assigned zero volume in the three-dimensional space.
 The volume of a solid (whether regularly or irregularly shaped) can be determined by fluid displacement. Displacement of liquid can also be used to determine the volume of a gas. The combined volume of two substances is usually greater than the volume of just one of the substances. However, sometimes one substance dissolves in the other and in such cases the combined volume is not additive.[2] In differential geometry, volume is expressed by means of the volume form, and is an important global Riemannian invariant.
In thermodynamics, volume is a fundamental parameter, and is a conjugate variable to pressure.
"
W_and_Z_bosons,Physics,2,"The W and Z bosons are together known as the weak or more generally as the intermediate vector bosons. These elementary particles mediate the weak interaction; the respective symbols are W+, W−, and Z0. The W± bosons have either a positive or negative electric charge of 1 elementary charge and are each other's antiparticles. The Z0 boson is electrically neutral and is its own antiparticle. The three particles have a spin of 1. The W± bosons have a magnetic moment, but the Z0 has none. All three of these particles are very short-lived, with a half-life of about 3×10−25 s. Their experimental discovery was pivotal in establishing what is now called the Standard Model of particle physics.
 The W bosons are named after the weak force. The physicist Steven Weinberg named the additional particle the ""Z particle"",[3] and later gave the explanation that it was the last additional particle needed by the model. The W bosons had already been named, and the Z bosons were named for having zero electric charge.[4] The two W bosons are verified mediators of neutrino absorption and emission. During these processes, the W± boson charge induces electron or positron emission or absorption, thus causing nuclear transmutation.
 The Z boson mediates the transfer of momentum, spin and energy when neutrinos scatter elastically from matter (a process which conserves charge). Such behavior is almost as common as inelastic neutrino interactions and may be observed in bubble chambers upon irradiation with neutrino beams. The Z boson is not involved in the absorption or emission of electrons or positrons. Whenever an electron is observed as a new free particle, suddenly moving with kinetic energy, it is inferred to be a result of a neutrino interacting directly with the electron, since this behavior happens more often when the neutrino beam is present. In this process, the neutrino simply strikes the electron and then scatters away from it, transferring some of the neutrino's momentum to the electron.[a]"
Watt,Physics,2,"The watt (symbol: W) is a unit of power. In the International System of Units (SI) it is defined as a derived unit of 1 joule per second,[1] and is used to quantify the rate of energy transfer. In SI base units, the watt is described as kg⋅m2⋅s−3.[2] The watt is named after James Watt, an 18th-century Scottish inventor.
"
Wave,Physics,2,"In physics, mathematics, and related fields, a wave is a propagating dynamic disturbance (change from equilibrium) of one or more quantities, sometimes as described by a wave equation.  In physical waves, at least two field quantities in the wave medium are involved. Waves can be periodic, in which case those quantities oscillate repeatedly about an equilibrium (resting) value at some frequency. When the entire waveform moves in one direction it is said to be a traveling wave; by contrast, a pair of superimposed periodic waves traveling in opposite directions makes a standing wave. In a standing wave, the amplitude of vibration has nulls at some positions where the wave amplitude appears smaller or even zero.
 The types of waves most commonly studied in classical physics are mechanical and electromagnetic. In a mechanical wave, stress and strain fields oscillate about a mechanical equilibrium.  A mechanical wave is a local deformation (strain) in some physical medium that propagates from particle to particle by creating local stresses that cause strain in neighboring particles too.  For example, sound waves are variations of the local pressure and particle motion that propagate through the medium.  Other examples of mechanical waves are seismic waves, gravity waves, surface waves, string vibrations (standing waves), and vortices[dubious  – discuss]. In an electromagnetic wave (such as light) energy is interchanged between the electric and magnetic fields which sustains propagation of a wave involving these fields according to Maxwell's equations.  Electromagnetic waves can travel through a vacuum and through some dielectric media (at wavelengths where they are considered transparent). Electromagnetic waves, according to their frequencies (or wavelengths) have more specific designations including radio waves, infrared radiation, terahertz waves, visible light, ultraviolet radiation, X-rays and gamma rays.
 Other types of waves include gravitational waves, which are disturbances in spacetime that propagate according to general relativity; heat diffusion waves[dubious  – discuss]; plasma waves that combine mechanical deformations and electromagnetic fields; reaction-diffusion waves, such as in the Belousov–Zhabotinsky reaction; and many more.
 Mechanical and electromagnetic waves transfer energy,[2], momentum, and information, but they do not transfer particles in the medium.  In mathematics and electronics waves are studied as signals.[3]  On the other hand, some waves have envelopes which do not move at all such as standing waves (which are fundamental to music) and hydraulic jumps. Some, like the probability waves of quantum mechanics, may be completely static[dubious  – discuss].
 A physical wave is almost always confined to some finite region of space, called its domain.  For example, the seismic waves generated by earthquakes are significant only in the interior and surface of the planet, so they can be ignored outside it.  However, waves with infinite domain, that extend over the whole space, are commonly studied in mathematics, and are very valuable tools for understanding physical waves in finite domains.
 A plane wave is an important mathematical idealization where the disturbance is identical along any (infinite) plane normal to a specific direction of travel. Mathematically, the simplest wave is a sinusoidal plane wave in which at any point the field experiences simple harmonic motion at one frequency. In linear media, complicated waves can generally be decomposed as the sum of many sinusoidal plane waves having different directions of propagation and/or different frequencies.  A plane wave is classified as a transverse wave if the field disturbance at each point is described by a vector perpendicular to the direction of propagation (also the direction of energy transfer); or longitudinal if those vectors are exactly in the propagation direction. Mechanical waves include both transverse and longitudinal waves; on the other hand electromagnetic plane waves are strictly transverse while sound waves in fluids (such as air) can only be longitudinal. That physical direction of an oscillating field relative to the propagation direction is also referred to as the wave's polarization which can be an important attribute for waves having more than one single possible polarization.
"
Wave_equation,Physics,2,"
 The wave equation is an important second-order linear partial differential equation for the description of waves—as they occur in classical physics—such as mechanical waves (e.g. water waves, sound waves and seismic waves) or light waves. It arises in fields like acoustics, electromagnetics, and fluid dynamics.
 Historically, the problem of a vibrating string such as that of a musical instrument was studied by Jean le Rond d'Alembert, Leonhard Euler, Daniel Bernoulli, and  Joseph-Louis Lagrange.[1][2][3][4][5] In 1746, d’Alembert discovered the one-dimensional wave equation, and within ten years Euler discovered the three-dimensional wave equation.[6]"
Wave_function,Physics,2,"A wave function in quantum physics is a mathematical description of the quantum state of an isolated quantum system.  The wave function is a complex-valued probability amplitude, and the probabilities for the possible results of measurements made on the system can be derived from it.  The most common symbols for a wave function are the Greek letters ψ and Ψ (lower-case and capital psi, respectively).
 The wave function is a function of the degrees of freedom corresponding to some maximal set of commuting observables.  Once such a representation is chosen, the wave function can be derived from the quantum state.
 For a given system, the choice of which commuting degrees of freedom to use is not unique, and correspondingly the domain of the wave function is also not unique.  For instance, it may be taken to be a function of all the position coordinates of the particles over position space, or the momenta of all the particles over momentum space; the two are related by a Fourier transform.  Some particles, like electrons and photons, have nonzero spin, and the wave function for such particles includes spin as an intrinsic, discrete degree of freedom; other discrete variables can also be included, such as isospin.  When a system has internal degrees of freedom, the wave function at each point in the continuous degrees of freedom (e.g., a point in space) assigns a complex number for each possible value of the discrete degrees of freedom (e.g., z-component of spin) – these values are often displayed in a column matrix (e.g., a 2 × 1 column vector for a non-relativistic electron with spin ​1⁄2).
 According to the superposition principle of quantum mechanics, wave functions can be added together and multiplied by complex numbers to form new wave functions and form a Hilbert space.  The inner product between two wave functions is a measure of the overlap between the corresponding physical states, and is used in the foundational probabilistic interpretation of quantum mechanics, the Born rule, relating transition probabilities to inner products.  The Schrödinger equation determines how wave functions evolve over time, and a wave function behaves qualitatively like other waves, such as water waves or waves on a string, because the Schrödinger equation is mathematically a type of wave equation.  This explains the name ""wave function"", and gives rise to wave–particle duality.  However, the wave function in quantum mechanics describes a kind of physical phenomenon, still open to different interpretations, which fundamentally differs from that of classic mechanical waves.[1][2][3][4][5][6][7] In Born's statistical interpretation in non-relativistic quantum mechanics,[8][9][10]
the squared modulus of the wave function, |ψ|2, is a real number interpreted as the probability density of measuring a particle as being at a given place – or having a given momentum – at a given time, and possibly having definite values for discrete degrees of freedom.  The integral of this quantity, over all the system's degrees of freedom, must be 1 in accordance with the probability interpretation.  This general requirement that a wave function must satisfy is called the normalization condition.  Since the wave function is complex valued, only its relative phase and relative magnitude can be measured—its value does not, in isolation, tell anything about the magnitudes or directions of measurable observables; one has to apply quantum operators, whose eigenvalues correspond to sets of possible results of measurements, to the wave function ψ and calculate the statistical distributions for measurable quantities.
"
Wave_function_collapse,Physics,2,"
 In quantum mechanics, wave function collapse occurs when a wave function—initially in a superposition of several eigenstates—reduces to a single eigenstate due to interaction with the external world. This interaction is called an ""observation"".  It is the essence of a measurement in quantum mechanics which connects the wave function with classical observables like position and momentum. Collapse is one of two processes by which quantum systems evolve in time; the other is the continuous evolution via the Schrödinger equation.[1] Collapse is a black box for a thermodynamically irreversible interaction with a classical environment.[2][3] Calculations of quantum decoherence show that when a quantum system interacts with the environment, the superpositions apparently reduce to mixtures of classical alternatives. Significantly, the combined wave function of the system and environment continue to obey the Schrödinger equation.[4] More importantly, this is not enough to explain wave function collapse, as decoherence does not reduce it to a single eigenstate.[2] In 1927, Werner Heisenberg used the idea of wave function reduction to explain quantum measurement.[5] However, if collapse were a fundamental physical phenomenon, rather than just the epiphenomenon of some other process, it would mean nature was fundamentally stochastic, i.e. nondeterministic, an undesirable property for a theory.[2][6]"
Wave%E2%80%93particle_duality,Physics,2,"Wave–particle duality is the concept in quantum mechanics  that every particle or quantum entity may be described as either a particle or a wave. It expresses the inability of the classical concepts ""particle"" or ""wave"" to fully describe the behaviour of quantum-scale objects. As Albert Einstein wrote:[1] It seems as though we must use sometimes the one theory and sometimes the other, while at times we may use either. We are faced with a new kind of difficulty. We have two contradictory pictures of reality; separately neither of them fully explains the phenomena of light, but together they do. Through the work of Max Planck, Albert Einstein, Louis de Broglie, Arthur Compton, Niels Bohr, and many others, current scientific theory holds that all particles exhibit a wave nature and vice versa.[2] This phenomenon has been verified not only for elementary particles, but also for compound particles like atoms and even molecules. For macroscopic particles, because of their extremely short wavelengths, wave properties usually cannot be detected.[3] Although the use of the wave-particle duality has worked well in physics, the meaning or interpretation has not been satisfactorily resolved; see Interpretations of quantum mechanics.
 Bohr regarded the ""duality paradox"" as a fundamental or metaphysical fact of nature. A given kind of quantum object will exhibit sometimes wave, sometimes particle, character, in respectively different physical settings. He saw such duality as one aspect of the concept of complementarity.[4] Bohr regarded renunciation of the cause-effect relation, or complementarity, of the space-time picture, as essential to the quantum mechanical account.[5] Werner Heisenberg considered the question further. He saw the duality as present for all quantic entities, but not quite in the usual quantum mechanical account considered by Bohr. He saw it in what is called second quantization, which generates an entirely new concept of fields that exist in ordinary space-time, causality still being visualizable. Classical field values (e.g. the electric and magnetic field strengths of Maxwell) are replaced by an entirely new kind of field value, as considered in quantum field theory. Turning the reasoning around, ordinary quantum mechanics can be deduced as a specialized consequence of quantum field theory.[6][7]"
Wavelength,Physics,2,"In physics, the wavelength is the spatial period of a periodic wave—the distance over which the wave's shape repeats.[1][2]  It is the distance between consecutive corresponding points of the same phase on the wave, such as two adjacent crests, troughs, or zero crossings, and is a characteristic of both traveling waves and standing waves, as well as other spatial wave patterns.[3][4] The inverse of the wavelength is called the spatial frequency. Wavelength is commonly designated by the Greek letter lambda (λ).
The term wavelength is also sometimes applied to modulated waves, and to the sinusoidal envelopes of modulated waves or waves formed by interference of several sinusoids.[5] Assuming a sinusoidal wave moving at a fixed wave speed, wavelength is inversely proportional to frequency of the wave: waves with higher frequencies have shorter wavelengths, and lower frequencies have longer wavelengths.[6] Wavelength depends on the medium (for example, vacuum, air, or water) that a wave travels through.  Examples of waves are sound waves, light, water waves and periodic electrical signals in a conductor. A sound wave is a variation in air pressure, while in light and other electromagnetic radiation the strength of the electric and the magnetic field vary. Water waves are variations in the height of a body of water. In a crystal lattice vibration, atomic positions vary.
 The range of wavelengths or frequencies for wave phenomena is called a spectrum. The name originated with the visible light spectrum but now can be applied to the entire electromagnetic spectrum as well as to a sound spectrum or vibration spectrum.
"
Weak_interaction,Physics,2,"
 In nuclear physics and particle physics, the weak interaction, which is also often called the weak force or weak nuclear force, is the mechanism of interaction between subatomic particles that is responsible for the radioactive decay of atoms. The weak interaction participates in nuclear fission, and the theory describing its behaviour and effects is sometimes called quantum flavourdynamics (QFD). However, the term QFD is rarely used, because the weak force is better understood by electroweak theory (EWT).[1] 
 The effective range of the weak force is limited to subatomic distances, and is less than the diameter of a proton. It is one of the four known force-related fundamental interactions of nature, alongside the strong interaction, electromagnetism, and gravitation.
"
Weber_(unit),Physics,2,"In physics, the weber (/ˈveɪb-, ˈwɛb.ər/ VAY-, WEH-bər;[1][2] symbol: Wb) is the SI derived unit of magnetic flux. A flux density of one Wb/m2 (one weber per square metre) is one tesla.
 The weber is named after the German physicist Wilhelm Eduard Weber (1804–1891).
"
Wedge_(mechanical_device),Physics,2,"A wedge is a triangular shaped tool, and is a portable inclined plane, and one of the six classical simple machines. It can be used to separate two objects or portions of an object, lift up an object, or hold an object in place. It functions by converting a force applied to its blunt end into forces perpendicular (normal) to its inclined surfaces.  The mechanical advantage of a wedge is given by the ratio of the length of its slope to its width.[1][2] Although a short wedge with a wide angle may do a job faster, it requires more force than a long wedge with a narrow angle.
 The force is applied on a flat, broad surface. This energy is transported to the pointy, sharp end of the wedge, hence the  force is transported.
 The wedge simply transports energy and collects it to the pointy end, consequently breaking the item. In this way, much pressure is put on a thin area.
"
Weight,Physics,2,"In science and engineering, the weight of an object is the force acting on the object due to gravity.[1][2][3] Some standard textbooks[4] define weight as a vector quantity, the gravitational force acting on the object. Others[5][6] define weight as a scalar quantity, the magnitude of the gravitational force. Yet others[7] define it as the magnitude of the reaction force exerted on a body by mechanisms that counteract the effects of gravity: the weight is the quantity that is measured by, for example, a spring scale. Thus, in a state of free fall, the weight would be zero. In this sense of weight, terrestrial objects can be weightless: ignoring air resistance, the famous apple falling from the tree, on its way to meet the ground near Isaac Newton, would be weightless.
 The unit of measurement for weight is that of force, which in the International System of Units (SI) is the newton. For example, an object with a mass of one kilogram has a weight of about 9.8 newtons on the surface of the Earth, and about one-sixth as much on the Moon. Although weight and mass are scientifically distinct quantities, the terms are often confused with each other in everyday use (i.e. comparing and converting force weight in pounds to mass in kilograms and vice versa).[8] Further complications in elucidating the various concepts of weight have to do with the theory of relativity according to which gravity is modeled as a consequence of the curvature of spacetime. In the teaching community, a considerable debate has existed for over half a century on how to define weight for their students. The current situation is that a multiple set of concepts co-exist and find use in their various contexts.[2]"
Wheel_and_axle,Physics,2,"The wheel and axle is a machine consisting of a wheel attached to a smaller axle so that these two parts rotate together in which a force is transferred from one to the other. The wheel and axle can be viewed as a version of the lever, with a drive force applied tangentially to the perimeter of the wheel and a load force applied to the axle, respectively, that are balanced around the hinge which is the fulcrum.
"
Wind,Physics,2,"Wind is the flow of gases on a large scale. On the surface of the Earth, wind consists of the bulk movement of air. In outer space, solar wind is the movement of gases or charged particles from the Sun through space, while planetary wind is the outgassing of light chemical elements from a planet's atmosphere into space. Winds are commonly classified by their spatial scale, their speed, the types of forces that cause them, the regions in which they occur, and their effect. The strongest observed winds on a planet in the Solar System occur on Neptune and Saturn. Winds have various aspects: velocity (wind speed); the density of the gas involved; energy content or wind energy. The wind is also an important means of transportation for seeds and small birds; with time things can travel thousands of miles in the wind.
 In meteorology, winds are often referred to according to their strength, and the direction from which the wind is blowing. Short bursts of high speed wind are termed gusts. Strong winds of intermediate duration (around one minute) are termed squalls. Long-duration winds have various names associated with their average strength, such as breeze, gale, storm, and hurricane. Wind occurs on a range of scales, from thunderstorm flows lasting tens of minutes, to local breezes generated by heating of land surfaces and lasting a few hours, to global winds resulting from the difference in absorption of solar energy between the climate zones on Earth. The two main causes of large-scale atmospheric circulation are the differential heating between the equator and the poles, and the rotation of the planet (Coriolis effect). Within the tropics, thermal low circulations over terrain and high plateaus can drive monsoon circulations. In coastal areas the sea breeze/land breeze cycle can define local winds; in areas that have variable terrain, mountain and valley breezes can dominate local winds.
 In human civilization, the concept of wind has been explored in mythology, influenced the events of history, expanded the range of transport and warfare, and provided a power source for mechanical work, electricity, and recreation. Wind powers the voyages of sailing ships across Earth's oceans. Hot air balloons use the wind to take short trips, and powered flight uses it to increase lift and reduce fuel consumption. Areas of wind shear caused by various weather phenomena can lead to dangerous situations for aircraft. When winds become strong, trees and human-made structures are damaged or destroyed.
 Winds can shape landforms, via a variety of aeolian processes such as the formation of fertile soils, such as loess, and by erosion. Dust from large deserts can be moved great distances from its source region by the prevailing winds; winds that are accelerated by rough topography and associated with dust outbreaks have been assigned regional names in various parts of the world because of their significant effects on those regions. Wind also affects the spread of wildfires. Winds can disperse seeds from various plants, enabling the survival and dispersal of those plant species, as well as flying insect populations. When combined with cold temperatures, the wind has a negative impact on livestock. Wind affects animals' food stores, as well as their hunting and defensive strategies.
"
Wind_shear,Physics,2,"Wind shear (or windshear), sometimes referred to as wind gradient, is a difference in wind speed or direction over a relatively short distance in the atmosphere. Atmospheric wind shear is normally described as either vertical or horizontal wind shear. Vertical wind shear is a change in wind speed or direction with change in altitude.  Horizontal wind shear is a change in wind speed with change in lateral position for a given altitude.[1] Wind shear is a microscale meteorological phenomenon occurring over a very small distance, but it can be associated with mesoscale or synoptic scale weather features such as squall lines and cold fronts. It is commonly observed near microbursts and downbursts caused by thunderstorms, fronts, areas of locally higher low-level winds referred to as low level jets, near mountains, radiation inversions that occur due to clear skies and calm winds, buildings, wind turbines, and sailboats. Wind shear has significant effects on control of an aircraft, and it has been a sole or contributing cause of many aircraft accidents.
 Wind shear is sometimes experienced by pedestrians at ground level when walking across a plaza towards a tower block and suddenly encountering a strong wind stream that is flowing around the base of the tower.
 Sound movement through the atmosphere is affected by wind shear, which can bend the wave front, causing sounds to be heard where they normally would not, or vice versa. Strong vertical wind shear within the troposphere also inhibits tropical cyclone development, but helps to organize individual thunderstorms into longer life cycles which can then produce severe weather. The thermal wind concept explains how differences in wind speed at different heights are dependent on horizontal temperature differences, and explains the existence of the jet stream.[2]"
Work_(physics),Physics,2,"
 In physics, work is the energy transferred to or from an object via the application of force along a displacement. In its simplest form, it is often represented as the product of force and displacement. A force is said to do positive work if (when applied) it has a component in the direction of the displacement of the point of application. A force does negative work if it has a component opposite to the direction of the displacement at the point of application of the force.
 For example, when a ball is held above the ground and then dropped, the work done by the gravitational force on the ball as it falls is equal to the weight of the ball (a force) multiplied by the distance to the ground (a displacement). When the force F is constant and the angle between the force and the displacement s is θ, then the work done is given by: 
 Work is a scalar quantity,[1] so it has only magnitude and no direction. Work transfers energy from one place to another, or one form to another. The SI unit of work is the joule (J).
"
Work_function,Physics,2,"In solid-state physics, the work function (sometimes spelled workfunction) is the minimum thermodynamic work (i.e., energy) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface. Here ""immediately"" means that the final electron position is far from the surface on the atomic scale, but still too close to the solid to be influenced by ambient electric fields in the vacuum.
The work function is not a characteristic of a bulk material, but rather a property of the surface of the material (depending on crystal face and contamination).
"
X-ray,Physics,2,"An X-ray, or X-radiation, is a penetrating form of high-energy electromagnetic radiation. Most X-rays have a wavelength ranging from 10 picometers to 10 nanometers, corresponding to frequencies in the range 30 petahertz to 30 exahertz (3×1016 Hz to 3×1019 Hz) and energies in the range 124 eV to 124 keV. X-ray wavelengths are shorter than those of UV rays and typically longer than those of gamma rays. In many languages, X-radiation is referred to as Röntgen radiation, after the German scientist Wilhelm Röntgen, who discovered it on November 8, 1895.[1] He named it X-radiation to signify an unknown type of radiation.[2] Spellings of X-ray(s) in English include the variants x-ray(s), xray(s), and X ray(s).[3]"
Young%E2%80%99s_modulus,Physics,2,"
 Young's modulus 



E


{  E}
, the Young modulus or the modulus of elasticity in tension, is a mechanical property that measures the tensile stiffness of a solid material. It quantifies the relationship between tensile stress 



σ


{  \sigma }
 (force per unit area) and axial strain 



ε


{  \varepsilon }
 (proportional deformation) in the linear elastic region of a material and is determined using the formula:[1] Young's moduli are typically so large that they are expressed not in pascals but in gigapascals (GPa).
 Although Young's modulus is named after the 19th-century British scientist Thomas Young, the concept was developed in 1727 by Leonhard Euler. The first experiments that used the concept of Young's modulus in its current form were performed by the Italian scientist Giordano Riccati in 1782, pre-dating Young's work by 25 years.[2]  The term modulus is derived from the Latin root term modus which means measure.
"
Zeeman_effect,Physics,2,"The Zeeman effect (/ˈzeɪmən/; Dutch pronunciation: [ˈzeːmɑn]), named after Dutch physicist Pieter Zeeman, is the effect of splitting of a spectral line into several components in the presence of a static magnetic field. It is analogous to the Stark effect, the splitting of a spectral line into several components in the presence of an electric field. Also similar to the Stark effect, transitions between different components have, in general, different intensities, with some being entirely forbidden (in the dipole approximation), as governed by the selection rules.
 Since the distance between the Zeeman sub-levels is a function of magnetic field strength, this effect can be used to measure magnetic field strength, e.g. that of the Sun and other stars or in laboratory plasmas.
The Zeeman effect is very important in applications such as nuclear magnetic resonance spectroscopy, electron spin resonance spectroscopy, magnetic resonance imaging (MRI) and Mössbauer spectroscopy. It may also be utilized to improve accuracy in atomic absorption spectroscopy.
A theory about the magnetic sense of birds assumes that a protein in the retina is changed due to the Zeeman effect.[1] When the spectral lines are absorption lines, the effect is called inverse Zeeman effect.
"
Abiotic_component,Biology,5,"
 In biology and ecology, abiotic components or abiotic factors are non-living chemical and physical parts of the environment that affect living organisms and the functioning of ecosystems. Abiotic factors and the phenomena associated with them underpin biology as a whole.
 Abiotic components include physical conditions and non-living resources that affect living organisms in terms of growth, maintenance, and reproduction.  Resources are distinguished as substances or objects in the environment required by one organism and consumed or otherwise made unavailable for use by other organisms.[1][2] Component degradation of a substance occurs by chemical or physical processes, e.g. hydrolysis.  All non-living components of an ecosystem, such as atmospheric conditions and water resources, are called abiotic components.[3]"
Abscission,Biology,5,"Abscission (from Latin ab, ""away"", and scindere, ""to cut'"") is the shedding of various parts of an organism, such as a plant dropping a leaf, fruit, flower, or seed. In zoology, abscission is the intentional shedding of a body part, such as the shedding of a claw, husk, or the autotomy of a tail to evade a predator. In mycology, it is the liberation of a fungal spore. In cell biology, abscission refers to the separation of two daughter cells at the completion of cytokinesis.
"
Absorption_(skin),Biology,5,"Skin absorption is a route by which substances can enter the body through the skin. Along with inhalation, ingestion and injection, dermal absorption is a route of exposure for toxic substances and route of administration for medication. Absorption of substances through the skin depends on a number of factors, the most important of which are concentration, duration of contact, solubility of medication, and physical condition of the skin and part of the body exposed.
 Skin (percutaneous, dermal) absorption is the transport of chemicals from the outer surface of the skin both into the skin and into circulation.  Skin absorption relates to the degree of exposure to and possible effect of a substance which may enter the body through the skin.  Human skin comes into contact with many agents intentionally and unintentionally. Skin absorption can occur from occupational, environmental, or consumer skin exposure to chemicals, cosmetics, or pharmaceutical products.  Some chemicals can be absorbed in enough quantity to cause detrimental systemic effects. Skin disease (dermatitis) is considered one of the most common occupational diseases.[1]  In order to assess if a chemical can be a risk of either causing dermatitis or other more systemic effects and how that risk may be reduced one must know the extent to which it is absorbed, thus dermal exposure is a key aspect of human health risk assessment.
"
Absorption_spectrum,Biology,5,"Absorption spectroscopy refers to spectroscopic techniques that measure the absorption of radiation, as a function of frequency or wavelength, due to its interaction with a sample. The sample absorbs energy, i.e., photons, from the radiating field. The intensity of the absorption varies as a function of frequency, and this variation is the absorption spectrum. Absorption spectroscopy is performed across the electromagnetic spectrum.
 Absorption spectroscopy is employed as an analytical chemistry tool to determine the presence of a particular substance in a sample and, in many cases, to quantify the amount of the substance present. Infrared and ultraviolet–visible spectroscopy are particularly common in analytical applications. Absorption spectroscopy is also employed in studies of molecular and atomic physics, astronomical spectroscopy and remote sensing.
 There are a wide range of experimental approaches for measuring absorption spectra. The most common arrangement is to direct a generated beam of radiation at a sample and detect the intensity of the radiation that passes through it. The transmitted energy can be used to calculate the absorption. The source, sample arrangement and detection technique vary significantly depending on the frequency range and the purpose of the experiment.
 Following are the major types of absorption spectroscopy:[1] Nuclear magnetic resonance spectroscopy
"
Acclimatization,Biology,5,"
 Acclimatization or acclimatisation (also called acclimation or acclimatation) is the process in which an individual organism adjusts to a change in its environment (such as a change in altitude, temperature, humidity, photoperiod, or pH), allowing it to maintain performance across a range of environmental conditions. Acclimatization occurs in a short period of time (hours to weeks), and within the organism's lifetime (compared to adaptation, which is a development that takes place over many generations). This may be a discrete occurrence (for example, when mountaineers acclimate to high altitude over hours or days) or may instead represent part of a periodic cycle, such as a mammal shedding heavy winter fur in favor of a lighter summer coat. Organisms can adjust their morphological, behavioral, physical, and/or biochemical traits in response to changes in their environment.[1] While the capacity to acclimate to novel environments has been well documented in thousands of species, researchers still know very little about how and why organisms acclimate the way that they do.
"
Acetyl-CoA,Biology,5,"Acetyl-CoA (acetyl coenzyme A) is a molecule that participates in many biochemical reactions in protein, carbohydrate and lipid metabolism.[1] Its main function is to deliver the acetyl group to the citric acid cycle (Krebs cycle) to be oxidized for energy production. Coenzyme A (CoASH or CoA) consists of a β-mercaptoethylamine group linked to the vitamin pantothenic acid (B5) through an amide linkage [2] and 3'-phosphorylated ADP. The acetyl group (indicated in blue in the structural diagram on the right) of acetyl-CoA is linked  to the sulfhydryl substituent of the β-mercaptoethylamine group. This thioester linkage is a ""high energy"" bond, which is particularly reactive. Hydrolysis of the thioester bond is exergonic (−31.5 kJ/mol).
 CoA is acetylated to acetyl-CoA by the breakdown of carbohydrates through glycolysis and by the breakdown of fatty acids through β-oxidation. Acetyl-CoA then enters the citric acid cycle, where the acetyl group is oxidized to carbon dioxide and water, and the energy released is captured in the form of 11 ATP and one GTP per acetyl group.
 Konrad Bloch and Feodor Lynen were awarded the 1964 Nobel Prize in Physiology and Medicine for their discoveries linking acetyl-CoA and fatty acid metabolism. Fritz Lipmann won the Nobel Prize in 1953 for his discovery of the cofactor coenzyme A.
"
Acoelomate,Biology,5,"The coelom is the main body cavity in most animals[1] and is positioned inside the body to surround and contain the digestive tract and other organs. In some animals, it is lined with mesothelium. In other animals, such as molluscs, it remains undifferentiated. In the past, and for practical purposes, coelom characteristics have been used to classify bilaterian animal phyla into informal groups.
"
Action_potential,Biology,5,"
 In physiology, an action potential (AP) occurs when the membrane potential of a specific cell location rapidly rises and falls:[1] this depolarization then causes adjacent locations to similarly depolarize. Action potentials occur in several types of animal cells, called excitable cells, which include neurons, muscle cells, endocrine cells, glomus cells, and in some plant cells.
 In neurons, action potentials play a central role in cell-to-cell communication by providing for—or with regard to saltatory conduction, assisting—the propagation of signals along the neuron's axon toward synaptic boutons situated at the ends of an axon; these signals can then connect with other neurons at synapses, or to motor cells or glands. In other types of cells, their main function is to activate intracellular processes. In muscle cells, for example, an action potential is the first step in the chain of events leading to contraction. In beta cells of the pancreas, they provoke release of insulin.[a] Action potentials in neurons are also known as ""nerve impulses"" or ""spikes"", and the temporal sequence of action potentials generated by a neuron is called its ""spike train"". A neuron that emits an action potential, or nerve impulse, is often said to ""fire"".
 Action potentials are generated by special types of voltage-gated ion channels embedded in a cell's plasma membrane.[b] These channels are shut when the membrane potential is near the (negative) resting potential of the cell, but they rapidly begin to open if the membrane potential increases to a precisely defined threshold voltage, depolarising the transmembrane potential.[b] When the channels open, they allow an inward flow of sodium ions, which changes the electrochemical gradient, which in turn produces a further rise in the membrane potential towards zero. This then causes more channels to open, producing a greater electric current across the cell membrane and so on. The process proceeds explosively until all of the available ion channels are open, resulting in a large upswing in the membrane potential. The rapid influx of sodium ions causes the polarity of the plasma membrane to reverse, and the ion channels then rapidly inactivate. As the sodium channels close, sodium ions can no longer enter the neuron, and they are then actively transported back out of the plasma membrane. Potassium channels are then activated, and there is an outward current of potassium ions, returning the electrochemical gradient to the resting state. After an action potential has occurred, there is a transient negative shift, called the afterhyperpolarization.
 In animal cells, there are two primary types of action potentials. One type is generated by voltage-gated sodium channels, the other by voltage-gated calcium channels. Sodium-based action potentials usually last for under one millisecond, but calcium-based action potentials may last for 100 milliseconds or longer.[citation needed] In some types of neurons, slow calcium spikes provide the driving force for a long burst of rapidly emitted sodium spikes. In cardiac muscle cells, on the other hand, an initial fast sodium spike provides a ""primer"" to provoke the rapid onset of a calcium spike, which then produces muscle contraction.[citation needed]"
Activation_energy,Biology,5,"In chemistry and physics, activation energy is the energy that must be provided to compounds to result in a chemical reaction.[1]The activation energy (Ea) of a reaction is measured in joules per mole (J/mol), kilojoules per mole (kJ/mol) or kilocalories per mole (kcal/mol).[2] Activation energy can be thought of as the magnitude of the potential barrier (sometimes called the energy barrier) separating minima of the potential energy surface pertaining to the initial and final thermodynamic state. For a chemical reaction to proceed at a reasonable rate, the temperature of the system should be high enough such that there exists an appreciable number of molecules with translational energy equal to or greater than the activation energy. The term Activation Energy was introduced in 1889 by the Swedish scientist Svante Arrhenius.[3]"
Active_site,Biology,5,"In biology, the active site is the region of an enzyme where substrate molecules bind and undergo a chemical reaction. The active site consists of amino acid residues that form temporary bonds with the substrate (binding site) and residues that catalyse a reaction of that substrate (catalytic site).[1] Although the active site occupies only ~10–20% of the volume of an enzyme,[2]:19 it is the most important part as it directly catalyzes the chemical reaction. It usually consists of three to four amino acids, while other amino acids within the protein are required to maintain the tertiary structure of the enzyme.[3] Each active site is evolved to be optimised to bind a particular substrate and catalyse a particular reaction, resulting in high specificity. This specificity is determined by the arrangement of amino acids within the active site and the structure of the substrates. Sometimes enzymes also need to bind with some cofactors to fulfil their function. The active site is usually a groove or pocket of the enzyme which can be located in a deep tunnel within the enzyme,[4] or between the interfaces of multimeric enzymes. An active site can catalyse a reaction repeatedly as residues are not altered at the end of the reaction (they may change during the reaction, but are regenerated by the end).[5] This process is achieved by lowering the activation energy of the reaction, so more substrates have enough energy to undergo reaction.[6]"
Active_transport,Biology,5,"In cellular biology, active transport is the movement of molecules across a cell membrane from a region of  lower concentration to a region of  higher concentration—against the concentration gradient. Active transport requires cellular energy to achieve this movement. There are two types of active transport: primary active transport that uses adenosine triphosphate (ATP), and secondary active transport that uses an electrochemical gradient. An example of active transport in human physiology is the uptake of glucose in the intestines.
"
Adaptation,Biology,5,"
 In biology, adaptation has three related meanings. Firstly, it is the dynamic evolutionary process that fits organisms to their environment, enhancing their evolutionary fitness. Secondly, it is a state reached by the population during that process. Thirdly, it is a phenotypic trait or adaptive trait, with a functional role in each individual organism, that is maintained and has evolved through natural selection.
 Historically, adaptation has been described from the time of the ancient Greek philosophers such as Empedocles and Aristotle. In 18th and 19th century natural theology, adaptation was taken as evidence for the existence of a deity. Charles Darwin proposed instead that it was explained by natural selection.
 Adaptation is related to biological fitness, which governs the rate of evolution as measured by change in gene frequencies. Often, two or more species co-adapt and co-evolve as they develop adaptations that interlock with those of the other species, such as with flowering plants and pollinating insects. In mimicry, species evolve to resemble other species; in Müllerian mimicry this is a mutually beneficial co-evolution as each of a group of strongly defended species (such as wasps able to sting) come to advertise their defences in the same way. Features evolved for one purpose may be co-opted for a different one, as when the insulating feathers of dinosaurs were co-opted for bird flight.
 Adaptation is a major topic in the philosophy of biology, as it concerns function and purpose (teleology). Some biologists try to avoid terms which imply purpose in adaptation, not least because it suggests a deity's intentions, but others note that adaptation is necessarily purposeful.
"
Adaptive_radiation,Biology,5,"In evolutionary biology, adaptive radiation is a process in which organisms diversify rapidly from an ancestral species into a multitude of new forms, particularly when a change in the environment makes new resources available, alters biotic interactions or opens new environmental niches.[1][2] Starting with a single ancestor, this process results in the speciation and phenotypic adaptation of an array of species exhibiting different morphological and physiological traits. The prototypical example of adaptive radiation is finch speciation on the Galapagos (""Darwin's finches""), but examples are known from around the world.
"
Adenine,Biology,5,"Adenine /ˈædɪnɪn/ (A, Ade) is a nucleobase (a purine derivative). It is one of the four nucleobases in the nucleic acid of DNA that are represented by the letters G–C–A–T. The three others are guanine, cytosine and thymine. Its derivatives have a variety of roles in biochemistry including cellular respiration, in the form of both the energy-rich adenosine triphosphate (ATP) and the cofactors nicotinamide adenine dinucleotide (NAD) and flavin adenine dinucleotide (FAD). It also has functions in protein synthesis and as a chemical component of DNA and RNA.[2] The shape of adenine is complementary to either thymine in DNA or uracil in RNA.
 The adjacent image shows pure adenine, as an independent molecule. When connected into DNA, a covalent bond is formed between deoxyribose sugar and the bottom left nitrogen (thereby removing the existing hydrogen atom). The remaining structure is called an adenine residue, as part of a larger molecule. Adenosine is adenine reacted with ribose, as used in RNA and ATP; deoxyadenosine is adenine attached to deoxyribose, as used to form DNA.
"
Adenosine_triphosphate,Biology,5,"Adenosine triphosphate (ATP) is an organic compound and hydrotrope that provides energy to drive many processes in living cells, e.g. muscle contraction, nerve impulse propagation, condensate dissolution, and chemical synthesis. Found in all known forms of life, ATP is often referred to as the ""molecular unit of currency"" of intracellular energy transfer.[2]  When consumed in metabolic processes, it converts either to adenosine diphosphate (ADP) or to adenosine monophosphate (AMP). Other processes regenerate ATP so that the human body recycles its own body weight equivalent in ATP each day.[3]  It is also a precursor to DNA and RNA, and is used as a coenzyme.
 From the perspective of biochemistry, ATP is classified as a nucleoside triphosphate, which indicates that it consists of three components: a nitrogenous base (adenine), the sugar ribose, and the triphosphate.
"
Adipose_tissue,Biology,5,"Adipose tissue, body fat, or simply fat is a loose connective tissue composed mostly of adipocytes.[1] In addition to adipocytes, adipose tissue contains the stromal vascular fraction (SVF) of cells including preadipocytes, fibroblasts, vascular endothelial cells and a variety of immune cells such as adipose tissue macrophages. Adipose tissue is derived from preadipocytes. Its main role is to store energy in the form of lipids, although it also cushions and insulates the body. Far from being hormonally inert, adipose tissue has, in recent years, been recognized as a major endocrine organ,[2] as it produces hormones such as leptin, estrogen, resistin, and cytokine (especially TNFα). The two types of adipose tissue are white adipose tissue (WAT), which stores energy, and brown adipose tissue (BAT), which generates body heat.  The formation of adipose tissue appears to be controlled in part by the adipose gene. Adipose tissue – more specifically brown adipose tissue – was first identified by the Swiss naturalist Conrad Gessner in 1551.[3]"
Aerobic_organism,Biology,5,"
 An aerobic organism or aerobe is an organism that can survive and grow in an oxygenated environment.[1] In contrast, an anaerobic organism (anaerobe) is any organism that does not require oxygen for growth. Some anaerobes react negatively or even die if oxygen is present.[2] In July 2020, marine biologists reported that aerobic microorganisms (mainly), in ""quasi-suspended animation"", were found in organically-poor sediments, up to 101.5 million years old, 250 feet below the seafloor in the South Pacific Gyre (SPG) (""the deadest spot in the ocean""), and could be the longest-living life forms ever found.[3][4]"
Aerobiology,Biology,5,"
Aerobiology (from Greek ἀήρ, aēr, ""air""; βίος, bios, ""life""; and -λογία, -logia) is a branch of biology that studies organic particles, such as bacteria, fungal spores, very small insects, pollen grains and viruses, which are passively transported by the air.[1] Aerobiologists have traditionally been involved in the measurement and reporting of airborne pollen and fungal spores as a service to allergy sufferers.[1] The first finding of airborne algae took place in Germany in 1910.[2] The minimum requirements published after a big consensus are an international standard to ensure the quality in Aerobiological method.[3][4]"
Agriculture,Biology,5,"
 
 Agriculture is the science and art of cultivating plants and livestock.[1] Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that enabled people to live in cities. The history of agriculture began thousands of years ago. After gathering wild grains beginning at least 105,000 years ago, nascent farmers began to plant them around 11,500 years ago. Pigs, sheep and cattle were domesticated over 10,000 years ago. Plants were independently cultivated in at least 11 regions of the world. Industrial agriculture based on large-scale monoculture in the twentieth century came to dominate agricultural output, though about 2 billion people still depended on subsistence agriculture into the twenty-first.
 Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have sharply increased yields, while causing widespread ecological and environmental damage. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and environmental damage. Environmental issues include contributions to global warming, depletion of aquifers, deforestation, antibiotic resistance, and growth hormones in industrial meat production. Genetically modified organisms are widely used, although some are banned in certain countries.
 The major agricultural products can be broadly grouped into foods, fibers, fuels and raw materials (such as rubber). Food classes include cereals (grains), vegetables, fruits, oils, meat, milk, fungi and eggs. Over one-third of the world's workers are employed in agriculture, second only to the service sector, although the number of agricultural workers in developed countries has decreased significantly over the centuries.
"
Agrobiology,Biology,5,"Agrobiology is the science of plant nutrition and growth in relation to soil conditions, especially to determine ways to increase crop yields. 
Agrobiology is generally concerned with:
 The term given by wilcox. There are several universities providing courses of agrobiology e.g. at Aarhus University.[1] There also is a Journal of Agrobiology published in the Czech Republic.
"
Alga,Biology,5,"
.mw-parser-output table.biota-infobox{text-align:center;width:200px;font-size:100%}.mw-parser-output table.biota-infobox th.section-header{text-align:center}.mw-parser-output table.biota-infobox td.section-content{text-align:left;padding:0 0.25em}.mw-parser-output table.biota-infobox td.list-section{text-align:left;padding:0 0.25em}.mw-parser-output table.biota-infobox td.taxon-section{text-align:center;padding:0 0.25em}.mw-parser-output table.biota-infobox td.image-section{text-align:center;font-size:88%}.mw-parser-output table.biota-infobox table.taxonomy{margin:0 auto;text-align:left;background:transparent;padding:2px}.mw-parser-output table.biota-infobox table.taxonomy tr{vertical-align:top}.mw-parser-output table.biota-infobox table.taxonomy td{padding:1px} Algae (/ˈældʒi, ˈælɡi/; singular alga /ˈælɡə/) is an informal term for a large and diverse group of photosynthetic eukaryotic organisms. It is a polyphyletic grouping, including species from multiple distinct clades. Included organisms range from unicellular microalgae, such as Chlorella and the diatoms, to multicellular forms, such as the giant kelp, a large brown alga which may grow up to 50 metres (160 ft) in length. Most are aquatic and autotrophic and lack many of the distinct cell and tissue types, such as stomata, xylem and phloem, which are found in land plants. The largest and most complex marine algae are called seaweeds, while the most complex freshwater forms are the Charophyta, a division of green algae which includes, for example, Spirogyra and stoneworts.
 No definition of algae is generally accepted. One definition is that algae ""have chlorophyll as their primary photosynthetic pigment and lack a sterile covering of cells around their reproductive cells"".[2] Although cyanobacteria are often referred to as ""blue-green algae"", most authorities exclude all prokaryotes from the definition of algae.[3][4] Algae constitute a polyphyletic group[3] since they do not include a common ancestor, and although their plastids seem to have a single origin, from cyanobacteria,[5] they were acquired in different ways. Green algae are examples of algae that have primary chloroplasts derived from endosymbiotic cyanobacteria. Diatoms and brown algae are examples of algae with secondary chloroplasts derived from an endosymbiotic red alga.[6] Algae exhibit a wide range of reproductive strategies, from simple asexual cell division to complex forms of sexual reproduction.[7] Algae lack the various structures that characterize land plants, such as the phyllids (leaf-like structures) of bryophytes, rhizoids in nonvascular plants, and the roots, leaves, and other organs found in tracheophytes (vascular plants). Most are phototrophic, although some are mixotrophic, deriving energy both from photosynthesis and uptake of organic carbon either by osmotrophy, myzotrophy, or phagotrophy. Some unicellular species of green algae, many golden algae, euglenids, dinoflagellates, and other algae have become heterotrophs (also called colorless or apochlorotic algae), sometimes parasitic, relying entirely on external energy sources and have limited or no photosynthetic apparatus.[8][9][10] Some other heterotrophic organisms, such as the apicomplexans, are also derived from cells whose ancestors possessed plastids, but are not traditionally considered as algae. Algae have photosynthetic machinery ultimately derived from cyanobacteria that produce oxygen as a by-product of photosynthesis, unlike other photosynthetic bacteria such as purple and green sulfur bacteria. Fossilized filamentous algae from the Vindhya basin have been dated back to 1.6 to 1.7 billion years ago.[11]"
Allopatric_speciation,Biology,5,"Allopatric speciation (from Ancient Greek ἄλλος, allos, meaning ""other"", and πατρίς, patris, ""fatherland""), also referred to as geographic speciation, vicariant speciation, or its earlier name, the dumbbell model,[1]:86 is a mode of speciation that occurs when biological populations become geographically isolated from each other to an extent that prevents or interferes with gene flow.
 Various geographic changes can arise such as the movement of continents, and the formation of mountains, islands, bodies of water, or glaciers. Human activity such as agriculture or developments can also change the distribution of species populations. These factors can substantially alter a region's geography, resulting in the separation of a species population into isolated subpopulations. The vicariant populations then undergo genetic changes as they become subjected to different selective pressures, experience genetic drift, and accumulate different mutations in the separated populations gene pools. The barriers prevent the exchange of genetic information between the two populations leading to reproductive isolation. If the two populations come into contact they will be unable to reproduce—effectively speciating. Other isolating factors such as population dispersal leading to emigration can cause speciation (for instance, the dispersal and isolation of a species on an oceanic island) and is considered a special case of allopatric speciation called peripatric speciation.
 Allopatric speciation is typically subdivided into two major models: vicariance and peripatric. Both models differ from one another by virtue of their population sizes and geographic isolating mechanisms. The terms allopatry and vicariance are often used in biogeography to describe the relationship between organisms whose ranges do not significantly overlap but are immediately adjacent to each other—they do not occur together or only occur within a narrow zone of contact. Historically, the language used to refer to modes of speciation directly reflected biogeographical distributions.[2] As such, allopatry is a geographical distribution opposed to sympatry (speciation within the same area). Furthermore, the terms allopatric, vicariant, and geographical speciation are often used interchangeably in the scientific literature.[2] This article will follow a similar theme, with the exception of special cases such as peripatric, centrifugal, among others.
 Observation of nature creates difficulties in witnessing allopatric speciation from ""start-to-finish"" as it operates as a dynamic process.[3] From this arises a host of various issues in defining species, defining isolating barriers, measuring reproductive isolation, among others. Nevertheless, verbal and mathematical models, laboratory experiments, and empirical evidence overwhelmingly supports the occurrence of allopatric speciation in nature.[4][1]:87–105 Mathematical modeling of the genetic basis of reproductive isolation supports the plausibility of allopatric speciation; whereas laboratory experiments of Drosophila and other animal and plant species have confirmed that reproductive isolation evolves as a byproduct of natural selection.[1]:87"
Amino_acid,Biology,5,"
 Amino acids are organic compounds that contain amine (–NH2) and carboxyl (–COOH) functional groups, along with a side chain (R group) specific to each amino acid.[1][2] The key elements of an amino acid are carbon (C), hydrogen (H), oxygen (O), and nitrogen (N), although other elements are found in the side chains of certain amino acids. About 500 naturally occurring amino acids are known (though only 20 appear in the genetic code) and can be classified in many ways.[3] They can be classified according to the core structural functional groups' locations as alpha- (α-), beta- (β-), gamma- (γ-) or delta- (δ-) amino acids; other categories relate to polarity, pH level, and side chain group type (aliphatic, acyclic, aromatic, containing hydroxyl or sulfur, etc.). In the form of proteins, amino acid residues form the second-largest component (water is the largest) of human muscles and other tissues.[4] Beyond their role as residues in proteins, amino acids participate in a number of processes such as neurotransmitter transport and biosynthesis.
 In biochemistry, amino acids which have the amine group attached to the (alpha-) carbon atom next to the carboxyl group have particular importance. They are known as 2-, alpha-, or α-amino acids (generic formula H2NCHRCOOH in most cases,[a] where R is an organic substituent known as a ""side chain"");[5] often the term ""amino acid"" is used to refer specifically to these. They include the 22 proteinogenic (""protein-building"") amino acids,[6][7][8] which combine into peptide chains (""polypeptides"") to form the building blocks of a vast array of proteins.[9] These are all L-stereoisomers (""left-handed"" isomers), although a few D-amino acids (""right-handed"") occur in bacterial envelopes, as a neuromodulator (D-serine), and in some antibiotics.[10] Twenty of the proteinogenic amino acids are encoded directly by triplet codons in the genetic code and are known as ""standard"" amino acids. The other two (""nonstandard"" or ""non-canonical"") are selenocysteine (present in many prokaryotes as well as most eukaryotes, but not coded directly by DNA), and pyrrolysine (found only in some archaea and one bacterium). Pyrrolysine and selenocysteine are encoded via variant codons; for example, selenocysteine is encoded by stop codon and SECIS element.[11][12][13] N-formylmethionine (which is often the initial amino acid of proteins in bacteria, mitochondria, and chloroplasts) is generally considered as a form of methionine rather than as a separate proteinogenic amino acid. Codon–tRNA combinations not found in nature can also be used to ""expand"" the genetic code and form novel proteins known as alloproteins incorporating non-proteinogenic amino acids.[14][15][16] Many important proteinogenic and non-proteinogenic amino acids have biological functions. For example, in the human brain, glutamate (standard glutamic acid) and gamma-aminobutyric acid (""GABA"", nonstandard gamma-amino acid) are, respectively, the main excitatory and inhibitory neurotransmitters.[17] Hydroxyproline, a major component of the connective tissue collagen, is synthesised from proline. Glycine is a biosynthetic precursor to porphyrins used in red blood cells. Carnitine is used in lipid transport. Nine proteinogenic amino acids are called ""essential"" for humans because they cannot be produced from other compounds by the human body and so must be taken in as food. Others may be conditionally essential for certain ages or medical conditions. Essential amino acids may also differ between species.[b] Because of their biological significance, amino acids are important in nutrition and are commonly used in nutritional supplements, fertilizers, feed, and food technology. Industrial uses include the production of drugs, biodegradable plastics, and chiral catalysts.
"
Amniote,Biology,5,"
 Amniotes (from Greek ἀμνίον amnion, ""membrane surrounding the fetus"", earlier ""bowl in which the blood of sacrificed animals was caught"", from  ἀμνός amnos, ""lamb""[3]) are a clade of tetrapod vertebrates comprising the reptiles (including dinosaurs and therefore birds) and mammals. Amniotes lay their eggs on land or retain the fertilized egg within the mother, and are distinguished from the anamniotes (fishes and amphibians), which typically lay their eggs in water. Older sources, particularly prior to the 20th century, may refer to amniotes as ""higher vertebrates"" and anamniotes as ""lower vertebrates"", based on the discredited idea of the evolutionary great chain of being.
 Amniotes are tetrapods (descendants of four-limbed and backboned animals) that are characterised by having an egg equipped with an amnion, an adaptation to lay eggs on land rather than in water as the anamniotes (including frogs) typically do. Amniotes include synapsids (mammals along with their extinct kin) and sauropsids (reptiles and birds), as well as their ancestors, back to amphibians. Amniote embryos, whether laid as eggs or carried by the female, are protected and aided by several extensive membranes. In eutherian mammals (such as humans), these membranes include the amniotic sac that surrounds the fetus. These embryonic membranes and the lack of a larval stage distinguish amniotes from tetrapod amphibians.[4] The first amniotes, referred to as ""basal amniotes"", resembled small lizards and evolved from the amphibian reptiliomorphs about 312 million years ago,[5] in the Carboniferous geologic period. Their eggs could survive out of the water, allowing amniotes to branch out into drier environments. The eggs could also ""breathe"" and cope with wastes, allowing the eggs and the amniotes themselves to evolve into larger forms.
 The amniotic egg represents a critical divergence within the vertebrates, one enabling amniotes to reproduce on dry land—free of the need to return to water for reproduction as required of the amphibians. From this point the amniotes spread around the globe, eventually to become the dominant land vertebrates. Very early in their evolutionary history, basal amniotes diverged into two main lines, the synapsids and the sauropsids, both of which persist into the modern era. The oldest known fossil synapsid is Protoclepsydrops from about 312 million years ago,[5] while the oldest known sauropsid is probably Paleothyris, in the order Captorhinida, from the Middle Pennsylvanian epoch (c. 306–312 million years ago).[5]"
Anaerobic_organism,Biology,5,"An anaerobic organism or anaerobe is any organism that does not require oxygen for growth. It may react negatively or even die if free oxygen is present. In contrast, an aerobic organism (aerobe) is an organism that requires an oxygenated environment.  Anaerobes may be unicellular (e.g. protozoans,[1] bacteria[2]) or multicellular.[3]
Most fungi are obligate aerobes, requiring oxygen to survive, however some species, such as the Chytridiomycota that reside in the rumen of cattle, are obligate anaerobes; for these species, anaerobic respiration is used because oxygen will disrupt their metabolism or kill them.
"
Analogous_structures,Biology,5,"
 Convergent evolution is the independent evolution of similar features in species of different periods or epochs in time. Convergent evolution creates analogous structures that have similar form or function but were not present in the last common ancestor of those groups. The cladistic term for the same phenomenon is homoplasy. The recurrent evolution of flight is a classic example, as flying insects, birds, pterosaurs, and bats have independently evolved the useful capacity of flight. Functionally similar features that have arisen through convergent evolution are analogous, whereas homologous structures or traits have a common origin but can have dissimilar functions.  Bird, bat, and pterosaur wings are analogous structures, but their forelimbs are homologous, sharing an ancestral state despite serving different functions.
 
 The opposite of convergence is divergent evolution, where related species evolve different traits. Convergent evolution is similar to parallel evolution, which occurs when two independent species evolve in the same direction and thus independently acquire similar characteristics; for instance, gliding frogs have evolved in parallel from multiple types of tree frog.
 Many instances of convergent evolution are known in plants, including the repeated development of C4 photosynthesis, seed dispersal by fleshy fruits adapted to be eaten by animals, and carnivory.
 Recent evidence suggests that even plants and animals share a convergently evolved developmental pattern whereby embryos of both lineages pass through a phylotypic stage marked by an organizational checkpoint during mid-embryogenesis.[1][2]"
Anatomy,Biology,5,"
 Anatomy (Greek anatomē, 'dissection') is the branch of biology concerned with the study of the structure of organisms and their parts.[1] Anatomy is a branch of natural science which deals with the structural organization of living things. It is an old science, having its beginnings in prehistoric times.[2] Anatomy is inherently tied to developmental biology, embryology, comparative anatomy, evolutionary biology, and phylogeny,[3] as these are the processes by which anatomy is generated, both over immediate and long-term timescales. Anatomy and physiology, which study the structure and function of organisms and their parts respectively, make a natural pair of related disciplines, and are often studied together. Human anatomy is one of the essential basic sciences that are applied in medicine.[4] The discipline of anatomy is divided into macroscopic and microscopic. Macroscopic anatomy, or gross anatomy, is the examination of an animal's body parts using unaided eyesight. Gross anatomy also includes the branch of superficial anatomy. Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology, and also in the study of cells.
 The history of anatomy is characterized by a progressive understanding of the functions of the organs and structures of the human body. Methods have also improved dramatically, advancing from the examination of animals by dissection of carcasses and cadavers (corpses) to 20th century medical imaging techniques including X-ray, ultrasound, and magnetic resonance imaging.
"
Animal,Biology,5,"
 
 Animals (also called Metazoa) are multicellular eukaryotic organisms that form the biological kingdom Animalia. With few exceptions, animals consume organic material, breathe oxygen, are able to move, can reproduce sexually, and grow from a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million animal species in total. Animals range in length from 8.5 micrometres (0.00033 in) to 33.6 metres (110 ft). They have complex interactions with each other and their environments, forming intricate food webs. The kingdom Animalia includes humans but in colloquial use the term animal often refers only to non-human animals. The scientific study of animals is known as zoology.
 Most living animal species are in Bilateria, a clade whose members have a bilaterally symmetric body plan. The Bilateria include the protostomes—in which many groups of invertebrates are found, such as nematodes, arthropods, and molluscs—and the deuterostomes, containing both the echinoderms as well as the chordates, the latter containing the vertebrates. Life forms interpreted as early animals were present in the Ediacaran biota of the late Precambrian. Many modern animal phyla became clearly established in the fossil record as marine species during the Cambrian explosion, which began around 542 million years ago. 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago.
 Historically, Aristotle divided animals into those with blood and those without. Carl Linnaeus created the first hierarchical biological classification for animals in 1758 with his Systema Naturae, which Jean-Baptiste Lamarck expanded into 14 phyla by 1809. In 1874, Ernst Haeckel divided the animal kingdom into the multicellular Metazoa (now synonymous for Animalia) and the Protozoa, single-celled organisms no longer considered animals. In modern times, the biological classification of animals relies on advanced techniques, such as molecular phylogenetics, which are effective at demonstrating the evolutionary relationships between taxa.
 Humans make use of many other animal species, such as for food (including meat, milk, and eggs), for materials (such as leather and wool), as pets, and as working animals including for transport. Dogs have been used in hunting, while many terrestrial and aquatic animals were hunted for sports. Non-human animals have appeared in art from the earliest times and are featured in mythology and religion.
"
Antibiotic,Biology,5,"
 An antibiotic is a type of antimicrobial substance active against bacteria. It is the most important type of antibacterial agent for fighting bacterial infections, and antibiotic medications are widely used in the treatment and prevention of such infections.[1][2] They may either kill or inhibit the growth of bacteria. A limited number of antibiotics also possess antiprotozoal activity.[3][4] Antibiotics are not effective against viruses such as the common cold or influenza;[5] drugs which inhibit viruses are termed antiviral drugs or antivirals rather than antibiotics.
 Sometimes, the term antibiotic—literally ""opposing life"", from the Greek roots ἀντι anti, ""against"" and βίος bios, ""life""—is broadly used to refer to any substance used against microbes, but in the usual medical usage, antibiotics (such as penicillin) are those produced naturally (by one microorganism fighting another), whereas nonantibiotic antibacterials (such as sulfonamides and antiseptics) are fully synthetic. However, both classes have the same goal of killing or preventing the growth of microorganisms, and both are included in antimicrobial chemotherapy. ""Antibacterials"" include antiseptic drugs, antibacterial soaps, and chemical disinfectants, whereas antibiotics are an important class of antibacterials used more specifically in medicine[6] and sometimes in livestock feed.
 Antibiotics have been used since ancient times. Many civilizations used topical application of mouldy bread, with many references to its beneficial effects arising from ancient Egypt, Nubia, China, Serbia, Greece, and Rome.[citation needed] The first person to directly document the use of molds to treat infections was John Parkinson (1567–1650). Antibiotics revolutionized medicine in the 20th century. Alexander Fleming (1881–1955) discovered modern day penicillin in 1928, the widespread use of which proved significantly beneficial during wartime. However, the effectiveness and easy access to antibiotics have also led to their overuse[7] and some bacteria have evolved resistance to them.[1][8][9][10] The World Health Organization has classified antimicrobial resistance as a widespread ""serious threat [that] is no longer a prediction for the future, it is happening right now in every region of the world and has the potential to affect anyone, of any age, in any country"".[11]"
Apoptosis,Biology,5,"Apoptosis (from Ancient Greek ἀπόπτωσις, apóptōsis, ""falling off"") is a form of programmed cell death that occurs in multicellular organisms.[1] Biochemical events lead to characteristic cell changes (morphology) and death. These changes include blebbing, cell shrinkage, nuclear fragmentation, chromatin condensation, chromosomal DNA fragmentation, and global[vague] mRNA decay. The average adult human loses between 50 and 70 billion cells each day due to apoptosis.[a] For an average human child between the ages of 8 and 14, approximately 20–30 billion cells die per day.[3] In contrast to necrosis, which is a form of traumatic cell death that results from acute cellular injury, apoptosis is a highly regulated and controlled process that confers advantages during an organism's life cycle. For example, the separation of fingers and toes in a developing human embryo occurs because cells between the digits undergo apoptosis. Unlike necrosis, apoptosis produces cell fragments called apoptotic bodies that phagocytic cells are able to engulf and remove before the contents of the cell can spill out onto surrounding cells and cause damage to them.[4] Because apoptosis cannot stop once it has begun, it is a highly regulated process. Apoptosis can be initiated through one of two pathways. In the intrinsic pathway the cell kills itself because it senses cell stress, while in the extrinsic pathway the cell kills itself because of signals from other cells. Weak external signals may also activate the intrinsic pathway of apoptosis.[5] Both pathways induce cell death by activating caspases, which are proteases, or enzymes that degrade proteins. The two pathways both activate initiator caspases, which then activate executioner caspases, which then kill the cell by degrading proteins indiscriminately.
 In addition to its importance as a biological phenomenon, defective apoptotic processes have been implicated in a wide variety of diseases. Excessive apoptosis causes atrophy, whereas an insufficient amount results in uncontrolled cell proliferation, such as cancer. Some factors like Fas receptors and caspases promote apoptosis, while some members of the Bcl-2 family of proteins inhibit apoptosis.
"
Arachnology,Biology,5,"Arachnology is the scientific study of spiders and related animals such as scorpions, pseudoscorpions, and harvestmen, collectively called arachnids. Those who study spiders and other arachnids are arachnologists. More narrowly, the study of spiders alone (order Araneae) is known as araneology.[1] The word ""arachnology"" derives from Greek ἀράχνη, arachnē, ""spider""; and -λογία, -logia.
"
Archaea,Biology,5,"
 Archaea (/ɑːrˈkiːə/ (listen) or /ɑːrˈkeɪə/ ar-KEE-ə or ar-KAY-ə) (singular archaeon) constitute a domain of single-celled organisms. These microorganisms lack cell nuclei and are therefore prokaryotes. Archaea were initially classified as bacteria, receiving the name archaebacteria (in the Archaebacteria kingdom), but this classification is obsolete.[6] Archaeal cells have unique properties separating them from the other two domains, Bacteria and Eukaryota. Archaea are further divided into multiple recognized phyla. Classification is difficult because most have not been isolated in a laboratory and have been detected only by their gene sequences in environmental samples.
 Archaea and bacteria are generally similar in size and shape, although a few archaea have very different shapes, such as the flat and square cells of Haloquadratum walsbyi.[7] Despite this morphological similarity to bacteria, archaea possess genes and several metabolic pathways that are more closely related to those of eukaryotes, notably for the enzymes involved in transcription and translation. Other aspects of archaeal biochemistry are unique, such as their reliance on ether lipids in their cell membranes,[8] including archaeols. Archaea use more energy sources than eukaryotes: these range from organic compounds, such as sugars, to ammonia, metal ions or even hydrogen gas. Salt-tolerant archaea (the Haloarchaea) use sunlight as an energy source, and other species of archaea fix carbon, but unlike plants and cyanobacteria, no known species of archaea does both. Archaea reproduce asexually by binary fission, fragmentation, or budding; unlike bacteria, no known species of Archaea form endospores.
The first observed archaea were extremophiles, living in extreme environments, such as hot springs and salt lakes with no other organisms. Improved molecular detection tools led to the discovery of archaea in almost every habitat, including soil, oceans, and marshlands. Archaea are particularly numerous in the oceans, and the archaea in plankton may be one of the most abundant groups of organisms on the planet.
 Archaea are a major part of Earth's life. They are part of the microbiota of all organisms. In the human microbiome, they are important in the gut, mouth, and on the skin.[9] Their morphological, metabolic, and geographical diversity permits them to play multiple ecological roles: carbon fixation; nitrogen cycling; organic compound turnover; and maintaining microbial symbiotic and syntrophic communities, for example.[10] No clear examples of archaeal pathogens or parasites are known. Instead they are often mutualists or commensals, such as the methanogens (methane-producing strains) that inhabit the gastrointestinal tract in humans and ruminants, where their vast numbers aid digestion. Methanogens are also used in biogas production and sewage treatment, and biotechnology exploits enzymes from extremophile archaea that can endure high temperatures and organic solvents.
"
Selective_breeding,Biology,5,"
 Selective breeding (also called artificial selection) is the process by which humans use animal breeding and plant breeding to selectively develop particular phenotypic traits (characteristics) by choosing which typically animal or plant males and females will sexually reproduce and have offspring together. Domesticated animals are known as breeds, normally bred by a professional breeder, while domesticated plants are known as varieties, cultigens, cultivars, or breeds.[1] Two purebred animals of different breeds produce a crossbreed, and crossbred plants are called hybrids. Flowers, vegetables and fruit-trees may be bred by amateurs and commercial or non-commercial professionals: major crops are usually the provenance of the professionals.
 In animal breeding, techniques such as inbreeding, linebreeding, and outcrossing are utilized. In plant breeding, similar methods are used. Charles Darwin discussed how selective breeding had been successful in producing change over time in his 1859 book, On the Origin of Species. Its first chapter discusses selective breeding and domestication of such animals as pigeons, cats, cattle, and dogs. Darwin used artificial selection as a springboard to introduce and support the theory of natural selection.[2] The deliberate exploitation of selective breeding to produce desired results has become very common in agriculture and experimental biology.
 Selective breeding can be unintentional, e.g., resulting from the process of human cultivation; and it may also produce unintended – desirable or undesirable – results. For example, in some grains, an increase in seed size may have resulted from certain ploughing practices rather than from the intentional selection of larger seeds. Most likely, there has been an interdependence between natural and artificial factors that have resulted in plant domestication.[3]"
Asexual_reproduction,Biology,5,"
 Asexual reproduction is a type of reproduction that does not involve the fusion of gametes or change in the number of chromosomes.  The offspring that arise by asexual reproduction from a single cell or from a multicellular organism inherit the genes of that parent. Asexual reproduction is the primary form of reproduction for single-celled organisms such as archaea and bacteria. Many multicellular animals, plants and fungi can also reproduce asexually.[1] While all prokaryotes reproduce without the formation and fusion of gametes, mechanisms for lateral gene transfer such as conjugation, transformation and transduction can be likened to sexual reproduction in the sense of genetic recombination in meiosis.[2]"
Astrobiology,Biology,5,"
 Astrobiology, formerly known as exobiology, is an interdisciplinary scientific field concerned with the origins, early evolution, distribution, and future of life in the universe. Astrobiology considers the question of whether extraterrestrial life exists, and if it does, how humans can detect it.[2][3] Astrobiology makes use of molecular biology, biophysics, biochemistry, chemistry, astronomy, physical cosmology, exoplanetology and geology to investigate the possibility of life on other worlds and help recognize biospheres that might be different from that on Earth.[4] The origin and early evolution of life is an inseparable part of the discipline of astrobiology.[5] Astrobiology concerns itself with interpretation of existing scientific data, and although speculation is entertained to give context, astrobiology concerns itself primarily with hypotheses that fit firmly into existing scientific theories.
 This interdisciplinary field encompasses research on the origin of planetary systems, origins of organic compounds in space, rock-water-carbon interactions, abiogenesis on Earth, planetary habitability, research on biosignatures for life detection, and studies on the potential for life to adapt to challenges on Earth and in outer space.[6][7][8] Biochemistry may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old.[9][10] According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe.[11][12] According to research published in August 2015, very large galaxies may be more favorable to the creation and development of habitable planets than such smaller galaxies as the Milky Way.[13] Nonetheless, Earth is the only place in the universe humans know to harbor life.[14][15] Estimates of habitable zones around other stars,[16][17] sometimes referred to as ""Goldilocks zones,""[18][19] along with the discovery of hundreds of extrasolar planets and new insights into extreme habitats here on Earth, suggest that there may be many more habitable places in the universe than considered possible until very recently.[20][21][22] Current studies on the planet Mars by the Curiosity and Opportunity rovers are searching for evidence of ancient life as well as plains related to ancient rivers or lakes that may have been habitable.[23][24][25][26] The search for evidence of habitability, taphonomy (related to fossils), and organic molecules on the planet Mars is now a primary NASA and ESA objective.
 Even if extraterrestrial life is never discovered, the interdisciplinary nature of astrobiology, and the cosmic and evolutionary perspectives engendered by it, may still result in a range of benefits here on Earth.[27]"
Autoimmunity,Biology,5,"Autoimmunity is the system of immune responses of an organism against its own healthy cells and tissues. Any disease that results from such an aberrant immune response is termed an ""autoimmune disease"". Prominent examples include celiac disease, post-infectious IBS, diabetes mellitus type 1, Henloch Scholein Pupura (HSP) sarcoidosis, systemic lupus erythematosus (SLE), Sjögren syndrome,  eosinophilic granulomatosis with polyangiitis, Hashimoto's thyroiditis, Graves' disease, idiopathic thrombocytopenic purpura, Addison's disease, rheumatoid arthritis (RA), ankylosing spondylitis, polymyositis (PM), dermatomyositis (DM) and multiple sclerosis (MS). Autoimmune diseases are very often treated with steroids.[1]"
Autotroph,Biology,5,"An autotroph or primary producer is an organism that produces complex organic compounds (such as carbohydrates, fats, and proteins) using carbon from simple substances such as carbon dioxide,[1] generally using energy from light (photosynthesis) or inorganic chemical reactions (chemosynthesis).[2] They convert an abiotic source of energy (e.g. light) into energy stored in organic compounds, which can be used by other organisms (e.g. heterotrophs).  Autotrophs do not need a living source of carbon or energy and are the producers in a food chain, such as plants on land or algae in water (in contrast to heterotrophs as consumers of autotrophs or other heterotrophs).  Autotrophs can reduce carbon dioxide to make organic compounds for biosynthesis and as stored chemical fuel.  Most autotrophs use water as the reducing agent, but some can use other hydrogen compounds such as hydrogen sulfide.
 The primary producers can convert the energy in the light (phototroph and photoautotroph) or the energy in inorganic chemical compounds (chemotrophs or chemolithotrophs) to build organic molecules, which is usually accumulated in the form of biomass and will be used as carbon and energy source by other organisms (e.g. heterotrophs and mixotrophs). The photoautotrophs are the main primary producers, converting the energy of the light into chemical energy through photosynthesis, ultimately building organic molecules from carbon dioxide, an inorganic carbon source.[3] Examples of chemolithotrophs are some archaea and bacteria (unicellular organisms) that produce biomass from the oxidation of inorganic chemical compounds, these organisms are called chemoautotrophs, and are frequently found in hydrothermal vents in the deep ocean. Primary producers are at the lowest trophic level, and are the reasons why Earth is sustainable for life to this day.[4] Most chemoautotrophs are lithotrophs, using inorganic electron donors such as hydrogen sulfide, hydrogen gas, elemental sulfur, ammonium and ferrous oxide as reducing agents and hydrogen sources for biosynthesis and chemical energy release. Autotrophs use a portion of the ATP produced during photosynthesis or the oxidation of chemical compounds to reduce NADP+ to NADPH to form organic compounds.[5]"
B_cell,Biology,5,"B cells, also known as B lymphocytes, are a type of white blood cell of the lymphocyte subtype.[1] They function in the humoral immunity component of the adaptive immune system by secreting antibodies.[1] Additionally, B cells present antigens (they are also classified as professional antigen-presenting cells (APCs)) and secrete cytokines.[1]
In mammals, B cells mature in the bone marrow, which is at the core of most bones.[2] In birds, B cells mature in the bursa of Fabricius, a lymphoid organ where they were first discovered by Chang and Glick,[2] (B for bursa) and not from bone marrow as commonly believed.
 B cells, unlike the other two classes of lymphocytes, T cells and natural killer cells, express B cell receptors (BCRs) on their cell membrane.[1] BCRs allow the B cell to bind to a specific antigen, against which it will initiate an antibody response.[1]"
Bacteria,Biology,5,"
 Bacteria (/bækˈtɪəriə/ (listen); common noun bacteria, singular bacterium) are a type of biological cell. They constitute a large domain of prokaryotic microorganisms. Typically a few micrometres in length, bacteria have a number of shapes, ranging from spheres to rods and spirals. Bacteria were among the first life forms to appear on Earth, and are present in most of its habitats. Bacteria inhabit soil, water, acidic hot springs, radioactive waste,[4] and the deep biosphere of the earth's crust. Bacteria also live in symbiotic and parasitic relationships with plants and animals. Most bacteria have not been characterised, and only about 27 percent of the bacterial phyla have species that can be grown in the laboratory.[5] The study of bacteria is known as bacteriology, a branch of microbiology.
 Nearly all animal life is dependent on bacteria for survival as only bacteria and some archaea possess the genes and enzymes necessary to synthesize vitamin B12, also known as cobalamin, and provide it through the food chain.  Vitamin B12 is a water-soluble vitamin that is involved in the metabolism of every cell of the human body.  It is a cofactor in DNA synthesis, and in both fatty acid and amino acid metabolism. It is particularly important in the normal functioning of the nervous system via its role in the synthesis of myelin.[6][7][8][9] There are typically 40 million bacterial cells in a gram of soil and a million bacterial cells in a millilitre of fresh water. There are approximately 5×1030 bacteria on Earth,[10] forming a biomass which is only exceeded by plants.[11] Bacteria are vital in many stages of the nutrient cycle by recycling nutrients such as the fixation of nitrogen from the atmosphere. The nutrient cycle includes the decomposition of dead bodies; bacteria are responsible for the putrefaction stage in this process.[12] In the biological communities surrounding hydrothermal vents and cold seeps, extremophile bacteria provide the nutrients needed to sustain life by converting dissolved compounds, such as hydrogen sulphide and methane, to energy.
 In humans and most animals, the largest number of bacteria exist in the gut, and a large number on the skin.[13] The vast majority of the bacteria in the body are rendered harmless by the protective effects of the immune system, though many are beneficial, particularly in the gut flora. However, several species of bacteria are pathogenic and cause infectious diseases, including cholera, syphilis, anthrax, leprosy, and bubonic plague. The most common fatal bacterial diseases are respiratory infections.  Tuberculosis alone kills about 2 million people per year, mostly in sub-Saharan Africa.[14] Antibiotics are used to treat bacterial infections and are also used in farming, making antibiotic resistance a growing problem. In industry, bacteria are important in sewage treatment and the breakdown of oil spills, the production of cheese and yogurt through fermentation, the recovery of gold, palladium, copper and other metals in the mining sector,[15] as well as in biotechnology, and the manufacture of antibiotics and other chemicals.[16] Once regarded as plants constituting the class Schizomycetes (""fission fungi""), bacteria are now classified as prokaryotes. Unlike cells of animals and other eukaryotes, bacterial cells do not contain a nucleus and rarely harbour membrane-bound organelles. Although the term bacteria traditionally included all prokaryotes, the scientific classification changed after the discovery in the 1990s that prokaryotes consist of two very different groups of organisms that evolved from an ancient common ancestor. These evolutionary domains are called Bacteria and Archaea.[1]"
Bacteriophage,Biology,5,"
 A bacteriophage (/bækˈtɪərioʊfeɪdʒ/), also known informally as a phage (/feɪdʒ/), is a virus that infects and replicates within bacteria and archaea. The term was derived from ""bacteria"" and the Greek φαγεῖν (phagein), meaning ""to devour"". Bacteriophages are composed of proteins that encapsulate a DNA or RNA genome, and may have structures that are either simple or elaborate. Their genomes may encode as few as four genes (e.g. MS2) and as many as hundreds of genes. Phages replicate within the bacterium following the injection of their genome into its cytoplasm.
 Bacteriophages are among the most common and diverse entities in the biosphere.[1] Bacteriophages are ubiquitous viruses, found wherever bacteria exist. It is estimated there are more than 1031 bacteriophages on the planet, more than every other organism on Earth, including bacteria, combined.[2] One of the densest natural sources for phages and other viruses is seawater, where up to 9x108 virions per millilitre have been found in microbial mats at the surface,[3] and up to 70% of marine bacteria may be infected by phages.[4] Phages have been used since the late 20th century as an alternative to antibiotics in the former Soviet Union and Central Europe, as well as in France.[5][6] They are seen as a possible therapy against multi-drug-resistant strains of many bacteria (see phage therapy).[7] On the other hand, phages of Inoviridae have been shown to complicate biofilms involved in pneumonia and cystic fibrosis and to shelter the bacteria from drugs meant to eradicate disease, thus promoting persistent infection.[8]"
Barr_body,Biology,5,"A Barr body (named after discoverer Murray Barr)[1] is an inactive X chromosome in a cell with more than one X chromosome,[2] rendered inactive in a process called lyonization, in species with  XY sex-determination  (including humans). The Lyon hypothesis states that in cells with multiple X chromosomes, all but one are inactivated during mammalian embryogenesis.[3]  This happens early in embryonic development at random in mammals,[4] except in marsupials and in some extra-embryonic tissues of some placental mammals, in which the X chromosome from the sperm is always deactivated.[5] In humans with more than one X chromosome, the number of Barr bodies visible at interphase is always one fewer than the total number of X chromosomes. For example, people with Klinefelter syndrome (47,XXY karyotype) have a single Barr body, and people with a 47, XXX karyotype have two Barr bodies. Barr bodies can be seen in the nucleus of neutrophils, at the rim of the nucleus in female somatic cells between divisions.
"
Basal_body,Biology,5,"A basal body (synonymous with basal granule, kinetosome, and in older cytological literature with blepharoplast) is a protein structure found at the base of a eukaryotic undulipodium (cilium or flagellum). It is formed from a centriole and several additional protein structures, and is, essentially, a modified centriole.[1][2] The basal body serves as a nucleation site for the growth of the axoneme microtubules.  Centrioles, from which basal bodies are derived, act as anchoring sites for proteins that in turn anchor microtubules, and are known as the microtubule organizing center (MTOC). These microtubules provide structure and facilitate movement of vesicles and organelles within many eukaryotic cells.
"
Behavioral_ecology,Biology,5,"Behavioral ecology, also spelled behavioural ecology, is the study of the evolutionary basis for animal behavior due to ecological pressures. Behavioral ecology emerged from ethology after Niko Tinbergen outlined four questions to address when studying animal behaviors: What are the proximate causes, ontogeny, survival value, and phylogeny of a behavior?
 If an organism has a trait that provides a selective advantage (i.e., has adaptive significance) in its environment, then natural selection favors it. Adaptive significance refers to the expression of a trait that affects fitness, measured by an individual's reproductive success. Adaptive traits are those that produce more copies of the individual's genes in future generations. Maladaptive traits are those that leave fewer. For example, if a bird that can call more loudly attracts more mates, then a loud call is an adaptive trait for that species because a louder bird mates more frequently than less loud birds—thus sending more loud-calling genes into future generations.
 Individuals are always in competition with others for limited resources, including food, territories, and mates. Conflict occurs between predators and prey, between rivals for mates, between siblings, mates, and even between parents and offspring.
"
Bile,Biology,5,"Bile (from latin bilis), or gall, is a dark-green-to-yellowish-brown fluid produced by the liver of most vertebrates that aids the digestion of lipids in the small intestine. In humans, bile is produced continuously by the liver (liver bile) and stored and concentrated in the gallbladder. After eating, this stored bile is discharged into the duodenum.
 The composition of hepatic bile is (97–98)% water, 0.7%[1] bile salts, 0.2% bilirubin, 0.51% fats (cholesterol, fatty acids, and lecithin),[1] and 200 meq/l inorganic salts.[2] The two main pigments of bile are bilirubin, which is orange–yellow, and its oxidised form biliverdin, which is green. When mixed, they are responsible for the brown color of feces.[3] About 400 to 800 millilitres of bile is produced per day in adult human beings.[4]"
Fission_(biology),Biology,5,"Fission, in biology, is the division of a single entity into two or more parts and the regeneration of those parts to separate entities resembling the original. The object experiencing fission is usually a cell, but the term may also refer to how organisms, bodies, populations, or species split into discrete parts.[1][2][3] The fission may be binary fission, in which a single organism produces two parts, or multiple fission, in which a single entity produces multiple parts.
"
Binomial_nomenclature,Biology,5,"
 Binomial nomenclature (""two-term naming system""), also called binominal nomenclature (""two-name naming system"") or binary nomenclature, is a formal system of naming species of living things by giving each a name composed of two parts, both of which use Latin grammatical forms, although they can be based on words from other languages. Such a name is called a binomial name (which may be shortened to just ""binomial""), a binomen, binominal name or a scientific name; more informally it is also called a Latin name.
 The first part of the name – the generic name – identifies the genus to which the species belongs, while the second part – the specific name or specific epithet – identifies the species within the genus. For example, modern humans belong to the genus Homo and within this genus to the species Homo sapiens. Tyrannosaurus rex is probably the most widely known binomial.[1] The formal introduction of this system of naming species is credited to Carl Linnaeus, effectively beginning with his work Species Plantarum in 1753.[2] But Gaspard Bauhin, in as early as 1622, had introduced in his book Pinax theatri botanici (English, Illustrated exposition of plants) many names of genera that were later adopted by Linnaeus.[3] The application of binomial nomenclature is now governed by various internationally agreed codes of rules, of which the two most important are the International Code of Zoological Nomenclature (ICZN) for animals and the International Code of Nomenclature for algae, fungi, and plants (ICNafp). Although the general principles underlying binomial nomenclature are common to these two codes, there are some differences, both in the terminology they use and in their precise rules.
 In modern usage, the first letter of the first part of the name, the genus, is always capitalized in writing, while that of the second part is not, even when derived from a proper noun such as the name of a person or place. Similarly, both parts are italicized when a binomial name occurs in normal text (or underlined in handwriting). Thus the binomial name of the annual phlox (named after botanist Thomas Drummond) is now written as Phlox drummondii.
 In scientific works, the authority for a binomial name is usually given, at least when it is first mentioned, and the date of publication may be specified.
"
Biocatalysis,Biology,5,"Biocatalysis refers to the use of living (biological) systems or their parts to speed up (catalyze) chemical reactions. In biocatalytic processes, natural catalysts, such as enzymes, perform chemical transformations on organic compounds. Both enzymes that have been more or less isolated and enzymes still residing inside living cells are employed for this task.[1][2][3] Modern biotechnology, specifically directed evolution, has made the production of modified or non-natural enzymes  possible. This has enabled the development of enzymes that can catalyze novel small molecule transformations that may be difficult or impossible using classical synthetic organic chemistry. Utilizing natural or modified enzymes to perform organic synthesis is termed chemoenzymatic synthesis; the reactions performed by the enzyme are classified as chemoenzymatic reactions.
"
Biochemistry,Biology,5,"Biochemistry or biological chemistry, is the study of chemical processes within and relating to living organisms.[1]  A sub-discipline of both biology and chemistry, biochemistry may be divided into three fields: structural biology, enzymology and metabolism.  Over the last decades of the 20th century, biochemistry has become successful at explaining living processes through these three disciplines.  Almost all areas of the life sciences are being uncovered and developed through biochemical methodology and research.[2]  Biochemistry focuses on understanding the chemical basis which allows biological molecules to give rise to the processes that occur within living cells and between cells,[3] in turn relating greatly to the understanding of tissues and organs, as well as organism structure and function.[4] Biochemistry is closely related to molecular biology which is the study of the molecular mechanisms of biological phenomena.[5] Much of biochemistry deals with the structures, functions, and interactions of biological macromolecules, such as proteins, nucleic acids, carbohydrates, and lipids.  They provide the structure of cells and perform many of the functions associated with life.[6]  The chemistry of the cell also depends upon the reactions of small molecules and ions.  These can be inorganic (for example, water and metal ions) or organic (for example, the amino acids, which are used to synthesize proteins).[7]  The mechanisms used by cells to harness energy from their environment via chemical reactions are known as metabolism.  The findings of biochemistry are applied primarily in medicine, nutrition and agriculture.  In medicine, biochemists investigate the causes and cures of diseases.[8]  Nutrition studies how to maintain health and wellness and also the effects of nutritional deficiencies.[9]  In agriculture, biochemists investigate soil and fertilizers. Improving crop cultivation, crop storage, and pest control are also goals.
"
Biodiversity,Biology,5,"
 Biodiversity is the variety and variability of life on Earth. Biodiversity is typically a measure of variation at the genetic, species, and ecosystem level.[1] Terrestrial biodiversity is usually greater near the equator,[2] which is the result of the warm climate and high primary productivity.[3] Biodiversity is not distributed evenly on Earth, and is richer in the tropics.[4] These tropical forest ecosystems cover less than 10 percent of earth's surface, and contain about 90 percent of the world's species.[5] Marine biodiversity is usually higher along coasts in the Western Pacific, where sea surface temperature is highest, and in the mid-latitudinal band in all oceans.[6] There are latitudinal gradients in species diversity.[6] Biodiversity generally tends to cluster in hotspots,[7] and has been increasing through time,[8][9] but will be likely to slow in the future.[10] Rapid environmental changes typically cause mass extinctions.[11][12][13] More than 99.9 percent of all species that ever lived on Earth, amounting to over five billion species,[14] are estimated to be extinct.[15][16] Estimates on the number of Earth's current species range from 10 million to 14 million,[17] of which about 1.2 million have been documented and over 86 percent have not yet been described.[18] More recently, in May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.[19] The total amount of related DNA base pairs on Earth is estimated at 5.0 x 1037 and weighs 50 billion tonnes.[20] In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).[21] In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.[22] The age of the Earth is about 4.54 billion years.[23][24][25] The earliest undisputed evidence of life on Earth dates at least from 3.5 billion years ago,[26][27][28] during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. There are microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia.[29][30][31] Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old meta-sedimentary rocks discovered in Western Greenland.[32] More recently, in 2015, ""remains of biotic life"" were found in 4.1 billion-year-old rocks in Western Australia.[33][34] According to one of the researchers, ""If life arose relatively quickly on Earth .. then it could be common in the universe.""[33] Since life began on Earth, five major mass extinctions and several minor events have led to large and sudden drops in biodiversity. The Phanerozoic eon (the last 540 million years) marked a rapid growth in biodiversity via the Cambrian explosion—a period during which the majority of multicellular phyla first appeared.[35] The next 400 million years included repeated, massive biodiversity losses classified as mass extinction events. In the Carboniferous, rainforest collapse led to a great loss of plant and animal life.[36] The Permian–Triassic extinction event, 251 million years ago, was the worst; vertebrate recovery took 30 million years.[37] The most recent, the Cretaceous–Paleogene extinction event, occurred 65 million years ago and has often attracted more attention than others because it resulted in the extinction of the non-avian dinosaurs.[38] The period since the emergence of humans has displayed an ongoing biodiversity reduction and an accompanying loss of genetic diversity. Named the Holocene extinction, the reduction is caused primarily by human impacts, particularly habitat destruction.[39] Conversely, biodiversity positively impacts human health in a number of ways, although a few negative effects are studied.[40] The United Nations designated 2011–2020 as the United Nations Decade on Biodiversity.[41] and 2021–2030 as the United Nations Decade on Ecosystem Restoration[42] According to a 2019 Global Assessment Report on Biodiversity and Ecosystem Services by IPBES, 25% of plant and animal species are threatened with extinction as the result of human activity.[43][44][45] An October 2020 IPBES report found the same human actions which drive biodiversity loss have also resulted in an increase in pandemics.[46] In 2020, the fifth edition of the UN's Global Biodiversity Outlook report,[47] which served as a “final report card” for the Aichi Biodiversity Targets, a series of 20 objectives set out in 2010, at the beginning of the UN's Decade on Biodiversity, most of which were supposed to be reached by the end of the year 2020, stated that none of the targets – which concern the safeguarding of ecosystems, and the promotion of sustainability – have been fully met.[48]"
Bioengineering,Biology,5,"Biological engineering, 
bioengineering, or bio-engineering is the application of principles of biology and the tools of engineering to create usable, tangible, economically-viable products.[1]  Biological engineering employs knowledge and expertise from a number of pure and applied sciences,[2] such as mass and heat transfer, kinetics, biocatalysts, biomechanics, bioinformatics, separation and purification processes, bioreactor design, surface science, fluid mechanics, thermodynamics, and polymer science. It is used in the design of medical devices, diagnostic equipment, biocompatible materials, renewable bioenergy, ecological engineering, agricultural engineering, and other areas that improve the living standards of societies. Examples of bioengineering research include bacteria engineered to produce chemicals, new medical imaging technology, portable and rapid disease diagnostic devices, prosthetics, biopharmaceuticals, and tissue-engineered organs.[3][4]  Bioengineering overlaps substantially with biotechnology and the biomedical sciences[5] in a way analogous to how various other forms of engineering and technology relate to various other sciences (such as aerospace engineering and other space technology to kinetics and astrophysics).
 In general, biological engineers (or biomedical engineers) attempt to either mimic biological systems to create products or modify and control biological systems so that they can replace, augment, sustain, or predict chemical and mechanical processes.[6] Bioengineers can apply their expertise to other applications of engineering and biotechnology, including genetic modification of plants and microorganisms, bioprocess engineering, and biocatalysis. Working with doctors, clinicians, and researchers, bioengineers use traditional engineering principles and techniques and apply them to real-world biological and medical problems.[7]"
Bioenergetics,Biology,5,"Bioenergetics is a field in biochemistry and cell biology that concerns energy flow through living systems.[1] This is an active area of biological research that includes the study of the transformation of energy in living organisms and the study of thousands of different cellular processes such as cellular respiration and the many other metabolic and enzymatic processes that lead to production and utilization of energy in forms such as adenosine triphosphate (ATP) molecules.[2][3] That is, the goal of bioenergetics is to describe how living organisms acquire and transform energy in order to perform biological work.[4] The study of metabolic pathways is thus essential to bioenergetics.
"
Biogeography,Biology,5,"Biogeography is the study of the distribution of species and ecosystems in geographic space and through geological time. Organisms and biological communities often vary in a regular fashion along geographic gradients of latitude, elevation, isolation and habitat area.[1] Phytogeography is the branch of biogeography that studies the distribution of plants. Zoogeography is the branch that studies distribution of animals. Mycogeography is the branch that studies distribution of fungi, such as mushrooms. 
 Knowledge of spatial variation in the numbers and types of organisms is as vital to us today as it was to our early human ancestors, as we adapt to heterogeneous but geographically predictable environments. Biogeography is an integrative field of inquiry that unites concepts and information from ecology, evolutionary biology, taxonomy, geology, physical geography, palaeontology,  and climatology.[2][3] Modern biogeographic research combines information and ideas from many fields, from the physiological and ecological constraints on organismal dispersal to geological and climatological phenomena operating at global spatial scales and evolutionary time frames.
 The short-term interactions within a habitat and species of organisms describe the ecological application of biogeography. Historical biogeography describes the long-term, evolutionary periods of time for broader classifications of organisms.[4] Early scientists, beginning with Carl Linnaeus, contributed to the development of biogeography as a science. Beginning in the mid-18th century, Europeans explored the world and discovered the biodiversity of life.
 The scientific theory of biogeography grows out of the work of Alexander von Humboldt (1769–1859),[5] Hewett Cottrell Watson (1804–1881),[6] Alphonse de Candolle (1806–1893),[7] Alfred Russel Wallace (1823–1913),[8] Philip Lutley Sclater (1829–1913) and other biologists and explorers.[9]"
Bioinformatics,Biology,5,"
 Bioinformatics /ˌbaɪ.oʊˌɪnfərˈmætɪks/ (listen) is an interdisciplinary field that develops methods and software tools for understanding biological data, in particular when the data sets are large and complex. As an interdisciplinary field of science, bioinformatics combines biology, computer science, information engineering, mathematics and statistics to analyze and interpret the biological data. Bioinformatics has been used for in silico analyses of biological queries using mathematical and statistical techniques.[clarification needed] Bioinformatics includes biological studies that use computer programming as part of their methodology, as well as a specific analysis ""pipelines"" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidates genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organizational principles within nucleic acid and protein sequences, called proteomics.[1]"
Biological_organization,Biology,5,"
 Biological organization is the hierarchy of complex biological structures and systems that define life using a reductionistic approach.[1] The traditional hierarchy, as detailed below, extends from atoms to biospheres. The higher levels of this scheme are often referred to as an ecological organization concept, or as the field,  hierarchical ecology.
 Each level in the hierarchy represents an increase in organizational complexity, with each ""object"" being primarily composed of the previous level's basic unit.[2] The basic principle behind the organization is the concept of emergence—the properties and functions found at a hierarchical level are not present and irrelevant at the lower levels.
 The biological organization of life is a fundamental premise for numerous areas of scientific research, particularly in the medical sciences. Without this necessary degree of organization, it would be much more difficult—and likely impossible—to apply the study of the effects of various physical and chemical phenomena to diseases and physiology (body function). For example, fields such as cognitive and behavioral neuroscience could not exist if the brain was not composed of specific types of cells, and the basic concepts of pharmacology could not exist if it was not known that a change at the cellular level can affect an entire organism. These applications extend into the ecological levels as well. For example, DDT's direct insecticidal effect occurs at the subcellular level, but affects higher levels up to and including multiple ecosystems. Theoretically, a change in one atom could change the entire biosphere.
"
Biology,Biology,5,"
 Biology is the natural science that studies life and living organisms, including their physical structure, chemical processes, molecular interactions, physiological mechanisms, development and evolution.[1] Despite the complexity of the science, certain unifying concepts consolidate it into a single, coherent field. Biology recognizes the cell as the basic unit of life, genes as the basic unit of heredity, and evolution as the engine that propels the creation and extinction of species. Living organisms are open systems that survive by transforming energy and decreasing their local entropy[2] to maintain a stable and vital condition defined as homeostasis.[3] Sub-disciplines of biology are defined by the research methods employed and the kind of system studied: theoretical biology uses mathematical methods to formulate quantitative models while experimental biology performs empirical experiments to test the validity of proposed theories and understand the mechanisms underlying life and how it appeared and evolved from non-living matter about 4 billion years ago through a gradual increase in the complexity of the system.[4][5][6]"
Biomass,Biology,5,"Biomass is a modern name for the ancient technology of burning plant or animal material for energy production (electricity or heat), or in various industrial processes as raw substance for a range of products.[1] It can be purposely grown energy crops (e.g. miscanthus, switchgrass), wood or forest residues, waste from food crops (wheat straw, bagasse), horticulture (yard waste), food processing (corn cobs), animal farming (manure, rich in nitrogen and phosphorus), or human waste from sewage plants.[2] Burning plant-derived biomass releases CO2.[3] [4] 
The issue of whether biomass is carbon neutral is contested. [3][5] 
Material directly combusted in cook stoves produces pollutants, leading to severe health and environmental consequences. Agriculture to grow the biomass inputs produces CO2 as well, through aeration (tilling), fertilizer, deforestation, and the supply chain.[6] Offsetting these impacts, photosynthesis eventually cycles the CO2 back into new crops and to the soil.[4]
The EU and UN consider biomass a renewable energy source. 
"
Biomathematics,Biology,5,"Mathematical and theoretical biology is a branch of biology which employs theoretical analysis, mathematical models and abstractions of the living organisms to investigate the principles that govern the structure, development and behavior of the systems, as opposed to experimental biology which deals with the conduction of experiments to prove and validate the scientific theories.[1] The field is sometimes called mathematical biology or biomathematics to stress the mathematical side, or theoretical biology to stress the biological side.[2] Theoretical biology focuses more on the development of theoretical principles for biology while mathematical biology focuses on the use of mathematical tools to study biological systems, even though the two terms are sometimes interchanged.[3][4] Mathematical biology aims at the mathematical representation and modeling of biological processes, using techniques and tools of applied mathematics and it can be useful in both theoretical and practical research. Describing systems in a quantitative manner means their behavior can be better simulated, and hence properties can be predicted that might not be evident to the experimenter. This requires precise mathematical models.
 Because of the complexity of the living systems, theoretical biology employs several fields of mathematics,[5] and has contributed to the development of new techniques.
"
Biome,Biology,5,"A biome /ˈbaɪoʊm/ is a community of plants and animals that have common characteristics for the environment they exist in. They can be found over a range of continents. Biomes are distinct biological communities that have formed in response to a shared physical climate.[1][2] Biome is a broader term than habitat; any biome can comprise a variety of habitats.
 While a biome can cover large areas, a microbiome is a mix of organisms that coexist in a defined space on a much smaller scale.  For example, the human microbiome is the collection of bacteria, viruses, and other microorganisms that are present on or in a human body.[3] A 'biota' is the total collection of organisms of a geographic region or a time period, from local geographic scales and instantaneous temporal scales all the way up to whole-planet and whole-timescale spatiotemporal scales. The biotas of the Earth make up the biosphere.
"
Biomechanics,Biology,5,"
 Biomechanics is the study of the structure, function and motion of the mechanical aspects of biological systems, at any level from whole organisms to organs, cells and cell organelles,[1] using the methods of mechanics.[2] Biomechanics is a branch of biophysics.
"
Biomedical_engineering,Biology,5,"Biomedical engineering (BME) or medical engineering is the application of engineering principles and design concepts to medicine and biology for healthcare purposes (e.g., diagnostic or therapeutic). BME is also traditionally known as ""bioengineering"", but this term has come to also refer to biological engineering. This field seeks to close the gap between engineering and medicine, combining the design and problem solving skills of engineering with medical biological sciences to advance health care treatment, including diagnosis, monitoring, and therapy.[1][2] Also included under the scope of a biomedical engineer is the management of current medical equipment within hospitals while adhering to relevant industry standards. This involves making equipment recommendations, procurement, routine testing and preventive maintenance, a role also known as a Biomedical Equipment Technician (BMET) or as clinical engineering.
 Biomedical engineering has recently emerged as its own study, as compared to many other engineering fields. Such an evolution is common as a new field transitions from being an interdisciplinary specialization among already-established fields, to being considered a field in itself. Much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). Prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, common imaging equipment such as MRIs and EKG/ECGs, regenerative tissue growth, pharmaceutical drugs and therapeutic biologicals.
"
Biomedical_research,Biology,5,"Medical research (or biomedical research), also known as experimental medicine, encompasses a wide array of research, extending from ""basic research"" (also called bench science or bench research),[1] – involving fundamental scientific principles that may apply to a preclinical understanding – to clinical research, which involves studies of people who may be subjects in clinical trials. Within this spectrum is applied research, or translational research, conducted to expand knowledge in the field of medicine.
 Both clinical and preclinical research phases exist in the pharmaceutical industry's drug development pipelines, where the clinical phase is denoted by the term clinical trial. However, only part of the clinical or preclinical research is oriented towards a specific pharmaceutical purpose. The need for fundamental and mechanism-based understanding, diagnostics, medical devices, and non-pharmaceutical therapies means that pharmaceutical research is only a small part of medical research.
 The increased longevity of humans over the past century can be significantly attributed to advances resulting from medical research. Among the major benefits of medical research have been vaccines for measles and polio, insulin treatment for diabetes, classes of antibiotics for treating a host of maladies, medication for high blood pressure, improved treatments for AIDS, statins and other treatments for atherosclerosis, new surgical techniques such as microsurgery, and increasingly successful treatments for cancer. New, beneficial tests and treatments are expected as a result of the Human Genome Project. Many challenges remain, however, including the appearance of antibiotic resistance and the obesity epidemic.
 Most of the research in the field is pursued by biomedical scientists, but significant contributions are made by other type of biologists. Medical research on humans, has to strictly follow the medical ethics sanctioned in the Declaration of Helsinki and hospital review board where the research is conducted. In all cases, research ethics are expected.
"
Biomolecule,Biology,5,"A biomolecule or biological molecule is a loosely used term for molecules present in organisms that are essential to one or more typically biological processes, such as cell division, morphogenesis, or development.[1] Biomolecules include large macromolecules (or polyanions) such as proteins, carbohydrates, lipids, and nucleic acids, as well as small molecules such as primary metabolites, secondary metabolites and natural products. A more general name for this class of material is biological materials. Biomolecules are an important element of living organisms, those biomolecules are often endogenous,[2] produced within the organism[3] but organisms usually need exogenous biomolecules, for example certain nutrients, to survive. 
 Biology and its subfields of biochemistry and molecular biology study biomolecules and their reactions. Most biomolecules are organic compounds, and just four elements—oxygen, carbon, hydrogen, and nitrogen—make up 96% of the human body's mass. But many other elements, such as the various biometals,  are present in small amounts.
 The uniformity of both specific types of molecules (the biomolecules) and of certain metabolic pathways are invariant features among the wide diversity of life forms; thus these biomolecules and metabolic pathways are referred to as ""biochemical universals""[4] or ""theory of material unity of the living beings"", a unifying concept in biology, along with cell theory and evolution theory.[5]"
Biophysics,Biology,5,"Biophysics is an interdisciplinary science that applies approaches and methods traditionally used in physics to study biological phenomena.[1][2][3] Biophysics covers all scales of biological organization, from molecular to organismic and populations. Biophysical research shares significant overlap with biochemistry, molecular biology, physical chemistry, physiology, nanotechnology, bioengineering, computational biology, biomechanics, developmental biology and systems biology.
 The term biophysics was originally introduced by Karl Pearson in 1892.[4][5] Ambiguously, the term biophysics is also regularly used in academia to indicate the study of the physical quantities (e.g. electric current, temperature, stress, entropy) in biological systems, which is, by definition, performed by physiology. Nevertheless, other biological sciences also perform research on the biophysical properties of living organisms including molecular biology, cell biology, biophysics, and biochemistry.
"
Biosynthesis,Biology,5,"Biosynthesis is a multi-step, enzyme-catalyzed process where substrates are converted into more complex products in living organisms. In biosynthesis, simple compounds are modified, converted into other compounds, or joined together to form macromolecules. This process often consists of metabolic pathways. Some of these biosynthetic pathways are located within a single cellular organelle, while others involve enzymes that are located within multiple cellular organelles. Examples of these biosynthetic pathways include the production of lipid membrane components and nucleotides. Biosynthesis is usually synonymous with anabolism.
 The prerequisite elements for biosynthesis include: precursor compounds, chemical energy (e.g. ATP), and catalytic enzymes which may require coenzymes (e.g.NADH, NADPH). These elements create monomers, the building blocks for macromolecules. Some important biological macromolecules include: proteins, which are composed of amino acid monomers joined via peptide bonds, and DNA molecules, which are composed of nucleotides joined via phosphodiester bonds.
"
Biotechnology,Biology,5,"
 Biotechnology is a broad area of biology, involving the use of living systems and organisms to develop or make products. Depending on the tools and applications, it often overlaps with related scientific fields. In the late 20th and early 21st centuries, biotechnology has expanded to include new and diverse sciences, such as genomics, recombinant gene techniques, applied immunology, and development of pharmaceutical therapies and diagnostic tests. The term ""Biotechnology"" was first used by ""Karl Ereky"" in 1919, meaning the production of products from raw materials with the aid of living organisms.
"
Bipedal,Biology,5,"Bipedalism is a form of terrestrial locomotion where an organism moves by means of its two rear limbs or legs. An animal or machine that usually moves in a bipedal manner is known as a biped /ˈbaɪpɛd/, meaning ""two feet"" (from the Latin bis for ""double"" and pes for ""foot""). Types of bipedal movement include walking, running and hopping.
 Few modern species are habitual bipeds whose normal method of locomotion is two-legged. Within mammals, habitual bipedalism has evolved multiple times, with the macropods, kangaroo rats and mice, springhare,[4] hopping mice, pangolins and hominin apes (australopithecines and humans) as well as various other extinct groups evolving the trait independently. In the Triassic period some groups of archosaurs (a group that includes crocodiles and dinosaurs) developed bipedalism; among the dinosaurs, all the early forms and many later groups were habitual or exclusive bipeds; the birds are members of a clade of exclusively bipedal dinosaurs, the theropods.
 A larger number of modern species intermittently or briefly use a bipedal gait. Several lizard species move bipedally when running, usually to escape from threats. Many primate and bear species will adopt a bipedal gait in order to reach food or explore their environment, though there are a few cases where they walk on their hind limbs only. Several arboreal primate species, such as gibbons and indriids, exclusively walk on two legs during the brief periods they spend on the ground. Many animals rear up on their hind legs while fighting or copulating. Some animals commonly stand on their hind legs to reach food, keep watch, threaten a competitor or predator, or pose in courtship, but do not move bipedally.
"
Birth,Biology,5,"Birth is the act or process of bearing or bringing forth offspring,[1] also referred to in technical contexts as parturition. In mammals, the process is initiated by hormones which cause the muscular walls of the uterus to contract, expelling the fetus at a developmental stage when it is ready to feed and breathe. 
 In some species the offspring is precocial and can move around almost immediately after birth but in others it is altricial and completely dependent on parenting. 
 In marsupials, the fetus is born at a very immature stage after a short gestational period and develops further in its mother's womb's  pouch.
 It is not only mammals that give birth. Some reptiles, amphibians, fish and invertebrates carry their developing young inside them. Some of these are ovoviviparous, with the eggs being hatched inside the mother's body, and others are viviparous, with the embryo developing inside her body, as in the case of mammals.
"
Blastocyst,Biology,5,"The blastocyst is a structure formed in the early development of mammals. It possesses an inner cell mass (ICM) which subsequently forms the embryo. The outer layer of the blastocyst consists of cells collectively called the trophoblast. This layer surrounds the inner cell mass and a fluid-filled cavity known as the blastocoel. The trophoblast gives rise to the placenta. The name ""blastocyst"" arises from the Greek βλαστός blastos (""a sprout"") and κύστις kystis (""bladder, capsule""). In other animals this is called a blastula.
 In humans, blastocyst formation begins about 5 days after fertilization when a fluid-filled cavity opens up in the morula, the early embryonic stage of a ball of 16 cells.
The blastocyst has a diameter of about 0.1–0.2 mm and comprises 200–300 cells following rapid cleavage (cell division). About seven days after fertilization,[1] the blastocyst undergoes implantation, embedding into the endometrium of the uterine wall. There it will undergo further developmental processes, including gastrulation. Embedding of the blastocyst into the endometrium requires that it hatches from the zona pellucida, which prevents adherence to the fallopian tube as the pre-embryo makes its way to the uterus.
 The use of blastocysts in in vitro fertilization (IVF) involves culturing a fertilized egg for five days before transferring it into the uterus. It can be a more viable method of fertility treatment than traditional IVF. The inner cell mass of blastocysts is the source of embryonic stem cells, which are broadly applicable in stem cell therapies including cell repair, replacement and regeneration.
"
Blood,Biology,5,"
 Blood is a body fluid in humans and other animals that delivers necessary substances such as nutrients and oxygen to the cells and transports metabolic waste products away from those same cells.[1] In vertebrates, it is composed of blood cells suspended in blood plasma. Plasma, which constitutes 55% of blood fluid, is mostly water (92% by volume),[2] and contains proteins, glucose, mineral ions, hormones, carbon dioxide (plasma being the main medium for excretory product transportation), and blood cells themselves. Albumin is the main protein in plasma, and it functions to regulate the colloidal osmotic pressure of blood. The blood cells are mainly red blood cells (also called RBCs or erythrocytes), white blood cells (also called WBCs or leukocytes) and platelets (also called thrombocytes). The most abundant cells in vertebrate blood are red blood cells. These contain hemoglobin, an iron-containing protein, which facilitates oxygen transport by reversibly binding to this respiratory gas and greatly increasing its solubility in blood. In contrast, carbon dioxide is mostly transported extracellularly as bicarbonate ion transported in plasma.
 Vertebrate blood is bright red when its hemoglobin is oxygenated and dark red when it is deoxygenated. Some animals, such as crustaceans and mollusks, use hemocyanin to carry oxygen, instead of hemoglobin. Insects and some mollusks use a fluid called hemolymph instead of blood, the difference being that hemolymph is not contained in a closed circulatory system. In most insects, this ""blood"" does not contain oxygen-carrying molecules such as hemoglobin because their bodies are small enough for their tracheal system to suffice for supplying oxygen.
 Jawed vertebrates have an adaptive immune system, based largely on white blood cells. White blood cells help to resist infections and parasites. Platelets are important in the clotting of blood. Arthropods, using hemolymph, have hemocytes as part of their immune system.
 Blood is circulated around the body through blood vessels by the pumping action of the heart. In animals with lungs, arterial blood carries oxygen from inhaled air to the tissues of the body, and venous blood carries carbon dioxide, a waste product of metabolism produced by cells, from the tissues to the lungs to be exhaled.
 Medical terms related to blood often begin with hemo- or hemato- (also spelled haemo- and haemato-) from the Greek word αἷμα (haima) for ""blood"". In terms of anatomy and histology, blood is considered a specialized form of connective tissue, given its origin in the bones and the presence of potential molecular fibers in the form of fibrinogen.
"
Blood-brain_barrier,Biology,5,"The blood–brain barrier (BBB) is a highly selective semipermeable border of endothelial cells that prevents solutes in the circulating blood from non-selectively crossing into the extracellular fluid of the central nervous system where neurons reside.[1] The blood-brain barrier is formed by endothelial cells of the capillary wall, astrocyte end-feet ensheathing the capillary, and pericytes embedded in the capillary basement membrane.[2] This system allows the passage of some molecules by passive diffusion, as well as the selective and active transport of various nutrients, ions, organic anions, and macromolecules such as glucose, water and amino acids that are crucial to neural function.[3] The blood-brain barrier restricts the passage of pathogens, the diffusion of solutes in the blood, and large or hydrophilic molecules into the cerebrospinal fluid, while allowing the diffusion of hydrophobic molecules (O2, CO2, hormones) and small polar molecules.[4] Cells of the barrier actively transport metabolic products such as glucose across the barrier using specific transport proteins.[5] The barrier also restricts the passage of peripheral immune factors, like signaling molecules, antibodies, and immune cells, into the CNS, thus insulating the brain from damage due to peripheral immune events.[6] Specialized brain structures participating in sensory and secretory integration within brain neural circuits—the circumventricular organs and choroid plexus—have highly permeable capillaries.[7]"
Botany,Biology,5,"Botany, also called plant science(s), plant biology or phytology, is the science of plant life and a branch of biology. A botanist, plant scientist or phytologist is a scientist who specialises in this field. The term ""botany"" comes from the Ancient Greek word βοτάνη (botanē) meaning ""pasture"", ""grass"", or ""fodder""; βοτάνη is in turn derived from βόσκειν (boskein), ""to feed"" or ""to graze"".[1][2][3]  Traditionally, botany has also included the study of fungi and algae by mycologists and phycologists respectively, with the study of these three groups of organisms remaining within the sphere of interest of the International Botanical Congress. Nowadays, botanists (in the strict sense) study approximately 410,000 species of land plants of which some 391,000 species are vascular plants (including approximately 369,000 species of flowering plants),[4] and approximately 20,000 are bryophytes.[5] 
 Botany originated in prehistory as herbalism with the efforts of early humans to identify – and later cultivate – edible, medicinal and poisonous plants, making it one of the oldest branches of science. Medieval physic gardens, often attached to monasteries, contained plants of medical importance. They were forerunners of the first botanical gardens attached to universities, founded from the 1540s onwards. One of the earliest was the Padua botanical garden. These gardens facilitated the academic study of plants. Efforts to catalogue and describe their collections were the beginnings of plant taxonomy, and led in 1753 to the binomial system of nomenclature of Carl Linnaeus that remains in use to this day for the naming of all biological species.
 In the 19th and 20th centuries, new techniques were developed for the study of plants, including methods of optical microscopy and live cell imaging, electron microscopy, analysis of chromosome number, plant chemistry and the structure and function of enzymes and other proteins. In the last two decades of the 20th century, botanists exploited the techniques of molecular genetic analysis, including genomics and proteomics and DNA sequences to classify plants more accurately.
 Modern botany is a broad, multidisciplinary subject with inputs from most other areas of science and technology. Research topics include the study of plant structure, growth and differentiation, reproduction, biochemistry and primary metabolism, chemical products, development, diseases, evolutionary relationships, systematics, and plant taxonomy. Dominant themes in 21st century plant science are molecular genetics and epigenetics, which study the mechanisms and control of gene expression during differentiation of plant cells and tissues. Botanical research has diverse applications in providing staple foods, materials such as timber, oil, rubber, fibre and drugs, in modern horticulture, agriculture and forestry, plant propagation, breeding and genetic modification, in the synthesis of chemicals and raw materials for construction and energy production, in environmental management, and the maintenance of biodiversity.
"
Light-independent_reactions#Coupling_to_other_metabolic_pathways,Biology,5,"The Calvin cycle, light-independent reactions, bio synthetic phase, dark reactions, or photosynthetic carbon reduction (PCR) cycle[1] of photosynthesis are the chemical reactions that convert carbon dioxide and other compounds into glucose. These reactions occur in the stroma, the fluid-filled area of a chloroplast outside the thylakoid membranes. These reactions take the products (ATP and NADPH) of light-dependent reactions and perform further chemical processes on them. [The Calvin cycle uses the reducing powers ATP and NADPH from the light dependent reactions to produce sugars for the plant to use. These substrates are used in a series of reduction-oxidation reactions to produce sugars in a step-wise process. There is no direct reaction that converts CO2 to a sugar because all of the energy would be lost to heat.] There are three phases to the light-independent reactions, collectively called the Calvin cycle: carbon fixation, reduction reactions, and ribulose 1,5-bisphosphate (RuBP) regeneration.
 Though it is called the ""dark reaction"", the Calvin cycle does not actually occur in the dark or during nighttime. This is because the process requires reduced NADP which is short-lived and comes from the light-dependent reactions. In the dark, plants instead release sucrose into the phloem from their starch reserves to provide energy for the plant. The Calvin cycle thus happens when light is available independent of the kind of photosynthesis (C3 carbon fixation, C4 carbon fixation, and Crassulacean Acid Metabolism (CAM)); CAM plants store malic acid in their vacuoles every night and release it by day to make this process work.[2]"
Carbon_fixation,Biology,5,"
 Carbon fixation or сarbon assimilation is the process by which inorganic carbon (particularly in the form of carbon dioxide) is converted to organic compounds by living organisms. The compounds are then used to store energy and as structure for other biomolecules. The most prominent example of carbon fixation is photosynthesis; another form known as chemosynthesis can take place in the absence of sunlight.
 Organisms that grow by fixing carbon are called autotrophs, which include photoautotrophs (which use sunlight), and lithoautotrophs (which use inorganic oxidation). Heterotrophs are not themselves capable of carbon fixation but are able to grow by consuming the carbon fixed by autotrophs or other heterotrophs. ""Fixed carbon"", ""reduced carbon"", and ""organic carbon"" may all be used interchangeably to refer to various organic compounds.[1]"
Carbonate,Biology,5,"In chemistry, a carbonate is a salt of carbonic acid (H2CO3),[2] characterized by the presence of the carbonate ion, a polyatomic ion with the formula of CO2−3. The name may also refer to a carbonate ester, an organic compound containing the carbonate group C(=O)(O–)2.
 The term is also used as a verb, to describe carbonation: the process of raising the concentrations of carbonate and bicarbonate ions in water to produce carbonated water and other carbonated beverages – either by the addition of carbon dioxide gas under pressure, or by dissolving carbonate or bicarbonate salts into the water.
 In geology and mineralogy, the term ""carbonate"" can refer both to carbonate minerals and carbonate rock (which is made of chiefly carbonate minerals), and both are dominated by the carbonate ion, CO2−3. Carbonate minerals are extremely varied and ubiquitous in chemically precipitated sedimentary rock. The most common are calcite or calcium carbonate, CaCO3, the chief constituent of limestone (as well as the main component of mollusc shells and coral skeletons); dolomite, a calcium-magnesium carbonate CaMg(CO3)2; and siderite, or iron(II) carbonate, FeCO3, an important iron ore. Sodium carbonate (""soda"" or ""natron"") and potassium carbonate (""potash"") have been used since antiquity for cleaning and preservation, as well as for the manufacture of glass. Carbonates are widely used in industry, such as in iron smelting, as a raw material for Portland cement and lime manufacture, in the composition of ceramic glazes, and more.
"
Carotenoid,Biology,5,"Carotenoids (/kəˈrɒtɪnɔɪd/), also called tetraterpenoids, are yellow, orange, and red organic pigments that are produced by plants and algae, as well as several bacteria, and fungi.[1] Carotenoids give the characteristic color to pumpkins, carrots, corn, tomatoes, canaries, flamingos, salmon, lobster, shrimp, and daffodils.[1] Carotenoids can be produced from fats and other basic organic metabolic building blocks by all these organisms. The only land dwelling arthropods known to produce carotenoids are  aphids, and spider mites, which acquired the ability and genes from fungi.,[2][3][4] It is also produced by endosymbiotic bacteria in whiteflies.[5] Carotenoids from the diet are stored in the fatty tissues of animals,[1] and exclusively carnivorous animals obtain the compounds from animal fat. In the human diet, absorption of carotenoids is improved when consumed with fat in a meal.[6] Cooking carotenoid-containing vegetables in oil increases carotenoid bioavailability.[1][6] There are over 1,100 known carotenoids [7] which can be further categorized into two classes, xanthophylls (which contain oxygen) and carotenes (which are purely hydrocarbons and contain no oxygen).[1] All are derivatives of tetraterpenes, meaning that they are produced from 8 isoprene molecules and contain 40 carbon atoms. In general, carotenoids absorb wavelengths ranging from 400 to 550 nanometers (violet to green light). This causes the compounds to be deeply colored yellow, orange, or red. Carotenoids are the dominant pigment in autumn leaf coloration of about 15-30% of tree species,[1] but many plant colors, especially reds and purples, are due to polyphenols.
 Carotenoids serve two key roles in plants and algae: they absorb light energy for use in photosynthesis, and they provide photoprotection via non-photochemical quenching.[8] Carotenoids that contain unsubstituted beta-ionone rings  (including beta-carotene, alpha-carotene, beta-cryptoxanthin, and gamma-carotene) have vitamin A activity (meaning that they can be converted to retinol). In the eye, lutein, meso-zeaxanthin, and zeaxanthin are present as macular pigments whose importance in visual function, as of 2016, remains under clinical research.[1][9]"
Catalase,Biology,5,"Catalase is a common enzyme found in nearly all living organisms exposed to oxygen (such as bacteria, plants, and animals) which catalyzes the decomposition of hydrogen peroxide to water and oxygen.[1] It is a very important enzyme in protecting the cell from oxidative damage by reactive oxygen species (ROS). Likewise, catalase has one of the highest turnover numbers of all enzymes; one catalase molecule can convert millions of hydrogen peroxide molecules to water and oxygen each second.[2] n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a Catalase is a tetramer of four polypeptide chains, each over 500 amino acids long.[3] It contains four iron-containing heme groups that allow the enzyme to react with the hydrogen peroxide. The optimum pH for human catalase is approximately 7,[4] and has a fairly broad maximum: the rate of reaction does not change appreciably between pH 6.8 and 7.5.[5] The pH optimum for other catalases varies between 4 and 11 depending on the species.[6] The optimum temperature also varies by species.[7]"
Cell_(biology),Biology,5,"
 The cell (from Latin cella, meaning ""small room""[1]) is the basic structural, functional, and biological unit of all known organisms. A cell is the smallest unit of life. Cells are often called the ""building blocks of life"". The study of cells is called cell biology, cellular biology, or cytology.
 Cells consist of  cytoplasm enclosed within a membrane, which contains many biomolecules such as proteins and nucleic acids.[2]  Most plant and animal cells are only visible under a light microscope, with dimensions between 1 and 100 micrometres.[3] Electron microscopy gives a much higher resolution showing greatly detailed cell structure. Organisms can be classified as unicellular (consisting of a single cell such as bacteria) or multicellular (including plants and animals).[4] Most unicellular organisms are classed as microorganisms.
 The number of cells in plants and animals varies from species to species; it has been estimated that humans contain somewhere around 40 trillion (4×1013) cells.[a][5] The human brain accounts for around 80 billion of these cells.[6] Cells were discovered by Robert Hooke in 1665, who named them for their resemblance to cells inhabited by Christian monks in a monastery.[7][8] Cell theory, first developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that cells are the fundamental unit of structure and function in all living organisms, and that all cells come from pre-existing cells.[9] Cells emerged on Earth at least 3.5 billion years ago.[10][11][12]"
Cell_biology,Biology,5,"
 Cell biology (also cellular biology or cytology) is a branch of biology studying the structure and function of the cell, also known as the basic unit of life.[1] Cell biology encompasses both prokaryotic and eukaryotic cells and can be divided into many sub-topics which may include the study of cell metabolism, cell communication, cell cycle, biochemistry, and cell composition. The study of cells is performed using several techniques such as cell culture, various types of microscopy, and cell fractionation. These have allowed for and are currently being used for discoveries and research pertaining to how cells function, ultimately giving insight into understanding larger organisms. Knowing the components of cells and how cells work is fundamental to all biological sciences while also being essential for research in biomedical fields such as cancer, and other diseases. Research in cell biology is interconnected to other fields such as genetics, molecular genetics, biochemistry, molecular biology, medical microbiology, immunology, and cytochemistry.
"
Cell_cycle,Biology,5,"
 The cell cycle, or cell-division cycle, is the series of events that take place in a cell that cause it to divide into two daughter cells. These events include the duplication of its DNA (DNA replication) and some of its organelles, and subsequently the partitioning of its cytoplasm and other components into two daughter cells in a process called cell division.
 In cells with nuclei (eukaryotes), (i.e., animal, plant, fungal, and protist cells), the cell cycle is divided into two main stages: interphase and the mitotic (M) phase (including mitosis and cytokinesis). During interphase, the cell grows, accumulating nutrients needed for mitosis, and replicates its DNA and some of its organelles. During the mitotic phase, the replicated chromosomes, organelles, and cytoplasm separate into two new daughter cells. To ensure the proper replication of cellular components and division, there are control mechanisms known as cell cycle checkpoints after each of the key steps of the cycle that determine if the cell can progress to the next phase.
 In cells without nuclei (prokaryotes), (i.e., bacteria and archaea), the cell cycle is divided into the B, C, and D periods. The B period extends from the end of cell division to the beginning of DNA replication. DNA replication occurs during the C period. The D period refers to the stage between the end of DNA replication and the splitting of the bacterial cell into two daughter cells.[1] The cell-division cycle is a vital process by which a single-celled fertilized egg develops into a mature organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed. After cell division, each of the daughter cells begin the interphase of a new cycle. Although the various stages of interphase are not usually morphologically distinguishable, each phase of the cell cycle has a distinct set of specialized biochemical processes that prepare the cell for initiation of the cell division.
"
Cell_division,Biology,5,"Cell division is the process by which a parent cell divides into two or more daughter cells.[1] Cell division usually occurs as part of a larger cell cycle. In eukaryotes, there are two distinct types of cell division; a vegetative division, whereby each daughter cell is genetically identical to the parent cell (mitosis), and a reproductive cell division, whereby the number of chromosomes in the daughter cells is reduced by half to produce haploid gametes (meiosis).[2] In cell biology, mitosis (/maɪˈtoʊsɪs/) is a part of the cell cycle, in which, replicated chromosomes are separated into two new nuclei. Cell division gives rise to genetically identical cells in which the total number of chromosomes is maintained. In general, mitosis (division of the nucleus) is preceded by the S stage of interphase (during which the DNA is replicated) and is often followed by telophase and cytokinesis; which divides the cytoplasm, organelles and cell membrane of one cell into two new cells containing roughly equal shares of these cellular components. The different stages of Mitosis all together define the mitotic (M) phase of an animal cell cycle—the division of the mother cell into two daughter cells genetically identical to each other[citation needed]. Meiosis results in four haploid daughter cells by undergoing one round of DNA replication followed by two divisions. Homologous chromosomes are separated in the first division, and sister chromatids are separated in the second division. Both of these cell division cycles are used in the process of sexual reproduction at some point in their life cycle. Both are believed to be present in the last eukaryotic common ancestor.
 Prokaryotes (bacteria and archaea) usually undergo a vegetative cell division known as binary fission, where their genetic material is segregated equally into two daughter cells. While binary fission may be the means of division by most prokaryotes, there are alternative manners of division, such as budding, that have been observed. All cell divisions, regardless of organism, are preceded by a single round of DNA replication.
 For simple unicellular microorganisms such as the amoeba, one cell division is equivalent to reproduction – an entire new organism is created. On a larger scale, mitotic cell division can create progeny from multicellular organisms, such as plants that grow from cuttings. Mitotic cell division enables sexually reproducing organisms to develop from the one-celled zygote, which itself was produced by meiotic cell division from gametes.[3][4] After growth, cell division by mitosis allows for continual construction and repair of the organism.[5] The human body experiences about 10 quadrillion cell divisions in a lifetime.[6] The primary concern of cell division is the maintenance of the original cell's genome. Before division can occur, the genomic information that is stored in chromosomes must be replicated, and the duplicated genome must be separated cleanly between cells.[7] A great deal of cellular infrastructure is involved in keeping genomic information consistent between generations.
"
Cell_membrane,Biology,5,"
 The cell membrane (also known as the plasma membrane,  or cytoplasmic membrane, and historically referred to as the plasmalemma) is the semipermeable membrane of a cell that surrounds and encloses its contents of cytoplasm and nucleoplasm. The cell membrane separates the cell from the surrounding interstitial fluid, the main component of the extracellular fluid.[1][2] The cell membrane consists of a lipid bilayer, including cholesterols (a lipid component) that sit between phospholipids to maintain their fluidity at various temperatures.  The membrane also contains membrane proteins, including integral proteins that go across the membrane serving as membrane transporters, and peripheral proteins that loosely attach to the outer (peripheral) side of the cell membrane, acting as enzymes shaping the cell.[3] The cell membrane controls the movement of substances in and out of cells and organelles. In this way, it is selectively permeable to ions and organic molecules.[4] In addition, cell membranes are involved in a variety of cellular processes such as cell adhesion, ion conductivity and cell signalling and serve as the attachment surface for several extracellular structures, including the cell wall, the carbohydrate layer called the glycocalyx, and the intracellular network of protein fibers called the cytoskeleton. In the field of synthetic biology, cell membranes can be artificially reassembled.[5][6][7]"
Cell_nucleus,Biology,5,"
 In cell biology, the nucleus (pl. nuclei; from Latin nucleus or nuculeus, meaning kernel or seed) is a membrane-bound organelle found in eukaryotic cells. Eukaryotes usually have a single nucleus, but a few cell types, such as mammalian red blood cells, have no nuclei, and a few others including osteoclasts have many. The main structures making up the nucleus are the nuclear envelope, a double membrane that encloses the entire organelle and isolates its contents from the cellular cytoplasm; and the nuclear matrix (which includes the nuclear lamina), a network within the nucleus that adds mechanical support, much like the cytoskeleton supports the cell as a whole.
 The cell nucleus contains all of the cell's genome, except for a small fraction of mitochondrial DNA, organized as multiple long linear DNA molecules in a complex with a large variety of proteins, such as histones, to form chromosomes. The genes within these chromosomes are structured in such a way to promote cell function. The nucleus maintains the integrity of genes and controls the activities of the cell by regulating gene expression—the nucleus is, therefore, the control center of the cell.
 Because the nuclear envelope is impermeable to large molecules, nuclear pores are required to regulate nuclear transport of molecules across the envelope. The pores cross both nuclear membranes, providing a channel through which larger molecules must be actively transported by carrier proteins while allowing free movement of small molecules and ions. Movement of large molecules such as proteins and RNA through the pores is required for both gene expression and the maintenance of chromosomes.
 Although the interior of the nucleus does not contain any membrane-bound subcompartments, its contents are not uniform, and a number of nuclear bodies exist, made up of unique proteins, RNA molecules, and particular parts of the chromosomes. The best-known of these is the nucleolus, which is mainly involved in the assembly of ribosomes. After being produced in the nucleolus, ribosomes are exported to the cytoplasm where they translate mRNA.
"
Cell_plate,Biology,5,"Cytokinesis in terrestrial plants occurs by cell plate formation. This process entails the delivery of Golgi-derived and endosomal vesicles carrying cell wall and cell membrane components to the plane of cell division and the subsequent fusion of these vesicles within this plate.
 After formation of an early tubulo-vesicular network at the center of the cell, the initially labile cell plate consolidates into a tubular network and eventually a fenestrated sheet. The cell plate grows outward from the center of the cell to the parental plasma membrane with which it will fuse, thus completing cell division. Formation and growth of the cell plate is dependent upon the phragmoplast, which is required for proper targeting of Golgi-derived vesicles to the cell plate. 
 As the cell plate matures in the central part of the cell, the phragmoplast disassembles in this region and new elements are added on its outside. This process leads to a steady expansion of the phragmoplast and, concomitantly, to a continuous retargeting of Golgi-derived vesicles to the growing edge of the cell plate. Once the cell plate reaches and fuses with the plasma membrane the phragmoplast disappears. This event not only marks the separation of the two daughter cells, but also initiates a range of biochemical modifications that transform the callose-rich, flexible cell plate into a cellulose-rich, stiff primary cell wall. 
 The heavy dependence of cell plate formation on active Golgi stacks explains why plant cells, unlike animal cells, do not disassemble their secretion machinery during cell division.
"
Cell_theory,Biology,5,"
 In biology, cell theory is the historic scientific theory, now universally accepted, that living organisms are made up of cells, that they are the basic structural/organizational unit of all organisms, and that all cells come from pre-existing cells. Cells are the basic unit of structure in all organisms and also the basic unit of reproduction. 
 The three tenets to the cell theory are as described below:
 There is no universally accepted definition of life. Some biologists consider non-cellular entities such as viruses living organisms,[1] and thus reasonably disagree with the first tenet.
"
Cell_wall,Biology,5,"
 A cell wall is a structural layer surrounding some types of cells, just outside the cell membrane. It can be tough, flexible, and sometimes rigid. It provides the cell with both structural support and protection, and also acts as a filtering mechanism.[1] Cell walls are present in most prokaryotes (except mollicute bacteria), in algae, fungi and eukaryotes including plants but are absent in animals.  A major function is to act as pressure vessels, preventing over-expansion of the cell when water enters.
 The composition of cell walls varies between species and may depend on cell type and developmental stage. The primary cell wall of land plants is composed of the polysaccharides cellulose, hemicelluloses and pectin. Often, other polymers such as lignin, suberin or cutin are anchored to or embedded in plant cell walls. Algae possess cell walls made of glycoproteins and polysaccharides such as carrageenan and agar that are absent from land plants. In bacteria, the cell wall is composed of peptidoglycan. The cell walls of archaea have various compositions, and may be formed of glycoprotein S-layers, pseudopeptidoglycan, or polysaccharides. Fungi possess cell walls made of the N-acetylglucosamine polymer chitin. Unusually, diatoms have a cell wall composed of biogenic silica.[2]"
Central_dogma_of_molecular_biology,Biology,5,"The central dogma of molecular biology is an explanation of the flow of genetic information within a biological system. It is often stated as ""DNA makes RNA, and RNA makes protein"",[1] although this is not its original meaning. It was first stated by Francis Crick in 1957,[2][3] then published in 1958:[4][5] The Central Dogma. This states that once ""information"" has passed into protein it cannot get out again. In more detail, the transfer of information from nucleic acid to nucleic acid, or from nucleic acid to protein may be possible, but transfer from protein to protein, or from protein to nucleic acid is impossible. Information means here the precise determination of sequence, either of bases in the nucleic acid or of amino acid residues in the protein. and re-stated in a Nature paper published in 1970:[6] The central dogma of molecular biology deals with the detailed residue-by-residue transfer of sequential information. It states that such information cannot be transferred back from protein to either protein or nucleic acid. A second version of the central dogma is popular but incorrect. This is the simplistic DNA → RNA → protein pathway published by James Watson in the first edition of The Molecular Biology of the Gene (1965). Watson's version differs from Crick's because Watson describes a two-step (DNA → RNA and RNA → protein) process as the central dogma.[7] While the dogma, as originally stated by Crick, remains valid today[citation needed], Watson's version does not[citation needed].
 The dogma is a framework for understanding the transfer of sequence information between information-carrying biopolymers, in the most common or general case, in living organisms. There are 3 major classes of such biopolymers: DNA and RNA (both nucleic acids), and protein. There are 3 × 3 = 9 conceivable direct transfers of information that can occur between these. The dogma classes these into 3 groups of 3: three general transfers (believed to occur normally in most cells), three special transfers (known to occur, but only under specific conditions in case of some viruses or in a laboratory), and three unknown transfers (believed never to occur). The general transfers describe the normal flow of biological information: DNA can be copied to DNA (DNA replication), DNA information can be copied into mRNA (transcription), and proteins can be synthesized using the information in mRNA as a template (translation). The special transfers describe: RNA being copied from RNA (RNA replication), DNA being synthesised using an RNA template (reverse transcription), and proteins being synthesised directly from a DNA template without the use of mRNA. The unknown transfers describe: a protein being copied from a protein, synthesis of RNA using the primary structure of a protein as a template, and DNA synthesis using the primary structure of a protein as a template - these are not thought to naturally occur.[6]"
Centriole,Biology,5,"In cell biology a centriole is a cylindrical organelle composed mainly of a protein called tubulin.[1] Centrioles are found in most eukaryotic cells. A bound pair of centrioles, surrounded by a highly ordered mass of dense material, called the pericentriolar material (PCM),[2] makes up a structure called a centrosome.[1] Centrioles are not present in all eukaryotes; for example, they are absent from conifers (pinophyta), flowering plants (angiosperms) and most fungi, and are only present in the male gametes of charophytes, bryophytes, seedless vascular plants, cycads, and ginkgo.[3][4] Centrioles are typically made up of nine sets of short microtubule triplets, arranged in a cylinder. Deviations from this structure include crabs and Drosophila melanogaster embryos, with nine doublets, and Caenorhabditis elegans sperm cells and early embryos, with nine singlets.[5][6] Additional proteins include centrin, cenexin and tektin.[7] The main function of centrioles is to produce cilia during interphase and the aster and the spindle during cell division.
"
Centrosome,Biology,5,"In cell biology, the centrosome (Latin centrum 'center' + Greek sōma 'body') is an organelle that serves as the main microtubule organizing center (MTOC) of the animal cell, as well as a regulator of cell-cycle progression. The centrosome is thought to have evolved only in the metazoan lineage of eukaryotic cells.[1] Fungi and plants lack centrosomes and therefore use other structures to organize their microtubules.[2][3] Although the centrosome has a key role in efficient mitosis in animal cells, it is not essential in certain fly and flatworm species.[4][5][6] Centrosomes are composed of two centrioles arranged at right angles to each other, and surrounded by a dense, highly structured[7] mass of protein termed the pericentriolar material (PCM). The PCM contains proteins responsible for microtubule nucleation and anchoring[8] — including γ-tubulin, pericentrin and ninein. In general, each centriole of the centrosome is based on a nine-triplet microtubule assembled in a cartwheel structure, and contains centrin, cenexin and tektin.[9]
In many cell types, the centrosome is replaced by a cilium during cellular differentiation. However, once the cell starts to divide, the cilium is replaced again by the centrosome.[10]"
Chemical_compound,Biology,5,"
 A chemical compound is a chemical substance composed of many identical molecules (or molecular entities) composed of atoms from more than one element held together by chemical bonds. A molecule consisting of atoms of only one element is therefore not a compound.
 There are four types of compounds, depending on how the constituent atoms are held together:
 A chemical formula specifies the number of atoms of each element in a compound molecule, using the standard abbreviations for the chemical elements and numerical subscripts. For example, a water molecule has formula H2O indicating two hydrogen atoms bonded to one oxygen atom. Many chemical compounds have a unique CAS number identifier assigned by the Chemical Abstracts Service. Globally, more than 350,000 chemical compounds (including mixtures of chemicals) have been registered for production and use.[1] A compound can be converted to a different chemical substance by interaction with a second substance via a chemical reaction. In this process, bonds between atoms may be broken in either or both of the interacting substances, and new bonds formed.
"
Chemical_equilibrium,Biology,5,"In a chemical reaction, chemical equilibrium is the state in which both reactants and products are present in concentrations which have no further tendency to change with time, so that there is no observable change in the properties of the system.[1] This state results when the forward reaction proceeds at the same rate as the reverse reaction. The reaction rates of the forward and backward reactions are generally not zero, but equal. Thus, there are no net changes in the concentrations of the reactants and products. Such a state is known as dynamic equilibrium.[2][3]"
Chemical_reaction,Biology,5,"
 A chemical reaction is a process that leads to the chemical transformation of one set of chemical substances to another.[1] Classically, chemical reactions encompass changes that only involve the positions of electrons in the forming and breaking of chemical bonds between atoms, with no change to the nuclei (no change to the elements present), and can often be described by a chemical equation. Nuclear chemistry is a sub-discipline of chemistry that involves the chemical reactions of unstable and radioactive elements where both electronic and nuclear changes can occur.
 The substance (or substances) initially involved in a chemical reaction are called reactants or reagents. Chemical reactions are usually characterized by a chemical change, and they yield one or more products, which usually have properties different from the reactants. Reactions often consist of a sequence of individual sub-steps, the so-called elementary reactions, and the information on the precise course of action is part of the reaction mechanism. Chemical reactions are described with chemical equations, which symbolically present the starting materials, end products, and sometimes intermediate products and reaction conditions.
 Chemical reactions happen at a characteristic reaction rate at a given temperature and chemical concentration. Typically, reaction rates increase with increasing temperature because there is more thermal energy available to reach the activation energy necessary for breaking bonds between atoms.
 Reactions may proceed in the forward or reverse direction until they go to completion or reach equilibrium. Reactions that proceed in the forward direction to approach equilibrium are often described as spontaneous, requiring no input of free energy to go forward. Non-spontaneous reactions require input of free energy to go forward (examples include charging a battery by applying an external electrical power source, or photosynthesis driven by absorption of electromagnetic radiation in the form of sunlight).
 Different chemical reactions are used in combinations during chemical synthesis in order to obtain a desired product. In biochemistry, a consecutive series of chemical reactions (where the product of one reaction is the reactant of the next reaction) form metabolic pathways. These reactions are often catalyzed by protein enzymes. Enzymes increase the rates of biochemical reactions, so that metabolic syntheses and decompositions impossible under ordinary conditions can occur at the temperatures and concentrations present within a cell.
 The general concept of a chemical reaction has been extended to reactions between entities smaller than atoms, including nuclear reactions, radioactive decays, and reactions between elementary particles, as described by quantum field theory.
"
Chemistry,Biology,5,"
 
 Chemistry is the scientific discipline involved with elements and compounds composed of atoms, molecules and ions: their composition, structure, properties, behavior and the changes they undergo during a reaction with other substances.[1][2][3][4] In the scope of its subject, chemistry occupies an intermediate position between physics and biology.[5] It is sometimes called the central science because it provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level.[6] For example, chemistry explains aspects of plant chemistry (botany), the formation of igneous rocks (geology), how atmospheric ozone is formed and how environmental pollutants are degraded (ecology), the properties of the soil on the moon (cosmochemistry), how medications work (pharmacology), and how to collect DNA evidence at a crime scene (forensics).
 Chemistry addresses topics such as how atoms and molecules interact via chemical bonds to form new chemical compounds. There are two types of chemical bonds: 1. Primary Chemical bonds e.g covalent bonds, in which atoms share one or more electron(s); ionic bonds, in which an atom donates one or more electrons to another atom to produce ions (cations and anions); Metallic bonds and 2. Secondary chemical bonds e.g. hydrogen bonds;Van der Waals force bonds, ion-ion interaction, ion-dipole interaction etc.
"
Chemosynthesis,Biology,5,"In biochemistry, chemosynthesis is the biological conversion of one or more carbon-containing molecules (usually carbon dioxide or methane) and nutrients into organic matter using the oxidation of inorganic compounds (e.g., hydrogen gas, hydrogen sulfide) or ferrous ions as a source of energy, rather than sunlight, as in photosynthesis. Chemoautotrophs, organisms that obtain carbon from carbon dioxide through chemosynthesis, are phylogenetically diverse, but also groups that include conspicuous or biogeochemically-important taxa include the sulfur-oxidizing gamma and epsilon proteobacteria, the Aquificae, the methanogenic archaea and the neutrophilic iron-oxidizing bacteria.
 Many microorganisms in dark regions of the oceans use chemosynthesis to produce biomass from single carbon molecules. Two categories can be distinguished. In the rare sites where hydrogen molecules (H2) are available, the energy available from the reaction between CO2 and H2 (leading to production of methane, CH4) can be large enough to drive the production of biomass. Alternatively, in most oceanic environments, energy for chemosynthesis derives from reactions in which substances such as hydrogen sulfide or ammonia are oxidized. This may occur with or without the presence of oxygen.
 Many chemosynthetic microorganisms are consumed by other organisms in the ocean, and symbiotic associations between chemosynthesizers and respiring heterotrophs are quite common. Large populations of animals can be supported by chemosynthetic secondary production at hydrothermal vents, methane clathrates, cold seeps, whale falls, and isolated cave water.
 It has been hypothesized that anaerobic chemosynthesis may support life below the surface of Mars, Jupiter's moon Europa, and other planets.[1] Chemosynthesis may have also been the first type of metabolism that evolved on Earth, leading the way for cellular respiration and photosynthesis to develop later.
"
Chlorophyll,Biology,5,"
 Chlorophyll (also chlorophyl) is any of several related green pigments found in the mesosomes of cyanobacteria and in the chloroplasts of algae and plants.[1] Its name is derived from the Greek words χλωρός, khloros (""pale green"") and φύλλον, phyllon (""leaf"").[2] Chlorophyll is essential in photosynthesis, allowing plants to absorb energy from light.
 Chlorophylls absorb light most strongly in the blue portion of the electromagnetic spectrum as well as the red portion.[3] Conversely, it is a poor absorber of green and near-green portions of the spectrum, which it reflects, producing the green color of chlorophyll-containing tissues. Two types of chlorophyll exist in the photosystems of green plants: chlorophyll a and b.[4]"
Chloroplast,Biology,5,"
 
 Chloroplasts /ˈklɔːrəˌplæsts, -plɑːsts/[1][2] are organelles that conduct photosynthesis, where the photosynthetic pigment chlorophyll captures the energy from sunlight, converts it, and stores it in the energy-storage molecules ATP and NADPH while freeing oxygen from water in plant and algal cells. They then use the ATP and NADPH to make organic molecules from carbon dioxide in a process known as the Calvin cycle. Chloroplasts carry out a number of other functions, including fatty acid synthesis, much amino acid synthesis, and the immune response in plants. The number of chloroplasts per cell varies from one, in unicellular algae, up to 100 in plants like Arabidopsis and wheat.
 A chloroplast is a type of organelle known as a plastid, characterized by its two membranes and a high concentration of chlorophyll. Other plastid types, such as the leucoplast and the chromoplast, contain little chlorophyll and do not carry out photosynthesis.
 Chloroplasts are highly dynamic—they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce. Their behavior is strongly influenced by environmental factors like light color and intensity. Chloroplasts, like mitochondria, contain their own DNA, which is thought to be inherited from their ancestor—a photosynthetic cyanobacterium that was engulfed by an early eukaryotic cell.[3] Chloroplasts cannot be made by the plant cell and must be inherited by each daughter cell during cell division.
 With one exception (the amoeboid Paulinella chromatophora), all chloroplasts can probably be traced back to a single endosymbiotic event, when a cyanobacterium was engulfed by the eukaryote. Despite this, chloroplasts can be found in an extremely wide set of organisms, some not even directly related to each other—a consequence of many secondary and even tertiary endosymbiotic events.
 The word chloroplast is derived from the Greek words chloros (χλωρός), which means green, and plastes (πλάστης), which means ""the one who forms"".[4]"
Cholesterol,Biology,5,"
 Cholesterol (from the Ancient Greek chole- (bile) and stereos (solid), followed by the chemical suffix -ol for an alcohol) is an organic molecule. It is a sterol (or modified steroid),[3] a type of lipid.[1] Cholesterol is biosynthesized by all animal cells and is an essential structural component of animal cell membranes.
 Cholesterol also serves as a precursor for the biosynthesis of steroid hormones, bile acid[4] and vitamin D. Cholesterol is the principal sterol synthesized by all animals. In vertebrates, hepatic cells typically produce the greatest amounts. It is absent among prokaryotes (bacteria and archaea), although there are some exceptions, such as Mycoplasma, which require cholesterol for growth.[5] François Poulletier de la Salle first identified cholesterol in solid form in gallstones in 1769. However, it was not until 1815 that chemist Michel Eugène Chevreul named the compound ""cholesterine"".[6][7]"
Chromosome,Biology,5,"
 A chromosome is a long DNA molecule with part or all of the genetic material of an organism. Most eukaryotic chromosomes include packaging proteins called histones which, aided by chaperone proteins, bind to and condense the DNA molecule to maintain its integrity.[1][2] These chromosomes display a complex three-dimensional structure, which plays a significant role in transcriptional regulation.[3] Chromosomes are normally visible under a light microscope only during the metaphase of cell division (where all chromosomes are aligned in the center of the cell in their condensed form).[4] Before this happens, each chromosome is duplicated (S phase), and both copies are joined by a centromere, resulting either in an X-shaped structure (pictured above), if the centromere is located equatorially, or a two-arm structure, if the centromere is located distally. The joined copies are now called sister chromatids. During metaphase the X-shaped structure is called a metaphase chromosome, which is highly condensed and thus easiest to distinguish and study.[5] In animal cells, chromosomes reach their highest compaction level in anaphase during chromosome segregation.[6] Chromosomal recombination during meiosis and subsequent sexual reproduction play a significant role in genetic diversity. If these structures are manipulated incorrectly, through processes known as chromosomal instability and translocation, the cell may undergo mitotic catastrophe. Usually, this will make the cell initiate apoptosis leading to its own death, but sometimes mutations in the cell hamper this process and thus cause progression of cancer.
 Some use the term chromosome in a wider sense, to refer to the individualized portions of chromatin in cells, either visible or not under light microscopy. Others use the concept in a narrower sense, to refer to the individualized portions of chromatin during cell division, visible under light microscopy due to high condensation.
"
Cilia,Biology,5,"The cilium (from Latin 'eyelash';[1] the plural is cilia) is an organelle found on eukaryotic cells in the shape of a slender protuberance that projects from the much larger cell body.[2] There are two types of cilia: motile and non-motile cilia. Non-motile cilia are also called primary cilia which serve as sensory organelles. Most mammalian cell types possess a single non-motile, primary cilium, which functions as a cellular antenna.[3][4] Exceptions include olfactory neurons which possess several non-motile cilia and cells of the transient embryonic node, which possess singular motile cilia known as nodal cilia, critical for the establishment of left to right body asymmetry.[5] In eukaryotes, motile cilia and flagella (together known as undulipodia) are structurally similar, although distinctions are sometimes made according to function or length.[6][7] Immotile cilia (called primary cilia) communicate signals from the environment or from other cells.[8][9]"
Circadian_rhythm,Biology,5,"A circadian rhythm is a natural, internal process that regulates the sleep-wake cycle and repeats on each rotation of the Earth roughly every 24 hours.[1] It can refer to any biological process that displays an endogenous, entrainable oscillation of about 24 hours. These 24-hour rhythms are driven by a circadian clock, and they have been widely observed in plants, animals, fungi, and cyanobacteria.[2] The term circadian comes from the Latin circa, meaning ""around"" (or ""approximately""), and diēm, meaning ""day"". The formal study of biological temporal rhythms, such as daily, tidal, weekly, seasonal, and annual rhythms, is called chronobiology. Processes with 24-hour oscillations are more generally called diurnal rhythms; strictly speaking, they should not be called circadian rhythms unless their endogenous nature is confirmed.[3] Although circadian rhythms are endogenous (""built-in"", self-sustained), they are adjusted (entrained) to the local environment by external cues called zeitgebers (from German, ""time giver""), which include light, temperature and redox cycles. In medical science,  an abnormal circadian rhythm in humans is known as circadian rhythm disorder.[4] In 2017, the Nobel Prize in Physiology or Medicine was awarded to Jeffrey C. Hall, Michael Rosbash and Michael W. Young ""for their discoveries of molecular mechanisms controlling the circadian rhythm"" in fruit flies.[5]"
Citric_acid_cycle,Biology,5,"The citric acid cycle (CAC) – also known as the TCA cycle (tricarboxylic acid cycle) or the Krebs cycle[1][2] – is a series of chemical reactions used by all aerobic organisms to release stored energy through the oxidation of acetyl-CoA derived from carbohydrates, fats, and proteins. In addition, the cycle provides precursors of certain amino acids, as well as the reducing agent NADH, that are used in numerous other reactions. Its central importance to many biochemical pathways suggests that it was one of the earliest components of metabolism and may have originated abiogenically.[3][4] Even though it is branded as a 'cycle', it is not necessary for metabolites to follow only one specific route; at least three segments of the citric acid cycle have been recognized.[5] The name of this metabolic pathway is derived from the citric acid (a tricarboxylic acid, often called citrate, as the ionized form predominates at biological pH[6]) that is consumed and then regenerated by this sequence of reactions to complete the cycle. The cycle consumes acetate (in the form of acetyl-CoA) and water, reduces NAD+ to NADH, releasing carbon dioxide. The NADH generated by the citric acid cycle is fed into the oxidative phosphorylation (electron transport) pathway. The net result of these two closely linked pathways is the oxidation of nutrients to produce usable chemical energy in the form of ATP.
 In eukaryotic cells, the citric acid cycle occurs in the matrix of the mitochondrion. In prokaryotic cells, such as bacteria, which lack mitochondria, the citric acid cycle reaction sequence is performed in the cytosol with the proton gradient for ATP production being across the cell's surface (plasma membrane) rather than the inner membrane of the mitochondrion. The overall yield of energy-containing compounds from the TCA cycle is three NADH, one FADH2, and one GTP.[7]"
Clade,Biology,5,"
 A clade (/kleɪd/;[1][2] from Ancient Greek: κλάδος, klados, ""branch""), also known as a monophyletic group or natural group,[3] is a group of organisms that are monophyletic—that is, composed of a common ancestor and all its lineal descendants.[4] Rather than the English term, the equivalent Latin term cladus (plural cladi) is often used in taxonomical literature.
 The common ancestor may be an individual, a population, a species (extinct or extant), and so on right up to a kingdom and further. Clades are nested, one in another, as each branch in turn splits into smaller branches. These splits reflect evolutionary history as populations diverged and evolved independently. Clades are termed monophyletic (Greek: ""one clan"") groups.
 Over the last few decades, the cladistic approach has revolutionized biological classification and revealed surprising evolutionary relationships among organisms.[5] Increasingly, taxonomists try to avoid naming taxa that are not clades; that is, taxa that are not monophyletic. Some of the relationships between organisms that the molecular biology arm of cladistics has revealed are that fungi are closer relatives to animals than they are to plants, archaea are now considered different from bacteria, and multicellular organisms may have evolved from archaea.[6] The term ""clade"" is also used with a similar meaning in other fields besides biology, such as historical linguistics; see Cladistics § In disciplines other than biology.
"
Class_(biology),Biology,5,"In biological classification, class (Latin: classis) is  a taxonomic rank, as well as a taxonomic unit, a taxon, in that rank.[a] Other well-known ranks in descending order of size are life, domain, kingdom, phylum, order, family, genus, and species, with class fitting between phylum and order.
"
Clonal_selection,Biology,5,"Clonal selection theory is a scientific theory in immunology that explains the functions of cells of the immune system (lymphocytes) in response to specific antigens invading the body. The concept was introduced by Australian doctor Frank Macfarlane Burnet in 1957, in an attempt to explain the great diversity of antibodies formed during initiation of the immune response.[1][2] The theory has become the widely accepted model for how the human immune system responds to infection and how certain types of B and T lymphocytes are selected for destruction of specific antigens.[3] The theory states that in a pre-existing group of lymphocytes (specifically B cells), a specific antigen activates (i.e. selects) only its counter-specific cell, which then induces that particular cell to multiply, producing identical clones for antibody production. This activation occurs in secondary lymphoid organs such as the spleen and the lymph nodes.[4] In short, the theory is an explanation of the mechanism for the generation of diversity of antibody specificity.[5] The first experimental evidence came in 1958, when Gustav Nossal and Joshua Lederberg showed that one B cell always produces only one antibody.[6] The idea turned out to be the foundation of molecular immunology, especially in adaptive immunity.[7]"
Cloning,Biology,5,"
 Cloning is the process of producing individuals with identical or virtually identical DNA, either naturally or artificially. In nature, many organisms produce clones through asexual reproduction. Cloning in biotechnology refers to the process of creating clones of organisms or copies of cells or DNA fragments (molecular cloning).
 The term clone, coined by Herbert J. Webber, is derived from the Ancient Greek word κλών klōn, ""twig"", referring to the process whereby a new plant can be created from a twig. In botany, the term lusus was traditionally used.[1] In horticulture, the spelling clon was used until the twentieth century; the final e came into use to indicate the vowel is a ""long o"" instead of a ""short o"".[2][3] Since the term entered the popular lexicon in a more general context, the spelling clone has been used exclusively.
"
Colony_(biology),Biology,5,"In biology, a colony is composed of two or more conspecific individuals living in close association with, or connected to, one another. This association is usually for mutual benefit such as stronger defense or the ability to attack bigger prey.[1] It is a cluster of identical cells (clones) on the surface of (or within) a solid medium, usually derived from a single parent cell, as in bacterial colony.[2] In contrast, solitary organisms are ones in which all individuals live independently and have all of the functions needed to survive and reproduce.
 Colonies, in the context of development, may be composed of two or more unitary (or solitary) organisms or be modular organisms. Unitary organisms have determinate development (set life stages) from zygote to adult form and individuals or groups of individuals (colonies) are visually distinct. Modular organisms[3] have indeterminate growth forms (life stages not set) through repeated iteration of genetically identical modules (or individuals), and it can be difficult to distinguish between the colony as a whole and the modules within.[4] In the latter case, modules may have specific functions within the colony.
 Some organisms are primarily independent and form facultative colonies in reply to environmental conditions while others must live in a colony to survive (obligate). For example, some carpenter bees will form colonies when a dominant hierarchy is formed between two or more nest foundresses[5] (facultative colony), while corals are animals that are physically connected by living tissue (the coenosarc) that contains a shared gastrovascular cavity.
"
Comparative_biology,Biology,5,"Comparative biology uses natural variation and disparity to understand the patterns of life at all levels—from genes to communities—and the critical role of organisms in ecosystems.  Comparative biology is a cross-lineage approach to understanding the phylogenetic history of individuals or higher taxa and the mechanisms and patterns that drives it. Comparative biology encompasses Evolutionary Biology, Systematics, Neontology, Paleontology, Ethology, Anthropology, and Biogeography as well as historical approaches to Developmental biology, Genomics, Physiology, Ecology and many other areas of the biological sciences. The comparative approach also has numerous applications in human health, genetics, biomedicine, and conservation biology. The biological relationships (phylogenies, pedigree) are important for comparative analyses and usually represented by a phylogenetic tree or cladogram to differentiate those features with single origins (Homology) from those with multiple origins (Homoplasy).
"
Conservation_biology,Biology,5,"Conservation biology is the management of nature and of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions.[1][2][3] It is an interdisciplinary subject drawing on natural and social sciences, and the practice of natural resource management.[4][5][6][7]:478 The conservation ethic is based on the findings of conservation biology.
"
Convergent_evolution,Biology,5,"
 Convergent evolution is the independent evolution of similar features in species of different periods or epochs in time. Convergent evolution creates analogous structures that have similar form or function but were not present in the last common ancestor of those groups. The cladistic term for the same phenomenon is homoplasy. The recurrent evolution of flight is a classic example, as flying insects, birds, pterosaurs, and bats have independently evolved the useful capacity of flight. Functionally similar features that have arisen through convergent evolution are analogous, whereas homologous structures or traits have a common origin but can have dissimilar functions.  Bird, bat, and pterosaur wings are analogous structures, but their forelimbs are homologous, sharing an ancestral state despite serving different functions.
 
 The opposite of convergence is divergent evolution, where related species evolve different traits. Convergent evolution is similar to parallel evolution, which occurs when two independent species evolve in the same direction and thus independently acquire similar characteristics; for instance, gliding frogs have evolved in parallel from multiple types of tree frog.
 Many instances of convergent evolution are known in plants, including the repeated development of C4 photosynthesis, seed dispersal by fleshy fruits adapted to be eaten by animals, and carnivory.
 Recent evidence suggests that even plants and animals share a convergently evolved developmental pattern whereby embryos of both lineages pass through a phylotypic stage marked by an organizational checkpoint during mid-embryogenesis.[1][2]"
Countercurrent_exchange,Biology,5,"Countercurrent exchange is a mechanism occurring in nature and mimicked in industry and engineering, in which there is a crossover of some property, usually heat or some chemical, between two flowing bodies flowing in opposite directions to each other. The flowing bodies can be liquids, gases, or even solid powders, or any combination of those. For example, in a distillation column, the vapors bubble up through the downward flowing liquid while exchanging both heat and mass.
 The maximum amount of heat or mass transfer that can be obtained is higher with countercurrent than co-current (parallel) exchange because countercurrent maintains a slowly declining difference or gradient (usually temperature or concentration difference). In cocurrent exchange the initial gradient is higher but falls off quickly, leading to wasted potential. For example, in the adjacent diagram, the fluid being heated (exiting top) has a higher exiting temperature than the cooled fluid (exiting bottom) that was used for heating. With cocurrent or parallel exchange the heated and cooled fluids can only approach one another. The result is that countercurrent exchange can achieve a greater amount of heat or mass transfer than parallel under otherwise similar conditions. See: flow arrangement.
 Countercurrent exchange when set up in a circuit or loop can be used for building up concentrations, heat, or other properties of flowing liquids. Specifically when set up in a loop with a buffering liquid between the incoming and outgoing fluid running in a circuit, and with active transport pumps on the outgoing fluid's tubes, the system is called a countercurrent multiplier, enabling a multiplied effect of many small pumps to gradually build up a large concentration in the buffer liquid.
 Other countercurrent exchange circuits where the incoming and outgoing fluids touch each other are used for retaining a high concentration of a dissolved substance or for retaining heat, or for allowing the external buildup of the heat or concentration at one point in the system.
 Countercurrent exchange circuits or loops are found extensively in nature, specifically in biologic systems. In vertebrates, they are called a rete mirabile, originally the name of an organ in fish gills for absorbing oxygen from the water. It is mimicked in industrial systems. Countercurrent exchange is a key concept in chemical engineering thermodynamics and manufacturing processes, for example in extracting sucrose from sugar beet roots.
 Countercurrent multiplication is a similar but different concept where liquid moves in a loop followed by a long length of movement in opposite directions with an intermediate zone. The tube leading to the loop passively building up a gradient of heat (or cooling) or solvent concentration while the returning tube has a constant small pumping action all along it, so that a gradual intensification of the heat or concentration is created towards the loop. Countercurrent multiplication has been found in the kidneys[1] as well as in many other biological organs.
"
Crista,Biology,5,"A crista (/ˈkrɪstə/; plural cristae) is a fold in the inner membrane of a mitochondrion. The name is from the Latin for crest or plume, and it gives the inner membrane its characteristic wrinkled shape, providing a large amount of surface area for chemical reactions to occur on. This aids aerobic cellular respiration, because the mitochondrion requires oxygen. Cristae are studded with proteins, including ATP synthase and a variety of cytochromes.
 1 Outer membrane 2 Intermembrane space 3 Lamella 4 Mitochondrial DNA5 Matrix granule6 Ribosome7 ATP synthase
"
Cryobiology,Biology,5,"Cryobiology is the branch of biology that studies the effects of low temperatures on living things within Earth's cryosphere or in science. The word cryobiology is derived from the Greek words κρῧος [kryos], ""cold"", βίος [bios], ""life"", and λόγος [logos], ""word"" (hence science). In practice, cryobiology is the study of biological material or systems at temperatures below normal. Materials or systems studied may include proteins, cells, tissues, organs, or whole organisms. Temperatures may range from moderately hypothermic conditions to cryogenic temperatures.
"
Cytology,Biology,5,"
 Cell biology (also cellular biology or cytology) is a branch of biology studying the structure and function of the cell, also known as the basic unit of life.[1] Cell biology encompasses both prokaryotic and eukaryotic cells and can be divided into many sub-topics which may include the study of cell metabolism, cell communication, cell cycle, biochemistry, and cell composition. The study of cells is performed using several techniques such as cell culture, various types of microscopy, and cell fractionation. These have allowed for and are currently being used for discoveries and research pertaining to how cells function, ultimately giving insight into understanding larger organisms. Knowing the components of cells and how cells work is fundamental to all biological sciences while also being essential for research in biomedical fields such as cancer, and other diseases. Research in cell biology is interconnected to other fields such as genetics, molecular genetics, biochemistry, molecular biology, medical microbiology, immunology, and cytochemistry.
"
Cytoplasm,Biology,5,"
 In cell biology, the cytoplasm is all of the material within a cell, enclosed by the cell membrane, except for the cell nucleus. The material inside the nucleus and contained within the nuclear membrane is termed the nucleoplasm. The main components of the cytoplasm are cytosol (a gel-like substance), the organelles (the cell's internal sub-structures), and various cytoplasmic inclusions. The cytoplasm is about 80% water and usually colorless.[1] The submicroscopic ground cell substance, or cytoplasmatic matrix which remains after exclusion the cell organelles and particles is groundplasm.  It is the hyaloplasm of light microscopy, and high complex, polyphasic system in which all of resolvable cytoplasmic elements of are suspended, including the larger organelles such as the ribosomes, mitochondria, the plant plastids, lipid droplets, and vacuoles.
 Most cellular activities take place within the cytoplasm, such as many metabolic pathways including glycolysis, and processes such as cell division. The concentrated inner area is called the endoplasm and the outer layer is called the cell cortex or the ectoplasm.
 Movement of calcium ions in and out of the cytoplasm is a signaling activity for metabolic processes.[2] In plants, movement of the cytoplasm around vacuoles is known as cytoplasmic streaming.
"
Cytosine,Biology,5,"Cytosine (/ˈsaɪtəˌsiːn, -ˌziːn, -ˌsɪn/;[2][3] C) is one of the four main bases found in DNA and RNA, along with adenine, guanine, and thymine (uracil in RNA). It is a pyrimidine derivative, with a heterocyclic aromatic ring and two substituents attached (an amine group at position 4 and a keto group at position 2). The nucleoside of cytosine is cytidine.  In Watson-Crick base pairing, it forms three hydrogen bonds with guanine.
"
Cytoskeleton,Biology,5,"The cytoskeleton is a complex, dynamic network of interlinking protein filaments present in the cytoplasm of all cells, including bacteria and archaea.[1] It extends from the cell nucleus to the cell membrane and is composed of similar proteins in the various organisms. In eukaryotes, it is composed of three main components, microfilaments, intermediate filaments and microtubules, and these are all capable of rapid growth or disassembly dependent on the cell's requirements.[2] A multitude of functions can be performed by the cytoskeleton. Its primary function is to give the cell its shape and mechanical resistance to deformation, and through association with extracellular connective tissue and other cells it stabilizes entire tissues.[3][4] The cytoskeleton can also contract, thereby deforming the cell and the cell's environment and allowing cells to migrate.[5] Moreover, it is involved in many cell signaling pathways and in the uptake of extracellular material (endocytosis),[6] the segregation of chromosomes during cellular division,[3] the cytokinesis stage of cell division,[7] as scaffolding to organize the contents of the cell in space[5] and in intracellular transport (for example, the movement of vesicles and organelles within the cell)[3] and can be a template for the construction of a cell wall.[3] Furthermore, it can form specialized structures, such as flagella, cilia, lamellipodia and podosomes. The structure, function and dynamic behavior of the cytoskeleton can be very different, depending on organism and cell type.[3][7] Even within one cell, the cytoskeleton can change through association with other proteins and the previous history of the network.[5] A large-scale example of an action performed by the cytoskeleton is muscle contraction. This is carried out by groups of highly specialized cells working together. A main component in the cytoskeleton that helps show the true function of this muscle contraction is the microfilament. Microfilaments are composed of the most abundant cellular protein known as actin.[8] During contraction of a muscle, within each muscle cell, myosin molecular motors collectively exert forces on parallel actin filaments. Muscle contraction starts from nerve impulses which then causes increased amounts of calcium to be released from the sarcoplasmic reticulum. Increases in calcium in the cytosol allows muscle contraction to begin with the help of two proteins, tropomyosin and troponin.[8] Tropomyosin inhibits the interaction between actin and myosin, while troponin senses the increase in calcium and releases the inhibition.[9] This action contracts the muscle cell, and through the synchronous process in many muscle cells, the entire muscle.
"
Darwinian_fitness,Biology,5,"Fitness (often denoted 



w


{  w}
 or ω in population genetics models) is the quantitative representation of natural and sexual selection within evolutionary biology. It can be defined either with respect to a genotype or to a phenotype in a given environment. In either case, it describes individual reproductive success and is equal to the average contribution to the gene pool of the next generation that is made by individuals of the specified genotype or phenotype. The fitness of a genotype is manifested through its phenotype, which is also affected by the developmental environment. The fitness of a given phenotype can also be different in different selective environments.
 With asexual reproduction, it is sufficient to assign fitnesses to genotypes. With sexual reproduction, genotypes have the opportunity to have a new frequency in the next generation. In this case, fitness values can be assigned to alleles by averaging over possible genetic backgrounds. Natural selection tends to make alleles with higher fitness more common over time, resulting in Darwinian evolution.
 The term ""Darwinian fitness"" can be used to make clear the distinction with physical fitness.[1] Fitness does not include a measure of survival or life-span; Herbert Spencer's  well-known phrase ""survival of the fittest"" should be interpreted as: ""Survival of the form (phenotypic or genotypic) that will leave the most copies of itself in successive generations.""
 Inclusive fitness differs from individual fitness by including the ability of an allele in one individual to promote the survival and/or reproduction of other individuals that share that allele, in preference to individuals with a different allele. One mechanism of inclusive fitness is kin selection.
"
Deciduous,Biology,5,"
 In the fields of horticulture and botany, the term deciduous (/dɪˈsɪdjuːəs/; US: /dɪˈsɪdʒuəs/)[1] means ""falling off at maturity""[2] and ""tending to fall off"",[3] in reference to trees and shrubs that seasonally shed leaves, usually in the autumn; to the shedding of petals, after flowering; and to the shedding of ripe fruit. The antonym of deciduous in the botanical sense is evergreen.
 Generally, the term ""deciduous"" means ""the dropping of a part that is no longer needed or useful"" and the ""falling away after its purpose is finished"". In plants, it is the result of natural processes. ""Deciduous"" has a similar meaning when referring to animal parts, such as deciduous antlers in deer,[4] deciduous teeth (baby teeth) in some mammals (including humans); or decidua, the uterine lining that sheds off after birth.
"
Decomposition,Biology,5,"Decomposition is the process by which dead organic substances are broken down into simpler organic or inorganic matter such as carbon dioxide, water, simple sugars and mineral salts. The process is a part of the nutrient cycle and is essential for recycling the finite matter that occupies physical space in the biosphere. Bodies of living organisms begin to decompose shortly after death. Animals, such as worms, also help decompose the organic materials. Organisms that do this are known as decomposers. Although no two organisms decompose in the same way, they all undergo the same sequential stages of decomposition. The science which studies decomposition is generally referred to as taphonomy from the Greek word taphos, meaning tomb. Decomposition can also be a gradual process for organisms that have extended periods of dormancy.[1] One can differentiate abiotic from biotic substance (biodegradation). The former means ""degradation of a substance by chemical or physical processes, e.g., hydrolysis.[2] The latter means ""the metabolic breakdown of materials into simpler components by living organisms"",[3] typically by microorganisms.
"
Dehydration_reaction,Biology,5,"In chemistry, a dehydration reaction is a conversion that involves the loss of water from the reacting molecule or ion.  Dehydration reactions are common processes, the reverse of a hydration reaction. Common dehydrating agents used in organic synthesis include sulfuric acid and alumina.  Often dehydration reactions are effected with heating.
"
Denaturation_(biochemistry),Biology,5,"Denaturation is a process in which proteins or nucleic acids lose the quaternary structure, tertiary structure, and secondary structure which is present in their native state, by application of some external stress or compound such as a strong acid or base, a concentrated inorganic salt, an organic solvent (e.g., alcohol or chloroform), radiation or heat.[3] If proteins in a living cell are denatured, this results in disruption of cell activity and possibly cell death. Protein denaturation is also a consequence of cell death.[4][5] Denatured proteins can exhibit a wide range of characteristics, from conformational change and loss of solubility to aggregation due to the exposure of hydrophobic groups. Denatured proteins lose their 3D structure and therefore cannot function.
 Note 1: Modified from the definition given in ref.[1] Note 2: Denaturation can occur when proteins and nucleic acids are subjected to elevated temperature or to extremes of pH, or to nonphysiological concentrations of salt, organic solvents, urea, or other chemical agents.
 Protein folding is key to whether a globular or membrane protein can do its job correctly; it must be folded into the right shape to function. However, hydrogen bonds, which play a big part in folding, are rather weak and thus easily affected by heat, acidity, varying salt concentrations, and other stressors which can denature the protein. This is one reason why homeostasis is physiologically necessary in many life forms.
 This concept is unrelated to denatured alcohol, which is alcohol that has been mixed with additives to make it unsuitable for human consumption.
"
Dendrite,Biology,5,"Dendrites (from Greek δένδρον déndron, ""tree""), also dendrons, are branched protoplasmic extensions of a nerve cell that propagate the electrochemical stimulation received from other neural cells to the cell body, or soma, of the neuron from which the dendrites project. Electrical stimulation is transmitted onto dendrites by upstream neurons (usually via their axons) via synapses which are located at various points throughout the dendritic tree. Dendrites play a critical role in integrating these synaptic inputs and in determining the extent to which action potentials are produced by the neuron.[1] Dendritic arborization, also known as dendritic branching, is a multi-step biological process by which neurons form new dendritic trees and branches to create new synapses.[1] The morphology of dendrites such as branch density and grouping patterns are highly correlated to the function of the neuron. Malformation of dendrites is also tightly correlated to impaired nervous system function.[2] Some disorders that are associated with the malformation of dendrites are autism, depression, schizophrenia, Down syndrome and anxiety.
 Certain classes of dendrites contain small projections referred to as dendritic spines that increase receptive properties of dendrites to isolate signal specificity. Increased neural activity and the establishment of long-term potentiation at dendritic spines change the sizes, shape, and conduction. This ability for dendritic growth is thought to play a role in learning and memory formation. There can be as many as 15,000 spines per cell, each of which serves as a postsynaptic process for individual presynaptic axons.[3] Dendritic branching can be extensive and in some cases is sufficient to receive as many as 100,000 inputs to a single neuron.[4] Dendrites are one of two types of protoplasmic protrusions that extrude from the cell body of a neuron, the other type being an axon. Axons can be distinguished from dendrites by several features including shape, length, and function. Dendrites often taper off in shape and are shorter, while axons tend to maintain a constant radius and be relatively long. Typically, axons transmit electrochemical signals and dendrites receive the electrochemical signals, although some types of neurons in certain species lack axons and simply transmit signals via their dendrites.[5] Dendrites provide an enlarged surface area to receive signals from the terminal buttons of other axons, and the axon also commonly divides at its far end into many branches (telodendria) each of which ends in a nerve terminal, allowing a chemical signal to pass simultaneously to many target cells.[4] Typically, when an electrochemical signal stimulates a neuron, it occurs at a dendrite and causes changes in the electrical potential across the neuron's plasma membrane. This change in the membrane potential will passively spread across the dendrite but becomes weaker with distance without an action potential. An action potential propagates the electrical activity along the membrane of the neuron's dendrites to the cell body and then afferently down the length of the axon to the axon terminal, where it triggers the release of neurotransmitters into the synaptic cleft.[4] However, synapses involving dendrites can also be axodendritic, involving an axon signaling to a dendrite, or dendrodendritic, involving signaling between dendrites.[6] An autapse is a synapse in which the axon of one neuron transmits signals to its own dendrites.
 There are three main types of neurons; multipolar, bipolar, and unipolar. Multipolar neurons, such as the one shown in the image, are composed of one axon and many dendritic trees. Pyramidal cells are multipolar cortical neurons with pyramid shaped cell bodies and large dendrites called apical dendrites that extend to the surface of the cortex. Bipolar neurons have one axon and one dendritic tree at opposing ends of the cell body. Unipolar neurons have a stalk that extends from the cell body that separates into two branches with one containing the dendrites and the other with the terminal buttons. Unipolar dendrites are used to detect sensory stimuli such as touch or temperature.[6][7][8]"
Denitrification,Biology,5,"Denitrification is a microbially facilitated process where nitrate (NO3−) is reduced and ultimately produces molecular nitrogen (N2) through a series of intermediate gaseous nitrogen oxide products. Facultative anaerobic bacteria perform denitrification as a type of respiration that reduces oxidized forms of nitrogen in response to the oxidation of an electron donor such as organic matter. The preferred nitrogen electron acceptors in order of most to least thermodynamically favorable include nitrate (NO3−), nitrite (NO2−), nitric oxide (NO), nitrous oxide (N2O) finally resulting in the production of dinitrogen (N2) completing the nitrogen cycle. Denitrifying microbes require a very low oxygen concentration of less than 10%, as well as organic C for energy. Since denitrification can remove NO3−, reducing its leaching to groundwater, it can be strategically used to treat sewage or animal residues of high nitrogen content. Denitrification can leak N2O, which is an ozone-depleting substance and a greenhouse gas that can have a considerable influence on global warming.
 The process is performed primarily by heterotrophic bacteria (such as Paracoccus denitrificans and various pseudomonads),[1] although autotrophic denitrifiers have also been identified (e.g., Thiobacillus denitrificans).[2] Denitrifiers are represented in all main phylogenetic groups.[3] Generally several species of bacteria are involved in the complete reduction of nitrate to N2, and more than one enzymatic pathway has been identified in the reduction process.[4] Direct reduction from nitrate to ammonium, a process known as dissimilatory nitrate reduction to ammonium or DNRA,[5] is also possible for organisms that have the nrf-gene.[6][7] This is less common than denitrification in most ecosystems as a means of nitrate reduction. Other genes known in microorganisms which denitrify include nir (nitrite reductase) and nos (nitrous oxide reductase) among others;[3] organisms identified as having these genes include Alcaligenes faecalis, Alcaligenes xylosoxidans, many in the genus Pseudomonas, Bradyrhizobium japonicum, and Blastobacter denitrificans.[8]"
Deoxyribonucleic_acid,Biology,5,"
 
 Deoxyribonucleic acid (/diːˈɒksɪˌraɪboʊnjuːˌkliːɪk, -ˌkleɪ-/ (listen);[1] DNA) is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organisms and many viruses. DNA and ribonucleic acid (RNA) are nucleic acids. Alongside proteins, lipids and complex carbohydrates (polysaccharides), nucleic acids are one of the four major types of macromolecules that are essential for all known forms of life.
 The two DNA strands are known as polynucleotides as they are composed of simpler monomeric units called nucleotides.[2][3] Each nucleotide is composed of one of four nitrogen-containing nucleobases (cytosine [C], guanine [G], adenine [A] or thymine [T]), a sugar called deoxyribose, and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds (known as the phospho-diester linkage) between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. The nitrogenous bases of the two separate polynucleotide strands are bound together, according to base pairing rules (A with T and C with G), with hydrogen bonds to make double-stranded DNA. The complementary nitrogenous bases are divided into two groups, pyrimidines and purines. In DNA, the pyrimidines are thymine and cytosine; the purines are adenine and guanine.
 Both strands of double-stranded DNA store the same biological information. This information is replicated as and when the two strands separate. A large part of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences. The two strands of DNA run in opposite directions to each other and are thus antiparallel. Attached to each sugar is one of four types of nucleobases (informally, bases). It is the sequence of these four nucleobases along the backbone that encodes genetic information. RNA strands are created using DNA strands as a template in a process called transcription, where DNA bases are exchanged for their corresponding bases except in the case of thymine (T), for which RNA substitutes uracil (U).[4] Under the genetic code, these RNA strands specify the sequence of amino acids within proteins in a process called translation.
 Within eukaryotic cells, DNA is organized into long structures called chromosomes. Before typical cell division, these chromosomes are duplicated in the process of DNA replication, providing a complete set of chromosomes for each daughter cell. Eukaryotic organisms (animals, plants, fungi and protists) store most of their DNA inside the cell nucleus as nuclear DNA, and some in the mitochondria as mitochondrial DNA or in chloroplasts as chloroplast DNA.[5] In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm, in circular chromosomes. Within eukaryotic chromosomes, chromatin proteins, such as histones, compact and organize DNA. These compacting structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.
"
Depolarization,Biology,5,"
In biology, depolarization (British English: Depolarisation) is a change within a cell, during which the cell undergoes a shift in electric charge distribution, resulting in less negative charge inside the cell. Depolarization is essential to the function of many cells, communication between cells, and the overall physiology of an organism.
 Most cells in higher organisms maintain an internal environment that is negatively charged relative to the cell's exterior. This difference in charge is called the cell's membrane potential. In the process of depolarization, the negative internal charge of the cell temporarily becomes more positive (less negative). This shift from a negative to a more positive membrane potential occurs during several processes, including an action potential. During an action potential, the depolarization is so large that the potential difference across the cell membrane briefly reverses polarity, with the inside of the cell becoming positively charged.
 The change in charge typically occurs due to an influx of sodium ions into a cell, although it can be mediated by an influx of any kind of cation or efflux of any kind of anion. The opposite of a depolarization is called a hyperpolarization.
 Usage of the term ""depolarization"" in biology differs from its use in physics, where it refers instead to situations in which any form of polarity ( i.e. the presence of any electrical charge, whether positive or negative) changes to a value of zero.
 Depolarization is sometimes referred to as ""hypopolarization"".[1][2]"
Desmosome,Biology,5,"A desmosome (/ˈdɛzməˌsoʊm/;[1][2] ""binding body""), also known as a macula adherens (plural: maculae adherentes) (Latin for adhering spot), is a cell structure specialized for cell-to-cell adhesion. A type of junctional complex, they are localized spot-like adhesions randomly arranged on the lateral sides of plasma membranes. Desmosomes are one of the stronger cell-to-cell adhesion types and are found in tissue that experience intense mechanical stress, such as cardiac muscle tissue, bladder tissue, gastrointestinal mucosa, and epithelia.[3]"
Developmental_biology,Biology,5,"Developmental biology is the study of the process by which animals and plants grow and develop. Developmental biology also encompasses the biology of regeneration, asexual reproduction, metamorphosis, and the growth and differentiation of stem cells in the adult organism.
"
Disease,Biology,5,"
 A disease is a particular abnormal condition that negatively affects the structure or function of all or part of an organism, and that is not due to any immediate external injury.[1][2] Diseases are often known to be medical conditions that are associated with specific symptoms and signs.[1][failed verification] A disease may be caused by external factors such as pathogens or by internal dysfunctions.  For example, internal dysfunctions of the immune system can produce a variety of different diseases, including various forms of immunodeficiency, hypersensitivity, allergies and autoimmune disorders.
 In humans, disease is often used more broadly to refer to any condition that causes pain, dysfunction, distress, social problems, or death to the person afflicted, or similar problems for those in contact with the person. In this broader sense, it sometimes includes injuries, disabilities, disorders, syndromes, infections, isolated symptoms, deviant behaviors, and atypical variations of structure and function, while in other contexts and for other purposes these may be considered distinguishable categories. Diseases can affect people not only physically, but also mentally, as contracting and living with a disease can alter the affected person's perspective on life.
 Death due to disease is called death by natural causes. There are four main types of disease: infectious diseases, deficiency diseases, hereditary diseases (including both genetic diseases and non-genetic hereditary diseases), and physiological diseases. Diseases can also be classified in other ways, such as communicable versus non-communicable diseases. The deadliest diseases in humans are coronary artery disease (blood flow obstruction), followed by cerebrovascular disease and lower respiratory infections.[3]  In developed countries, the diseases that cause the most sickness overall are neuropsychiatric conditions, such as depression and anxiety.
 The study of disease is called pathology, which includes the study of etiology, or cause.
"
DNA,Biology,5,"
 
 Deoxyribonucleic acid (/diːˈɒksɪˌraɪboʊnjuːˌkliːɪk, -ˌkleɪ-/ (listen);[1] DNA) is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organisms and many viruses. DNA and ribonucleic acid (RNA) are nucleic acids. Alongside proteins, lipids and complex carbohydrates (polysaccharides), nucleic acids are one of the four major types of macromolecules that are essential for all known forms of life.
 The two DNA strands are known as polynucleotides as they are composed of simpler monomeric units called nucleotides.[2][3] Each nucleotide is composed of one of four nitrogen-containing nucleobases (cytosine [C], guanine [G], adenine [A] or thymine [T]), a sugar called deoxyribose, and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds (known as the phospho-diester linkage) between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. The nitrogenous bases of the two separate polynucleotide strands are bound together, according to base pairing rules (A with T and C with G), with hydrogen bonds to make double-stranded DNA. The complementary nitrogenous bases are divided into two groups, pyrimidines and purines. In DNA, the pyrimidines are thymine and cytosine; the purines are adenine and guanine.
 Both strands of double-stranded DNA store the same biological information. This information is replicated as and when the two strands separate. A large part of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences. The two strands of DNA run in opposite directions to each other and are thus antiparallel. Attached to each sugar is one of four types of nucleobases (informally, bases). It is the sequence of these four nucleobases along the backbone that encodes genetic information. RNA strands are created using DNA strands as a template in a process called transcription, where DNA bases are exchanged for their corresponding bases except in the case of thymine (T), for which RNA substitutes uracil (U).[4] Under the genetic code, these RNA strands specify the sequence of amino acids within proteins in a process called translation.
 Within eukaryotic cells, DNA is organized into long structures called chromosomes. Before typical cell division, these chromosomes are duplicated in the process of DNA replication, providing a complete set of chromosomes for each daughter cell. Eukaryotic organisms (animals, plants, fungi and protists) store most of their DNA inside the cell nucleus as nuclear DNA, and some in the mitochondria as mitochondrial DNA or in chloroplasts as chloroplast DNA.[5] In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm, in circular chromosomes. Within eukaryotic chromosomes, chromatin proteins, such as histones, compact and organize DNA. These compacting structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.
"
DNA_replication,Biology,5,"In molecular biology, DNA replication is the biological process of producing two identical replicas of DNA from one original DNA molecule.[1] DNA replication occurs in all living organisms acting as the most essential part for biological inheritance. The cell possesses the distinctive property of division, which makes replication of DNA essential.
 DNA is made up of a double helix of two complementary strands. During replication, these strands are separated.  Each strand of the original DNA molecule then serves as a template for the production of its counterpart, a process referred to as semiconservative replication. As a result of semi-conservative replication, the new helix will be composed of an original DNA strand as well as a newly synthesized strand.[2] Cellular proofreading and error-checking mechanisms ensure near perfect fidelity for DNA replication.[3][4] In a cell, DNA replication begins at specific locations, or origins of replication, in the genome.[5] Unwinding of DNA at the origin and synthesis of new strands, accommodated by an enzyme known as helicase, results in replication forks growing bi-directionally from the origin. A number of proteins are associated with the replication fork to help in the initiation and continuation of DNA synthesis. Most prominently, DNA polymerase synthesizes the new strands by adding nucleotides that complement each (template) strand. DNA  replication occurs during the S-stage of interphase.
 DNA replication (DNA amplification) can also be performed in vitro (artificially, outside a cell). DNA polymerases isolated from cells and artificial DNA primers can be used to start DNA synthesis at known sequences in a template DNA molecule. Polymerase chain reaction (PCR), ligase chain reaction (LCR), and transcription-mediated amplification (TMA) are examples.
"
DNA_sequencing,Biology,5,"
 DNA sequencing is the process of determining the nucleic acid sequence – the order of nucleotides in DNA. It includes any method or technology that is used to determine the order of the four bases: adenine, guanine, cytosine, and thymine. The advent of rapid DNA sequencing methods has greatly accelerated biological and medical research and discovery.[1][2] Knowledge of DNA sequences has become indispensable for basic biological research, and in numerous applied fields such as medical diagnosis, biotechnology, forensic biology, virology and biological systematics. Comparing healthy and mutated DNA sequences can diagnose different diseases including various cancers,[3] characterize antibody repertoire,[4] and can be used to guide patient treatment.[5] Having a quick way to sequence DNA allows for faster and more individualized medical care to be administered, and for more organisms to be identified and cataloged.[4] 
The rapid speed of sequencing attained with modern DNA sequencing technology has been instrumental in the sequencing of complete DNA sequences, or genomes, of numerous types and species of life, including the human genome and other complete DNA sequences of many animal, plant, and microbial species. The first DNA sequences were obtained in the early 1970s by academic researchers using laborious methods based on two-dimensional chromatography. Following the development of fluorescence-based sequencing methods with a DNA sequencer,[6] DNA sequencing has become easier and orders of magnitude faster.[7]"
Drug,Biology,5,"
 A drug is any substance that causes a change in an organism's physiology or psychology when consumed.[3][4] Drugs are typically distinguished from food and substances that provide nutritional support. Consumption of drugs can be via inhalation, injection, smoking, ingestion, absorption via a patch on the skin, suppository, or dissolution under the tongue.
 In pharmacology, a drug is a chemical substance, typically of known structure, which, when administered to a living organism, produces a biological effect.[5] A pharmaceutical drug, also called a medication or medicine, is a chemical substance used to treat, cure, prevent, or diagnose a disease or to promote well-being.[3] Traditionally drugs were obtained through extraction from medicinal plants, but more recently also by organic synthesis.[6] Pharmaceutical drugs may be used for a limited duration, or on a regular basis for chronic disorders.[7] Pharmaceutical drugs are often classified into drug classes—groups of related drugs that have similar chemical structures, the same mechanism of action (binding to the same biological target), a related mode of action, and that are used to treat the same disease.[8][9] The Anatomical Therapeutic Chemical Classification System (ATC), the most widely used drug classification system, assigns drugs a unique ATC code, which is an alphanumeric code that assigns it to specific drug classes within the ATC system. Another major classification system is the Biopharmaceutics Classification System. This classifies drugs according to their solubility and permeability or absorption properties.[10] Psychoactive drugs are chemical substances that affect the function of the central nervous system, altering perception, mood or consciousness.[11] These drugs are divided into different groups like: stimulants, depressants, antidepressants, anxiolytics, antipsychotics, and hallucinogens. These psychoactive drugs have been proven useful in treating wide range of medical conditions including mental disorders around the world. The most widely used drugs in the world include caffeine, nicotine and alcohol,[12] which are also considered recreational drugs, since they are used for pleasure rather than medicinal purposes.[13] All drugs can have potential side effects.[14] Abuse of several psychoactive drugs can cause psychological or physical addiction.[15] Excessive use of stimulants can promote stimulant psychosis. Many recreational drugs are illicit and international treaties such as the Single Convention on Narcotic Drugs exist for the purpose of their prohibition.
"
Dynein,Biology,5,"Dynein is a family of cytoskeletal motor proteins that move along microtubules in cells. They convert the chemical energy stored in ATP to mechanical work. Dynein transports various cellular cargos, provides forces and displacements important in mitosis, and drives the beat of eukaryotic cilia and flagella. All of these functions rely on dynein's ability to move towards the minus-end of the microtubules, known as retrograde transport, thus, they are called ""minus-end directed motors"". In contrast, most kinesin motor proteins move toward the microtubules' plus end.
"
Ecological_efficiency,Biology,5,"Ecological efficiency describes the efficiency with which energy is transferred from one trophic level to the next. It is determined by a combination of efficiencies relating to organismic resource acquisition and assimilation in an ecosystem.
"
Ecological_pyramid,Biology,5,"An ecological pyramid (also trophic pyramid, Eltonian pyramid, energy pyramid, or sometimes food pyramid) is a graphical representation designed to show the biomass or bioproductivity at each trophic level in a given ecosystem.
 A pyramid of energy shows how much energy is retained in the form of new biomass at each trophic level, while a pyramid of biomass shows how much biomass (the amount of living or organic matter present in an organism) is present in the organisms. There is also a pyramid of numbers representing the number of individual organisms at each trophic level. Pyramids of energy are normally upright, but other pyramids can be inverted or take other shapes.
 Ecological pyramids begin with producers on the bottom (such as plants) and proceed through the various trophic levels (such as herbivores that eat plants, then carnivores that eat flesh, then omnivores that eat both plants and flesh, and so on).  The highest level is the top of the food chain.
 Biomass can be measured by a bomb calorimeter.
"
Ecological_succession,Biology,5,"Ecological succession is the process of change in the species structure of an ecological community over time. The time scale can be decades (for example, after a wildfire), or even millions of years after a mass extinction.[1] The community begins with relatively few pioneering plants and animals and develops through increasing complexity until it becomes stable or self-perpetuating as a climax community. The ""engine"" of succession, the cause of ecosystem change, is the impact of established organisms  upon their own environments. A consequence of living is the sometimes subtle and sometimes overt alteration of one's own environment.[2] It is a phenomenon or process by which an ecological community undergoes more or less orderly and predictable changes following a disturbance or the initial colonization of a new habitat. Succession may be initiated either by formation of new, unoccupied habitat, such as from a lava flow or a severe landslide, or by some form of disturbance of a community, such as from a fire, severe windthrow, or logging. Succession that begins in new habitats, uninfluenced by pre-existing communities is called primary succession, whereas succession that follows disruption of a pre-existing community is called secondary succession.
 Succession was among the first theories advanced in ecology. Ecological succession was first documented in the Indiana Dunes of Northwest Indiana and remains at the core of much ecological science.[3]"
Ecology,Biology,5,"
 Ecology (from Greek: οἶκος, ""house"" and -λογία, ""study of"")[A] is a branch of biology[1] concerning the spatial and temporal patterns of the distribution and abundance of organisms, including the causes and consequences.[2] Topics of interest include the biodiversity, distribution, biomass, and populations of organisms, as well as cooperation and competition within and between species. Ecosystems are dynamically interacting systems of organisms, the communities they make up, and the non-living components of their environment. Ecosystem processes, such as primary production, pedogenesis, nutrient cycling, and niche construction, regulate the flux of energy and matter through an environment. These processes are sustained by organisms with specific life history traits.
 Ecology is not synonymous with environmentalism or strictly natural history. Ecology overlaps with the closely related sciences of evolutionary biology, genetics, and ethology. An important focus for ecologists is to improve the understanding of how biodiversity affects ecological function. Ecologists seek to explain:
 Ecology has practical applications in conservation biology, wetland management, natural resource management (agroecology, agriculture, forestry, agroforestry, fisheries), city planning (urban ecology), community health, economics, basic and applied science, and human social interaction (human ecology). It is not treated as separate from humans. Organisms (including humans) and resources compose ecosystems which, in turn, maintain biophysical feedback mechanisms that moderate processes acting on living (biotic) and non-living (abiotic) components of the planet. Ecosystems sustain life-supporting functions and produce natural capital like biomass production (food, fuel, fiber, and medicine), the regulation of climate, global biogeochemical cycles, water filtration, soil formation, erosion control, flood protection, and many other natural features of scientific, historical, economic, or intrinsic value.
 The word ""ecology"" (""Ökologie"") was coined in 1866 by the German scientist Ernst Haeckel. Ecological thought is derivative of established currents in philosophy, particularly from ethics and politics.[3] Ancient Greek philosophers such as Hippocrates and Aristotle laid the foundations of ecology in their studies on natural history. Modern ecology became a much more rigorous science in the late 19th century. Evolutionary concepts relating to adaptation and natural selection became the cornerstones of modern ecological theory.
"
Ecophysiology,Biology,5,"
Ecophysiology (from Greek οἶκος, oikos, ""house(hold)""; φύσις, physis, ""nature, origin""; and -λογία, -logia), environmental physiology or physiological ecology is a biological discipline that studies the response of an organism's physiology to environmental conditions.  It is closely related to comparative physiology and evolutionary physiology. Ernst Haeckel's coinage bionomy is sometimes employed as a synonym.[1]"
Ecosystem,Biology,5,"
 An ecosystem is a community of living organisms in conjunction with the nonliving components of their environment, interacting as a system.[2] These biotic and abiotic components are linked together through nutrient cycles and energy flows.[3] Energy enters the system through photosynthesis and is incorporated into plant tissue. By feeding on plants and on one another, animals play an important role in the movement of matter and energy through the system. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter, decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and other microbes.[4] Ecosystems are controlled by external and internal factors. External factors such as climate, parent material which forms the soil and topography, control the overall structure of an ecosystem but are not themselves influenced by the ecosystem.[5] Unlike external factors, internal factors are controlled, for example, decomposition, root competition, shading, disturbance, succession, and the types of species present.
 Ecosystems are dynamic entities—they are subject to periodic disturbances and are in the process of recovering from some past disturbance.[6] Ecosystems in similar environments that are located in different parts of the world can end up doing things very differently simply because they have different pools of species present.[5] Internal factors not only control ecosystem processes but are also controlled by them and are often subject to feedback loops.[5] Resource inputs are generally controlled by external processes like climate and parent material. Resource availability within the ecosystem is controlled by internal factors like decomposition, root competition or shading.[5] Although humans operate within ecosystems, their cumulative effects are large enough to influence external factors like climate.[5] Biodiversity affects ecosystem functioning, as do the processes of disturbance and succession. Ecosystems provide a variety of goods and services upon which people depend.
"
Ecotype,Biology,5,"In evolutionary ecology, an ecotype,[note 1] sometimes called ecospecies, describes a genetically distinct geographic variety, population or race within a species, which is genotypically adapted to specific environmental conditions.
 Typically, though ecotypes exhibit phenotypic differences (such as in morphology or physiology) stemming from environmental heterogeneity, they are capable of interbreeding with other geographically adjacent ecotypes without loss of fertility or vigor.
[1][2][3][4][5]"
Ectoderm,Biology,5,"The ectoderm is the most exterior of the three primary germ layers formed in the very early embryo. The other two layers are the mesoderm (middle layer), and endoderm (innermost layer).[1] It emerges and originates from the outer layer of germ cells. The word ectoderm comes from the Greek ektos meaning ""outside"", and derma meaning ""skin.""[2] Generally speaking, the ectoderm differentiates to form some types of epithelial tissue – of the nervous system (spinal cord, peripheral nerves and brain), tooth enamel and the skin. It also forms the lining of mouth, anus, nostrils, sweat glands, hair and nails.[3] Other types of epithelium are derived from the endoderm.[3] In vertebrate embryos, the ectoderm has four parts: the external ectoderm also known as surface ectoderm, the neural plate, neural crest, and neurogenic placodes.[4] The neural plate and neural crest are known as neuroectoderm.
"
Ectotherm,Biology,5,"An ectotherm (from the Greek ἐκτός (ektós) ""outside"" and θερμός (thermós) ""hot"") is an organism in which internal physiological sources of heat are of relatively small or of quite negligible importance in controlling body temperature.[1] Such organisms (for example frogs) rely on environmental heat sources,[2] which permit them to operate at very economical metabolic rates.[3] Some of these animals live in environments where temperatures are practically constant, as is typical of regions of the abyssal ocean and hence can be regarded as homeothermic ectotherms. In contrast, in places where temperature varies so widely as to limit the physiological activities of other kinds of ectotherms, many species habitually seek out external sources of heat or shelter from heat; for example, many reptiles regulate their body temperature by basking in the sun, or seeking shade when necessary in addition to a whole host of other behavioral thermoregulation mechanisms. For home captivity of pet reptiles, owners can use a UVB/UVA light system to assist the animals' basking behaviour.[4] In contrast to ectotherms, endotherms rely largely, even predominantly, on heat from internal metabolic processes, and mesotherms use an intermediate strategy.
 In ectotherms, fluctuating ambient temperatures may affect the body temperature. Such variation in body temperature is called poikilothermy, though the concept is not widely satisfactory and the use of the term is declining. In small aquatic creatures such as Rotifera, the poikilothermy is practically absolute, but other creatures (like crabs) have wider physiological options at their disposal, and they can move to preferred temperatures, avoid ambient temperature changes, or moderate their effects.[1][5]  Ectotherms can also display the features of homeothermy, especially within aquatic organisms.  Normally their range of ambient environmental temperatures is relatively constant, and there are few in number that attempt to maintain a higher internal temperature due to the high associated costs.[6]"
Effector_(biology),Biology,5,"
 In biochemistry, an effector molecule is usually a small molecule that selectively binds to a protein and regulates its biological activity. In this manner, effector molecules act as ligands that can increase or decrease enzyme activity, gene expression, or cell signaling. Effector molecules can also directly regulate the activity of some mRNA molecules (riboswitches).
 In some cases, proteins can be considered to function as effector molecules, especially in cellular signal transduction cascades.
 The term effector is used in other fields of biology. For instance, the effector end of a neuron is the terminus where an axon makes contact with the muscle or organ that it stimulates or suppresses.
"
Egg_(biology),Biology,5,"
 The egg is the organic vessel containing the zygote in which an embryo develops until it can survive on its own, at which point the animal hatches. An egg results from fertilization of an egg cell. Most arthropods, vertebrates (excluding live-bearing mammals), and mollusks lay eggs, although some, such as scorpions, do not.
 Reptile eggs, bird eggs, and monotreme eggs are laid out of water and are surrounded by a protective shell, either flexible or inflexible. Eggs laid on land or in nests are usually kept within a warm and favorable temperature range while the embryo grows. When the embryo is adequately developed it hatches, i.e., breaks out of the egg's shell. Some embryos have a temporary egg tooth they use to crack, pip, or break the eggshell or covering.
 The largest recorded egg is from a whale shark and was 30 cm × 14 cm × 9 cm (11.8 in × 5.5 in × 3.5 in) in size.[1] Whale shark eggs typically hatch within the mother. At 1.5 kg (3.3 lb) and up to 17.8 cm × 14 cm (7.0 in × 5.5 in), the ostrich egg is the largest egg of any living bird,[2] though the extinct elephant bird and some non-avian dinosaurs laid larger eggs. The bee hummingbird produces the smallest known bird egg, which weighs half of a gram (around 0.02 oz). Some eggs laid by reptiles and most fish, amphibians, insects, and other invertebrates can be even smaller.
 Reproductive structures similar to the egg in other kingdoms are termed ""spores,"" or in spermatophytes ""seeds,"" or in gametophytes ""egg cells"".
"
Electrochemical_gradient,Biology,5,"An electrochemical gradient is a gradient of electrochemical potential, usually for an ion that can move across a membrane. The gradient consists of two parts, the chemical gradient, or difference in solute concentration across a membrane, and the electrical gradient, or difference in charge across a membrane. When there are unequal concentrations of an ion across a permeable membrane, the ion will move across the membrane from the area of higher concentration to the area of lower concentration through simple diffusion. Ions also carry an electric charge that forms an electric potential across a membrane. If there is an unequal distribution of charges across the membrane, then the difference in electric potential generates a force that drives ion diffusion until the charges are balanced on both sides of the membrane.[1]"
Electron_acceptor,Biology,5,"An electron acceptor is a chemical entity that accepts electrons transferred to it from another compound.[1] It is an oxidizing agent that, by virtue of its accepting electrons, is itself reduced in the process. Electron acceptors are sometimes mistakenly called electron receptors. 
 Typical[citation needed] oxidizing agents undergo permanent chemical alteration through covalent or ionic reaction chemistry, resulting in the complete[clarification needed] and irreversible transfer of one or more electrons. In many chemical circumstances, however, the transfer of electronic charge from an electron donor may be only fractional, meaning an electron is not completely transferred, but results in an electron resonance[clarification needed] between the donor and acceptor. This leads to the formation of charge transfer complexes in which the components largely retain their chemical identities.
 The electron accepting power of an acceptor molecule is measured by its electron affinity which is the energy released when filling the lowest unoccupied molecular orbital (LUMO).
 The energy required to remove one electron from the electron donor is its ionization energy (I). The energy liberated by attachment of an electron to the electron acceptor is the negative of its electron affinity (A). The overall system energy change (ΔE) for the charge transfer is then 




Δ

E
=
I
−
A



{  {\Delta }E=I-A\,}
. For an exothermic reaction, the energy liberated is of interest and is equal to 



−

Δ

E
=
A
−
I



{  -{\Delta }E=A-I\,}
.
 In chemistry, a class of electron acceptors that acquire not just one, but a set of two paired electrons that form a covalent bond with an electron donor molecule, is known as a Lewis acid. This phenomenon gives rise to the wide field of Lewis acid-base chemistry.[2] The driving forces for electron donor and acceptor behavior in chemistry is based on the concepts of electropositivity (for donors) and electronegativity (for acceptors) of atomic or molecular entities.
"
Electron_carrier,Biology,5,"The electron transport chain (ETC) is a series of complexes that transfer electrons from electron donors to electron acceptors via redox (both reduction and oxidation occurring simultaneously) reactions, and couples this electron transfer with the transfer of protons (H+ ions) across a membrane. The electron transport chain is built up of peptides, enzymes, and other molecules.
 The flow of electrons through the electron transport chain is an exergonic process. The energy from the redox reactions create an electrochemical proton gradient that drives the synthesis of adenosine triphosphate (ATP). In aerobic respiration, the flow of electrons terminates with molecular oxygen being the final electron acceptor. In anaerobic respiration, other electron acceptors are used, such as sulfate.
 In the electron transport chain, the redox reactions are driven by the Gibbs free energy state of the components. Gibbs free energy is related to a quantity called the redox potential. The complexes in the electron transport chain harvest the energy of the redox reactions that occur when transferring electrons from a low redox potential to a higher redox potential, creating an electrochemical gradient. It is the electrochemical gradient created that drives the synthesis of ATP via coupling with oxidative phosphorylation with ATP synthase.[1] The electron transport chain, and site of oxidative phosphorylation is found on the inner mitochondrial membrane. The energy stored from the process of respiration in reduced compounds (such as NADH and FADH) is used by the electron transport chain to pump protons into the inter membrane space, generating the electrochemical gradient over the inner mitochrondrial membrane. In photosynthetic eukaryotes, the electron transport chain is found on the thylakoid membrane. Here, light energy drives the reduction of components of the electron transport chain and therefore causes subsequent synthesis of ATP. In bacteria, the electron transport chain can vary over species but it always constitutes a set of redox reactions that are coupled to the synthesis of ATP, through the generation of an electrochemical gradient, and oxidative phosphorylation through ATP synthase.[2]"
Electron_donor,Biology,5,"An electron donor is a chemical entity that donates electrons to another compound. It is a reducing agent that, by virtue of its donating electrons, is itself oxidized in the process.
 Typical reducing agents undergo permanent chemical alteration through covalent or ionic reaction chemistry. This results in the complete and irreversible transfer of one or more electrons. In many chemical circumstances, however, the transfer of electronic charge to an electron acceptor may be only fractional, meaning an electron is not completely transferred, but results in an electron resonance between the donor and acceptor. This leads to the formation of charge transfer complexes in which the components largely retain their chemical identities.
 The electron donating power of a donor molecule is measured by its ionization potential which is the energy required to remove an electron from the highest occupied molecular orbital.
 The overall energy balance (ΔE), i.e., energy gained or lost, in an electron donor-acceptor transfer is determined by the difference between the acceptor's electron affinity (A) and the ionization potential (I):
 In chemistry, the class of electron donors that donate not just one, but a set of two paired electrons that form a covalent bond with an electron acceptor molecule, is known as a Lewis base. This phenomenon gives rise to the wide field of Lewis acid-base chemistry.[1] The driving forces for electron donor and acceptor behavior in chemistry is based on the concepts of electropositivity (for donors) and electronegativity (for acceptors) of atomic or molecular entities.
"
Electron_microscope,Biology,5,"An electron microscope is a microscope that uses a beam of accelerated electrons as a source of illumination. As the wavelength of an electron can be up to 100,000 times shorter than that of visible light photons, electron microscopes have a higher resolving power than light microscopes and can reveal the structure of smaller objects. A scanning transmission electron microscope has achieved better than 50 pm resolution in annular dark-field imaging mode[1] and magnifications of up to about 10,000,000× whereas most light microscopes are limited by diffraction to about 200 nm resolution and useful magnifications below 2000×.
 Electron microscopes use shaped magnetic fields to form electron optical lens systems that are analogous to the glass lenses of an optical light microscope.
 Electron microscopes are used to investigate the ultrastructure of a wide range of biological and inorganic specimens including microorganisms, cells, large molecules, biopsy samples, metals, and crystals. Industrially, electron microscopes are often used for quality control and failure analysis. Modern electron microscopes produce electron micrographs using specialized digital cameras and frame grabbers to capture the images.
"
Electron_transport_chain,Biology,5,"The electron transport chain (ETC) is a series of complexes that transfer electrons from electron donors to electron acceptors via redox (both reduction and oxidation occurring simultaneously) reactions, and couples this electron transfer with the transfer of protons (H+ ions) across a membrane. The electron transport chain is built up of peptides, enzymes, and other molecules.
 The flow of electrons through the electron transport chain is an exergonic process. The energy from the redox reactions create an electrochemical proton gradient that drives the synthesis of adenosine triphosphate (ATP). In aerobic respiration, the flow of electrons terminates with molecular oxygen being the final electron acceptor. In anaerobic respiration, other electron acceptors are used, such as sulfate.
 In the electron transport chain, the redox reactions are driven by the Gibbs free energy state of the components. Gibbs free energy is related to a quantity called the redox potential. The complexes in the electron transport chain harvest the energy of the redox reactions that occur when transferring electrons from a low redox potential to a higher redox potential, creating an electrochemical gradient. It is the electrochemical gradient created that drives the synthesis of ATP via coupling with oxidative phosphorylation with ATP synthase.[1] The electron transport chain, and site of oxidative phosphorylation is found on the inner mitochondrial membrane. The energy stored from the process of respiration in reduced compounds (such as NADH and FADH) is used by the electron transport chain to pump protons into the inter membrane space, generating the electrochemical gradient over the inner mitochrondrial membrane. In photosynthetic eukaryotes, the electron transport chain is found on the thylakoid membrane. Here, light energy drives the reduction of components of the electron transport chain and therefore causes subsequent synthesis of ATP. In bacteria, the electron transport chain can vary over species but it always constitutes a set of redox reactions that are coupled to the synthesis of ATP, through the generation of an electrochemical gradient, and oxidative phosphorylation through ATP synthase.[2]"
Embryo,Biology,5,"An embryo is the early stage of development of a multicellular organism. In general, in organisms that reproduce sexually, embryonic development is the part of the life cycle that begins just after fertilization and continues through the formation of body structures, such as tissues and organs. Each embryo starts development as a zygote, a single cell resulting from the fusion of gametes (i.e. fertilization of a female egg cell by a male sperm cell). In the first stages of embryonic development, a single-celled zygote undergoes many rapid cell divisions, called cleavage, to form a blastula, which looks similar to a ball of cells. Next, the cells in a blastula-stage embryo start rearranging themselves into layers in a process called gastrulation. These layers will each give rise to different parts of the developing multicellular organism, such as the nervous system, connective tissue, and organs...
 A newly developing human is typically referred to as an embryo until the ninth week after conception, when it is then referred to as a fetus. In other multicellular organisms, the word “embryo” can be used more broadly to any early developmental or life cycle stage prior to birth or hatching.
"
Embryology,Biology,5,"
 Embryology (from Greek ἔμβρυον, embryon, ""the unborn, embryo""; and -λογία, -logia) is the branch of biology that studies the prenatal development of gametes (sex cells), fertilization, and development of embryos and fetuses. Additionally, embryology encompasses the study of congenital disorders that occur before birth, known as teratology.
 Early embryology was proposed by Marcello Malpighi, and known as preformationism, the theory that organisms develop from pre-existing miniature versions of themselves. Then Aristotle proposed the theory that is now accepted, epigenesis. Epigenesis is the idea that organisms develop from seed or egg in a sequence of steps. Modern embryology developed from the work of Karl Ernst von Baer, though accurate observations had been made in Italy by anatomists such as Aldrovandi and Leonardo da Vinci in the Renaissance.
"
Endangered_species,Biology,5,"An endangered species is a species that is very likely to become extinct in the near future, either worldwide or in a particular political jurisdiction. Endangered species may be at risk due to factors such as habitat loss, poaching and invasive species. The International Union for Conservation of Nature (IUCN) Red List lists the global conservation status of many species, and various other agencies assess the status of species within particular areas. Many nations have laws that protect conservation-reliant species which, for example, forbid hunting, restrict land development, or create protected areas. Some endangered species are the target of extensive conservation efforts such as captive breeding and habitat restoration.
"
Endemism,Biology,5,"Endemism is the ecological state of a species being native to a single defined geographic location, such as an island, region, state or other defined zone, or habitat type; organisms that are indigenous to a place are not endemic to it if they are also found elsewhere. For example, the orange-breasted sunbird is exclusively found in the fynbos vegetation zone of southwestern South Africa and the glacier bear is a subspecies endemic to Southeast Alaska. The extreme opposite of an endemic species is one with a cosmopolitan distribution, having a global or widespread range. An alternative term for a species that is endemic is precinctive, which applies to species (and other taxonomic levels) that are restricted to a defined geographical area.
"
Endergonic_reaction,Biology,5,"In chemical thermodynamics, an endergonic reaction (also called a heat absorb nonspontaneous reaction or an unfavorable reaction) is a chemical reaction in which the standard change in free energy is positive, and an additional driving force is needed to perform this reaction. In layman's terms, the total amount of useful energy is negative (it takes more energy to start the reaction than what is received out of it) so the total energy is a negative net result. For an overall gain in the net result see exergonic reaction. Another way to phrase this is that useful energy must be absorbed from the surroundings into the workable system for the reaction to happen.
 Under constant temperature and constant pressure conditions, this means that the change in the standard Gibbs free energy would be positive,
 for the reaction at standard state (i.e. at standard pressure (1 bar), and standard concentrations (1 molar) of all the reagents).
 In metabolism, an endergonic process is anabolic, meaning that energy is stored; in many such anabolic processes energy is supplied by coupling the reaction to adenosine triphosphate (ATP) and consequently resulting in a high energy, negatively charged organic phosphate and positive adenosine diphosphate.
"
Endocrine_gland,Biology,5,"Endocrine glands are ductless glands of the endocrine system that secrete their products, hormones, directly into the blood. The major glands of the endocrine system include the pineal gland, pituitary gland, pancreas, ovaries, testes, thyroid gland, parathyroid gland, hypothalamus and adrenal glands. The hypothalamus and pituitary glands are neuroendocrine organs.
 1 Pineal gland
2 Pituitary gland
3 Thyroid gland
4 Thymus
5 Adrenal gland
6 Pancreas
7 Ovary (female)
 The pituitary gland hangs from the base of the brain by the pituitary stalk, and is enclosed by bone. It consists of a hormone-producing glandular portion of the anterior pituitary and a neural portion of the posterior pituitary, which is an extension of the hypothalamus. The hypothalamus regulates the hormonal output of the anterior pituitary and creates two hormones that it exports to the posterior pituitary for storage and later release.
 Four of the six anterior pituitary hormones are tropic hormones that regulate the function of other endocrine organs. Most anterior pituitary hormones exhibit a diurnal rhythm of release, which is subject to modification by stimuli influencing the hypothalamus.
 Somatotropic hormone or growth hormone (GH) is an anabolic hormone that stimulates the growth of all body tissues especially skeletal muscle and bone. It may act directly, or indirectly via insulin-like growth factors (IGFs). GH mobilizes fats, stimulates protein synthesis, and inhibits glucose uptake and metabolism. Secretion is regulated by growth hormone-releasing hormone (GHRH) and growth hormone inhibiting hormone (GHIH), or somatostatin. Hypersecretion causes gigantism in children and acromegaly in adults; hyposecretion in children causes pituitary dwarfism.
 Thyroid-stimulating hormone promotes normal development and activity of the thyroid gland. Thyrotropin-releasing hormone stimulates its release; negative feedback of thyroid hormone inhibits it.
 Adrenocorticotropic hormone stimulates the adrenal cortex to release corticosteroids. Adrenocorticotropic hormone release is triggered by corticotropin-releasing hormone and inhibited by rising glucocorticoid levels.
 The gonadotropins—follicle-stimulating hormone and luteinizing hormone regulate the functions of the gonads in both sexes. Follicle-stimulating hormone stimulates sex cell production; luteinizing hormone stimulates gonadal hormone production. Gonadotropin levels rise in response to gonadotropin-releasing hormone. Negative feedback of gonadal hormones inhibits gonadotropin release.
 Prolactin promotes milk production in human females. Its secretion is prompted by prolactin-releasing hormone and inhibited by prolactin-inhibiting hormone.
 The intermediate lobe of the pituitary gland secretes only one enzyme that is melanocyte stimulating hormone. It is linked with the formation of the black pigment in our skin called melanin.
 The neurohypophysis stores and releases two hypothalamic hormones:
"
Endocrine_system,Biology,5,"
 The endocrine system is a chemical messenger system comprising feedback loops of the hormones released by internal glands of an organism directly into the circulatory system, regulating distant target organs. In humans, the major endocrine glands are the thyroid gland and the adrenal glands.  In vertebrates, the hypothalamus is the neural control center for all endocrine systems. The study of the endocrine system and its disorders is known as endocrinology. Endocrinology is a branch of internal medicine.[1] A number of glands that signal each other in sequence are usually referred to as an axis, such as the hypothalamic-pituitary-adrenal axis. In addition to the specialized endocrine organs mentioned above, many other organs that are part of other body systems have secondary endocrine functions, including bone, kidneys, liver, heart and gonads. For example, the kidney secretes the endocrine hormone erythropoietin. Hormones can be amino acid complexes, steroids, eicosanoids, leukotrienes, or prostaglandins.[1] The endocrine system can be contrasted to both exocrine glands, which secrete hormones to the outside of the body, and paracrine signalling between cells over a relatively short distance.  Endocrine glands have no ducts, are vascular, and commonly have intracellular vacuoles or granules that store their hormones. In contrast, exocrine glands, such as salivary glands, sweat glands, and glands within the gastrointestinal tract, tend to be much less vascular and have ducts or a hollow lumen. 
 The word endocrine derives via New Latin from the Greek words ἔνδον, endon, ""inside, within,"" and ""crine"" from the κρίνω, krīnō, ""to separate, distinguish"".
"
Endocytosis,Biology,5,"Endocytosis is a cellular process in which substances are brought into the cell. The material to be internalized is surrounded by an area of cell membrane, which then buds off inside the cell to form a vesicle containing the ingested material. Endocytosis includes pinocytosis (cell drinking) and phagocytosis (cell eating). It is a form of active transport.
"
Endoderm,Biology,5,"Endoderm is one of the three primary germ layers in the very early embryo. The other two layers are the ectoderm (outside layer) and mesoderm (middle layer), with the endoderm being the innermost layer.[1] Cells migrating inward along the archenteron form the inner layer of the gastrula, which develops into the endoderm.[citation needed] The endoderm consists at first of flattened cells, which subsequently become columnar. It forms the epithelial lining of multiple systems.
 In plant biology, endoderm corresponds to the innermost part of the cortex (bark) in young shoots and young roots often consisting of a single cell layer. As the plant becomes older, more endoderm will lignify.
"
Endogenous,Biology,5,"If Wiktionary has a definition already, change this tag to {{TWCleanup2}} or else consider a soft redirect to Wiktionary by replacing the text on this page with {{Wi}}. 
If Wiktionary does not have the definition yet, consider moving the whole article to Wiktionary by replacing this tag with the template {{Copy to Wiktionary}}. 
 This template will no longer automatically categorize articles as candidates to move to Wiktionary.
 Endogenous substances and processes are those that originate from within a system such as an organism, tissue, or cell.[1] Endogenous substances and processes contrast with exogenous ones, such as drugs, which originate from outside of the organism.
"
Endoplasmic_reticulum,Biology,5,"The endoplasmic reticulum (ER) is, in essence, the transportation system of the eukaryotic cell, and has many other important functions such as protein folding. It is a type of organelle made up of two subunits – rough endoplasmic reticulum (RER), and smooth endoplasmic reticulum (SER). The endoplasmic reticulum is found in most eukaryotic cells and forms an interconnected network of flattened, membrane-enclosed sacs known as cisternae (in the RER), and tubular structures in the SER. The membranes of the ER are continuous with the outer nuclear membrane. The endoplasmic reticulum is not found in red blood cells, or spermatozoa.
 The two types of ER share many of the same proteins and engage in certain common activities such as the synthesis of certain lipids and cholesterol. Different types of cells contain different ratios of the two types of ER depending on the activities of the cell.
 The outer (cytosolic) face of the rough endoplasmic reticulum is studded with ribosomes that are the sites of protein synthesis. The rough endoplasmic reticulum is especially prominent in cells such as hepatocytes. The smooth endoplasmic reticulum lacks ribosomes and functions in lipid synthesis but not metabolism, the production of steroid hormones, and detoxification.[1] The smooth endoplasmic reticulum is especially abundant in mammalian liver and gonad cells.
"
Endosperm,Biology,5,"The endosperm is a tissue produced inside the seeds of most of the flowering plants following fertilization. It is triploid (meaning three chromosome sets per nucleus) in most species.[1] It surrounds the embryo and provides nutrition in the form of starch, though it can also contain oils and protein. This can make endosperm a source of nutrition in animal diet. For example, wheat endosperm is ground into flour for bread (the rest of the grain is included as well in whole wheat flour), while barley endosperm is the main source of sugars for beer production. Other examples of endosperm that forms the bulk of the edible portion are coconut ""meat"" and coconut ""water"",[2] and corn. Some plants, such as orchids, lack endosperm in their seeds.
"
Endosymbiotic_theory,Biology,5,"Symbiogenesis, or endosymbiotic theory, is the leading evolutionary theory of the origin of eukaryotic cells from prokaryotic organisms.[1] The theory holds that mitochondria, plastids such as chloroplasts, and possibly other organelles of eukaryotic cells are descended from formerly free-living prokaryotes (more closely related to bacteria than archaea) taken one inside the other in endosymbiosis.
 Mitochondria appear to be phylogenetically related to Rickettsiales proteobacteria, and chloroplasts to nitrogen-fixing filamentous cyanobacteria. The theory was first articulated in 1905 and 1910 by the Russian botanist Konstantin Mereschkowski, and advanced and substantiated with microbiological evidence by Lynn Margulis in 1967. Among the many lines of evidence supporting symbiogenesis are that new mitochondria and plastids are formed only through binary fission, and that cells cannot create new ones otherwise; that the transport proteins called porins are found in the outer membranes of mitochondria, chloroplasts and bacterial cell membranes; that cardiolipin is found only in the inner mitochondrial membrane and bacterial cell membranes; and that some mitochondria and plastids contain single circular DNA molecules similar to the chromosomes of bacteria.
"
Endotherm,Biology,5,"An endotherm (from Greek ἔνδον endon ""within"" and θέρμη thermē ""heat"") is an organism that maintains its body at a metabolically favorable temperature, largely by  the use of heat set free by its internal bodily functions instead of relying almost purely on ambient heat. Such internally generated heat is mainly an incidental product of the animal's routine metabolism, but under conditions of excessive cold or low activity an endotherm might apply special mechanisms adapted specifically to heat production. Examples include special-function muscular exertion such as shivering, and uncoupled oxidative metabolism such as within brown adipose tissue.
Only birds and mammals are extant universally endothermic groups of animals.  Certain lamnid sharks, tuna and billfishes are also endothermic.
 In common parlance, endotherms are characterized as ""warm-blooded"". The opposite of endothermy is ectothermy, although in general, there is no absolute or clear separation between the nature of endotherms and ectotherms.
"
Entomology,Biology,5,"Entomology (from Ancient Greek  ἔντομον (entomon) 'insect', and  -λογία (-logia) 'study of'[1]) is the scientific study of insects, a branch of zoology. In the past the term ""insect"" was vaguer, and historically the definition of entomology included the study of animals in other arthropod groups, such as arachnids, myriapods, and crustaceans. This wider meaning may still be encountered in informal use.
 Like several of the other fields that are categorized within zoology, entomology is a taxon-based category; any form of scientific study in which there is a focus on insect-related inquiries is, by definition, entomology. Entomology therefore overlaps with a cross-section of topics as diverse as molecular genetics, behavior, biomechanics, biochemistry, systematics, physiology, developmental biology, ecology, morphology, and paleontology.
 At some 1.3 million described species, insects account for more than two-thirds of all known organisms,[2] some dating back around 400 million years.  They have many kinds of interactions with humans and other forms of life on earth.
"
Environmental_biology,Biology,5,"
 Environmental science is an interdisciplinary academic field that integrates physical, biological and information sciences (including ecology, biology, physics, chemistry, plant science, zoology, mineralogy, oceanography, limnology, soil science, geology and physical geography, and atmospheric science) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment.[1]  Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.[2] Environmental studies incorporates more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect.
 Environmental scientists study subjects like the understanding of earth processes, evaluating alternative energy systems, pollution control and mitigation, natural resource management, and the effects of global climate change. Environmental issues almost always include an interaction of physical, chemical, and biological processes. Environmental scientists bring a systems approach to the analysis of environmental problems. Key elements of an effective environmental scientist include the ability to relate space, and time relationships as well as quantitative analysis.
 Environmental science came alive as a substantive, active field of scientific investigation in the 1960s and 1970s driven by (a) the need for a multi-disciplinary approach to analyze complex environmental problems, (b) the arrival of substantive environmental laws requiring specific environmental protocols of investigation and (c) the growing public awareness of a need for action in addressing environmental problems. Events that spurred this development included the publication of Rachel Carson's landmark environmental book Silent Spring[3] along with major environmental issues becoming very public, such as the 1969 Santa Barbara oil spill, and the Cuyahoga River of Cleveland, Ohio, ""catching fire"" (also in 1969), and helped increase the visibility of environmental issues and create this new field of study.
"
Enzyme,Biology,5,"
 Enzymes /ˈɛnzaɪmz/ are proteins that act as biological catalysts (biocatalysts). Catalysts accelerate chemical reactions. The molecules upon which enzymes may act are called substrates, and the enzyme converts the substrates into different molecules known as products. Almost all metabolic processes in the cell need enzyme catalysis in order to occur at rates fast enough to sustain life.[1]:8.1 Metabolic pathways depend upon enzymes to catalyze individual steps. The study of enzymes is called enzymology and a new field of pseudoenzyme analysis has recently grown up, recognising that during evolution, some enzymes have lost the ability to carry out biological catalysis, which is often reflected in their amino acid sequences and unusual 'pseudocatalytic' properties.[2][3] Enzymes are known to catalyze more than 5,000 biochemical reaction types.[4] Other biocatalysts are catalytic RNA molecules, called ribozymes. Enzymes' specificity comes from their unique three-dimensional structures.
 Like all catalysts, enzymes increase the reaction rate by lowering its activation energy. Some enzymes can make their conversion of substrate to product occur many millions of times faster. An extreme example is orotidine 5'-phosphate decarboxylase, which allows a reaction that would otherwise take millions of years to occur in milliseconds.[5][6] Chemically, enzymes are like any catalyst and are not consumed in chemical reactions, nor do they alter the equilibrium of a reaction. Enzymes differ from most other catalysts by being much more specific. Enzyme activity can be affected by other molecules: inhibitors are molecules that decrease enzyme activity, and activators are molecules that increase activity. Many therapeutic drugs and poisons are enzyme inhibitors. An enzyme's activity decreases markedly outside its optimal temperature and pH, and many enzymes are (permanently) denatured when exposed to excessive heat, losing their structure and catalytic properties.
 Some enzymes are used commercially, for example, in the synthesis of antibiotics. Some household products use enzymes to speed up chemical reactions: enzymes in biological washing powders break down protein, starch or fat stains on clothes, and enzymes in meat tenderizer break down proteins into smaller molecules, making the meat easier to chew.
"
Epidemiology,Biology,5,"
Epidemiology is the study and analysis of the distribution (who, when, and where), patterns and determinants of health and disease conditions in defined populations.
 It is a cornerstone of public health, and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare. Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review).  Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences.[1] Major areas of epidemiological study include disease causation, transmission, outbreak investigation, disease surveillance, environmental epidemiology, forensic epidemiology, occupational epidemiology, screening, biomonitoring, and comparisons of treatment effects such as in clinical trials. Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment.
 Epidemiology, literally meaning ""the study of what is upon the people"", is derived from Greek  epi 'upon, among',  demos  'people, district', and  logos 'study, word, discourse', suggesting that it applies only to human populations. However, the term is widely used in studies of zoological populations (veterinary epidemiology), although the term ""epizoology"" is available, and it has also been applied to studies of plant populations (botanical or plant disease epidemiology).[2] The distinction between ""epidemic"" and ""endemic"" was first drawn by Hippocrates,[3] to distinguish between diseases that are ""visited upon"" a population (epidemic) from those that ""reside within"" a population (endemic).[4] The term ""epidemiology"" appears to have first been used to describe the study of epidemics in 1802 by the Spanish physician Villalba in Epidemiología Española.[4] Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic.
 The term epidemiology is now widely applied to cover the description and causation of not only epidemic disease, but of disease in general, and even many non-disease, health-related conditions, such as high blood pressure, depression and obesity. Therefore, this epidemiology is based upon how the pattern of the disease causes change in the function of human beings.
"
Epigenetics,Biology,5,"
 In biology, epigenetics is the study of heritable phenotype changes that do not involve alterations in the DNA sequence.[1] The Greek prefix epi- (ἐπι- ""over, outside of, around"") in epigenetics implies features that are ""on top of"" or ""in addition to"" the traditional genetic basis for inheritance.[2] Epigenetics most often involves changes that affect gene activity and expression, but the term can also be used to describe any heritable phenotypic change. Such effects on cellular and physiological phenotypic traits may result from external or environmental factors, or be part of normal development. The standard definition of epigenetics requires these alterations to be heritable[3][4] in the progeny of either cells or organisms.
 The term also refers to the changes themselves: functionally relevant changes to the genome that do not involve a change in the nucleotide sequence. Examples of mechanisms that produce such changes are DNA methylation and histone modification, each of which alters how genes are expressed without altering the underlying DNA sequence. Gene expression can be controlled through the action of repressor proteins that attach to silencer regions of the DNA. These epigenetic changes may last through cell divisions for the duration of the cell's life, and may also last for multiple generations, even though they do not involve changes in the underlying DNA sequence of the organism;[5] instead, non-genetic factors cause the organism's genes to behave (or ""express themselves"") differently.[6] One example of an epigenetic change in eukaryotic biology is the process of cellular differentiation. During morphogenesis, totipotent stem cells become the various pluripotent cell lines of the embryo, which in turn become fully differentiated cells. In other words, as a single fertilized egg cell – the zygote – continues to divide, the resulting daughter cells change into all the different cell types in an organism, including neurons, muscle cells, epithelium, endothelium of blood vessels, etc., by activating some genes while inhibiting the expression of others.[7] Historically, some phenomena not necessarily heritable have also been described as epigenetic. For example, the term ""epigenetic"" has been used to describe any modification of chromosomal regions, especially histone modifications, whether or not these changes are heritable or associated with a phenotype. The consensus definition now requires a trait to be heritable for it to be considered epigenetic.[4] A similar concept to epigenetics can be found in the study of Traditional Chinese medicine's understanding of postnatal Jing or essence.[8]"
Epiphyte,Biology,5,"An epiphyte is an organism that grows on the surface of a plant and derives its moisture and nutrients from the air, rain, water (in marine environments) or from debris accumulating around it. Epiphytes take part in nutrient cycles and add to both the diversity and biomass of the ecosystem in which they occur, like any other organism. They are an important source of food for many species. Typically, the older parts of a plant will have more epiphytes growing on them. Epiphytes differ from parasites in that they grow on other plants for physical support and do not necessarily affect the host negatively. An organism that grows on another organism that is not a plant may be called an epibiont.[1] Epiphytes are usually found in the temperate zone (e.g., many mosses, liverworts, lichens, and algae) or in the tropics (e.g., many ferns, cacti, orchids, and bromeliads).[2] Epiphyte species make good houseplants due to their minimal water and soil requirements.[3] Epiphytes provide a rich and diverse habitat for other organisms including animals, fungi, bacteria, and myxomycetes.[4] Epiphyte is one of the subdivisions of the Raunkiær system.
The term epiphytic derives from the Greek epi- (meaning 'upon') and phyton (meaning 'plant'). Epiphytic plants are sometimes called ""air plants"" because they do not root in soil. However, there are many aquatic species of algae that are epiphytes on other aquatic plants (seaweeds or aquatic angiosperms).
"
Essential_nutrient,Biology,5,"A nutrient is a substance used by an organism to survive, grow, and reproduce. The requirement for dietary nutrient intake applies to animals, plants, fungi, and protists. Nutrients can be incorporated into cells for metabolic purposes or excreted by cells to create non-cellular structures, such as hair, scales, feathers, or exoskeletons. Some nutrients can be metabolically converted to smaller molecules in the process of releasing energy, such as for carbohydrates, lipids, proteins, and fermentation products (ethanol or vinegar), leading to end-products of water and carbon dioxide. All organisms require water. Essential nutrients for animals are the energy sources, some of the amino acids that are combined to create proteins, a subset of fatty acids, vitamins and certain minerals. Plants require more diverse minerals absorbed through roots, plus carbon dioxide and oxygen absorbed through leaves. Fungi live on dead or living organic matter and meet nutrient needs from their host.
 Different types of organism have different essential nutrients. Ascorbic acid (vitamin C) is essential, meaning it must be consumed in sufficient amounts, to humans and some other animal species, but not to all animals and not to plants, which are able to synthesize it. Nutrients may be organic or inorganic: organic compounds include most compounds containing carbon, while all other chemicals are inorganic. Inorganic nutrients include nutrients such as iron, selenium, and zinc, while organic nutrients include, among many others, energy-providing compounds and vitamins.
 A classification used primarily to describe nutrient needs of animals divides nutrients into macronutrients and micronutrients. Consumed in relatively large amounts (grams or ounces), macronutrients (carbohydrates, fats, proteins, water) are primarily used to generate energy or to incorporate into tissues for growth and repair. Micronutrients are needed in smaller amounts (milligrams or micrograms); they have subtle biochemical and physiological roles in cellular processes, like vascular functions or nerve conduction. Inadequate amounts of essential nutrients, or diseases that interfere with absorption, result in a deficiency state that compromises growth, survival and reproduction. Consumer advisories for dietary nutrient intakes, such as the United States Dietary Reference Intake, are based on deficiency outcomes[clarification needed] and provide macronutrient and micronutrient guides for both lower and upper limits of intake. In many countries, macronutrients and micronutrients in significant content[clarification needed] are required by regulations to be displayed on food product labels. Nutrients in larger quantities than the body needs may have harmful effects.[1] Edible plants also contain thousands of compounds generally called phytochemicals which have unknown effects on disease or health, including a diverse class with non-nutrient status called polyphenols, which remain poorly understood as of 2017.
"
Estrogen,Biology,5,"
 Estrogen, or oestrogen, is a category of sex hormone responsible for the development and regulation of the female reproductive system and secondary sex characteristics. There are three major endogenous estrogens that have estrogenic hormonal activity: estrone (E1), estradiol (E2), and estriol (E3). Estradiol, an estrane, is the most potent and prevalent. Another estrogen called estetrol (E4) is produced only during pregnancy. 
 Estrogens are synthesized in all vertebrates[1] and some insects.[2] Their presence in both vertebrates and insects suggests that estrogenic sex hormones have an ancient evolutionary history. Quantitatively, estrogens circulate at lower levels than androgens in both men and women.[3] While estrogen levels are significantly lower in males compared to females, estrogens nevertheless have important physiological roles in males.[4] Like all steroid hormones, estrogens readily diffuse across the cell membrane. Once inside the cell, they bind to and activate estrogen receptors (ERs) which in turn modulate the expression of many genes.[5] Additionally, estrogens bind to and activate rapid-signaling membrane estrogen receptors (mERs),[6][7] such as GPER (GPR30).[8] In addition to their role as natural hormones, estrogens are used as medications, for instance in menopausal hormone therapy, hormonal birth control and transgender hormone therapy for transgender women and nonbinary people.
"
Ethology,Biology,5,"
 Ethology is the scientific and objective study of animal behaviour, usually with a focus on behaviour under natural conditions, and viewing behaviour as an evolutionarily adaptive trait.[1] Behaviourism as a term also describes the scientific and objective study of animal behaviour, usually referring to measured responses to stimuli or to trained behavioural responses in a laboratory context, without a particular emphasis on evolutionary adaptivity.[2] Throughout history, different naturalists have studied aspects of animal behaviour. Ethology has its scientific roots in the work of Charles Darwin (1809-1882) and of American and German ornithologists of the late 19th and early 20th century,[citation needed] including Charles O. Whitman, Oskar Heinroth (1871-1945), and Wallace Craig. The modern discipline of ethology is generally considered to have begun during the 1930s with the work of Dutch biologist Nikolaas Tinbergen (1907-1988) and of Austrian biologists Konrad Lorenz and Karl von Frisch (1886-1982), the three recipients of the 1973 Nobel Prize in Physiology or Medicine.[3] Ethology combines laboratory and field science, with a strong relation to some other disciplines such as neuroanatomy, ecology, and evolutionary biology. Ethologists typically show interest in a behavioural process rather than in a particular animal group,[4] and often study one type of behaviour, such as aggression, in a number of unrelated species.
 Ethology is a rapidly growing field. Since the dawn of the 21st century researchers have re-examined and reached new conclusions in many aspects of animal communication, emotions, culture, learning and sexuality that the scientific community long thought it understood. New fields, such as neuroethology, have developed.
 Understanding ethology or animal behaviour can be important in animal training. Considering the natural behaviours of different species or breeds enables trainers to select the individuals best suited to perform the required task. It also enables trainers to encourage the performance of naturally occurring behaviours and the discontinuance of undesirable behaviours.[5]"
Eukaryote,Biology,5,"
 Eukaryotes (/juːˈkærioʊts, -əts/) are organisms whose cells have a nucleus enclosed within a nuclear envelope.[3][4][5] Eukaryotes belong to the domain Eukaryota or Eukarya; their name comes from the Greek εὖ (eu, ""well"" or ""good"") and κάρυον (karyon, ""nut"" or ""kernel"").[6] The domain Eukaryota makes up one of the domains of life in the three-domain system, which is typically regarded as obsolete: The two other domains are Bacteria and Archaea (together known as prokaryotes), and the Eukaryote are usually now regarded as having emerged in the Archaea in or as sister of the now cultivated Asgard Archaea.[7][8][9][10] Eukaryotes represent a tiny minority of the number of living organisms;[11] however, due to their generally much larger size, their collective worldwide biomass is estimated to be about equal to that of prokaryotes.[11] Eukaryotes evolved approximately 1.6–2.1 billion years ago, during the Proterozoic eon.
 Eukaryotic organisms that cannot be classified under the kingdoms Plantae, Animalia or Fungi are sometimes grouped in the kingdom Protista.
 Eukaryotic cells typically contain membrane-bound organelles such as mitochondria and Golgi apparatus, and chloroplasts can be found in plants and algae; these organelles are unique to eukaryotes, although primitive organelles can be found in prokaryotes.[12] As well as being unicellular, eukaryotes may also be multicellular and include many cell types forming different kinds of tissue; in comparison, prokaryotes are typically unicellular. Animals, plants, and fungi are the most familiar eukaryotes; other eukaryotes are sometimes called protists.[13] Eukaryotes can reproduce both asexually through mitosis and sexually through meiosis and gamete fusion. In mitosis, one cell divides to produce two genetically identical cells. In meiosis, DNA replication is followed by two rounds of cell division to produce four haploid daughter cells. These act as sex cells (gametes). Each gamete has just one set of chromosomes, each a unique mix of the corresponding pair of parental chromosomes resulting from genetic recombination during meiosis.[14]"
Evolution,Biology,5,"
 Evolution is change in the heritable characteristics of biological populations over successive generations.[1][2] These characteristics are the expressions of genes that are passed on from parent to offspring during reproduction. Different characteristics tend to exist within any given population as a result of mutation, genetic recombination and other sources of genetic variation.[3] Evolution occurs when evolutionary processes such as natural selection (including sexual selection) and genetic drift act on this variation, resulting in certain characteristics becoming more common or rare within a population.[4] It is this process of evolution that has given rise to biodiversity at every level of biological organisation, including the levels of species, individual organisms and molecules.[5][6] The scientific theory of evolution by natural selection was conceived independently by Charles Darwin and Alfred Russel Wallace in the mid-19th century and was set out in detail in Darwin's book On the Origin of Species.[7] Evolution by natural selection was first demonstrated by the observation that more offspring are often produced than can possibly survive. This is followed by three observable facts about living organisms: (1) traits vary among individuals with respect to their morphology, physiology and behaviour (phenotypic variation), (2) different traits confer different rates of survival and reproduction (differential fitness) and (3) traits can be passed from generation to generation (heritability of fitness).[8] Thus, in successive generations members of a population are more likely to be replaced by the progenies of parents with favourable characteristics that have enabled them to survive and reproduce in their respective environments. In the early 20th century, other competing ideas of evolution such as mutationism and orthogenesis were refuted as the modern synthesis reconciled Darwinian evolution with classical genetics, which established adaptive evolution as being caused by natural selection acting on Mendelian genetic variation.[9] All life on Earth shares a last universal common ancestor (LUCA)[10][11][12] that lived approximately 3.5–3.8 billion years ago.[13] The fossil record includes a progression from early biogenic graphite,[14] to microbial mat fossils,[15][16][17] to fossilised multicellular organisms. Existing patterns of biodiversity have been shaped by repeated formations of new species (speciation), changes within species (anagenesis) and loss of species (extinction) throughout the evolutionary history of life on Earth.[18] Morphological and biochemical traits are more similar among species that share a more recent common ancestor, and can be used to reconstruct phylogenetic trees.[19][20] Evolutionary biologists have continued to study various aspects of evolution by forming and testing hypotheses as well as constructing theories based on evidence from the field or laboratory and on data generated by the methods of mathematical and theoretical biology. Their discoveries have influenced not just the development of biology but numerous other scientific and industrial fields, including agriculture, medicine and computer science.[21]"
Evolutionary_biology,Biology,5,"
Evolutionary biology  is the subfield of biology that studies the evolutionary processes (natural selection, common descent, speciation) that produced the diversity of life on Earth. In the 1930s, the discipline of evolutionary biology emerged through what Julian Huxley called the modern synthesis of understanding, from previously unrelated fields of biological research, such as genetics and ecology, systematics and paleontology.
 The investigational range of current research widened to encompass the genetic architecture of adaptation, molecular evolution, and the different forces that contribute to evolution, such as sexual selection, genetic drift, and biogeography. Moreover, the newer field of evolutionary developmental biology (""evo-devo"") investigates how embryogenesis, the development of the embryo, is controlled, thus yielding a wider synthesis that integrates developmental biology with the fields of study covered by the earlier evolutionary synthesis.
"
Exocytosis,Biology,5,"Exocytosis (/ˌɛksoʊsaɪˈtoʊsɪs/[1][2]) is a form of active transport and  bulk transport in which a cell transports molecules (e.g., neurotransmitters and proteins) out of the cell (exo- + cytosis) by secreting them through an energy-dependent process. Exocytosis and its counterpart, endocytosis, are used by all cells because most chemical substances important to them are large polar molecules that cannot pass through the hydrophobic portion of the cell membrane by passive means. Exocytosis is the process by which a large amount of molecules are released; thus it is a form of bulk transport.
 In exocytosis, membrane-bound secretory vesicles are carried to the cell membrane, and their contents (i.e., water-soluble molecules) are secreted into the extracellular environment. This secretion is possible because the vesicle transiently fuses with the plasma membrane. In the context of neurotransmission, neurotransmitters are typically released from synaptic vesicles into the synaptic cleft via exocytosis; however, neurotransmitters can also be released via reverse transport through membrane transport proteins.
 Exocytosis is also a mechanism by which cells are able to insert membrane proteins (such as ion channels and cell surface receptors), lipids, and other components into the cell membrane. Vesicles containing these membrane components fully fuse with and become part of the outer cell membrane.
"
Exogenous,Biology,5,"If Wiktionary has a definition already, change this tag to {{TWCleanup2}} or else consider a soft redirect to Wiktionary by replacing the text on this page with {{Wi}}. 
If Wiktionary does not have the definition yet, consider moving the whole article to Wiktionary by replacing this tag with the template {{Copy to Wiktionary}}. 
 This template will no longer automatically categorize articles as candidates to move to Wiktionary.
 In a variety of contexts, exogeny or exogeneity (from Greek  ἔξω éxō 'outside' and  -γένεια -géneia 'to produce') is the fact of an action or object originating externally. It contrasts with endogeneity or endogeny, the fact of being influenced within a system.
"
Exponential_growth,Biology,5,"Exponential growth is a specific way that a quantity may increase over time. It occurs when the instantaneous rate of change (that is, the derivative) of a quantity with respect to time is proportional to the quantity itself. Described as a function, a quantity undergoing exponential growth is an exponential function of time, that is, the variable representing time is the exponent (in contrast to other types of growth, such as quadratic growth). 
 If the constant of proportionality is negative, then the quantity decreases over time, and is said to be undergoing exponential decay instead. In the case of a discrete domain of definition with equal intervals, it is also called geometric growth or geometric decay since the function values form a geometric progression.
 The formula for exponential growth of a variable x at the growth rate r, as time t goes on in discrete intervals (that is, at integer times 0, 1, 2, 3, ...), is
 where x0 is the value of x at time 0. The growth of a bacterial colony is often used to illustrate it. One bacterium splits itself into two, each of which splits itself resulting in four, then eight, 16, 32, and so on. The rate of increase keeps increasing because it is proportional to the ever-increasing number of bacteria. Growth like this is observed in real-life activity or phenomena, such as the spread of virus infection, the growth of debt due to compound interest, and the spread of viral videos. In real cases, initial exponential growth often does not last forever, instead slowing down eventually due to upper limits caused by external factors and turning into logistic growth.
"
External_fertilization,Biology,5,"External fertilization is a mode of reproduction in which a male organism's sperm fertilizes a female organism's egg outside of the female's body.[1]
It is contrasted with internal fertilization, in which sperm are introduced via insemination and then combine with an egg inside the body of a female organism.[2] External fertilization typically occurs in water or a moist area to facilitate the movement of sperm to the egg.[3] The release of eggs and sperm into the water is known as spawning.[4] In motile species, spawning females often travel to a suitable location to release their eggs. However, sessile species are less able to move to spawning locations and must release gametes locally.[4] Among vertebrates, external fertilization is most common in amphibians and fish.[5] Invertebrates utilizing external fertilization are mostly benthic, sessile, or both, including animals such as coral, sea anemones, and tube-dwelling polychaetes.[3] Benthic marine plants also use external fertilization to reproduce.[3] Environmental factors and timing are key challenges to the success of external fertilization. While in the water, the male and female must both release gametes at similar times in order to fertilize the egg.[3] Gametes spawned into the water may also be washed away, eaten, or damaged by external factors.
"
Extinction,Biology,5,"Extinction is the termination of a kind of organism or of a group of kinds (taxon), usually a species. The moment of extinction is generally considered to be the death of the last individual of the species, although the capacity to breed and recover may have been lost before this point. Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively. This difficulty leads to phenomena such as Lazarus taxa, where a species presumed extinct abruptly ""reappears"" (typically in the fossil record) after a period of apparent absence.
 More than 99% of all species that ever lived on Earth, amounting to over five billion species,[1] are estimated to have died out.[2][3][4][5] It is estimated that there are currently around 8.7 million species of eukaryote globally,[6] and possibly many times more if microorganisms, like bacteria, are included.[7] Notable extinct animal species include non-avian dinosaurs, saber-toothed cats, dodos, mammoths, ground sloths, thylacines, trilobites and golden toads.
 Through evolution, species arise through the process of speciation—where new varieties of organisms arise and thrive when they are able to find and exploit an ecological niche—and species become extinct when they are no longer able to survive in changing conditions or against superior competition. The relationship between animals and their ecological niches has been firmly established.[8] A typical species becomes extinct within 10 million years of its first appearance,[5] although some species, called living fossils, survive with little to no morphological change for hundreds of millions of years.
 Mass extinctions are relatively rare events; however, isolated extinctions are quite common. Only recently have extinctions been recorded and scientists have become alarmed at the current high rate of extinctions.[9][10][11][12] Most species that become extinct are never scientifically documented. Some scientists estimate that up to half of presently existing plant and animal species may become extinct by 2100.[13] A 2018 report indicated that the phylogenetic diversity of 300 mammalian species erased during the human era since the Late Pleistocene would require 5 to 7 million years to recover.[14] According to the 2019 Global Assessment Report on Biodiversity and Ecosystem Services by IPBES, the biomass of wild mammals has fallen by 82%, natural ecosystems have lost about half their area and a million species are at risk of extinction—all largely as a result of human actions. Twenty-five percent of plant and animal species are threatened with extinction.[15][16][17] In June 2019, one million species of plants and animals were at risk of extinction. At least 571 species are lost since 1750 but likely many more. The main cause of the extinctions is the destruction of natural habitats by human activities, such as cutting down forests and converting land into fields for farming.[18] A dagger symbol (†) placed next to the name of a species or other taxon normally indicates its status as extinct.
"
Extranuclear_inheritance,Biology,5,Extranuclear inheritance or cytoplasmic inheritance is the transmission of genes that occur outside the nucleus. It is found in most eukaryotes and is commonly known to occur in cytoplasmic organelles such as mitochondria and chloroplasts or from cellular parasites like viruses or bacteria.[1][2][3]
Facultative_anaerobic_organism,Biology,5,"A facultative anaerobe is an organism that makes ATP by aerobic respiration if oxygen is present, but is capable of switching to fermentation if oxygen is absent.
 Some examples of facultatively anaerobic bacteria are Staphylococcus spp.,[1] Escherichia coli, Salmonella, Listeria spp.,[2] Shewanella oneidensis and Yersinia pestis. Certain eukaryotes are also facultative anaerobes, including fungi such as Saccharomyces cerevisiae[3] and many aquatic invertebrates such as nereid polychaetes.[4]"
Family_(biology),Biology,5,"
 Family (Latin: familia, plural familiae) is one of the eight major hierarchical taxonomic ranks in Linnaean taxonomy; it is classified between order and genus. A family may be divided into subfamilies, which are intermediate ranks between the ranks of family and genus. The official family names are Latin in origin; however, popular names are often used: for example, walnut trees and hickory trees belong to the family Juglandaceae, but that family is commonly referred to as being the ""walnut family"".
 What belongs to a family—or if a described family should be recognized at all—are proposed and determined by practicing taxonomists. There are no hard rules for describing or recognizing a family. Taxonomists often take different positions about descriptions, and there may be no broad consensus across the scientific community for some time. The publishing of new data and opinions often enables adjustments and consensus.
"
Fermentation,Biology,5,"Fermentation is a metabolic process that produces chemical changes in organic substrates through the action of enzymes. In biochemistry, it is narrowly defined as the extraction of energy from carbohydrates in the absence of oxygen. In food production, it may more broadly refer to any process in which the activity of microorganisms brings about a desirable change to a foodstuff or beverage.[1] The science of fermentation is known as zymology.
 In microorganisms, fermentation is the primary means of producing adenosine triphosphate (ATP) by the degradation of organic nutrients anaerobically.[2] Humans have used fermentation to produce foodstuffs and beverages since the Neolithic age. For example, fermentation is used for preservation in a process that produces lactic acid found in such sour foods as pickled cucumbers, kombucha, kimchi, and yogurt, as well as for producing alcoholic beverages such as wine and beer. Fermentation also occurs within the gastrointestinal tracts of all animals, including humans.[3]"
Fitness_(biology),Biology,5,"Fitness (often denoted 



w


{  w}
 or ω in population genetics models) is the quantitative representation of natural and sexual selection within evolutionary biology. It can be defined either with respect to a genotype or to a phenotype in a given environment. In either case, it describes individual reproductive success and is equal to the average contribution to the gene pool of the next generation that is made by individuals of the specified genotype or phenotype. The fitness of a genotype is manifested through its phenotype, which is also affected by the developmental environment. The fitness of a given phenotype can also be different in different selective environments.
 With asexual reproduction, it is sufficient to assign fitnesses to genotypes. With sexual reproduction, genotypes have the opportunity to have a new frequency in the next generation. In this case, fitness values can be assigned to alleles by averaging over possible genetic backgrounds. Natural selection tends to make alleles with higher fitness more common over time, resulting in Darwinian evolution.
 The term ""Darwinian fitness"" can be used to make clear the distinction with physical fitness.[1] Fitness does not include a measure of survival or life-span; Herbert Spencer's  well-known phrase ""survival of the fittest"" should be interpreted as: ""Survival of the form (phenotypic or genotypic) that will leave the most copies of itself in successive generations.""
 Inclusive fitness differs from individual fitness by including the ability of an allele in one individual to promote the survival and/or reproduction of other individuals that share that allele, in preference to individuals with a different allele. One mechanism of inclusive fitness is kin selection.
"
Fitness_landscape,Biology,5,"In evolutionary biology, fitness landscapes or adaptive landscapes  (types of evolutionary landscapes) are used to visualize the relationship between genotypes and reproductive success. It is assumed that every genotype has a well-defined replication rate (often referred to as fitness). This fitness is the ""height"" of the landscape. Genotypes which are similar are said to be ""close"" to each other, while those that are very different are ""far"" from each other. The set of all possible genotypes, their degree of similarity, and their related fitness values is then called a fitness landscape. The idea of a fitness landscape is a metaphor to help explain flawed forms in evolution by natural selection, including exploits and glitches in animals like their reactions to supernormal stimuli.
 The idea of studying evolution by visualizing the distribution of fitness values as a kind of landscape was first introduced by Sewall Wright in 1932.[1] In evolutionary optimization problems, fitness landscapes are evaluations of a fitness function for all candidate solutions (see below).
"
Fertilization,Biology,5,"
 Fertilisation or fertilization (see spelling differences), also known as generative fertilisation, insemination, pollination,[1] fecundation, syngamy and impregnation,[2] is the fusion of gametes to give rise to a new individual organism or offspring and initiate its development. This cycle of fertilisation and development of new individuals is called sexual reproduction. During double fertilisation in angiosperms the haploid male gamete combines with two haploid polar nuclei to      form a triploid primary endosperm nucleus by the process of vegetative fertilisation.
"
Fetus,Biology,5,"A fetus or foetus (/ˈfiːtəs/; plural fetuses, feti, foetuses, or foeti) is the unborn offspring of an animal that develops from an embryo.[1] Following embryonic development the fetal stage of development takes place. In human prenatal development, fetal development begins from the ninth week after fertilisation (or eleventh week gestational age) and continues until birth.[2] Prenatal development is a continuum, with no clear defining feature distinguishing an embryo from a fetus. However, a fetus is characterized by the presence of all the major body organs, though they will not yet be fully developed and functional and some not yet situated in their final anatomical location.
"
Flagellum,Biology,5,"
 A flagellum (/fləˈdʒɛləm/; plural: flagella) is a lash-like appendage that protrudes from the cell body of certain bacteria and eukaryotic cells termed as flagellates. A flagellate can have one or several flagella. The primary function of a flagellum is that of locomotion, but it also often functions as a sensory organelle, being sensitive to chemicals and temperatures outside the cell.[1][2][3][4] The similar structure in the archaea functions in the same way but is structurally different and has been termed the archaellum.[5] Flagella are organelles defined by function rather than structure. Flagella vary greatly. Both prokaryotic and eukaryotic flagella can be used for swimming but they differ greatly in protein composition, structure, and mechanism of propulsion. The word flagellum in Latin means whip.
 An example of a flagellated bacterium is the ulcer-causing Helicobacter pylori, which uses multiple flagella to propel itself through the mucus lining to reach the stomach epithelium.[6] An example of a eukaryotic flagellate cell is the mammalian sperm cell, which uses its flagellum to propel itself through the female reproductive tract.[7] Eukaryotic flagella are structurally identical to eukaryotic cilia, although distinctions are sometimes made according to function or length.[8] Fimbriae and pili are also thin appendages, but have different functions and are usually smaller.
"
Flavin_adenine_dinucleotide,Biology,5,"In biochemistry, flavin adenine dinucleotide (FAD) is a redox-active coenzyme associated with various proteins, which is involved with several enzymatic reactions in metabolism. A flavoprotein is a protein that contains a flavin group, which may be in the form of FAD or flavin mononucleotide (FMN). Many flavoproteins are known: components of the succinate dehydrogenase complex, α-ketoglutarate dehydrogenase, and a component of the pyruvate dehydrogenase complex.
 FAD can exist in four redox states, which are the flavin-N(5)-oxide, quinone, semiquinone, and hydroquinone.[1] FAD is converted between these states by accepting or donating electrons. FAD, in its fully oxidized form, or quinone form, accepts two electrons and two protons to become FADH2 (hydroquinone form). The semiquinone (FADH·) can be formed by either reduction of FAD or oxidation of FADH2 by accepting or donating one electron and one proton, respectively. Some proteins, however, generate and maintain a superoxidized form of the flavin cofactor, the flavin-N(5)-oxide.[2][3]"
Food_chain,Biology,5,"A food chain is a linear network of links in a food web starting from producer organisms (such as grass or trees which use radiation from the Sun to make their food) and ending at apex predator species (like grizzly bears or killer whales), detritivores (like earthworms or woodlice), or decomposer species (such as fungi or bacteria). A food chain also shows how the organisms are related with each other by the food they eat. Each level of a food chain represents a different trophic level. A food chain differs from a food web, because the complex network of different animals' feeding relations are aggregated and the chain only follows a direct, linear pathway of one animal at a time. Natural interconnections between food chains make it a food web. 
 A common metric used to the quantify food web trophic structure is food chain length. In its simplest form, the length of a chain is the number of links between a trophic consumer and the base of the web. The mean chain length of an entire web is the arithmetic average of the lengths of all chains in the food web.[1][2] The food chain is an energy source diagram. The food chain begins with a producer, which is eaten by a primary consumer. The primary consumer may be eaten by a secondary consumer, which in turn may be consumed by a tertiary consumer. For example, a food chain might start with a green plant as the producer, which is eaten by a snail, the primary consumer. The snail might then be the prey of a secondary consumer such as a frog, which itself may be eaten by a tertiary consumer such as a snake.
 Food chains are very important for the survival of most species. When only one element is removed from the food chain it can result in extinction of a species in some cases. A producer can use either solar energy or chemical energy to convert organisms into usable compounds. Because the sun is necessary for photosynthesis, life could not exist if the sun disappeared. Decomposers, which feed on dead animals, break down the organic compounds into simple nutrients that are returned to the soil. These are the simple nutrients that plants require to create organic compounds. It is estimated that there are more than 100,000 different decomposers in existence.
 Many food webs have a keystone species. A keystone species is a species that has a large impact on the surrounding environment and can directly affect the food chain. If this keystone species dies off it can set the entire food chain off balance. Keystone species keep herbivores from depleting all of the foliage in their environment and preventing a mass extinction.[3] Food chains were first introduced by the Arab scientist and philosopher Al-Jahiz in the 10th century and later popularized in a book published in 1927 by Charles Elton, which also introduced the food web concept.[4][5][6]"
Founder_effect,Biology,5,"In population genetics, the founder effect is the loss of genetic variation that occurs when a new population is established by a very small number of individuals from a larger population. It was first fully outlined by Ernst Mayr in 1942,[1] using existing theoretical work by those such as Sewall Wright.[2] As a result of the loss of genetic variation, the new population may be distinctively different, both genotypically and phenotypically, from the parent population from which it is derived. In extreme cases, the founder effect is thought to lead to the speciation and subsequent evolution of new species.[3] In the figure shown, the original population has nearly equal numbers of blue and red individuals. The three smaller founder populations show that one or the other color may predominate (founder effect), due to random sampling of the original population. A population bottleneck may also cause a founder effect, though it is not strictly a new population.
 The founder effect occurs when a small group of migrants that is not genetically representative of the population from which they came establish in a new area.[4][5] In addition to founder effects, the new population is often a very small population, so shows increased sensitivity to genetic drift, an increase in inbreeding, and relatively low genetic variation.
"
Fungi,Biology,5,"
 
 A fungus (plural: fungi[2] or funguses[3]) is any member of the group of eukaryotic organisms that includes microorganisms such as yeasts and molds, as well as the more familiar mushrooms. These organisms are classified as a kingdom, which is separate from the other eukaryotic life kingdoms of plants and animals.
 A characteristic that places fungi in a different kingdom from plants, bacteria, and some protists is chitin in their cell walls. Similar to animals, fungi are heterotrophs; they acquire their food by absorbing dissolved molecules, typically by secreting digestive enzymes into their environment. Fungi do not photosynthesize. Growth is their means of mobility, except for spores (a few of which are flagellated), which may travel through the air or water. Fungi are the principal decomposers in ecological systems. These and other differences place fungi in a single group of related organisms, named the Eumycota (true fungi or Eumycetes), which share a common ancestor (from a monophyletic group), an interpretation that is also strongly supported by molecular phylogenetics. This fungal group is distinct from the structurally similar myxomycetes (slime molds) and oomycetes (water molds). The discipline of biology devoted to the study of fungi is known as mycology (from the Greek μύκης mykes, mushroom). In the past, mycology was regarded as a branch of botany, although it is now known fungi are genetically more closely related to animals than to plants.
 Abundant worldwide, most fungi are inconspicuous because of the small size of their structures, and their cryptic lifestyles in soil or on dead matter. Fungi include symbionts of plants, animals, or other fungi and also parasites. They may become noticeable when fruiting, either as mushrooms or as molds. Fungi perform an essential role in the decomposition of organic matter and have fundamental roles in nutrient cycling and exchange in the environment. They have long been used as a direct source of human food, in the form of mushrooms and truffles; as a leavening agent for bread; and in the fermentation of various food products, such as wine, beer, and soy sauce. Since the 1940s, fungi have been used for the production of antibiotics, and, more recently, various enzymes produced by fungi are used industrially and in detergents. Fungi are also used as biological pesticides to control weeds, plant diseases and insect pests. Many species produce bioactive compounds called mycotoxins, such as alkaloids and polyketides, that are toxic to animals including humans. The fruiting structures of a few species contain psychotropic compounds and are consumed recreationally or in traditional spiritual ceremonies. Fungi can break down manufactured materials and buildings, and become significant pathogens of humans and other animals. Losses of crops due to fungal diseases (e.g., rice blast disease) or food spoilage can have a large impact on human food supplies and local economies.
 The fungus kingdom encompasses an enormous diversity of taxa with varied ecologies, life cycle strategies, and morphologies ranging from unicellular aquatic chytrids to large mushrooms. However, little is known of the true biodiversity of Kingdom Fungi, which has been estimated at 2.2 million to 3.8 million species.[4] Of these, only about 120,000 have been described, with over 8,000 species known to be detrimental to plants and at least 300 that can be pathogenic to humans.[5] Ever since the pioneering 18th and 19th century taxonomical works of Carl Linnaeus, Christian Hendrik Persoon, and Elias Magnus Fries, fungi have been classified according to their morphology (e.g., characteristics such as spore color or microscopic features) or physiology. Advances in molecular genetics have opened the way for DNA analysis to be incorporated into taxonomy, which has sometimes challenged the historical groupings based on morphology and other traits. Phylogenetic studies published in the first decade of the 21st century have helped reshape the classification within Kingdom Fungi, which is divided into one subkingdom, seven phyla, and ten subphyla.
"
G_protein,Biology,5,"
 G proteins, also known as guanine nucleotide-binding proteins, are a family of proteins that act as molecular switches inside cells, and are involved in transmitting signals from a variety of stimuli outside a cell to its interior.  Their activity is regulated by factors that control their ability to bind to and hydrolyze guanosine triphosphate (GTP) to guanosine diphosphate (GDP). When they are bound to GTP, they are 'on', and, when they are bound to GDP, they are 'off'. G proteins belong to the larger group of enzymes called GTPases.
 There are two classes of G proteins. The first function as monomeric small GTPases (small G-proteins), while the second function as heterotrimeric G protein complexes. The latter class of complexes is made up of alpha (α), beta (β) and gamma (γ) subunits.[1] In addition, the beta and gamma subunits can form a stable dimeric complex referred to as the beta-gamma complex
.[2] Heterotrimeric G proteins located within the cell are activated by G protein-coupled receptors (GPCRs) that span the cell membrane.[3] Signaling molecules bind to a domain of the GPCR located outside the cell, and an intracellular GPCR domain then in turn activates a particular G protein. Some active-state GPCRs have also been shown to be ""pre-coupled"" with G proteins.[4] The G protein activates a cascade of further signaling events that finally results in a change in cell function. G protein-coupled receptor and G proteins working together transmit signals from many hormones, neurotransmitters, and other signaling factors.[5] G proteins regulate metabolic enzymes, ion channels, transporter proteins, and other parts of the cell machinery, controlling transcription, motility, contractility, and secretion, which in turn regulate diverse systemic functions such as embryonic development, learning and memory, and homeostasis.[6]"
Gamete,Biology,5,"
 A gamete (/ˈɡæmiːt/; from Ancient Greek γαμετή gamete from gamein ""to marry"") is a haploid cell that fuses with another haploid cell during fertilization in organisms that sexually reproduce and possess only one set of dissimilar chromosomes[1]. Gametes are an organism’s reproductive cells, also referred to as sex cells[2].In species that produce two morphologically distinct types of gametes, and in which each individual produces only one type, a female is any individual that produces the larger type of gamete—called an ovum— and a male produces the smaller tadpole-like type—called a sperm.  Sperm cells or spermatozoon are small and motile due to the flagellum, a tail-shaped structure that allows the cell to propel and move. In contrast, each egg cell or ovum is relatively large and non-motile[2]. In short a gamete is an egg cell (female gamete) or a sperm (male gamete). Ova mature in the ovaries of females and sperm develop in the testes of males. During fertilization, a spermatozoon and ovum unite to form a new diploid organism[2]. Gametes carry half the genetic information of an individual, one ploidy of each type, and are created through meiosis, in which a germ cell undergoes two fissions, resulting in the production of four gametes[1]. In biology, the type of gamete one produces determines the classification of their sex[3].
 This is an example of anisogamy or heterogamy, the condition in which females and males produce gametes of different sizes (this is the case in humans; the human ovum has approximately 100,000 times the volume of a single human sperm cell). In contrast, isogamy is the state of gametes from both sexes being the same size and shape, and given arbitrary designators for mating type. The name gamete was introduced by the German cytologist Eduard Strasburger.
 Oogenesis is the process of female gamete formation in animals. This process involves meiosis (including meiotic recombination) occurring in the diploid primary oocyte to produce the haploid ovum. Spermatogenesis is the process of male gamete formation in animals. This process also involves meiosis occurring in the diploid primary spermatocyte to produce the haploid spermatozoon. 
"
Gene,Biology,5,"
 In biology, a gene is a sequence of nucleotides in DNA or RNA that encodes the synthesis of a gene product, either RNA or protein.
 During gene expression, the DNA is first copied into RNA. The RNA can be directly functional or be the intermediate template for a protein that performs a function. The transmission of genes to an organism's offspring is the basis of the inheritance of phenotypic traits. These genes make up different DNA sequences called genotypes. Genotypes along with environmental and developmental factors determine what the phenotypes will be. Most biological traits are under the influence of polygenes (many different genes) as well as gene–environment interactions. Some genetic traits are instantly visible, such as eye color or the number of limbs, and some are not, such as blood type, the risk for specific diseases, or the thousands of basic biochemical processes that constitute life.
 Genes can acquire mutations in their sequence, leading to different variants, known as alleles, in the population. These alleles encode slightly different versions of a protein, which cause different phenotypical traits. Usage of the term ""having a gene"" (e.g., ""good genes,"" ""hair colour gene"") typically refers to containing a different allele of the same, shared gene.[1] Genes evolve due to natural selection / survival of the fittest and genetic drift of the alleles.
 The concept of gene continues to be refined as new phenomena are discovered.[2] For example, regulatory regions of a gene can be far removed from its coding regions, and coding regions can be split into several exons. Some viruses store their genome in RNA instead of DNA and some gene products are functional non-coding RNAs. Therefore, a broad, modern working definition of a gene is any discrete locus of heritable, genomic sequence which affect an organism's traits by being expressed as a functional product or by regulation of gene expression.[3][4] The term gene was introduced by Danish botanist, plant physiologist and geneticist Wilhelm Johannsen in 1909.[5] It is inspired by the ancient Greek: γόνος, gonos, that means offspring and procreation.
"
Gene_pool,Biology,5,"The gene pool is the set of all genes, or genetic information, in any population, usually of a particular species.[1]"
Genetic_code,Biology,5,"The genetic code is the set of rules used by living cells to translate information encoded within genetic material (DNA or mRNA sequences of nucleotide triplets, or codons) into proteins. Translation is accomplished by the ribosome, which links proteinogenic amino acids in an order specified by messenger RNA (mRNA), using transfer RNA (tRNA) molecules to carry amino acids and to read the mRNA three nucleotides at a time. The genetic code is highly similar among all organisms and can be expressed in a simple table with 64 entries.[1] The code defines how codons specify which amino acid will be added next during protein synthesis. With some exceptions,[2] a three-nucleotide codon in a nucleic acid sequence specifies a single amino acid. The vast majority of genes are encoded with a single scheme (see the RNA codon table). That scheme is often referred to as the canonical or standard genetic code, or simply the genetic code, though variant codes (such as in human mitochondria) exist.
 While the ""genetic code"" is what determines a protein's amino acid sequence, other genomic regions determine when and where these proteins are produced according to various ""gene regulatory codes"".
"
Genetic_drift,Biology,5,"
 Genetic drift (also known as allelic drift or the Sewall Wright effect)[1] is the change in the frequency of an existing gene variant (allele) in a population due to random sampling of organisms.[2] The alleles in the offspring are a sample of those in the parents, and chance has a role in determining whether a given individual survives and reproduces. A population's allele frequency is the fraction of the copies of one gene that share a particular form.[3] Genetic drift may cause gene variants to disappear completely and thereby reduce genetic variation.[4] It can also cause initially rare alleles to become much more frequent and even fixed.
 When there are few copies of an allele, the effect of genetic drift is larger, and when there are many copies the effect is smaller. In the middle of the 20th century, vigorous debates occurred over the relative importance of natural selection versus neutral processes, including genetic drift. Ronald Fisher, who explained natural selection using Mendelian genetics,[5] held the view that genetic drift plays at the most a minor role in evolution, and this remained the dominant view for several decades. In 1968, population geneticist Motoo Kimura rekindled the debate with his neutral theory of molecular evolution, which claims that most instances where a genetic change spreads across a population (although not necessarily changes in phenotypes) are caused by genetic drift acting on neutral mutations.[6][7]"
Genetic_variation,Biology,5,"Genetic variation is the difference in DNA among individuals [2] or the differences between populations.[3]  There are multiple sources of genetic variation, including mutation and genetic recombination.[4]   The mutation is the ultimate source of genetic variation, but mechanisms such as sexual reproduction and genetic drift contribute to it as well.[5]"
Genetics,Biology,5,"
 
 Genetics is a branch of biology concerned with the study of genes, genetic variation, and heredity in organisms.[1][2][3] Though heredity had been observed for millennia, Gregor Mendel, a scientist and Augustinian friar working in the 19th century, was the first to study genetics scientifically. Mendel studied ""trait inheritance"", patterns in the way traits are handed down from parents to offspring. He observed that organisms (pea plants) inherit traits by way of discrete ""units of inheritance"". This term, still used today, is a somewhat ambiguous definition of what is referred to as a gene.
 Trait inheritance and molecular inheritance mechanisms of genes are still primary principles of genetics in the 21st century, but modern genetics has expanded beyond inheritance to studying the function and behavior of genes. Gene structure and function, variation, and distribution are studied within the context of the cell, the organism (e.g. dominance), and within the context of a population. Genetics has given rise to a number of subfields, including molecular genetics, epigenetics and population genetics. Organisms studied within the broad field span the domains of life (archaea, bacteria, and eukarya).
 Genetic processes work in combination with an organism's environment and experiences to influence development and behavior, often referred to as nature versus nurture. The intracellular or extracellular environment of a living cell or organism may switch gene transcription on or off. A classic example is two seeds of genetically identical corn, one placed in a temperate climate and one in an arid climate (lacking sufficient waterfall or rain). While the average height of the two corn stalks may be genetically determined to be equal, the one in the arid climate only grows to half the height of the one in the temperate climate due to lack of water and nutrients in its environment.
"
Genome,Biology,5,"In the fields of molecular biology and genetics, a genome is all genetic material of an organism. It consists of DNA (or RNA in RNA viruses). The genome includes both the genes (the coding regions) and the noncoding DNA,[1] as well as mitochondrial DNA[2] and chloroplast DNA. The study of the genome is called genomics.
 
"
Genotype,Biology,5,"A genotype is an organism’s set of heritable genes that can be passed down from parents to offspring.[1] The genes take part in determining the characteristics that are observable (phenotype) in an organism, such as hair color, height, etc.[2] An example of a characteristic determined by a genotype is the petal color in a pea plant. The collection of all genetic possibilities for a single trait are called alleles; two alleles for petal color are purple and white.[3] The genotype is one of three factors that determine phenotype. The other two are the environmental (not inherited) and the epigenetic (inherited) factors. Not all individuals with the same genotype look or act the same way because appearance and behavior are modified by environmental and growing conditions. Likewise, not all organisms that look alike necessarily have the same genotype. One would typically refer to an individual's genotype with regard to a particular gene of interest and the combination of alleles the individual carries (see homozygous, heterozygous).[4] Genotypes are often denoted with letters, for example Bb, where B stands for one allele and b for another.
 Somatic mutations that are acquired rather than inherited, such as those in cancers, are not part of the individual's genotype. Hence, scientists and physicians sometimes talk about the genotype of a particular cancer, that is, of the disease as distinct from the diseased.
 The term genotype was coined by the Danish botanist Wilhelm Johannsen in 1903.[5]"
Genus,Biology,5,"A genus (plural genera) is a taxonomic rank used in the biological classification of living and fossil organisms, as well as viruses,[1] in biology. In the hierarchy of biological classification, genus comes above species and below family. In binomial nomenclature, the genus name forms the first part of the binomial species name for each species within the genus.
 The composition of a genus is determined by a taxonomist. The standards for genus classification are not strictly codified, so different authorities often produce different classifications for genera. There are some general practices used, however,[2][3] including the idea that a newly defined genus should fulfill these three criteria to be descriptively useful:
 Moreover, genera should be composed of phylogenetic units of the same kind as other (analogous) genera.[4] 
"
Gizzard,Biology,5,"The gizzard, also referred to as the ventriculus, gastric mill, and gigerium, is an organ found in the digestive tract of some animals, including archosaurs (pterosaurs, crocodiles, alligators, dinosaurs including birds), earthworms, some gastropods, some fish, and some crustaceans. This specialized stomach constructed of thick muscular walls is used for grinding up food, often aided by particles of stone or grit. In certain insects and molluscs, the gizzard features chitinous plates or teeth.
"
Guanine,Biology,5,"Guanine (/ˈɡwɑːnɪn/; or G, Gua) is one of the four main nucleobases found in the nucleic acids DNA and RNA, the others being adenine, cytosine, and thymine (uracil in RNA). In DNA, guanine is paired with cytosine. The guanine nucleoside is called guanosine.
 With the formula C5H5N5O, guanine is a derivative of purine, consisting of a fused pyrimidine-imidazole ring system with conjugated double bonds. This unsaturated arrangement means the bicyclic molecule is planar.
"
Habitat,Biology,5,"
 In ecology, habitat identifies is the array of resources, physical and biotic factors, present in an area that allow the survival and reproduction of a particular species. A species habitat can be seen as the physical manifestation its ecological niche. Thus, habitat is a specie-specific term, fundamentally different from concepts such as environment or vegetation assemblages, for which the therm habitat-type is more appropriate.[1] The physical factors may include (for example): soil, moisture, range of temperature, and light intensity. Biotic factors will include the availability of food and the presence or absence of predators. Every organism has certain habitat needs for the conditions in which it will thrive, but some are tolerant of wide variations while others are very specific in their requirements. A species habitat is not necessarily a geographical area, it can be the interior of a stem, a rotten log, a rock or a clump of moss; for a parasitic organism has as its habitat the body of its host, part of the host's body (such as the digestive tract), or a single cell within the host's body.
 Geographic habitat-types include polar, temperate, subtropical and tropical. The terrestrial vegetation type may be forest, steppe, grassland, semi-arid or desert. Fresh-water habitats include marshes, streams, rivers, lakes, and ponds; marine habitats include salt marshes, the coast, the intertidal zone, estuaries, reefs, bays, the open sea, the sea bed, deep water and submarine vents.
 Habitats may change over time. Causes of change may include a violent event (such as the eruption of a volcano, an earthquake, a tsunami, a wildfire or a change in oceanic currents); or change may occur more gradually over millennia with alterations in the climate, as ice sheets and glaciers advance and retreat, and as different weather patterns bring changes of precipitation and solar radiation. Other changes come as a direct result of human activities, such as deforestation, the plowing of ancient grasslands, the diversion and damming of rivers, the draining of marshland and the dredging of the seabed. The introduction of alien species can have a devastating effect on native wildlife, through increased predation, through competition for resources or through the introduction of pests and diseases to which the indigenous species have no immunity.
"
Habituation,Biology,5,"Habituation is a form of non-associative learning in which an innate (non-reinforced) response to a stimulus decreases after repeated or prolonged presentations of that stimulus.[1] Responses that habituate include those that involve the intact organism (e.g., full-body startle response) or those that involve only components of the organism (e.g., habituation of neurotransmitter release from in vitro Aplysia sensory neurons). The broad ubiquity of habituation across all biologic phyla has resulted in it being called ""the simplest, most universal form of learning...as fundamental a characteristic of life as DNA."" [2] Functionally-speaking, by diminishing the response to an inconsequential stimulus, habituation is thought to free-up cognitive resources to other stimuli that are associated with biologically important events (i.e., punishment/reward). For example, organisms may habituate to repeated sudden loud noises when they learn these have no consequences.[3]  A progressive decline of a behavior in a habituation procedure may also reflect nonspecific effects such as fatigue, which must be ruled out when the interest is in habituation.[4] Habituation is clinically relevant, as a number of neuropsychiatric conditions, including autism, schizophrenia, migraine, and Tourette's, show reductions in habituation to a variety of stimulus-types both simple (tone) and complex (faces).[5]"
Heredity,Biology,5,"Heredity, also called inheritance or biological inheritance, is the passing on of traits from parents to their offspring; either through asexual reproduction or sexual reproduction, the offspring cells or organisms acquire the genetic information of their parents. Through heredity, variations between individuals can accumulate and cause species to evolve by natural selection. The study of heredity in biology is genetics.
"
Hermaphrodite,Biology,5,"In reproductive biology, a hermaphrodite (/hɜːrˈmæfrədaɪt/) is an organism that has complete or partial reproductive organs and produces gametes normally associated with both male and female sexes.[1][2][3] Many taxonomic groups of animals (mostly invertebrates) do not have separate sexes.[4] In these groups, hermaphroditism is a normal condition, enabling a form of sexual reproduction in which either partner can act as the ""female"" or ""male"". For example, the great majority of tunicates, pulmonate snails, opisthobranch snails, earthworms, and slugs are hermaphrodites. Hermaphroditism is also found in some fish species and to a lesser degree in other vertebrates. Most plants are also hermaphrodites.
 Historically, the term hermaphrodite has also been used to describe ambiguous genitalia and gonadal mosaicism in individuals of gonochoristic species, especially human beings. The word intersex has come into usage for humans, since the word hermaphrodite is considered to be misleading and stigmatizing,[5][6] as well as ""scientifically specious and clinically problematic.""[7] A rough estimate of the number of hermaphroditic animal species is 65,000.[8] The percentage of animal species that are hermaphroditic is about 5%. (Although the current estimated total number of animal species is about 7.7 million, the study,[8] which estimated the number, 65,000, used an estimated total number of animal species, 1,211,577 from ""Classification phylogénétique du vivant (Vol. 2)"" - Lecointre and Le Guyader (2001)). Most hermaphroditic species exhibit some degree of self-fertilization. The distribution of self-fertilization rates among animals is similar to that of plants, suggesting that similar processes are operating to direct the evolution of selfing in animals and plants.[8]"
Herpetology,Biology,5,"Herpetology (from Greek ἑρπετόν herpetón, meaning ""reptile"" or ""creeping animal"") is the branch of zoology concerned with the study of amphibians (including frogs, toads, salamanders, newts, and caecilians (gymnophiona)) and reptiles (including snakes, lizards, amphisbaenids, turtles, terrapins, tortoises, crocodilians, and the tuataras).[1] Birds, which are cladistically included within Reptilia, are traditionally excluded here; the scientific study of birds is the subject of ornithology.
 Thus, the definition of herpetology can be more precisely stated as the study of ectothermic (cold-blooded) tetrapods. Under this definition ""herps"" (or sometimes ""herptiles"" or ""herpetofauna"") exclude fish, but it is not uncommon for herpetological and ichthyological scientific societies to ""team up"", publishing joint journals and holding conferences in order to foster the exchange of ideas between the fields, as the American Society of Ichthyologists and Herpetologists does. Many herpetological societies have been formed to promote interest in reptiles and amphibians, both captive and wild.
 Herpetology offers benefits to humanity in the study of the role of amphibians and reptiles in global ecology, especially because amphibians are often very sensitive to environmental changes, offering a visible warning to humans that significant changes are taking place. Some toxins and venoms produced by reptiles and amphibians are useful in human medicine. Currently, some snake venom has been used to create anti-coagulants that work to treat strokes and heart attacks.
"
Heterosis,Biology,5,"Heterosis, hybrid vigor, or outbreeding enhancement is the improved or increased function of any biological quality in a hybrid offspring. An offspring is heterotic if its traits are enhanced as a result of mixing the genetic contributions of its parents. These effects can be due to Mendelian or non-Mendelian inheritance.
"
Heterotroph,Biology,5,"A heterotroph (/ˈhɛtərəˌtroʊf, -ˌtrɒf/;[1] Ancient Greek ἕτερος héteros = ""other"" plus trophe = ""nutrition"") is an organism that cannot produce its own food,  instead taking nutrition from other sources of organic carbon, mainly plant or animal matter. In the food chain, heterotrophs are primary, secondary and tertiary consumers, but not producers.[2][3] Living organisms that are heterotrophic include all animals and fungi, some bacteria and protists,[4] and many parasitic plants. The term heterotroph arose in microbiology in 1946 as part of a classification of microorganisms based on their type of nutrition.[5] The term is now used in many fields, such as ecology in describing the food chain.
 Heterotrophs may be subdivided according to their energy source. If the heterotroph uses chemical energy, it is a chemoheterotroph (e.g., humans and mushrooms). If it uses light for energy, then it is a photoheterotroph (e.g., green non-sulfur bacteria).
 Heterotrophs represent one of the two mechanisms of nutrition (trophic levels), the other being autotrophs (auto = self, troph = nutrition). Autotrophs use energy from sunlight (photoautotrophs) or oxidation of inorganic compounds (lithoautotrophs) to convert inorganic carbon dioxide to organic carbon compounds and energy to sustain their life. Comparing the two in basic terms, heterotrophs (such as animals) eat either autotrophs (such as plants) or other heterotrophs, or both.
 Detritivores are heterotrophs which obtain nutrients by consuming detritus (decomposing plant and animal parts as well as feces).[6] Saprotrophs (also called lysotrophs) are chemoheterotrophs that use extracellular digestion in processing decayed organic matter; the term most often used to describe fungi. The process is most often facilitated through the active transport of such materials through endocytosis within the internal mycelium and its constituent hyphae.[7]"
Histology,Biology,5,"Histology,[help 1]
also known as microscopic anatomy or microanatomy,[1] is the branch of biology which studies the microscopic anatomy of biological tissues.[2][3][4][5] Histology is the microscopic counterpart to gross anatomy, which looks at larger structures visible without a microscope.[5][6] Although one may divide microscopic anatomy into organology, the study of organs, histology, the study of tissues, and cytology, the study of cells, modern usage places these topics under the field of histology.[5] In medicine, histopathology is the branch of histology that includes the microscopic identification and study of diseased tissue.[5][6] In the field of paleontology, the term paleohistology refers to the histology of fossil organisms.[7][8]"
Hormone,Biology,5,"A hormone (from the Greek participle ὁρμῶν, ""setting in motion"") is any member of a class of signaling molecules, produced by glands in multicellular organisms, that are transported by the circulatory system to target distant organs to regulate physiology and behavior.[1] Hormones have diverse chemical structures, mainly of three classes:
 The glands that secrete hormones comprise the endocrine signaling system. The term ""hormone"" is sometimes extended to include chemicals produced by cells that affect the same cell (autocrine or  intracrine signaling) or nearby cells (paracrine signalling).
 Hormones serve to communicate between organs and tissues for physiological regulation and behavioral activities such as digestion, metabolism, respiration, tissue function, sensory perception, sleep, excretion, lactation, stress induction, growth and development, movement, reproduction, and mood manipulation.[2][3] Hormones affect distant cells by binding to specific receptor proteins in the target cell, resulting in a change in cell function. When a hormone binds to the receptor, it results in the activation of a signal transduction pathway that typically activates gene transcription, resulting in increased expression of target proteins; non-genomic effects are more rapid, and can be synergistic with genomic effects.[4] Amino acid–based hormones (amines and peptide or protein hormones) are water-soluble and act on the surface of target cells via second messengers; steroid hormones, being lipid-soluble, move through the plasma membranes of target cells (both cytoplasmic and nuclear) to act within their nuclei.
 Hormone secretion may occur in many tissues. Endocrine glands provide the cardinal example, but specialized cells in various other organs also secrete hormones. Hormone secretion occurs in response to specific biochemical signals from a wide range of regulatory systems. For instance, serum calcium concentration affects parathyroid hormone synthesis; blood sugar (serum glucose concentration) affects insulin synthesis; and because the outputs of the stomach and exocrine pancreas (the amounts of gastric juice and pancreatic juice) become the input of the small intestine, the small intestine secretes hormones to stimulate or inhibit the stomach and pancreas based on how busy it is. Regulation of hormone synthesis of gonadal hormones, adrenocortical hormones, and thyroid hormones often depends on complex sets of direct-influence and feedback interactions involving the hypothalamic-pituitary-adrenal (HPA), -gonadal (HPG), and -thyroid (HPT) axes.
 Upon secretion, certain hormones, including protein hormones and catecholamines, are water-soluble and are thus readily transported through the circulatory system. Other hormones, including steroid and thyroid hormones, are lipid-soluble; to achieve widespread distribution, these hormones must bond to carrier plasma glycoproteins (e.g., thyroxine-binding globulin (TBG)) to form ligand-protein complexes. Some hormones are completely active[which?] when released into the bloodstream (as is the case for insulin and growth hormones), while others are prohormones that must be activated in specific cells through a series of activation steps that are commonly highly regulated. The endocrine system secretes hormones directly into the bloodstream, typically via fenestrated capillaries, whereas the exocrine system secretes its hormones indirectly using ducts. Hormones with paracrine function diffuse through the interstitial spaces to nearby target tissue.
"
Host_(biology),Biology,5,"
 In biology and medicine, a host is a larger organism that harbours a smaller organism;[1] whether a parasitic, a mutualistic, or a commensalist guest (symbiont). The guest is typically provided with nourishment and shelter. Examples include animals playing host to parasitic worms (e.g. nematodes), cells harbouring pathogenic (disease-causing) viruses, a bean plant hosting mutualistic (helpful) nitrogen-fixing bacteria. More specifically in botany, a host plant supplies food resources to micropredators, which have an evolutionarily stable relationship with their hosts similar to ectoparasitism. The host range is the collection of hosts that an organism can use as a partner.
"
Hybrid_(biology),Biology,5,"
 In biology, a hybrid is the offspring resulting from combining the qualities of two organisms of different breeds, varieties, species or genera through sexual reproduction. Hybrids are not always intermediates between their parents (such as in blending inheritance), but can show hybrid vigour, sometimes growing larger or taller than either parent. The concept of a hybrid is interpreted differently in animal and plant breeding, where there is interest in the individual parentage. In genetics, attention is focused on the numbers of chromosomes. In taxonomy, a key question is how closely related the parent species are.
 Species are reproductively isolated by strong barriers to hybridisation, which include genetic and morphological differences, differing times of fertility, mating behaviors and cues, and physiological rejection of sperm cells or the developing embryo. Some act before fertilization and others after it. Similar barriers exist in plants, with differences in flowering times, pollen vectors, inhibition of pollen tube growth, somatoplastic sterility, cytoplasmic-genic male sterility and the structure of the chromosomes. A few animal species and many plant species, however, are the result of hybrid speciation, including important crop plants such as wheat, where the number of chromosomes has been doubled.
 Human impact on the environment has resulted in an increase in the interbreeding between regional species, and the proliferation of introduced species worldwide has also resulted in an increase in hybridisation. This genetic mixing may threaten many species with extinction, while genetic erosion from monoculture in crop plants may be damaging the gene pools of many species for future breeding.  A form of often intentional human-mediated hybridisation is the crossing of wild and domesticated species. This is common in both traditional horticulture and modern agriculture; many commercially useful fruits, flowers, garden herbs, and trees have been produced by hybridisation. One such flower, Oenothera lamarckiana, was central to early genetics research into mutationism and polyploidy.  It is also more occasionally done in the livestock and pet trades; some well-known wild × domestic hybrids are beefalo and wolfdogs.  Human selective breeding of domesticated animals and plants has resulted in the development of distinct breeds (usually called cultivars in reference to plants); crossbreeds between them (without any wild stock) are sometimes also imprecisely referred to as ""hybrids"".
 Hybrid humans existed in prehistory. For example, Neanderthals and anatomically modern humans are thought to have interbred as recently as 40,000 years ago.
 Mythological hybrids appear in human culture in forms as diverse as the Minotaur, blends of animals, humans and mythical beasts such as centaurs and sphinxes, and the Nephilim of the Biblical apocrypha described as the wicked sons of fallen angels and attractive women.
"
Hydrocarbon,Biology,5," In organic chemistry, a hydrocarbon is an organic compound consisting entirely of hydrogen and carbon.[1]:620 Hydrocarbons are examples of group 14 hydrides. Hydrocarbons from which one hydrogen atom has been removed are functional groups called hydrocarbyls.[2] Hydrocarbons are generally colourless and hydrophobic with only weak odours. Because of their diverse molecular structures, it is difficult to generalize further. Most anthropogenic emissions of hydrocarbons are from the burning of fossil fuels including fuel production and combustion. Natural sources of hydrocarbons such as ethylene, isoprene, and monoterpenes come from the emissions of vegetation.[3]"
Ichthyology,Biology,5,"Ichthyology is the branch of zoology devoted to the study of fish, including bony fish (Osteichthyes), cartilaginous fish (Chondrichthyes), and jawless fish (Agnatha). According to FishBase, 33,400 species of fish had been described as of October 2016, with approximately 250 new species described each year.[1][citation needed]"
Immune_response,Biology,5,"An immune response is a reaction which occurs within an organism for the purpose of defending against foreign invaders. These invaders include a wide variety of different microorganisms including viruses, bacteria, parasites, and fungi which could cause serious problems to the health of the host organism if not cleared from the body.[1] There are two distinct aspects of the immune response, the innate and the adaptive, which work together to protect against pathogens. The innate branch—the body's first reaction to an invader—is known to be a non-specific and quick response to any sort of pathogen. Components of the innate immune response include physical barriers like the skin and mucous membranes, immune cells such as neutrophils, macrophages, and monocytes, and soluble factors including cytokines and complement.[2] On the other hand, the adaptive branch is the body's immune response which is catered against specific antigens and thus, it takes longer to activate the components involved. The adaptive branch include cells such as dendritic cells, T cell, and B cells as well as antibodies—also known as immunoglobulins—which directly interact with antigen and are a very important component for a strong response against an invader.[1] The first contact that an organism has with a particular antigen will result in the production of effector T and B cells which are activated cells that defend against the pathogen. The production of these effector cells as a result of the first-time exposure is called a primary immune response. Memory T and memory B cells are also produced in the case that the same pathogen enters the organism again. If the organism does happen to become re-exposed to the same pathogen, the secondary immune response will kick in and the immune system will be able to respond in both a fast and strong manner because of the memory cells from the first exposure.[3] Vaccines introduce a weakened, killed, or fragmented microorganism in order to evoke a primary immune response. This is so that in the case that an exposure to the real pathogen occurs, the body can rely on the secondary immune response to quickly defend against it.[4]"
Immunity_(medical),Biology,5,"In biology, immunity is the capability of multicellular organisms to resist harmful microorganisms. Immunity involves both specific and nonspecific components. The nonspecific components act as barriers or eliminators of a wide range of pathogens irrespective of their antigenic make-up. Other components of the immune system adapt themselves to each new disease encountered and can generate pathogen-specific immunity.
 Immunity can be defined as a complex biological system endowed with the capacity to recognize and tolerate whatever belongs to the self, and to recognize and reject what is foreign (non-self).[1]"
Immunoglobulin,Biology,5,"
 An antibody (Ab), also known as an immunoglobulin (Ig),[1] is a large, Y-shaped protein used by the immune system to identify and neutralize foreign objects such as pathogenic bacteria and viruses. The antibody recognizes a unique molecule of the pathogen, called an antigen.[2][3] Each tip of the ""Y"" of an antibody contains a paratope (analogous to a lock) that is specific for one particular epitope (analogous to a key) on an antigen, allowing these two structures to bind together with precision. Using this binding mechanism, an antibody can tag a microbe or an infected cell for attack by other parts of the immune system, or can neutralize it directly (for example, by blocking a part of a virus that is essential for its invasion). 
 To allow the immune system to recognize millions of different antigens, the antigen-binding sites at both tips of the antibody come in an equally wide variety.
In contrast, the remainder of the antibody is relatively constant. It only occurs in a few variants, which define the antibody's class or isotype: IgA, IgD, IgE, IgG, or IgM.
The constant region at the trunk of the antibody includes sites involved in interactions with other components of the immune system. The class hence determines the function triggered by an antibody after binding to an antigen, in addition to some structural features.
Antibodies from different classes also differ in where they are released in the body and at what stage of an immune response.
 Together with B and T cells, antibodies are the most important part of the adaptive immune system.
They occur in two forms: attached to a B cell or in soluble form in extracellular fluids such as blood plasma.
Initially, antibodies are attached to the surface of a B cell – they are then referred to as B-cell receptors (BCR).
After an antigen binds to a BCR, the B cell activates to proliferate and differentiate into either plasma cells, which secrete soluble antibodies with the same paratope, or memory B cells, which survive in the body to enable long-lasting immunity to the antigen.[4]
Soluble antibodies are released into the blood and tissue fluids, as well as many secretions.
Because these fluids were traditionally known as humors, antibody-mediated immunity is sometimes known as, or considered a part of, humoral immunity.[5] 
The soluble Y-shaped units can occur individually as monomers, or in complexes of two to five units.
 Antibodies are glycoproteins belonging to the immunoglobulin superfamily.
The terms antibody and immunoglobulin are often used interchangeably,[1] though the term 'antibody' is sometimes reserved for the secreted, soluble form, i.e. excluding B-cell receptors.[6]"
Infection,Biology,5,"An infection is the invasion of an organism's body tissues by disease-causing agents, their multiplication, and the reaction of host tissues to the infectious agents and the toxins they produce.[1][2] An infectious disease, also known as a transmissible disease or communicable disease, is an illness resulting from an infection.
 Infections are caused by infectious agents (pathogens) including:
 Hosts can fight infections using their immune system. Mammalian hosts react to infections with an innate response, often involving inflammation, followed by an adaptive response.[7] Specific medications used to treat infections include antibiotics, antivirals, antifungals, antiprotozoals, and antihelminthics. Infectious diseases resulted in 9.2 million deaths in 2013 (about 17% of all deaths).[8] The branch of medicine that focuses on infections is referred to as infectious disease.[9]"
Insulin,Biology,5,"Insulin (/ˈɪn.sjʊ.lɪn/,[5][6] from Latin insula, 'island') is a peptide hormone produced by beta cells of the pancreatic islets; it is considered to be the main anabolic hormone of the body.[7] It regulates the metabolism of carbohydrates, fats and protein by promoting the absorption of glucose from the blood into liver, fat and skeletal muscle cells.[8]  In these tissues the absorbed glucose is converted into either glycogen via glycogenesis or fats (triglycerides) via lipogenesis, or, in the case of the liver, into both.[8] Glucose production and secretion by the liver is strongly inhibited by high concentrations of insulin in the blood.[9] Circulating insulin also affects the synthesis of proteins in a wide variety of tissues. It is therefore an anabolic hormone, promoting the conversion of small molecules in the blood into large molecules inside the cells. Low insulin levels in the blood have the opposite effect by promoting widespread catabolism, especially of reserve body fat.
 1A7F, 1AI0, 1AIY, 1B9E, 1BEN, 1EV3, 1EV6, 1EVR, 1FU2, 1FUB, 1G7A, 1G7B, 1GUJ, 1HIQ, 1HIS, 1HIT, 1HLS, 1HTV, 1HUI, 1IOG, 1IOH, 1J73, 1JCA, 1JCO, 1K3M, 1KMF, 1LKQ, 1LPH, 1MHI, 1MHJ, 1MSO, 1OS3, 1OS4, 1Q4V, 1QIY, 1QIZ, 1QJ0, 1RWE, 1SF1, 1T1K, 1T1P, 1T1Q, 1TRZ, 1TYL, 1TYM, 1UZ9, 1VKT, 1W8P, 1XDA, 1XGL, 1XW7, 1ZEG, 1ZEH, 1ZNJ, 2AIY, 2C8Q, 2C8R, 2CEU, 2H67, 2HH4, 2HHO, 2HIU, 2JMN, 2JUM, 2JUU, 2JUV, 2JV1, 2JZQ, 2K91, 2K9R, 2KJJ, 2KJU, 2KQQ, 2KXK, 2L1Y, 2L1Z, 2LGB, 2M1D, 2M1E, 2M2M, 2M2N, 2M2O, 2M2P, 2OLY, 2OLZ, 2OM0, 2OM1, 2OMG, 2OMH, 2OMI, 2QIU, 2R34, 2R35, 2R36, 2RN5, 2VJZ, 2VK0, 2W44, 2WBY, 2WC0, 2WRU, 2WRV, 2WRW, 2WRX, 2WS0, 2WS1, 2WS4, 2WS6, 2WS7, 3AIY, 3BXQ, 3E7Y, 3E7Z, 3EXX, 3FQ9, 3I3Z, 3I40, 3ILG, 3INC, 3IR0, 3Q6E, 3ROV, 3TT8, 3U4N, 3UTQ, 3UTS, 3UTT, 3V19, 3V1G, 3W11, 3W12, 3W13, 3W7Y, 3W7Z, 3W80, 3ZI3, 3ZQR, 3ZS2, 3ZU1, 4AIY, 4AJX, 4AJZ, 4AK0, 4AKJ, 4EFX, 4EWW, 4EWX, 4EWZ, 4EX0, 4EX1, 4EXX, 4EY1, 4EY9, 4EYD, 4EYN, 4EYP, 4F0N, 4F0O, 4F1A, 4F1B, 4F1C, 4F1D, 4F1F, 4F1G, 4F4T, 4F4V, 4F51, 4F8F, 4FG3, 4FKA, 4GBC, 4GBI, 4GBK, 4GBL, 4GBN, 4IUZ, 5AIY, 2LWZ, 3JSD, 3KQ6, 3P2X, 3P33, 1JK8, 2MLI, 2MPG, 2MPI, 2MVC, 2MVD, 4CXL, 4CXN, 4CY7, 4NIB, 4OGA, 4P65, 4Q5Z, 4RXW, 4UNE, 4UNG, 4UNH, 4XC4, 4WDI, 4Z76, 4Z77, 4Z78, 2N2W, 5CO6, 5ENA, 4Y19, 5BQQ, 5BOQ, 2N2V, 5CNY, 5CO9, 5EN9, 4Y1A, 2N2X, 5BPO, 5CO2, 5BTS, 5HYJ, 5C0D,%%s1EFE, 1SJT, 1SJU, 2KQP,%%s1T0C,%%s2G54, 2G56, 3HYD, 2OMQ 3630 16334 ENSG00000254647 ENSMUSG00000000215 P01308 P01326 NM_000207NM_001185097NM_001185098NM_001291897 NM_001185083NM_001185084NM_008387 NP_001172026.1NP_001172027.1NP_001278826.1NP_000198NP_000198NP_000198NP_000198 NP_001172012NP_001172013NP_032413 Beta cells are sensitive to blood sugar levels so that they secrete insulin into the blood in response to high level of glucose; and inhibit secretion of insulin when glucose levels are low.[10] Insulin enhances glucose uptake and metabolism in the cells, thereby reducing blood sugar level. Their neighboring alpha cells, by taking their cues from the beta cells,[10] secrete glucagon into the blood in the opposite manner: increased secretion when blood glucose is low, and decreased secretion when glucose concentrations are high. Glucagon increases blood glucose level by stimulating glycogenolysis and gluconeogenesis in the liver.[8][10] The secretion of insulin and glucagon into the blood in response to the blood glucose concentration is the primary mechanism of glucose homeostasis.[10] Decreased or absent insulin activity results in diabetes mellitus, a condition of high blood sugar level (hyperglycaemia). There are two types of the disease. In type 1 diabetes mellitus, the beta cells are destroyed by an autoimmune reaction so that insulin can no longer be synthesized or be secreted into the blood.[11] In type 2 diabetes mellitus, the destruction of beta cells is less pronounced than in type 1 diabetes, and is not due to an autoimmune process. Instead, there is an accumulation of amyloid in the pancreatic islets, which likely disrupts their anatomy and physiology.[10] The pathogenesis of type 2 diabetes is not well understood but reduced population of islet beta-cells, reduced secretory function of islet beta-cells that survive, and peripheral tissue insulin resistance are known to be involved.[7] Type 2 diabetes is characterized by increased glucagon secretion which is unaffected by, and unresponsive to the concentration of blood glucose. But insulin is still secreted into the blood in response to the blood glucose.[10] As a result, glucose accumulates in the blood.
 The human insulin protein is composed of 51 amino acids, and has a molecular mass of 5808 Da. It is a heterodimer of an A-chain and a B-chain, which are linked together by disulfide bonds. Insulin's structure varies slightly between species of animals. Insulin from animal sources differs somewhat in effectiveness (in carbohydrate metabolism effects) from human insulin because of these variations. Porcine insulin is especially close to the human version, and was widely used to treat type 1 diabetics before human insulin could be produced in large quantities by recombinant DNA technologies.[12][13][14][15] Insulin was the first peptide hormone discovered.[16] Frederick Banting and Charles Herbert Best, working in the laboratory of J.J.R. Macleod at the University of Toronto, were the first to isolate insulin from dog pancreas in 1921. Frederick Sanger sequenced the amino acid structure in 1951, which made insulin the first protein to be fully sequenced.[17]  The crystal structure of insulin in the solid state was determined by Dorothy Hodgkin in 1969. Insulin is also the first protein to be chemically synthesised and produced by DNA recombinant technology.[18] It is on the WHO Model List of Essential Medicines, the most important medications needed in a basic health system.[19]"
Integrative_biology,Biology,5,"Integrative Biology is a monthly peer-reviewed scientific journal covering the interface between biology and the fields of physics, chemistry, engineering, imaging, and informatics. It was published by the Royal Society of Chemistry from its launch in 2008 until 2018. Since 2019 it has been published by Oxford University Press.
"
Interferon,Biology,5,"Interferons (IFNs, /ˌɪntərˈfɪərɒn/[1]) are a group of signaling proteins[2] made and released by host cells in response to the presence of several  viruses. In a typical scenario, a virus-infected cell will release interferons causing nearby cells to heighten their anti-viral defenses.
 IFNs belong to the large class of proteins known as cytokines, molecules used for communication between cells to trigger the protective defenses of the immune system that help eradicate pathogens.[3] Interferons are named for  their ability to ""interfere"" with viral replication[3] by protecting cells from virus infections. IFNs also have various other functions: they activate immune cells, such as natural killer cells and macrophages; they increase host defenses by up-regulating antigen presentation by virtue of increasing the expression of major histocompatibility complex (MHC) antigens.  Certain symptoms of infections, such as fever, muscle pain and ""flu-like symptoms"", are also caused by the production of IFNs and other cytokines.
 More than twenty distinct IFN genes and proteins have been identified in animals, including humans.  They are typically divided among three classes: Type I IFN, Type II IFN, and Type III IFN.  IFNs belonging to all three classes are important for fighting viral infections and for the regulation of the immune system.
"
Internal_fertilization,Biology,5,"Internal fertilization is the union of an egg cell with a sperm during sexual reproduction inside the female body. Internal fertilization, unlike its counterpart, external fertilization, brings more control to the female with reproduction.[1] For internal fertilization to happen there needs to be a method for the male to introduce the sperm into the female's reproductive tract. In mammals, reptiles, and certain other groups of animals, this is done by copulation, an intromittent organ being introduced into the vagina or cloaca.[2][3] In most birds, the cloacal kiss is used, the two animals pressing their cloacas together while transferring sperm.[4] Salamanders, spiders, some insects and some molluscs undertake internal fertilization by transferring a spermatophore, a bundle of sperm, from the male to the female. Following fertilization, the embryos are laid as eggs in oviparous organisms, or continue to develop inside the reproductive tract of the mother to be born later as live young in viviparous organisms.
"
International_System_of_Units,Biology,5,"
 The  International System of Units (SI, abbreviated from the French Système international (d'unités)) is the modern form of the metric system. It is the only system of measurement with an official status in nearly every country in the world. It comprises a coherent system of units of measurement starting with seven base units, which are the second (the unit of time with the symbol s), metre (length, m), kilogram (mass, kg), ampere (electric current, A), kelvin (thermodynamic temperature, K), mole (amount of substance, mol), and candela (luminous intensity, cd). The system allows for an unlimited number of additional units, called derived units, which can always be represented as products of powers of the base units.[Note 1] Twenty-two derived units have been provided with special names and symbols.[Note 2] The seven base units and the 22 derived units with special names and symbols may be used in combination to express other derived units,[Note 3] which are adopted to facilitate measurement of diverse quantities. The SI system also provides twenty prefixes to the unit names and unit symbols that may be used when specifying power-of-ten (i.e. decimal) multiples and sub-multiples of SI units. The SI is intended to be an evolving system; units and prefixes are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves.
 Since 2019, the magnitudes of all SI units have been defined by declaring exact numerical values for seven defining constants when expressed in terms of their SI units. These defining constants are the speed of light in vacuum, c, the hyperfine transition frequency of caesium ΔνCs, the Planck constant h, the elementary charge e, the Boltzmann constant k, the Avogadro constant NA, and the luminous efficacy Kcd. The nature of the defining constants ranges from fundamental constants of nature such as c to the purely technical constant Kcd. Prior to 2019, h, e, k, and NA were not defined a priori but were rather very precisely measured quantities. In 2019, their values were fixed by definition to their best estimates at the time, ensuring continuity with previous definitions of the base units. One consequence of the redefinition of the SI is that the distinction between the base units and derived units is in principle not needed, since any unit can be constructed directly from the seven defining constants.[2]:129 The current way of defining the SI system is a result of a decades-long move towards increasingly abstract and idealised formulation in which the realisations of the units are separated conceptually from the definitions. A consequence is that as science and technologies develop, new and superior realisations may be introduced without the need to redefine the unit.  One problem with artefacts is that they can be lost, damaged, or changed; another is that they introduce uncertainties that cannot be reduced by advancements in science and technology. The last artefact used by the SI was the International Prototype of the Kilogram, a cylinder of platinum-iridium.
 The original motivation for the development of the SI was the diversity of units that had sprung up within the centimetre–gram–second (CGS) systems (specifically the inconsistency between the systems of electrostatic units and electromagnetic units) and the lack of coordination between the various disciplines that used them. The General Conference on Weights and Measures (French: Conférence générale des poids et mesures – CGPM), which was established by the Metre Convention of 1875, brought together many international organisations to establish the definitions and standards of a new system and to standardise the rules for writing and presenting measurements. The system was published in 1960 as a result of an initiative that began in 1948,so is based on the metre–kilogram–second system of units (MKS) rather than any variant of the CGS.
"
Interphase,Biology,5,"Interphase is the portion of the cell cycle that is not accompanied by observable changes under the microscope, and includes the G1, S and G2 phases. During interphase, the cell grows (G1), replicates its DNA (S) and prepares for mitosis (G2). A cell in interphase should not be confused with a cell in quiescent state, which represents most of the cell’s lifetime. The term quiescent (i.e. state of dormancy) is misleading since a quiescent cell is very busy synthesizing proteins, copying DNA into RNA, engulfing extracellular material, processing signals, to name just a few activities. The cell is quiescent only in the sense of cell division (i.e. the cell is out of the cell cycle, G0). Interphase is the phase of the cell cycle in which a typical cell spends most of its life. During interphase, the cell copies its DNA in preparation for mitosis.[1] Interphase is the 'daily living' or metabolic phase of the cell, in which the cell obtains nutrients and metabolizes them, grows, reads its DNA, and conducts other ""normal"" cell functions.[1] This phase was formerly called the resting phase. However, interphase does not describe a cell that is merely resting; rather, the cell is living and preparing for later cell division, so the name was changed. A common misconception is that interphase is the first stage of mitosis, but since mitosis is the division of the nucleus, prophase is actually the first stage.[2] In interphase, the cell gets itself ready for mitosis or meiosis. Somatic cells, or normal diploid cells of the body, go through mitosis in order to reproduce themselves through cell division, whereas diploid germ cells (i.e., primary spermatocytes and primary oocytes) go through meiosis in order to create haploid gametes (i.e., sperm and ova) for the purpose of sexual reproduction.
"
Introduced_species,Biology,5,"An introduced species, alien species, exotic species, adventive species, immigrant species, foreign species, non-indigenous species, or non-native species is a species living outside its native distributional range, but which has arrived there by human activity, directly or indirectly, and either deliberately or accidentally. Non-native species can have various effects on the local ecosystem. Introduced species that become established and spread beyond the place of introduction are considered ""naturalized"". The process of human-caused introduction is distinguished from biological colonization, in which species spread to new areas through ""natural"" (non-human) means such as storms and rafting.
 The impact of introduced species is highly variable. Some have a negative effect on a local ecosystem, while other introduced species may have no negative effect or only minor impact. Some species have been introduced intentionally to combat pests. They are called biocontrols and may be regarded as beneficial as an alternative to pesticides in agriculture for example. In some instances the potential for being beneficial or detrimental in the long run remains unknown.[1][2][3]  The effects of introduced species on natural environments have gained much scrutiny from scientists, governments, farmers and others.
"
Invertebrate,Biology,5,"
.mw-parser-output table.biota-infobox{text-align:center;width:200px;font-size:100%}.mw-parser-output table.biota-infobox th.section-header{text-align:center}.mw-parser-output table.biota-infobox td.section-content{text-align:left;padding:0 0.25em}.mw-parser-output table.biota-infobox td.list-section{text-align:left;padding:0 0.25em}.mw-parser-output table.biota-infobox td.taxon-section{text-align:center;padding:0 0.25em}.mw-parser-output table.biota-infobox td.image-section{text-align:center;font-size:88%}.mw-parser-output table.biota-infobox table.taxonomy{margin:0 auto;text-align:left;background:transparent;padding:2px}.mw-parser-output table.biota-infobox table.taxonomy tr{vertical-align:top}.mw-parser-output table.biota-infobox table.taxonomy td{padding:1px} Invertebrates are animals that neither possess nor develop a vertebral column (commonly known as a backbone or spine), derived from the notochord. This includes all animals apart from the subphylum Vertebrata. Familiar examples of invertebrates include arthropods (insects, arachnids, crustaceans, and myriapods), mollusks (chitons, snail, bivalves, squids, and octopuses), annelid (earthworms and leeches), and cnidarians (hydras, jellyfishes, sea anemones, and corals).
 The majority of animal species are invertebrates; one estimate puts the figure at 97%.[1]  Many invertebrate taxa have a greater number and variety of species than the entire subphylum of Vertebrata.[2] Invertebrates vary widely in size, from 50 μm (0.002 in) rotifers[3] to the 9–10 m (30–33 ft) colossal squid.[4] Some so-called invertebrates, such as the Tunicata and Cephalochordata, are more closely related to vertebrates than to other invertebrates. This makes the invertebrates paraphyletic, so the term has little meaning in taxonomy.
"
Ion,Biology,5,"
 An ion (/ˈaɪɒn, -ən/)[1] is an particle,atom or molecule with a net electrical charge. 
 The charge of the electron is considered negative by convention.  The negative charge of an ion is equal and opposite to charged proton(s) considered positive by convention.  The net charge of an ion is non-zero due to its total number of electrons being unequal to its total number of protons. 
 A cation is a positively charged ion, with fewer electrons than protons, while an anion is negatively charged, with more electrons than protons. Because of their opposite electric charges, cations and anions attract each other and readily form ionic compounds.
 Ions consisting of only a single atom are termed atomic or monatomic ions, while two or more atoms form molecular ions or polyatomic ions. In the case of physical ionization in a fluid (gas or liquid), ""ion pairs"" are created by spontaneous molecule collisions, where each generated pair consists of a free electron and a positive ion.[2] Ions are also created by chemical interactions, such as the dissolution of a salt in liquids, or by other means, such as passing a direct current through a conducting solution, dissolving an anode via ionization.
"
Ionic_bond,Biology,5,"
 Ionic bonding is a type of chemical bonding that involves the electrostatic attraction between oppositely charged ions[citation needed], or between two atoms with sharply different electronegativities,[1] and is the primary interaction occurring in ionic compounds. It is one of the main types of bonding along with covalent bonding and metallic bonding. Ions are atoms (or groups of atoms) with an electrostatic charge. Atoms that gain electrons make negatively charged ions (called anions). Atoms that lose electrons make positively charged ions (called cations). This transfer of electrons is known as electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complex nature, e.g. molecular ions like NH+4 or SO2−4. In simpler words, an ionic bond results from the transfer of electrons from a metal to a non-metal in order to obtain a full valence shell for both atoms.
 It is important to recognize that clean ionic bonding — in which one atom or molecule completely transfers an electron to another — cannot exist: all ionic compounds have some degree of covalent bonding, or electron sharing. Thus, the term ""ionic bonding"" is given when the ionic character is greater than the covalent character – that is, a bond in which a large electronegativity difference exists between the two atoms, causing the bonding to be more polar (ionic) than in covalent bonding where electrons are shared more equally. Bonds with partially ionic and partially covalent character are called polar covalent bonds. 
 Ionic compounds conduct electricity when molten or in solution, typically not when solid. Ionic compounds generally have a high melting point, depending on the charge of the ions they consist of. The higher the charges the stronger the cohesive forces and the higher the melting point. They also tend to be soluble in water; the stronger the cohesive forces, the lower the solubility.[2]"
Isomer,Biology,5,"
 In chemistry, isomers are molecules or polyatomic ions with identical molecular formulas — that is, same number of atoms of each element — but distinct arrangements of atoms in space.[1] Isomerism is existence or possibility of isomers.
 Isomers do not necessarily share similar chemical or physical properties. Two main forms of isomerism are structural or constitutional isomerism, in which bonds between the atoms differ; and stereoisomerism or spatial isomerism, in which the bonds are the same but the relative positions of the atoms differ.
 Isomeric relationships form a hierarchy. Two chemicals might be the same constitutional isomer, but upon deeper analysis be stereoisomers of each other. Two molecules that are the same stereoisomer as each other might be in different conformational forms or be different isotopologues. The depth of analysis depends on the field of study or the chemical and physical properties of interest.
 The English word ""isomer"" (/ˈaɪsəmər/) is a back-formation from ""isomeric"",[2] which was borrowed through German isomerisch[3] from Swedish isomerisk;  which in turn was coined from Greek ἰσόμερoς  isómeros, with roots isos = ""equal"",  méros = ""part"".[4]"
Isotonic_solutions,Biology,5,"Tonicity is a measure of the effective osmotic pressure gradient; the water potential of two solutions separated by a semipermeable cell membrane. In other words, tonicity is the relative concentration of solutes dissolved in solution which determine the direction and extent of diffusion. It is commonly used when describing the response of cells immersed in an external solution.
 Unlike osmotic pressure, tonicity is influenced only by solutes that cannot cross the membrane, as only these exert an effective osmotic pressure. Solutes able to freely cross the membrane do not affect tonicity because they will always equilibrate with equal concentrations on both sides of the membrane without net solvent movement. It is also a factor affecting imbibition.
 There are three classifications of tonicity that one solution can have relative to another: hypertonic, hypotonic, and isotonic.[1]"
Jejunum,Biology,5,"The jejunum is the second part of the small intestine in humans and most higher vertebrates, including mammals, reptiles, and birds.  Its lining is specialised for the absorption by enterocytes of small nutrient molecules which have been previously digested by enzymes in the duodenum.
 The jejunum lies between the duodenum and the ileum and is considered to start at the suspensory muscle of the duodenum, a location called the duodenojejunal flexure.[4] The division between the jejunum and ileum is not anatomically distinct.[5] In adult humans, the small intestine is usually 6–7 metres (20–23 ft) long (post mortem), about two-fifths of which (about 2.5 m (8.2 ft)) is the jejunum.[4]"
Kinase,Biology,5,"In biochemistry, a kinase is an enzyme that catalyzes the transfer of phosphate groups from high-energy, phosphate-donating molecules to specific substrates. This process is known as phosphorylation, where the substrate gains a phosphate group and the high-energy ATP molecule donates a phosphate group. This transesterification produces a phosphorylated substrate and ADP. Conversely, it is referred to as dephosphorylation when the phosphorylated substrate donates a phosphate group and ADP gains a phosphate group (producing a dephosphorylated substrate and the high energy molecule of ATP). These two processes, phosphorylation and dephosphorylation, occur four times during glycolysis.[2][3][4] Kinases are part of the larger family of phosphotransferases. Kinases should not be confused with phosphorylases, which catalyze the addition of inorganic phosphate groups to an acceptor, nor with phosphatases, which remove phosphate groups (dephosphorylation). The phosphorylation state of a molecule, whether it be a protein, lipid or carbohydrate, can affect its activity, reactivity and its ability to bind other molecules. Therefore, kinases are critical in metabolism, cell signalling, protein regulation, cellular transport, secretory processes and many other cellular pathways, which makes them very important to human physiology.
"
Kingdom_(biology),Biology,5,"In biology, kingdom (Latin: regnum, plural regna) is the second highest taxonomic rank, just below domain. Kingdoms are divided into smaller groups called phyla.
 Traditionally, some textbooks from the United States and Canada used a system of six kingdoms (Animalia, Plantae, Fungi, Protista, Archaea/Archaebacteria, and Bacteria/Eubacteria) while textbooks in countries like Great Britain, India, Greece, Brazil and other countries use five kingdoms only (Animalia, Plantae, Fungi, Protista and Monera).
 Some recent classifications based on modern cladistics have explicitly abandoned the term ""kingdom"", noting that the traditional kingdoms are not monophyletic, i.e., do not consist of all the descendants of a common ancestor.
"
Krebs_cycle,Biology,5,"The citric acid cycle (CAC) – also known as the TCA cycle (tricarboxylic acid cycle) or the Krebs cycle[1][2] – is a series of chemical reactions used by all aerobic organisms to release stored energy through the oxidation of acetyl-CoA derived from carbohydrates, fats, and proteins. In addition, the cycle provides precursors of certain amino acids, as well as the reducing agent NADH, that are used in numerous other reactions. Its central importance to many biochemical pathways suggests that it was one of the earliest components of metabolism and may have originated abiogenically.[3][4] Even though it is branded as a 'cycle', it is not necessary for metabolites to follow only one specific route; at least three segments of the citric acid cycle have been recognized.[5] The name of this metabolic pathway is derived from the citric acid (a tricarboxylic acid, often called citrate, as the ionized form predominates at biological pH[6]) that is consumed and then regenerated by this sequence of reactions to complete the cycle. The cycle consumes acetate (in the form of acetyl-CoA) and water, reduces NAD+ to NADH, releasing carbon dioxide. The NADH generated by the citric acid cycle is fed into the oxidative phosphorylation (electron transport) pathway. The net result of these two closely linked pathways is the oxidation of nutrients to produce usable chemical energy in the form of ATP.
 In eukaryotic cells, the citric acid cycle occurs in the matrix of the mitochondrion. In prokaryotic cells, such as bacteria, which lack mitochondria, the citric acid cycle reaction sequence is performed in the cytosol with the proton gradient for ATP production being across the cell's surface (plasma membrane) rather than the inner membrane of the mitochondrion. The overall yield of energy-containing compounds from the TCA cycle is three NADH, one FADH2, and one GTP.[7]"
Larva,Biology,5,"A larva /ˈlɑːrvə/ (plural larvae /ˈlɑːrviː/) is a distinct juvenile form many  animals undergo before metamorphosis into adults. Animals with indirect development such as insects, amphibians, or cnidarians typically have a larval phase of their life cycle.
 The larva's appearance is generally very different from the adult form (e.g. caterpillars and butterflies) including different unique structures and organs that do not occur in the adult form. Their diet may also be considerably different.
 Larvae are frequently adapted to environments separate from adults. For example, some larvae such as tadpoles live almost exclusively in aquatic environments, but can live outside water as adult frogs. By living in a distinct environment, larvae may be given shelter from predators and reduce competition for resources with the adult population.
 Animals in the larval stage will consume food to fuel their transition into the adult form. In some organisms like polychaetes and barnacles, adults are immobile but their larvae are mobile, and use their mobile larval form to distribute themselves.[1][2] Some larvae are dependent on adults to feed them. In many eusocial Hymenoptera species, the larvae are fed by female workers. In Ropalidia marginata (a paper wasp) the males are also capable of feeding larvae but they are much less efficient, spending more time and getting less food to the larvae.[3] The larvae of some organisms (for example, some newts) can become pubescent and do not develop further into the adult form. This is a type of neoteny.[4] It is a misunderstanding that the larval form always reflects the group's evolutionary history. This could be the case, but often the larval stage has evolved secondarily, as in insects.[5][6] In these cases the larval form may differ more than the adult form from the group's common origin.[7]"
Law_of_Independent_Assortment,Biology,5,"
 Mendelian inheritance is a type of biological inheritance that follows the principles originally proposed by Gregor Mendel in 1865 and 1866, re-discovered in 1900 and popularized by William Bateson.[1] These principles were initially controversial. When Mendel's theories were integrated with the Boveri–Sutton chromosome theory of inheritance by Thomas Hunt Morgan in 1915, they became the core of classical genetics. Ronald Fisher combined these ideas with the theory of natural selection in his 1930 book The Genetical Theory of Natural Selection, putting evolution onto a mathematical footing and forming the basis for population genetics within the modern evolutionary synthesis.[2]"
Leukocyte,Biology,5,"
 White blood cells (WBCs), also called leukocytes or leucocytes, are the cells of the immune system that are involved in protecting the body against both infectious disease and foreign invaders. All white blood cells are produced and derived from multipotent cells in the bone marrow known as hematopoietic stem cells. Leukocytes are found throughout the body, including the blood and lymphatic system.[1] All white blood cells have nuclei, which distinguishes them from the other blood cells, the anucleated red blood cells (RBCs) and platelets. The different white blood cell types are classified in standard ways; two pairs of broadest categories classify them either by structure (granulocytes or agranulocytes) or by cell lineage (myeloid cells or lymphoid cells). These broadest categories can be further divided into the five main types: neutrophils, eosinophils (acidophiles), basophils, lymphocytes, and monocytes.[2] These types are distinguished by their physical and functional characteristics. Monocytes and neutrophils are phagocytic. Further subtypes can be classified; for example, among lymphocytes, there are B cells (named from bursa or bone marrow cells), T cells (named from thymus cells), and natural killer cells.
 The number of leukocytes in the blood is often an indicator of disease, and thus the white blood cell count is an important subset of the complete blood count. The normal white cell count is usually between 4 × 109/L and 1.1 × 1010/L. In the US, this is usually expressed as 4,000 to 11,000 white blood cells per microliter of blood.[3] White blood cells make up approximately 1% of the total blood volume in a healthy adult,[4] making them substantially less numerous than the red blood cells at 40% to 45%. However, this 1% of the blood makes a large difference to health, because immunity depends on it. An increase in the number of leukocytes over the upper limits is called leukocytosis. It is normal when it is part of healthy immune responses, which happen frequently. It is occasionally abnormal, when it is neoplastic or autoimmune in origin. A decrease below the lower limit is called leukopenia. This indicates a weakened immune system.
"
Lichen,Biology,5,"
 A lichen (/ˈlaɪkən/ LY-ken or, sometimes in the UK, /ˈlɪtʃən/, LICH-en) is a composite organism that arises from algae or cyanobacteria living among filaments of multiple fungi species[1] in a mutualistic relationship.[2][3][4] Lichens have properties different from those of their component organisms. Lichens come in many colors, sizes, and forms and are sometimes plant-like, but lichens are not plants. Lichens may have tiny, leafless branches (fruticose), flat leaf-like structures (foliose), flakes that lie on the surface like peeling paint (crustose),[5] a powder-like appearance (leprose), or other growth forms.[6] A macrolichen is a lichen that is either bush-like or leafy; all other lichens are termed microlichens.[2] Here, ""macro"" and ""micro"" do not refer to size, but to the growth form.[2] Common names for lichens may contain the word moss (e.g., ""reindeer moss"", ""Iceland moss""), and lichens may superficially look like and grow with mosses, but lichens are not related to mosses or any plant.[4]:3 Lichens do not have roots that absorb water and nutrients as plants do,[7]:2 but like plants, they produce their own nutrition by photosynthesis.[8] When they grow on plants, they do not live as parasites, but instead use the plants as a substrate.
 Lichens occur from sea level to high alpine elevations, in many environmental conditions, and can grow on almost any surface.[8] Lichens are abundant growing on bark, leaves, mosses, on other lichens,[7] and hanging from branches ""living on thin air"" (epiphytes) in rain forests and in temperate woodland. They grow on rock, walls, gravestones, roofs, exposed soil surfaces, rubber, bones, and in the soil as part of biological soil crusts. Different kinds of lichens have adapted to survive in some of the most extreme environments on Earth: arctic tundra, hot dry deserts, rocky coasts, and toxic slag heaps. They can even live inside solid rock, growing between the grains.
 It is estimated that 6–8% of Earth's land surface is covered by lichens.[9] There are about 20,000 known species of lichens.[10] Some lichens have lost the ability to reproduce sexually, yet continue to speciate.[7][11] Lichens can be seen as being relatively self-contained miniature ecosystems, where the fungi, algae, or cyanobacteria have the potential to engage with other microorganisms in a functioning system that may evolve as an even more complex composite organism.[12][13][14][15] Lichens may be long-lived, with some considered to be among the oldest living things.[4][16] They are among the first living things to grow on fresh rock exposed after an event such as a landslide. The long life-span and slow and regular growth rate of some lichens can be used to date events (lichenometry).
"
Life,Biology,5,"
 Life is a characteristic that distinguishes physical entities that have biological processes, such as signaling and self-sustaining processes, from those that do not, either because such functions have ceased (they have died), or because they never had such functions and are classified as inanimate. Various forms of life exist, such as plants, animals, fungi, protists, archaea, and bacteria. Biology is the science concerned with the study of life.
 Life on Earth:
 
There is currently no consensus regarding the definition of life. One popular definition is that organisms are open systems that maintain homeostasis, are composed of cells, have a life cycle, undergo metabolism, can grow, adapt to their environment, respond to stimuli, reproduce and evolve. Other definitions sometimes include non-cellular life forms such as viruses and viroids.
 Abiogenesis is the natural process of life arising from non-living matter, such as simple organic compounds. The prevailing scientific hypothesis is that the transition from non-living to living entities was not a single event, but a gradual process of increasing complexity. Life on Earth first appeared as early as 4.28 billion years ago, soon after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago.[1][2][3][4] The earliest known life forms are microfossils of bacteria.[5][6] Researchers generally think that current life on Earth descends from an RNA world,[7] although RNA-based life may not have been the first life to have existed.[8][9] The classic 1952 Miller–Urey experiment and similar research demonstrated that most amino acids, the chemical constituents of the proteins used in all living organisms, can be synthesized from inorganic compounds under conditions intended to replicate those of the early Earth. Complex organic molecules occur in the Solar System and in interstellar space, and these molecules may have provided starting material for the development of life on Earth.[10][11][12][13] Since its primordial beginnings, life on Earth has changed its environment on a geologic time scale, but it has also adapted to survive in most ecosystems and conditions. Some microorganisms, called extremophiles, thrive in physically or geochemically extreme environments that are detrimental to most other life on Earth. The cell is considered the structural and functional unit of life.[14][15] There are two kinds of cells, prokaryotic and eukaryotic, both of which consist of cytoplasm enclosed within a membrane and contain many biomolecules such as proteins and nucleic acids. Cells reproduce through a process of cell division, in which the parent cell divides into two or more daughter cells.
 In the past, there have been many attempts to define what is meant by ""life"" through obsolete concepts such as odic force, hylomorphism, spontaneous generation and vitalism, that have now been disproved by biological discoveries. Aristotle is considered to be the first person to classify organisms. Later, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Eventually new groups and categories of life were discovered, such as cells and microorganisms, forcing dramatic revisions of the structure of relationships between living organisms. Though currently only known on Earth, life need not be restricted to it, and many scientists speculate in the existence of extraterrestrial life. Artificial life is a computer simulation or human-made reconstruction of any aspect of life, which is often used to examine systems related to natural life.
 Death is the permanent termination of all biological processes which sustain an organism, and as such, is the end of its life. Extinction is the term describing the dying out of a group or taxon, usually a species. Fossils are the preserved remains or traces of organisms.
"
Biological_life_cycle,Biology,5,"In biology, a biological life cycle (or just life cycle or lifecycle when the biological context is clear) is a series of changes in form that an organism undergoes, returning to the starting state. ""The concept is closely related to those of the life history, development and ontogeny, but differs from them in stressing renewal.""[1][2] Transitions of form may involve growth, asexual reproduction, or sexual reproduction.
 In some organisms, different ""generations"" of the species succeed each other during the life cycle. For plants and many algae, there are two multicellular stages, and the life cycle is referred to as alternation of generations. The term life history is often used, particularly for organisms such as the red algae which have three multicellular stages (or more), rather than two.[3] Life cycles that include sexual reproduction involve alternating haploid (n) and diploid (2n) stages, i.e., a change of ploidy is involved. To return from a diploid stage to a haploid stage, meiosis must occur. In regard to changes of ploidy, there are 3 types of cycles:
 The cycles differ in when mitosis (growth) occurs. Zygotic meiosis and gametic meiosis have one mitotic stage: mitosis occurs during the n phase in zygotic meiosis and during the 2n phase in gametic meiosis. Therefore, zygotic and gametic meiosis are collectively termed ""haplobiontic"" (single mitotic phase, not to be confused with haplontic). Sporic meiosis, on the other hand, has mitosis in two stages, both the diploid and haploid stages, termed ""diplobiontic"" (not to be confused with diplontic).
"
Ligament,Biology,5,"A ligament is the fibrous connective tissue that connects bones to other bones.  It is also known as articular ligament, articular larua,[1] fibrous ligament, or true ligament. Other ligaments in the body include the:
 Ligaments are similar to tendons  as they are all made of connective tissue. The differences in them are in the connections that they make: ligaments connect one bone to another bone, tendons connect muscle to bone, and fasciae connect muscles to other muscles. These are all found in the skeletal system of the human body. Ligaments cannot usually be regenerated naturally; however, there are periodontal ligament stem cells located near the periodontal ligament which are involved in the adult regeneration of periodontist ligament.
 The study of ligaments is known as desmology.
"
Light-independent_reactions,Biology,5,"The Calvin cycle, light-independent reactions, bio synthetic phase, dark reactions, or photosynthetic carbon reduction (PCR) cycle[1] of photosynthesis are the chemical reactions that convert carbon dioxide and other compounds into glucose. These reactions occur in the stroma, the fluid-filled area of a chloroplast outside the thylakoid membranes. These reactions take the products (ATP and NADPH) of light-dependent reactions and perform further chemical processes on them. [The Calvin cycle uses the reducing powers ATP and NADPH from the light dependent reactions to produce sugars for the plant to use. These substrates are used in a series of reduction-oxidation reactions to produce sugars in a step-wise process. There is no direct reaction that converts CO2 to a sugar because all of the energy would be lost to heat.] There are three phases to the light-independent reactions, collectively called the Calvin cycle: carbon fixation, reduction reactions, and ribulose 1,5-bisphosphate (RuBP) regeneration.
 Though it is called the ""dark reaction"", the Calvin cycle does not actually occur in the dark or during nighttime. This is because the process requires reduced NADP which is short-lived and comes from the light-dependent reactions. In the dark, plants instead release sucrose into the phloem from their starch reserves to provide energy for the plant. The Calvin cycle thus happens when light is available independent of the kind of photosynthesis (C3 carbon fixation, C4 carbon fixation, and Crassulacean Acid Metabolism (CAM)); CAM plants store malic acid in their vacuoles every night and release it by day to make this process work.[2]"
Linked_genes,Biology,5,"Genetic linkage is the tendency of DNA sequences that are close together on a chromosome to be inherited together during the meiosis phase of sexual reproduction. Two genetic markers that are physically near to each other are unlikely to be separated onto different chromatids during chromosomal crossover, and are therefore said to be more linked than markers that are far apart. In other words, the nearer two genes are on a chromosome, the lower the chance of recombination between them, and the more likely they are to be inherited together. Markers on different chromosomes are perfectly unlinked.
 Genetic linkage is the most prominent exception to Gregor Mendel's Law of Independent Assortment. The first experiment to demonstrate linkage was carried out in 1905. At the time, the reason why certain traits tend to be inherited together was unknown. Later work revealed that genes are physical structures related by physical distance.
 The typical unit of genetic linkage is the centimorgan (cM). A distance of 1 cM between two markers means that the markers are separated to different chromosomes on average once per 100 meiotic product, thus once per 50 meioses.
"
Lipid,Biology,5,"In biology and biochemistry, a lipid is a macrobiomolecule that is soluble in nonpolar solvents.[3] Non-polar solvents are typically hydrocarbons used to dissolve other naturally occurring hydrocarbon lipid molecules that do not (or do not easily) dissolve in water, including fatty acids, waxes, sterols, fat-soluble vitamins (such as vitamins A, D, E, and K), monoglycerides, diglycerides, triglycerides, and phospholipids.
 The functions of lipids include storing energy, signaling, and acting as structural components of cell membranes.[4][5] Lipids have applications in the cosmetic and food industries as well as in nanotechnology.[6] Scientists sometimes define lipids as hydrophobic or amphiphilic small molecules; the amphiphilic nature of some lipids allows them to form structures such as vesicles, multilamellar/unilamellar liposomes, or membranes in an aqueous environment. Biological lipids originate entirely or in part from two distinct types of biochemical subunits or ""building-blocks"": ketoacyl and isoprene groups.[4] Using this approach, lipids may be divided into eight categories: fatty acids, glycerolipids, glycerophospholipids, sphingolipids, saccharolipids, and polyketides (derived from condensation of ketoacyl subunits); and sterol lipids and prenol lipids (derived from condensation of isoprene subunits).[4] Although the term ""lipid"" is sometimes used as a synonym for fats, fats are a subgroup of lipids called triglycerides. Lipids also encompass molecules such as fatty acids and their derivatives (including tri-, di-, monoglycerides, and phospholipids), as well as other sterol-containing metabolites such as cholesterol.[7] Although humans and other mammals use various biosynthetic pathways both to break down and to synthesize lipids, some essential lipids can't be made this way and must be obtained from the diet.
"
Lipoprotein,Biology,5,"A lipoprotein is a biochemical assembly whose primary function is to transport hydrophobic lipid (also known as fat) molecules in water, as in blood plasma or other extracellular fluids. They consist of a Triglyceride and Cholesterol center, surrounded by a phospholipid outer shell, with the hydrophilic portions oriented outward toward the surrounding water and lipophilic portions oriented inward toward the lipid center. A special kind of protein, called apolipoprotein, is embedded in the outer shell, both stabilising the complex and giving it a functional identity that determines its fate.
 Many enzymes, transporters, structural proteins, antigens, adhesins, and toxins are lipoproteins. Examples include plasma lipoprotein particles (HDL, LDL, IDL, VLDL and chylomicrons). Subgroups of these plasma particles are primary drivers or modulators of atherosclerosis.[1]"
M_phase,Biology,5,"
 The cell cycle, or cell-division cycle, is the series of events that take place in a cell that cause it to divide into two daughter cells. These events include the duplication of its DNA (DNA replication) and some of its organelles, and subsequently the partitioning of its cytoplasm and other components into two daughter cells in a process called cell division.
 In cells with nuclei (eukaryotes), (i.e., animal, plant, fungal, and protist cells), the cell cycle is divided into two main stages: interphase and the mitotic (M) phase (including mitosis and cytokinesis). During interphase, the cell grows, accumulating nutrients needed for mitosis, and replicates its DNA and some of its organelles. During the mitotic phase, the replicated chromosomes, organelles, and cytoplasm separate into two new daughter cells. To ensure the proper replication of cellular components and division, there are control mechanisms known as cell cycle checkpoints after each of the key steps of the cycle that determine if the cell can progress to the next phase.
 In cells without nuclei (prokaryotes), (i.e., bacteria and archaea), the cell cycle is divided into the B, C, and D periods. The B period extends from the end of cell division to the beginning of DNA replication. DNA replication occurs during the C period. The D period refers to the stage between the end of DNA replication and the splitting of the bacterial cell into two daughter cells.[1] The cell-division cycle is a vital process by which a single-celled fertilized egg develops into a mature organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed. After cell division, each of the daughter cells begin the interphase of a new cycle. Although the various stages of interphase are not usually morphologically distinguishable, each phase of the cell cycle has a distinct set of specialized biochemical processes that prepare the cell for initiation of the cell division.
"
Macroevolution,Biology,5,"
 Macroevolution in the modern sense is evolution that is guided by selection among interspecific variation, as opposed to selection among intraspecific variation in microevolution.[1][2][3] This modern definition differs from the original concept, which referred macroevolution to the evolution of taxa above the species level (genera, families, orders etc.).[4]"
Macromolecule,Biology,5,"A macromolecule is a very large molecule, such as protein, commonly composed of the polymerization of smaller subunits called monomers. They are typically composed of thousands of atoms or more. A substance that is composed of monomers is called a polymer. The most common macromolecules in biochemistry are biopolymers (nucleic acids, proteins, and carbohydrates) and large non-polymeric molecules (such as lipids and macrocycles),[1] synthetic fibers as well as experimental materials such as carbon nanotubes.[2][3] Macromolecules are large molecules composed of thousands of covalently connected atoms. Carbohydrates, lipids, proteins, and nucleic acids are all macromolecules. Macromolecules are formed by many monomers linking together, forming a polymer. Carbohydrates are composed of carbon, oxygen, and hydrogen. The monomer of carbohydrates are monosaccharides. There are three forms of carbohydrates: energy, storage, and structural molecules. A disaccharide is formed when a dehydration reaction joins two monosaccharides. Another type of macromolecules are lipids. Lipids are hydrocarbons that do not form polymers. Fats are constructed from glycerol and fatty acids. Phospholipids are commonly found in the phospholipid bilayer of membranes. They have hydrophilic heads and hydrophopic tails. A protein is another type of macromolecules. Amino acids are the monomers of proteins. Proteins have many different functions. There are proteins that are used for structural support, storage, transport, cellular communication, movement, defense against foreign substances, and more. Nucleic acids transmit and help express hereditary information. They are made up of monomers called nucleotides. Two types of nucleic acids are DNA and RNA.
"
Macronutrient,Biology,5,"A nutrient is a substance used by an organism to survive, grow, and reproduce. The requirement for dietary nutrient intake applies to animals, plants, fungi, and protists. Nutrients can be incorporated into cells for metabolic purposes or excreted by cells to create non-cellular structures, such as hair, scales, feathers, or exoskeletons. Some nutrients can be metabolically converted to smaller molecules in the process of releasing energy, such as for carbohydrates, lipids, proteins, and fermentation products (ethanol or vinegar), leading to end-products of water and carbon dioxide. All organisms require water. Essential nutrients for animals are the energy sources, some of the amino acids that are combined to create proteins, a subset of fatty acids, vitamins and certain minerals. Plants require more diverse minerals absorbed through roots, plus carbon dioxide and oxygen absorbed through leaves. Fungi live on dead or living organic matter and meet nutrient needs from their host.
 Different types of organism have different essential nutrients. Ascorbic acid (vitamin C) is essential, meaning it must be consumed in sufficient amounts, to humans and some other animal species, but not to all animals and not to plants, which are able to synthesize it. Nutrients may be organic or inorganic: organic compounds include most compounds containing carbon, while all other chemicals are inorganic. Inorganic nutrients include nutrients such as iron, selenium, and zinc, while organic nutrients include, among many others, energy-providing compounds and vitamins.
 A classification used primarily to describe nutrient needs of animals divides nutrients into macronutrients and micronutrients. Consumed in relatively large amounts (grams or ounces), macronutrients (carbohydrates, fats, proteins, water) are primarily used to generate energy or to incorporate into tissues for growth and repair. Micronutrients are needed in smaller amounts (milligrams or micrograms); they have subtle biochemical and physiological roles in cellular processes, like vascular functions or nerve conduction. Inadequate amounts of essential nutrients, or diseases that interfere with absorption, result in a deficiency state that compromises growth, survival and reproduction. Consumer advisories for dietary nutrient intakes, such as the United States Dietary Reference Intake, are based on deficiency outcomes[clarification needed] and provide macronutrient and micronutrient guides for both lower and upper limits of intake. In many countries, macronutrients and micronutrients in significant content[clarification needed] are required by regulations to be displayed on food product labels. Nutrients in larger quantities than the body needs may have harmful effects.[1] Edible plants also contain thousands of compounds generally called phytochemicals which have unknown effects on disease or health, including a diverse class with non-nutrient status called polyphenols, which remain poorly understood as of 2017.
"
Macrophage,Biology,5,"
 Macrophages (abbreviated as Mφ, MΦ or MP) (Greek: large eaters, from Greek μακρός (makrós) = large, φαγεῖν (phagein) = to eat) are a type of white blood cell of the immune system that engulfs and digests cellular debris, foreign substances, microbes, cancer cells, and anything else that does not have the type of proteins specific to healthy body cells on its surface[2] in a process called phagocytosis.
 These large phagocytes are found in essentially all tissues,[3] where they patrol for potential pathogens by amoeboid movement. They take various forms (with various names) throughout the body (e.g., histiocytes, Kupffer cells, alveolar macrophages, microglia, and others), but all are part of the mononuclear phagocyte system. Besides phagocytosis, they play a critical role in nonspecific defense (innate immunity) and also help initiate specific defense mechanisms (adaptive immunity) by recruiting other immune cells such as lymphocytes. For example, they are important as antigen presenters to T cells. In humans, dysfunctional macrophages cause severe diseases such as chronic granulomatous disease that result in frequent infections.
 Beyond increasing inflammation and stimulating the immune system, macrophages also play an important anti-inflammatory role and can decrease immune reactions through the release of cytokines. Macrophages that encourage inflammation are called M1 macrophages, whereas those that decrease inflammation and encourage tissue repair are called M2 macrophages.[4] This difference is reflected in their metabolism; M1 macrophages have the unique ability to metabolize arginine to the ""killer"" molecule nitric oxide, whereas rodent M2 macrophages have the unique ability to metabolize arginine to the ""repair"" molecule ornithine.[citation needed] However, this dichotomy has been recently questioned as further complexity has been discovered.[5] Human macrophages are about 21 micrometres (0.00083 in) in diameter[6] and are produced by the differentiation of monocytes in tissues. They can be identified using flow cytometry or immunohistochemical staining by their specific expression of proteins such as CD14, CD40, CD11b, CD64, F4/80 (mice)/EMR1 (human), lysozyme M, MAC-1/MAC-3 and CD68.[7] Macrophages were first discovered by Élie Metchnikoff, a Russian zoologist, in 1884.[8]"
Mammalogy,Biology,5,"In zoology, mammalogy is the study of mammals – a class of vertebrates with characteristics such as homeothermic metabolism, fur, four-chambered hearts, and complex nervous systems.[1] Mammalogy has also been known as ""mastology,"" ""theriology,"" and ""therology."" The archive of number of mammals on earth is constantly growing, but is currently set at 6,495 different mammal species including recently extinct.[2] There are 5,416 living mammals identified on earth and roughly 1,251 have been newly discovered since 2006.[2]  The major branches of mammalogy include natural history, taxonomy and systematics, anatomy and physiology, ethology, ecology, and management and control.[3] The approximate salary of a mammalogist varies from $20,000 to $60,000 a year, depending on their experience.  Mammalogists are typically involved in activities such as conducting research, managing personnel, and writing proposals.[4][5] Mammalogy branches off into other taxonomically-oriented disciplines such as primatology (study of primates), and cetology (study of cetaceans). Like other studies, mammalogy is also  a part of zoology which is also a part of biology, the study of all living things. 
"
Marine_biology,Biology,5,"Marine biology is the scientific study of marine life, organisms in the sea.  Given that in biology many phyla, families and genera have some species that live in the sea and others that live on land, marine biology classifies species based on the environment rather than on taxonomy.
 A large proportion of all life on Earth lives in the ocean. The exact size of this large proportion is unknown, since many ocean species are still to be discovered. The ocean is a complex three-dimensional world[3] covering approximately 71% of the Earth's surface. The habitats studied in marine biology include everything from the tiny layers of surface water in which organisms and abiotic items may be trapped in surface tension between the ocean and atmosphere, to the depths of the oceanic trenches, sometimes 10,000 meters or more beneath the surface of the ocean. Specific habitats include coral reefs, kelp forests, seagrass meadows, the surrounds of seamounts and thermal vents, tidepools, muddy, sandy and rocky bottoms, and the open ocean (pelagic) zone, where solid objects are rare and the surface of the water is the only visible boundary. The organisms studied range from microscopic phytoplankton and zooplankton to huge cetaceans (whales) 25–32 meters (82–105 feet) in length. Marine ecology is the study of how marine organisms interact with each other and the environment.
 Marine life is a vast resource, providing food, medicine, and raw materials, in addition to helping to support recreation and tourism all over the world. At a fundamental level, marine life helps determine the very nature of our planet. Marine organisms contribute significantly to the oxygen cycle, and are involved in the regulation of the Earth's climate.[4] Shorelines are in part shaped and protected by marine life, and some marine organisms even help create new land.[5] Many species are economically important to humans, including both finfish and shellfish. It is also becoming understood that the well-being of marine organisms and other organisms are linked in fundamental ways. The human body of knowledge regarding the relationship between life in the sea and important cycles is rapidly growing, with new discoveries being made nearly every day. These cycles include those of matter (such as the carbon cycle) and of air (such as Earth's respiration, and movement of energy through ecosystems including the ocean). Large areas beneath the ocean surface still remain effectively unexplored.
"
Mast_cell,Biology,5,"
 A mast cell (also known as a mastocyte or a labrocyte[1]) is a migrant cell of connective tissue that contains many granules rich in histamine and heparin. Specifically, it is a type of granulocyte derived from the myeloid stem cell that is a part of the immune and neuroimmune systems. Mast cells were discovered by Paul Ehrlich in 1877.[2] Although best known for their role in allergy and anaphylaxis, mast cells play an important protective role as well, being intimately involved in wound healing, angiogenesis, immune tolerance, defense against pathogens, and vascular permeability in brain tumours.[3][4] The mast cell is very similar in both appearance and function to the basophil, another type of white blood cell. Although mast cells were once thought to be tissue resident basophils, it has been shown that the two cells develop from different hematopoietic lineages and thus cannot be the same cells.[5]"
Mating,Biology,5,"In biology, mating is the pairing of either opposite-sex or hermaphroditic organisms, usually for the purposes of sexual reproduction. Some definitions limit the term to pairing between animals,[1] while other definitions extend the term to mating in plants and fungi. Fertilization is the fusion of two gametes.[2] Copulation is the union of the sex organs of two sexually reproducing animals for insemination and subsequent internal fertilization. Mating may also lead to external fertilization, as seen in amphibians, fishes and plants. For the majority of species, mating is between two individuals of opposite sexes. However, for some hermaphroditic species, copulation is not required because the parent organism is capable of self-fertilization (autogamy); for example, banana slugs.
 The term mating is also applied to related processes in bacteria, archaea and viruses.  Mating in these cases involves the pairing of individuals, accompanied by the pairing of their homologous chromosomes and then exchange of genomic information leading to formation of recombinant progeny (see mating systems).
"
Medulla_oblongata,Biology,5,"
 The medulla oblongata or simply medulla is a long stem-like structure which makes up the lower part of the brainstem.[1] It is anterior and partially inferior to the cerebellum. It is a cone-shaped neuronal mass responsible for autonomic (involuntary) functions, ranging from vomiting to sneezing.[2] The medulla contains the cardiac, respiratory, vomiting and vasomotor centers, and therefore deals with the autonomic functions of breathing, heart rate and blood pressure as well as the sleep wake cycle.[2] During embryonic development, the medulla oblongata develops from the myelencephalon. The myelencephalon is a secondary vesicle which forms during the maturation of the rhombencephalon, also referred to as the hindbrain.
 The bulb is an archaic term for the medulla oblongata.[1] In modern clinical usage, the word bulbar (as in bulbar palsy) is retained for terms that relate to the medulla oblongata, particularly in reference to medical conditions. The word bulbar can refer to the nerves and tracts connected to the medulla, and also by association to those muscles innervated, such as those of the tongue, pharynx and larynx.
"
Meiosis,Biology,5,"
 Meiosis (/maɪˈoʊsɪs/ (listen); from Greek μείωσις, meiosis, meaning ""lessening"") is a special type of cell division of germ cells in sexually-reproducing organisms used to produce the gametes, such as sperm or egg cells. It involves two rounds of division that ultimately result in four cells with only one copy of each paternal and maternal chromosome (haploid). Additionally, prior to the division, genetic material from the paternal and maternal copies of each chromosome is crossed over, creating new combinations of code on each chromosome.[1] Later on, during fertilisation, the haploid cells produced by meiosis from a male and female will fuse to create a cell with two copies of each chromosome again, the zygote.
 Errors in meiosis resulting in aneuploidy (an abnormal number of chromosomes) are the leading known cause of miscarriage and the most frequent genetic cause of developmental disabilities.[2] In meiosis, DNA replication is followed by two rounds of cell division to produce four daughter cells, each with half the number of chromosomes as the original parent cell.[1] The two meiotic divisions are known as meiosis I and meiosis II. Before meiosis begins, during S phase of the cell cycle, the DNA of each chromosome is replicated so that it consists of two identical sister chromatids, which remain held together through sister chromatid cohesion. This S-phase can be referred to as ""premeiotic S-phase"" or ""meiotic S-phase"".  Immediately following DNA replication, meiotic cells enter a prolonged G2-like stage known as meiotic prophase. During this time, homologous chromosomes pair with each other and undergo genetic recombination, a programmed process in which DNA may be cut and then repaired, which allows them to exchange some of their genetic information. A subset of recombination events results in crossovers, which create physical links known as chiasmata (singular: chiasma, for the Greek letter Chi (X)) between the homologous chromosomes. In most organisms, these links can help direct each pair of homologous chromosomes to segregate away from each other during Meiosis I, resulting in two haploid cells that have half the number of chromosomes as the parent cell.
 During meiosis II, the cohesion between sister chromatids is released and they segregate from one another, as during mitosis. In some cases, all four of the meiotic products form gametes such as sperm, spores or pollen. In female animals, three of the four meiotic products are typically eliminated by extrusion into polar bodies, and only one cell develops to produce an ovum. Because the number of chromosomes is halved during meiosis, gametes can fuse (i.e. fertilization) to form a diploid zygote that contains two copies of each chromosome, one from each parent. Thus, alternating cycles of meiosis and fertilization enable sexual reproduction, with successive generations maintaining the same number of chromosomes. For example, diploid human
cells contain 23 pairs of chromosomes including 1 pair of sex chromosomes (46 total), half of maternal origin and half of paternal origin. Meiosis produces haploid gametes (ova or sperm) that contain one set of 23 chromosomes. When two gametes (an egg and a sperm) fuse, the resulting zygote is once again diploid, with the mother and father each contributing 23 chromosomes. This same pattern, but not the same number of chromosomes, occurs in all organisms that utilize meiosis.
 Meiosis occurs in all sexually-reproducing single-celled and multicellular organisms (which are all eukaryotes), including animals, plants and fungi.[3][4][5] It is an essential process for oogenesis and spermatogenesis.
"
Membrane_potential,Biology,5,"Membrane potential (also transmembrane potential or membrane voltage) is the difference in electric potential between the interior and the exterior of a biological cell. For the exterior of the cell, typical values of membrane potential, normally given in units of millivolts and denoted as mV, range from –40 mV to –80 mV.
 All animal cells are surrounded by a membrane composed of a lipid bilayer with proteins embedded in it. The membrane serves as both an insulator and a diffusion barrier to the movement of ions. Transmembrane proteins, also known as ion transporter or ion pump proteins, actively push ions across the membrane and establish concentration gradients across the membrane, and ion channels allow ions to move across the membrane down those concentration gradients. Ion pumps and ion channels are electrically equivalent to a set of batteries and resistors inserted in the membrane, and therefore create a voltage between the two sides of the membrane.
 Almost all plasma membranes have an electrical potential across them, with the inside usually negative with respect to the outside.[1] The membrane potential has two basic functions. First, it allows a cell to function as a battery, providing power to operate a variety of ""molecular devices"" embedded in the membrane. Second, in electrically excitable cells such as neurons and muscle cells, it is used for transmitting signals between different parts of a cell. Signals are generated by opening or closing of ion channels at one point in the membrane, producing a local change in the membrane potential. This change in the electric field can be quickly affected by either adjacent or more distant ion channels in the membrane. Those ion channels can then open or close as a result of the potential change, reproducing the signal.
 In non-excitable cells, and in excitable cells in their baseline states, the membrane potential is held at a relatively stable value, called the resting potential. For neurons, typical values of the resting potential range from –70 to –80 millivolts; that is, the interior of a cell has a negative baseline voltage of a bit less than one-tenth of a volt. The opening and closing of ion channels can induce a departure from the resting potential. This is called a depolarization if the interior voltage becomes less negative (say from –70 mV to –60 mV), or a hyperpolarization if the interior voltage becomes more negative (say from –70 mV to –80 mV).  In excitable cells, a sufficiently large depolarization can evoke an action potential, in which the membrane potential changes rapidly and significantly for a short time (on the order of 1 to 100 milliseconds), often reversing its polarity.  Action potentials are generated by the activation of certain voltage-gated ion channels.
 In neurons, the factors that influence the membrane potential are diverse. They include numerous types of ion channels, some of which are chemically gated and some of which are voltage-gated. Because voltage-gated ion channels are controlled by the membrane potential, while the membrane potential itself is influenced by these same ion channels, feedback loops that allow for complex temporal dynamics arise, including oscillations and regenerative events such as action potentials.
"
Messenger_RNA,Biology,5,"In molecular biology, messenger RNA (mRNA) is a single-stranded molecule of RNA that corresponds to the genetic sequence of a gene, and is read by a ribosome in the process of synthesizing a protein. 
 During transcription, RNA polymerase makes a copy of a gene from the DNA to mRNA as needed. This process is slightly different in eukaryotes and prokaryotes, including that prokaryotic RNA polymerase associates with DNA-processing enzymes during transcription so that processing can proceed during transcription. Therefore, this causes the new mRNA strand to become double-stranded by producing a complementary strand known as the tRNA strand. In addition, the RNA is unable to form structures from base-pairing. Moreover, the template for mRNA is the complementary strand of tRNA, which is identical in sequence to the anticodon sequence that the DNA binds to. The short-lived, unprocessed or partially processed product is termed precursor mRNA, or pre-mRNA; once completely processed, it is termed mature mRNA. 
 mRNA is created during the process of transcription, where an enzyme (RNA polymerase) converts the gene into primary transcript mRNA (also known as pre-mRNA). This pre-mRNA usually still contains introns, regions that will not go on to code for the final amino acid sequence. These are removed in the process of RNA splicing, leaving only exons, regions that will encode the protein. This exon sequence constitutes mature mRNA. Mature mRNA is then read by the ribosome, and, utilising amino acids carried by transfer RNA (tRNA), the ribosome creates the protein. This process is known as translation. All of these processes form part of the central dogma of molecular biology, which describes the flow of genetic information in a biological system.
 As in DNA, genetic information in mRNA is contained in the sequence of nucleotides, which are arranged into codons consisting of three ribonucleotides each. Each codon codes for a specific amino acid, except the stop codons, which terminate protein synthesis. The translation of codons into amino acids requires two other types of RNA: transfer RNA, which recognizes the codon and provides the corresponding amino acid, and ribosomal RNA (rRNA), the central component of the ribosome's protein-manufacturing machinery.
 The existence of mRNA was first suggested by Jacques Monod and François Jacob, and was subsequently discovered by Jacob, Sydney Brenner and Matthew Meselson at the California Institute of Technology in 1961.[1]"
Metabolism,Biology,5,"
 Metabolism (/məˈtæbəlɪzəm/, from Greek: μεταβολή metabolē, ""change"") is the set of life-sustaining chemical reactions in organisms. The three main purposes of metabolism are: the conversion of food to energy to run cellular processes; the conversion of food/fuel to building blocks for proteins, lipids, nucleic acids, and some carbohydrates; and the elimination of metabolic wastes. These enzyme-catalyzed reactions allow organisms to grow and reproduce, maintain their structures, and respond to their environments. (The word metabolism can also refer to the sum of all chemical reactions that occur in living organisms, including digestion and the transport of substances into and between different cells, in which case the above described set of reactions within the cells is called intermediary metabolism or intermediate metabolism).
 Metabolic reactions may be categorized as catabolic – the breaking down of compounds (for example, the breaking down of glucose to pyruvate by cellular respiration); or anabolic – the building up (synthesis) of compounds (such as proteins, carbohydrates, lipids, and nucleic acids). Usually, catabolism releases energy, and anabolism consumes energy.
 The chemical reactions of metabolism are organized into metabolic pathways, in which one chemical is transformed through a series of steps into another chemical, each step being facilitated by a specific enzyme. Enzymes are crucial to metabolism because they allow organisms to drive desirable reactions that require energy that will not occur by themselves, by coupling them to spontaneous reactions that release energy. Enzymes act as catalysts – they allow a reaction to proceed more rapidly – and they also allow the regulation of the rate of a  metabolic reaction, for example in response to changes in the cell's environment or to signals from other cells.
 The metabolic system of a particular organism determines which substances it will find nutritious and which poisonous. For example, some prokaryotes use hydrogen sulfide as a nutrient, yet this gas is poisonous to animals.[1] The basal metabolic rate of an organism is the measure of the amount of energy consumed by all of these chemical reactions.
 A striking feature of metabolism is the similarity of the basic metabolic pathways among vastly different species.[2] For example, the set of carboxylic acids that are best known as the intermediates in the citric acid cycle are present in all known organisms, being found in species as diverse as the unicellular bacterium Escherichia coli and huge multicellular organisms like elephants.[3] These similarities in metabolic pathways are likely due to their early appearance in evolutionary history, and their retention because of their efficacy.[4][5] The metabolism of cancer cells is also different from the metabolism of normal cells and these differences can be used to find targets for therapeutic intervention in cancer.[6]"
Metamorphosis,Biology,5,"Metamorphosis is a biological process by which an animal physically develops after birth or hatching, involving a conspicuous and relatively abrupt change in the animal's body structure through cell growth and differentiation. Some insects, fish, amphibians, mollusks, crustaceans, cnidarians, echinoderms, and tunicates undergo metamorphosis, which is often accompanied by a change of nutrition source or behavior. Animals can be divided into species that undergo complete metamorphosis (""holometaboly""), incomplete metamorphosis (""hemimetaboly""), or no metamorphosis (""ametaboly"").
 Scientific usage of the term is technically precise, and it is not applied to general aspects of cell growth, including rapid growth spurts. References to ""metamorphosis"" in mammals are imprecise and only colloquial, but historically idealist ideas of transformation and morphology (biology), as in Goethe's Metamorphosis of Plants, have influenced the development of ideas of evolution.
"
Metaphase,Biology,5,"Metaphase (from the Greek μετά, ""adjacent"" and φάσις, ""stage"") is a stage of mitosis in the eukaryotic cell cycle in which chromosomes are at their second-most condensed and coiled stage (they are at their most condensed in anaphase).[1] These chromosomes, carrying genetic information, align in the equator of the cell before being separated into each of the two daughter cells. Metaphase accounts for approximately 4% of the cell cycle's duration.[citation needed]
Preceded by events in prometaphase and followed by anaphase, microtubules formed in prophase have already found and attached themselves to kinetochores in metaphase.
 In metaphase, the centromeres of the chromosomes convene themselves on the metaphase plate (or equatorial plate),[2] an imaginary line that is equidistant from the two centrosome poles. This even alignment is due to the counterbalance of the pulling powers generated by the opposing kinetochore microtubules,[3] analogous to a tug-of-war between two people of equal strength, ending with the destruction of B cyclin.[4] In certain types of cells, chromosomes do not line up at the metaphase plate and instead move back and forth between the poles randomly, only roughly lining up along the middleline.[citation needed] Early events of metaphase can coincide with the later events of prometaphase, as chromosomes with connected kinetochores will start the events of metaphase individually before other chromosomes with unconnected kinetochores that are still lingering in the events of prometaphase.[citation needed] One of the cell cycle checkpoints occurs during prometaphase and metaphase. Only after all chromosomes have become aligned at the metaphase plate, when every kinetochore is properly attached to a bundle of microtubules, does the cell enter anaphase. It is thought that unattached or improperly attached kinetochores generate a signal to prevent premature progression to anaphase, even if most of  kinetochores have been attached and most of the chromosomes have been aligned. Such a signal creates the mitotic spindle checkpoint. This would be accomplished by regulation of the anaphase-promoting complex, securin, and separase.
"
Microbiology,Biology,5,"Microbiology (from Greek μῑκρος, mīkros, ""small""; βίος, bios, ""life""; and -λογία, -logia) is the study of microorganisms, those being unicellular (single cell), multicellular (cell colony), or acellular (lacking cells).[1][2] Microbiology encompasses numerous sub-disciplines including virology, bacteriology, protistology, mycology, immunology and parasitology.
 Eukaryotic microorganisms possess membrane-bound organelles and include fungi and protists, whereas prokaryotic organisms—all of which are microorganisms—are conventionally classified as lacking membrane-bound organelles and include Bacteria and Archaea.[3][4] Microbiologists traditionally relied on culture, staining, and microscopy. However, less than 1% of the microorganisms present in common environments can be cultured in isolation using current means.[5] Microbiologists often rely on molecular biology tools such as DNA sequence based identification, for example the 16S rRNA gene sequence used for bacteria identification.
 Viruses have been variably classified as organisms,[6] as they have been considered either as very simple microorganisms or very complex molecules. Prions, never considered as microorganisms, have been investigated by virologists, however, as the clinical effects traced to them were originally presumed due to chronic viral infections, and virologists took search—discovering ""infectious proteins"".
 The existence of microorganisms was predicted many centuries before they were first observed, for example by the Jains in India and by Marcus Terentius Varro in ancient Rome. The first recorded microscope observation was of the fruiting bodies of moulds, by Robert Hooke in 1666, but the Jesuit priest Athanasius Kircher was likely the first to see microbes, which he mentioned observing in milk and putrid material in 1658. Antonie van Leeuwenhoek is considered a father of microbiology as he observed and experimented with microscopic organisms in the 1670s, using simple microscopes of his own design. Scientific microbiology developed in the 19th century through the work of Louis Pasteur and in medical microbiology Robert Koch.
"
Microevolution,Biology,5,"
 Microevolution is the change in allele frequencies that occurs over time within a population.[1] This change is due to four different processes: mutation, selection (natural and artificial), gene flow and genetic drift. This change happens over a relatively short (in evolutionary terms) amount of time compared to the changes termed macroevolution.
 Population genetics is the branch of biology that provides the mathematical structure for the study of the process of microevolution. Ecological genetics concerns itself with observing microevolution in the wild. Typically, observable instances of evolution are examples of microevolution; for example, bacterial strains that have antibiotic resistance.
 Microevolution may lead to speciation, which provides the raw material for macroevolution.[2][3]"
Mitochondria,Biology,5,"The mitochondrion (/ˌmaɪtəˈkɒndrɪən/,[1] plural mitochondria) is a double-membrane-bound organelle found in most eukaryotic organisms. Some cells in some multicellular organisms lack mitochondria (for example, mature mammalian red blood cells). A number of unicellular organisms, such as microsporidia, parabasalids, and diplomonads, have reduced or transformed their mitochondria into other structures.[2] To date, only one eukaryote, Monocercomonoides, is known to have completely lost its mitochondria,[3] and one multicellular organism, Henneguya salminicola, is known to have retained mitochondrion-related organelles in association with a complete loss of their mitochondrial genome.[3][4][5] Mitochondria generate most of the cell's supply of adenosine triphosphate (ATP), used as a source of chemical energy.[6] A mitochondrion is thus termed the powerhouse of the cell.[7] Mitochondria are commonly between 0.75 and 3 μm² in area[8] but vary considerably in size and structure. Unless specifically stained, they are not visible. In addition to supplying cellular energy, mitochondria are involved in other tasks, such as signaling, cellular differentiation, and cell death, as well as maintaining control of the cell cycle and cell growth.[9] Mitochondrial biogenesis is in turn temporally coordinated with these cellular processes.[10][11] Mitochondria have been implicated in several human diseases and conditions, such as mitochondrial disorders,[12] cardiac dysfunction,[13] heart failure[14] and autism.[15] The number of mitochondria in a cell can vary widely by organism, tissue, and cell type. Red blood cells have no mitochondria, whereas liver cells can have more than 2000.[16][17]  The organelle is composed of compartments that carry out specialized functions. These compartments or regions include the outer membrane, intermembrane space, inner membrane, cristae and matrix.
 Although most of a cell's DNA is contained in the cell nucleus, the mitochondrion has its own genome (""mitogenome"") that is substantially similar to bacterial genomes.[18]  Mitochondrial proteins (proteins transcribed from mitochondrial DNA) vary depending on the tissue and the species. In humans, 615 distinct types of proteins have been identified from cardiac mitochondria,[19] whereas in rats, 940 proteins have been reported.[20] The mitochondrial proteome is thought to be dynamically regulated.[21]"
Mitosis,Biology,5,"
 In cell biology, mitosis (/maɪˈtoʊsɪs/) is a part of the cell cycle, in which, replicated chromosomes are separated into two new nuclei. Cell division gives rise to genetically identical cells in which the total number of chromosomes is maintained.[1] In general, mitosis (division of the nucleus) is preceded by the S stage of interphase (during which the DNA is replicated) and is often followed by telophase and cytokinesis; which divides the cytoplasm, organelles and cell membrane of one cell into two new cells containing roughly equal shares of these cellular components.[2] The different stages of Mitosis all together define the mitotic (M) phase of an animal cell cycle—the division of the mother cell into two daughter cells genetically identical to each other[3].
 The process of mitosis is divided into stages corresponding to the completion of one set of activities and the start of the next. These stages are prophase, prometaphase, metaphase, anaphase, and telophase. During mitosis, the chromosomes, which have already duplicated, condense and attach to spindle fibers that pull one copy of each chromosome to opposite sides of the cell.[4] The result is two genetically identical daughter nuclei. The rest of the cell may then continue to divide by cytokinesis to produce two daughter cells.[5] The different phases of mitosis can be visualized in real time, using live cell imaging.[6] Producing three or more daughter cells instead of the normal two is a mitotic error called tripolar mitosis or multipolar mitosis (direct cell triplication / multiplication).[7] Other errors during mitosis can induce apoptosis (programmed cell death) or cause mutations. Certain types of cancer can arise from such mutations.[8] Mitosis occurs only in eukaryotic cells. Prokaryotic cells, which lack a nucleus, divide by a different process called binary fission[citation needed]. Mitosis varies between organisms.[9] For example, animal cells undergo an ""open"" mitosis, where the nuclear envelope breaks down before the chromosomes separate, whereas fungi undergo a ""closed"" mitosis, where chromosomes divide within an intact cell nucleus.[10] Most animal cells undergo a shape change, known as mitotic cell rounding, to adopt a near spherical morphology at the start of mitosis. Most human cells are produced by mitotic cell division. Important exceptions include the gametes – sperm and egg cells – which are produced by meiosis.
"
Molecule,Biology,5,"
 A molecule is an electrically neutral group of two or more atoms held together by chemical bonds.[4][5][6][7][8] Molecules are distinguished from ions by their lack of electrical charge. 
 In quantum physics, organic chemistry, and biochemistry, the distinction from ions is dropped and molecule is often used when referring to polyatomic ions.
 In the kinetic theory of gases, the term molecule is often used for any gaseous particle regardless of its composition. This violates the definition that a molecule contain two or more atoms, since the noble gases are individual atoms.[9] A molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with two atoms in the oxygen molecule (O2); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (two hydrogen atoms and one oxygen atom; H2O). 
 Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are typically not considered single molecules.[10] Molecules as components of matter are common.  They also make up most of the oceans and atmosphere. Most organic substances are molecules.  The substances of life are molecules, e.g. proteins, the amino acids they are made of, the nucleic acids (DNA & RNA), sugars, carbohydrates, fats, and vitamins. The nutrient minerals ordinarily are not molecules, e.g. iron sulfate.
 However, the majority of familiar solid substances on Earth are not made of molecules. These include all of the minerals that make up the substance of the Earth, soil, dirt, sand, clay, pebbles, rocks, boulders, bedrock, the molten interior, and the core of the Earth. All of these contain many chemical bonds, but are not made of identifiable molecules. 
 No typical molecule can be defined for salts nor for covalent crystals, although these are often composed of repeating unit cells that extend either in a plane, e.g. graphene; or three-dimensionally e.g. diamond, quartz, sodium chloride. The theme of repeated unit-cellular-structure also holds for most metals which are condensed phases with metallic bonding. Thus solid metals are not made of molecules. 
 In glasses, which are solids that exist in a vitreous disordered state, the atoms are held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating unit-cellular-structure that characterizes salts, covalent crystals, and metals.
"
Molecular_biology,Biology,5,"Molecular biology /məˈlɛkjʊlər/ is the branch of biology that concerns the molecular basis of biological activity in and between cells, including molecular synthesis, modification, mechanisms and interactions.[1][2] The central dogma of molecular biology describes the process in which DNA is transcribed into RNA, then translated into protein. [2][3] William Astbury described molecular biology in 1961 in Nature, as:
 ...not so much a technique as an approach, an approach from the viewpoint of the so-called basic sciences with the leading idea of searching below the large-scale manifestations of classical biology for the corresponding molecular plan. It is concerned particularly with the forms of biological molecules and [...] is predominantly three-dimensional and structural – which does not mean, however, that it is merely a refinement of morphology. It must at the same time inquire into genesis and function.[4] Some clinical research and medical therapies arising from molecular biology are covered under gene therapy whereas the use of molecular biology or molecular cell biology in medicine is now referred to as molecular medicine. Molecular biology also plays important role in understanding formations, actions, and regulations of various parts of cells which can be used to efficiently target new drugs, diagnose disease, and understand the physiology of the cell. [5]"
Molecular_switch,Biology,5,"
 A molecular switch is a molecule that can be reversibly shifted between two or more stable states.[1][page needed] The molecules may be shifted between the states in response to environmental stimuli, such as changes in pH, light, temperature, an electric current, microenvironment, or in the presence of ions[2] and other ligands. In some cases, a combination of stimuli is required. The oldest forms of synthetic molecular switches are pH indicators, which display distinct colors as a function of pH. Currently synthetic molecular switches are of interest in the field of nanotechnology for application in molecular computers or responsive drug delivery systems.[3] Molecular switches are also important in biology because many biological functions are based on it, for instance allosteric regulation and vision. They are also one of the simplest examples of molecular machines.
"
Monomer,Biology,5,"A monomer (/ˈmɒnəmər/ MON-ə-mər; mono-, ""one"" + -mer, ""part"") is a molecule that can react together with other monomer molecules to form a larger polymer chain or three-dimensional network in a process called polymerization.[1][2][3]"
Morphology_(biology),Biology,5,"Morphology is a branch of biology dealing with the study of the form and structure of organisms and their specific structural features.[1] This includes aspects of the outward appearance (shape, structure, colour, pattern, size), i.e. external morphology (or eidonomy), as well as the form and structure of the internal parts like bones and organs, i.e. internal morphology (or anatomy). This is in contrast to physiology, which deals primarily with function. Morphology is a branch of life science dealing with the study of gross structure of an organism or taxon and its component parts.
"
Motility,Biology,5,"Motility is the ability of an organism to move independently, using metabolic energy.
"
Motor_neuron,Biology,5,"A motor neuron (or motoneuron) is a neuron whose cell body is located in the motor cortex, brainstem or the spinal cord, and whose axon (fiber) projects to the spinal cord or outside of the spinal cord to directly or indirectly control effector organs, mainly muscles and glands.[1]  There are two types of motor neuron – upper motor neurons and lower motor neurons. Axons from upper motor neurons synapse onto interneurons in the spinal cord and occasionally directly onto lower motor neurons.[2] The axons from the lower motor neurons are efferent nerve fibers that carry signals from the spinal cord to the effectors.[3] Types of lower motor neurons are alpha motor neurons, beta motor neurons, and gamma motor neurons.
 A single motor neuron may innervate many muscle fibres and a muscle fibre can undergo many action potentials in the time taken for a single muscle twitch. Innervation takes place at a neuromuscular junction and twitches can become superimposed as a result of summation or a tetanic contraction. Individual twitches can become indistinguishable, and tension rises smoothly eventually reaching a plateau.[4]"
Mucous_membrane,Biology,5,"A mucous membrane or mucosa is a membrane that lines various cavities in the body and covers the surface of internal organs. It consists of one or more layers of epithelial cells overlying a layer of loose connective tissue. It is mostly of endodermal origin and is continuous with the skin at various body openings such as the eyes, ears, inside the nose, inside the mouth, lip, vagina, the urethral opening and the anus. Some mucous membranes secrete mucus, a thick protective fluid. The function of the membrane is to stop pathogens and dirt from entering the body and to prevent bodily tissues from becoming dehydrated.
"
Multicellular_organism,Biology,5,"Multicellular organisms are organisms that consist of more than one cell, in contrast to unicellular organisms.[1] All species of animals, land plants and most fungi are multicellular, as are many algae, whereas a few organisms are partially uni- and partially multicellular, like slime molds and social amoebae such as the genus Dictyostelium.[2][3] Multicellular organisms arise in various ways, for example by cell division or by aggregation of many single cells.[4][3] Colonial organisms are the result of many identical individuals joining together to form a colony.  However, it can often be hard to separate colonial protists from true multicellular organisms, because the two concepts are not distinct; colonial protists have been dubbed ""pluricellular"" rather than ""multicellular"".[5][6]"
Mycology,Biology,5,"Mycology is the branch of biology concerned with the study of fungi, including their genetic and biochemical properties, their taxonomy and their use to humans  as a source for tinder, traditional medicine, food, and entheogens, as well as their dangers, such as toxicity or infection. 
 A biologist specializing in mycology is called a mycologist.
 Mycology branches into the field of phytopathology, the study of plant diseases, and the two disciplines remain closely related because the vast majority of plant pathogens are fungi.
"
Myofibril,Biology,5,"A myofibril (also known as a muscle fibril) is a basic rod-like unit of a muscle cell.[1]  Muscles are composed of tubular cells called myocytes, known as muscle fibres in striated muscle, and these cells in turn contain many chains of myofibrils.  They are created during embryonic development in a process known as myogenesis.
 Myofibrils are composed of long proteins including actin, myosin, and titin, and other proteins that hold them together. These proteins are organized into thick and thin filaments called myofilaments, which repeat along the length of the myofibril in sections called sarcomeres. Muscles contract by sliding the thick (myosin) and thin (actin) filaments along each other.
"
Myosin,Biology,5,"Myosins (/ˈmaɪəsɪn, -oʊ-/[1][2]) are a superfamily of motor proteins best known for their roles in muscle contraction and in a wide range of other motility processes in eukaryotes. They are ATP-dependent and responsible for actin-based motility. The term was originally used to describe a group of similar ATPases found in the cells of both striated muscle tissue and smooth muscle tissue.[3]  Following the discovery by Pollard and Korn (1973) of enzymes with myosin-like function in Acanthamoeba castellanii, a global range of divergent myosin genes have been discovered throughout the realm of eukaryotes.
 Although myosin was originally thought to be restricted to muscle cells (hence myo-(s) + -in), there is no single ""myosin""; rather it is a very large superfamily of genes whose protein products share the basic properties of actin binding, ATP hydrolysis (ATPase enzyme activity), and force transduction. Virtually all eukaryotic cells contain myosin isoforms.  Some isoforms have specialized functions in certain cell types (such as muscle), while other isoforms are ubiquitous.  The structure and function of myosin is globally conserved across species, to the extent that rabbit muscle myosin II will bind to actin from an amoeba.[4]"
Natural_selection,Biology,5,"
 Natural selection is the differential survival and reproduction of individuals due to differences in phenotype. It is a key mechanism of evolution, the change in the heritable traits characteristic of a population over generations. Charles Darwin popularised the term ""natural selection"", contrasting it with artificial selection, which in his view is intentional, whereas natural selection is not.
 Variation exists within all populations of organisms. This occurs partly because random mutations arise in the genome of an individual organism, and their offspring can inherit such mutations. Throughout the lives of the individuals, their genomes interact with their environments to cause variations in traits. The environment of a genome includes the molecular biology in the cell, other cells, other individuals, populations, species, as well as the abiotic environment. Because individuals with certain variants of the trait tend to survive and reproduce more than individuals with other less successful variants, the population evolves. Other factors affecting reproductive success include sexual selection (now often included in natural selection) and fecundity selection.
 Natural selection acts on the phenotype, the characteristics of the organism which actually interact with the environment, but the genetic
(heritable) basis of any phenotype that gives that phenotype a reproductive advantage may become more common in a population. Over time, this process can result in populations that specialise for particular ecological niches (microevolution) and may eventually result in speciation  (the emergence of new species, macroevolution). In other words, natural selection is a key process in the evolution of a population.
 Natural selection is a cornerstone of modern biology. The concept, published by Darwin and Alfred Russel Wallace in a joint presentation of papers in 1858, was elaborated in Darwin's influential 1859 book On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life. He described natural selection as analogous to artificial selection, a process by which animals and plants with traits considered desirable by human breeders are systematically favoured for reproduction. The concept of natural selection originally developed in the absence of a valid theory of heredity; at the time of Darwin's writing, science had yet to develop modern theories of genetics. The union of traditional Darwinian evolution with subsequent discoveries in classical genetics formed the modern synthesis of the mid-20th century. The addition of molecular genetics has led to evolutionary developmental biology, which explains evolution at the molecular level. While genotypes can slowly change by random genetic drift, natural selection remains the primary explanation for adaptive evolution.
"
Neurobiology,Biology,5,"Neuroscience (or neurobiology) is the scientific study of the nervous system.[1] It is a multidisciplinary science that combines physiology, anatomy, molecular biology, developmental biology, cytology, mathematical modeling, and psychology to understand the fundamental and emergent properties of neurons and neural circuits.[2][3][4][5][6] The understanding of the biological basis of learning, memory, behavior, perception, and consciousness has been described by Eric Kandel as the ""ultimate challenge"" of the biological sciences.[7] The scope of neuroscience has broadened over time to include different approaches used to study the nervous system at different scales and the techniques used by neuroscientists have expanded enormously, from molecular and cellular studies of individual neurons to imaging of sensory, motor and cognitive tasks in the brain.
"
Neuron,Biology,5,"A neuron or nerve cell is an electrically excitable cell[1] that communicates with other cells via specialized connections called synapses. It is the main component of nervous tissue in all animals except sponges and placozoa. Plants and fungi do not have nerve cells. The spelling neurone has become uncommon.[2] Neurons are typically classified into three types based on their function. Sensory neurons respond to stimuli such as touch, sound, or light that affect the cells of the sensory organs, and they send signals to the spinal cord or brain. Motor neurons receive signals from the brain and spinal cord to control everything from muscle contractions to glandular output. Interneurons connect neurons to other neurons within the same region of the brain or spinal cord. A group of connected neurons is called a neural circuit.
 A typical neuron consists of a cell body (soma), dendrites, and a single axon. The soma is usually compact. The axon and dendrites are filaments that extrude from it. Dendrites typically branch profusely and extend a few hundred micrometers from the soma. The axon leaves the soma at a swelling called the axon hillock, and travels for as far as 1 meter in humans or more in other species. It branches but usually maintains a constant diameter. At the farthest tip of the axon's branches are axon terminals, where the neuron can transmit a signal across the synapse to another cell. Neurons may lack dendrites or have no axon. The term neurite is used to describe either a dendrite or an axon, particularly when the cell is undifferentiated.
 Most neurons receive signals via the dendrites and soma and send out signals down the axon. At the majority of synapses, signals cross from the axon of one neuron to a dendrite of another. However, synapses can connect an axon to another axon or a dendrite to another dendrite.
 The signaling process is partly electrical and partly chemical. Neurons are electrically excitable, due to maintenance of voltage gradients across their membranes. If the voltage changes by a large enough amount over a short interval, the neuron generates an all-or-nothing electrochemical pulse called an action potential. This potential travels rapidly along the axon, and activates synaptic connections as it reaches them. Synaptic signals may be excitatory or inhibitory, increasing or reducing the net voltage that reaches the soma.
 In most cases, neurons are generated by neural stem cells during brain development and childhood. Neurogenesis largely ceases during adulthood in most areas of the brain. However, strong evidence supports generation of substantial numbers of new neurons in the hippocampus and olfactory bulb.[3][4]"
Neurotransmitter,Biology,5,"
 Neurotransmitters are chemical messengers that transmit a message from a nerve cell across the synapse to a target cell. The target can be another nerve cell, or a muscle cell, or a gland cell. They are chemicals made by the nerve cell specifically to transmit the message.[1] Neurotransmitters are released from synaptic vesicles in synapses into the synaptic cleft, where they are received by neurotransmitter receptors on the target cell. Many neurotransmitters are synthesized from simple and plentiful precursors such as amino acids, which are readily available and only require a small number of biosynthetic steps for conversion. Neurotransmitters are essential to the function of complex neural systems. The exact number of unique neurotransmitters in humans is unknown, but more than 200 have been identified.[2][3][4]"
Ecological_niche,Biology,5,"In ecology, a niche is the match of a species to a specific environmental condition.[1][2] It describes how an organism or population responds to the distribution of resources and competitors (for example, by growing when resources are abundant, and when predators, parasites and pathogens are scarce) and how it in turn alters those same factors (for example, limiting access to resources by other organisms, acting as a food source for predators and a consumer of prey). ""The type and number of variables comprising the dimensions of an environmental niche vary from one species to another [and] the relative importance of particular environmental variables for a species may vary according to the geographic and biotic contexts"".[3] A Grinnellian niche is determined by the habitat in which a species lives and its accompanying behavioral adaptations. An Eltonian niche emphasizes that a species not only grows in and responds to an environment, it may also change the environment and its behavior as it grows. The Hutchinsonian niche uses mathematics and statistics to try to explain how species coexist within a given community.
 The concept of ecological niche is central to ecological biogeography, which focuses on spatial patterns of ecological communities.[4]  ""Species distributions and their dynamics over time result from properties of the species, environmental variation..., and interactions between the two—in particular the abilities of some species, especially our own, to modify their environments and alter the range dynamics of many other species.""[5] Alteration of an ecological niche by its inhabitants is the topic of niche construction.[6] The majority of species exist in a standard ecological niche, sharing behaviors, adaptations, and functional traits similar to the other closely related species within the same broad taxonomic class, but there are exceptions. A premier example of a non-standard niche filling species is the flightless, ground-dwelling kiwi bird of New Zealand, which feeds on worms and other ground creatures, and lives its life in a mammal-like niche. Island biogeography can help explain island species and associated unfilled niches.
"
Nucleic_acid,Biology,5,"Nucleic acids are the biopolymers, or large biomolecules, essential to all known forms of life. The term nucleic acid is the overall name for DNA and RNA. They are composed of nucleotides, which are the monomers made of three components: a 5-carbon sugar, a phosphate group and a nitrogenous base. If the sugar is a compound  ribose, the polymer is RNA (ribonucleic acid); if the sugar is derived from ribose as deoxyribose, the polymer is DNA (deoxyribonucleic acid).
 Nucleic acids are the most important of all biomolecules. These are found in abundance in all living things, where they function to create and encode and then store information of every living cell of every life-form organism on Earth. In turn, they function to transmit and express that information inside and outside the cell nucleus—to the interior operations of the cell and ultimately to the next generation of each living organism. The encoded information is contained and conveyed via the nucleic acid sequence, which provides the 'ladder-step' ordering of nucleotides within the molecules of RNA and DNA.
 Strings of nucleotides are bonded to form helical backbones—typically, one for RNA, two for DNA—and assembled into chains of base-pairs selected from the five primary, or canonical, nucleobases, which are: adenine, cytosine, guanine, thymine, and uracil. Thymine occurs only in DNA and uracil only in RNA. Using amino acids and the process known as protein synthesis,[1] the specific sequencing in DNA of these nucleobase-pairs enables storing and transmitting coded instructions as genes. In RNA, base-pair sequencing provides for manufacturing new proteins that determine the frames and parts and most chemical processes of all life forms.
"
Nucleic_acid_sequence,Biology,5,"A nucleic acid sequence is a succession of bases signified by a series of a set of five different letters that indicate the order of nucleotides forming alleles within a DNA (using GACT) or RNA (GACU) molecule. By convention, sequences are usually presented  from the 5' end to the 3' end. For DNA, the sense strand is used. Because nucleic acids are normally linear (unbranched) polymers, specifying the sequence is equivalent to defining the covalent structure of the entire molecule. For this reason, the nucleic acid sequence is also termed the primary structure.
 The sequence has capacity to represent information. Biological deoxyribonucleic acid represents the information which directs the functions of a living thing.
 Nucleic acids also have a secondary structure and tertiary structure. Primary structure is sometimes mistakenly referred to as primary sequence. Conversely, there is no parallel concept of secondary or tertiary sequence.
"
Nucleobase,Biology,5,"Nucleobases, also known as nitrogenous bases or often simply bases, are nitrogen-containing biological compounds that form nucleosides, which, in turn, are components of nucleotides, with all of these monomers constituting the basic building blocks of nucleic acids. The ability of nucleobases to form base pairs and to stack one upon another leads directly to long-chain helical structures such as ribonucleic acid (RNA) and deoxyribonucleic acid (DNA).
 Five nucleobases—adenine (A), cytosine (C), guanine (G), thymine (T), and uracil (U)—are called primary or canonical. They function as the fundamental units of the genetic code, with the bases A, G, C, and T being found in DNA while A, G, C, and U are found in RNA. Thymine and uracil are identical except that T includes a methyl group that U lacks.
 Adenine and guanine have a fused-ring skeletal structure derived of purine, hence they are called purine bases.  The purine nitrogenous bases are characterized by their single amino group (NH2), at the C6 carbon in adenine and C2 in guanine.[1] Similarly, the simple-ring structure of cytosine, uracil, and thymine is derived of pyrimidine, so those three bases are called the pyrimidine bases. Each of the base pairs in a typical double-helix DNA comprises a purine and a pyrimidine: either an A paired with a T or a C paired with a G. These purine-pyrimidine pairs, which are called base complements, connect the two strands of the helix and are often compared to the rungs of a ladder. The pairing of purines and pyrimidines may result, in part, from dimensional constraints, as this combination enables a geometry of constant width for the DNA spiral helix. The A-T and C-G pairings function to form double or triple hydrogen bonds between the amine and carbonyl groups on the complementary bases.
 Nucleobases such as adenine, guanine, xanthine, hypoxanthine, purine, 2,6-diaminopurine, and 6,8-diaminopurine may have formed in outer space as well as on earth.[2][3][4] The origin of the term base reflects these compounds' chemical properties in acid-base reactions, but those properties are not especially important for understanding most of the biological functions of nucleobases.
"
Nucleoid,Biology,5,"The nucleoid (meaning nucleus-like) is an irregularly shaped region within the prokaryotic cell that contains all or most of the genetic material.[1][2][3] The chromosome of a prokaryote is circular, and its length is very large compared to the cell dimensions needing it to be compacted in order to fit. In contrast to the nucleus of a eukaryotic cell, it is not surrounded by a nuclear membrane. Instead, the nucleoid forms by condensation and functional arrangement with the help of chromosomal architectural proteins and RNA molecules as well as DNA supercoiling. The length of a genome widely varies (generally at least a few million base pairs) and a cell may contain multiple copies of it.
 There is not yet a high-resolution structure known of a bacterial nucleoid, however key features have been researched in Escherichia coli as a model organism. In E. coli, the chromosomal DNA is on average negatively supercoiled and folded into plectonemic loops, which are confined to different physical regions, and rarely diffuse into each other. These loops spatially organize into megabase-sized regions called macrodomains, within which DNA sites frequently interact, but between which interactions are rare. The condensed and spatially organized DNA forms a helical ellipsoid that is radially confined in the cell. The 3D structure of the DNA in the nuceoid appears to vary depending on conditions and is linked to gene expression so that the nucleoid architecture and gene transcription are tightly interdependent, influencing each other reciprocally.
"
Nucleolus,Biology,5,"
 The nucleolus (/nuː-, njuːˈkliːələs, -kliˈoʊləs/, plural: nucleoli /-laɪ/) is the largest structure in the nucleus of eukaryotic cells.[1] It is best known as the site of ribosome biogenesis. Nucleoli also participate in the formation of signal recognition particles and play a role in the cell's response to stress.[2] Nucleoli are made of proteins, DNA and RNA and form around specific chromosomal regions called nucleolar organizing regions.  Malfunction of nucleoli can be the cause of several human conditions called ""nucleolopathies""[3] and the nucleolus is being investigated as a target for cancer chemotherapy.[4][5]"
Nucleotide,Biology,5,"Nucleotides are organic molecules consisting of a nucleoside and a phosphate. They serve as monomeric units of the nucleic acid polymers deoxyribonucleic acid (DNA) and ribonucleic acid (RNA), both of which are essential biomolecules within all life-forms on Earth. 
 Nucleotides are composed of three subunit molecules: a nitrogenous base (also known as nucleobase), a five-carbon sugar (ribose or deoxyribose), and a phosphate group consisting of one to three phosphates.  The four nitrogenous bases in DNA are guanine, adenine, cytosine and thymine; in RNA, uracil is used in place of thymine.
 Nucleotides also play a central role in metabolism at a fundamental, cellular level. They provide chemical energy—in the form of the nucleoside triphosphates, adenosine triphosphate (ATP), guanosine triphosphate (GTP), cytidine triphosphate (CTP) and uridine triphosphate (UTP)—throughout the cell for the many cellular functions that demand energy, including: amino acid, protein and cell membrane synthesis, moving the cell and cell parts (both internally and intercellularly), cell division, etc.[1] In addition, nucleotides participate in cell signaling (cyclic guanosine monophosphate or cGMP and cyclic adenosine monophosphate or cAMP), and are incorporated into important cofactors of enzymatic reactions (e.g. coenzyme A, FAD, FMN, NAD, and NADP+).
 In experimental biochemistry, nucleotides can be radiolabeled using radionuclides to yield radionucleotides.
"
Offspring,Biology,5,"In biology, offspring are the young born of living organisms, produced either by a single organism or, in the case of sexual reproduction, two organisms. Collective offspring may be known as a brood or progeny in a more general way. This can refer to a set of simultaneous offspring, such as the chicks hatched from one clutch of eggs, or to all the offspring, as with the honeybee.
 Human offspring (descendants) are referred to as children (without reference to age, thus one can refer to a parent's ""minor children"" or ""adult children"" or ""infant children"" or ""teenage children"" depending on their age); male children are sons and female children are daughters (see kinship and descent). Offspring can occur after mating or after artificial insemination.
 Offspring contains many parts and properties that are precise and accurate in what they consist of, and what they define. As the offspring of a new species, also known as a child or f1 generation, consist of genes of the father and the mother, which is also known as the parent generation.[1] Each of these offspring contains numerous genes which have coding for specific tasks and properties. Males and females both contribute equally to the genotypes of their offspring, in which gametes fuse and form. An important aspect of the formation of the parent offspring is the chromosome, which is a structure of DNA which contains many genes.[2] To focus more on the offspring and how it results in the formation of the f1 generation, is an inheritance called sex-linkage,[3] which is a gene which is located on the sex chromosome and patterns of these inheritance differ in both male and female. The explanation that proves the theory of the offspring having genes from both parent generations, is proven through a process called crossing-over, which consists of taking genes from the male chromosomes and genes from the female chromosome, resulting in a process of meiosis occurring, and leading to the splitting of the chromosomes evenly.[4] Depending on which genes are dominantly expressed in the gene will result in the sex of the offspring. The female will always give an X chromosome, whereas the male, depending on the situation, will either give an X chromosome or a Y chromosome. If a male offspring is produced, the gene will consist of an X and a Y chromosome. If two X chromosomes are expressed and produced, it produces a female offspring.[5] Cloning is the production of an offspring which represents the identical genes as its parent. Reproductive cloning begins with the removal of the nucleus from an egg, which holds the genetic material.[6] In order to clone an organ, a stem cell is to be produced and then utilized to clone that specific organ.[7] A common misconception of cloning is that it produces an exact copy of the parent being cloned. Cloning copies the DNA/genes of the parent and then creates a genetic duplicate. The clone will not be a similar copy as he or she will grow up in different surroundings from the parent and may encounter different opportunities and experiences. Although mostly positive, cloning also faces some setbacks in terms of ethics and human health. Though cell division and DNA replication is a vital part of survival, there are many steps involved and mutations can occur with permanent change in an organism's and their offspring's DNA.[8] Some mutations can be good as they result in random evolution periods in which may be good for the species, but most mutations are bad as they can change the genotypes of offspring, which can result in changes that harm the species.
"
Order_(biology),Biology,5,"In biological classification, the order (Latin: ordo) is
 What does and does not belong to each order is determined by a taxonomist, as is whether a particular order should be recognized at all. Often there is no exact agreement, with different taxonomists each taking a different position. There are no hard rules that a taxonomist needs to follow in describing or recognizing an order. Some taxa are accepted almost universally, while others are recognised only rarely.[1] For some groups of organisms, consistent suffixes are used to denote that the rank is an order. The Latin suffix -(i)formes meaning ""having the form of"" is used for the scientific name of orders of birds and fishes, but not for those of mammals and invertebrates. The suffix -ales is for the name of orders of plants, fungi, and algae.[2] The name of an order is usually written with a capital letter.[3]"
Organ_(anatomy),Biology,5,"An organ is a group of tissues with similar functions. Plant life and animal life rely on many organs that coexist in organ systems.[1] A given organ's tissues can be broadly categorized as parenchyma, the tissue peculiar to (or at least archetypal of) the organ and that does the organ's specialized job, and stroma, the tissues with supportive, structural, connective, or ancillary functions. For example, in a gland, the tissue that makes the hormones is the parenchyma, whereas the stroma includes the nerves that innervate the parenchyma, the blood vessels that oxygenate and nourish it and carry away its metabolic wastes, and the connective tissues that provide a suitable place for it to be situated and anchored. The main tissues that make up an organ tend to have common embryologic origins, such as arising from the same germ layer. Functionally related organs often cooperate to form whole organ systems. Organs exist in most multicellular organisms. In single-celled organisms such as bacteria, the functional analogue of an organ is known as an organelle. In plants, there are three main organs.[2] A hollow organ is an internal organ that forms a hollow tube, or pouch such as the stomach, intestine, or bladder.
 In the study of anatomy, the term viscus refers to an internal organ. Viscera is the plural form.[3][4] The number of organs in any organism depends on which precise definition of the term one uses. By one widely used definition, 79 organs have been identified in the human body.[5]"
Organism,Biology,5,"
 In biology, an organism (from Greek: ὀργανισμός, organismos) is any individual entity that embodies the properties of life. It is a synonym for ""life form"".
 Organisms are classified by taxonomy into groups such as multicellular animals, plants, and fungi; or unicellular microorganisms such as protists, bacteria, and archaea.[1] All types of organisms are capable of reproduction, growth and development, maintenance, and some degree of response to stimuli. Humans, squids, mushrooms, and vascular plants are examples of multicellular organisms that differentiate specialized tissues and organs during development.
 An organism may be either a prokaryote or a eukaryote. Prokaryotes are represented by two separate domains – bacteria and archaea. Eukaryotic organisms are characterized by the presence of a membrane-bound cell nucleus and contain additional membrane-bound compartments called organelles (such as mitochondria in animals and plants and plastids in plants and algae, all generally considered to be derived from endosymbiotic bacteria).[2] Fungi, animals and plants are examples of kingdoms of organisms within the eukaryotes.
 Estimates on the number of Earth's current species range from 2 million to 1 trillion,[3] of which over 1.7 million have been documented.[4] More than 99% of all species, amounting to over five billion species,[5] that ever lived are estimated to be extinct.[6][7] In 2016, a set of 355 genes from the last universal common ancestor (LUCA) of all organisms was identified.[8][9]"
Ornithology,Biology,5,"Ornithology is a branch of zoology that concerns the ""methodological study and consequent knowledge of birds with all that relates to them"".[1] Several aspects of ornithology differ from related disciplines, due partly to the high visibility and the aesthetic appeal of birds.[2] It has also been an area with a large contribution made by amateurs in terms of time, resources, and financial support. Studies on birds have helped develop key concepts in biology including evolution, behaviour and ecology such as the definition of species, the process of speciation, instinct, learning, ecological niches, guilds, island biogeography, phylogeography, and conservation.[3] While early ornithology was principally concerned with descriptions and distributions of species, ornithologists today seek answers to very specific questions, often using birds as models to test hypotheses or predictions based on theories. Most modern biological theories apply across life forms, and the number of scientists who identify themselves as ""ornithologists"" has therefore declined.[4] A wide range of tools and techniques are used in ornithology, both inside the laboratory and out in the field, and innovations are constantly made.[5]"
Osmosis,Biology,5,"
 Osmosis (/ɒzˈmoʊ.sɪs/)[1] is the spontaneous net movement of solvent molecules through a selectively permeable membrane into a region of higher solute concentration, in the direction that tends to equalize the solute concentrations on the two sides.[2][3][4] It may also be used to describe a physical process in which any solvent moves across a selectively permeable membrane (permeable to the solvent, but not the solute) separating two solutions of different concentrations.[5][6] Osmosis can be made to do work.[7] Osmotic pressure is defined as the external pressure required to be applied so that there is no net movement of solvent across the membrane.  Osmotic pressure is a colligative property, meaning that the osmotic pressure depends on the molar concentration of the solute but not on its identity.
 Osmosis is a vital process in biological systems, as biological membranes are semipermeable. In general, these membranes are impermeable to large and polar molecules, such as ions, proteins, and polysaccharides, while being permeable to non-polar or hydrophobic molecules like lipids as well as to small molecules like oxygen, carbon dioxide, nitrogen, and nitric oxide. Permeability depends on solubility, charge, or chemistry, as well as solute size. Water molecules travel through the plasma membrane, tonoplast membrane (vacuole) or protoplast by diffusing across the phospholipid bilayer via aquaporins (small transmembrane proteins similar to those responsible for facilitated diffusion and ion channels). Osmosis provides the primary means by which water is transported into and out of cells. The turgor pressure of a cell is largely maintained by osmosis across the cell membrane between the cell interior and its relatively hypotonic environment.
"
Paleontology,Biology,5,"Paleontology, also spelled palaeontology or palæontology (/ˌpeɪliɒnˈtɒlədʒi, ˌpæli-, -ən-/), is the scientific study of life that existed prior to, and sometimes including, the start of the Holocene Epoch (roughly 11,700 years before present). It includes the study of fossils to classify organisms and study interactions with each other and their environments (their paleoecology). Paleontological observations have been documented as far back as the 5th century BCE. The science became established in the 18th century as a result of Georges Cuvier's work on comparative anatomy, and developed rapidly in the 19th century. The term itself originates from Greek παλαιός, palaios, ""old, ancient"", ὄν, on (gen. ontos), ""being, creature"", and λόγος, logos, ""speech, thought, study"".[1] Paleontology lies on the border between biology and geology, but differs from archaeology in that it excludes the study of anatomically modern humans. It now uses techniques drawn from a wide range of sciences, including biochemistry, mathematics, and engineering. Use of all these techniques has enabled paleontologists to discover much of the evolutionary history of life, almost all the way back to when Earth became capable of supporting life, about 3.8 billion years ago. As knowledge has increased, paleontology has developed specialised sub-divisions, some of which focus on different types of fossil organisms while others study ecology and environmental history, such as ancient climates.
 Body fossils and trace fossils are the principal types of evidence about ancient life, and geochemical evidence has helped to decipher the evolution of life before there were organisms large enough to leave body fossils. Estimating the dates of these remains is essential but difficult: sometimes adjacent rock layers allow radiometric dating, which provides absolute dates that are accurate to within 0.5%, but more often paleontologists have to rely on relative dating by solving the ""jigsaw puzzles"" of biostratigraphy (arrangement of rock layers from youngest to oldest). Classifying ancient organisms is also difficult, as many do not fit well into the Linnaean taxonomy classifying living organisms, and paleontologists more often use cladistics to draw up evolutionary ""family trees"". The final quarter of the 20th century saw the development of molecular phylogenetics, which investigates how closely organisms are related by measuring the similarity of the DNA in their genomes. Molecular phylogenetics has also been used to estimate the dates when species diverged, but there is controversy about the reliability of the molecular clock on which such estimates depend.
"
Parallel_evolution,Biology,5,"Parallel evolution is the similar development of a trait in distinct species that are not closely related, but share a similar original trait in response to similar evolutionary pressure.[1][2]"
Parasite,Biology,5,"
 
 Parasitism is a symbiotic relationship between species, where one organism, the parasite, lives on or inside another organism, the host, causing it some harm, and is adapted structurally to this way of life.[1] The entomologist E. O. Wilson has characterised parasites as ""predators that eat prey in units of less than one"".[2] Parasites include protozoans such as the agents of malaria, sleeping sickness, and amoebic dysentery; animals such as hookworms, lice, mosquitoes, and vampire bats; fungi such as honey fungus and the agents of ringworm; and plants such as mistletoe, dodder, and the broomrapes. There are six major parasitic strategies of exploitation of animal hosts, namely parasitic castration, directly transmitted parasitism (by contact), trophically transmitted parasitism (by being eaten), vector-transmitted parasitism, parasitoidism, and micropredation.
 Like predation, parasitism is a type of consumer-resource interaction,[3] but unlike predators, parasites, with the exception of parasitoids, are typically much smaller than their hosts, do not kill them, and often live in or on their hosts for an extended period. Parasites of animals are highly specialised, and reproduce at a faster rate than their hosts. Classic examples include interactions between vertebrate hosts and tapeworms, flukes, the malaria-causing Plasmodium species, and fleas.
 Parasites reduce host fitness by general or specialised pathology, from parasitic castration to modification of host behaviour. Parasites increase their own fitness by exploiting hosts for resources necessary for their survival, in particular by feeding on them and by using intermediate (secondary) hosts to assist in their transmission from one definitive (primary) host to another. Although parasitism is often unambiguous, it is part of a spectrum of interactions between species, grading via parasitoidism into predation, through evolution into mutualism, and in some fungi, shading into being saprophytic.
 People have known about parasites such as roundworms and tapeworms since ancient Egypt, Greece, and Rome. In Early Modern times, Antonie van Leeuwenhoek observed Giardia lamblia in his microscope in 1681, while Francesco Redi described internal and external parasites including sheep liver fluke and ticks. Modern parasitology developed in the 19th century. In human culture, parasitism has negative connotations. These were exploited to satirical effect in Jonathan Swift's 1733 poem ""On Poetry: A Rhapsody"", comparing poets to hyperparasitical ""vermin"". In fiction, Bram Stoker's 1897 Gothic horror novel Dracula and its many later adaptations featured a blood-drinking parasite. Ridley Scott's 1979 film Alien was one of many works of science fiction to feature a parasitic alien species.[4]"
Parasitology,Biology,5,"Parasitology is the study of parasites, their hosts, and the relationship between them. As a biological discipline, the scope of parasitology is not determined by the organism or environment in question but by their way of life. This means it forms a synthesis of other disciplines, and draws on techniques from fields such as cell biology, bioinformatics, biochemistry, molecular biology, immunology, genetics, evolution and ecology.
"
Pathobiology,Biology,5,"Pathology is the study of the causes and effects of disease or injury. The word pathology also refers to the study of disease in general, incorporating a wide range of bioscience research fields and medical practices. However, when used in the context of modern medical treatment, the term is often used in a more narrow fashion to refer to processes and tests which fall within the contemporary medical field of ""general pathology"", an area which includes a number of distinct but inter-related medical specialties that diagnose disease, mostly through analysis of tissue, cell, and body fluid samples.  Idiomatically, ""a pathology"" may also refer to the predicted or actual progression of particular diseases (as in the statement ""the many different forms of cancer have diverse pathologies""), and the affix pathy is sometimes used to indicate a state of disease in cases of both physical ailment (as in cardiomyopathy) and psychological conditions (such as psychopathy).[1]  A physician practicing pathology is called a pathologist.
 As a field of general inquiry and research, pathology addresses  components of disease: cause, mechanisms of development (pathogenesis), structural alterations of cells (morphologic changes), and the consequences of changes (clinical manifestations).[2] In common medical practice, general pathology is mostly concerned with analyzing known clinical abnormalities that are markers or precursors for both infectious and non-infectious disease, and is conducted by experts in one of two major specialties, anatomical pathology and clinical pathology.[3] Further divisions in specialty exist on the basis of the involved sample types (comparing, for example, cytopathology, hematopathology, and histopathology), organs (as in renal pathology), and physiological systems (oral pathology), as well as on the basis of the focus of the examination (as with forensic pathology).
 Pathology is a significant field in modern medical diagnosis and medical research.
"
Pathogen,Biology,5,"In biology, a pathogen (Greek: πάθος pathos ""suffering"", ""passion"" and -γενής -genēs ""producer of"") in the oldest and broadest sense, is any organism that can produce disease. A pathogen may also be referred to as an infectious agent, or simply a germ.
 The term pathogen came into use in the 1880s.[1][2]  Typically, the term is used to describe an infectious microorganism or agent, such as a virus, bacterium, protozoan, prion, viroid, or fungus.[3][4][5] Small animals, such as certain kinds of worms and insect larvae, can also produce disease. However, these animals are usually, in common parlance, referred to as parasites rather than pathogens.  The scientific study of microscopic organisms, including microscopic pathogenic organisms, is called microbiology, while the study of disease that may include these pathogens is called pathology. Parasitology, meanwhile, is the scientific study of parasites and the organisms that host them.
 There are several pathways through which pathogens can invade a host. The principal pathways have different episodic time frames, but soil has the longest or most persistent potential for harboring a pathogen. 
 Diseases in humans that are caused by infectious agents are known as pathogenic diseases. Not all diseases are caused by pathogens, other causes are, for example, toxins, genetic disorders and the host's own immune system.
"
Pathology,Biology,5,"Pathology is the study of the causes and effects of disease or injury. The word pathology also refers to the study of disease in general, incorporating a wide range of bioscience research fields and medical practices. However, when used in the context of modern medical treatment, the term is often used in a more narrow fashion to refer to processes and tests which fall within the contemporary medical field of ""general pathology"", an area which includes a number of distinct but inter-related medical specialties that diagnose disease, mostly through analysis of tissue, cell, and body fluid samples.  Idiomatically, ""a pathology"" may also refer to the predicted or actual progression of particular diseases (as in the statement ""the many different forms of cancer have diverse pathologies""), and the affix pathy is sometimes used to indicate a state of disease in cases of both physical ailment (as in cardiomyopathy) and psychological conditions (such as psychopathy).[1]  A physician practicing pathology is called a pathologist.
 As a field of general inquiry and research, pathology addresses  components of disease: cause, mechanisms of development (pathogenesis), structural alterations of cells (morphologic changes), and the consequences of changes (clinical manifestations).[2] In common medical practice, general pathology is mostly concerned with analyzing known clinical abnormalities that are markers or precursors for both infectious and non-infectious disease, and is conducted by experts in one of two major specialties, anatomical pathology and clinical pathology.[3] Further divisions in specialty exist on the basis of the involved sample types (comparing, for example, cytopathology, hematopathology, and histopathology), organs (as in renal pathology), and physiological systems (oral pathology), as well as on the basis of the focus of the examination (as with forensic pathology).
 Pathology is a significant field in modern medical diagnosis and medical research.
"
PH,Biology,5,"
 In chemistry, pH (/piːˈeɪtʃ/, denoting 'potential of hydrogen' or 'power of hydrogen'[1]) is a scale used to specify the acidity or basicity of an aqueous solution. Acidic solutions (solutions with higher concentrations of  H+ ions) are measured to have lower pH values than basic or alkaline solutions.
 The pH scale is logarithmic and inversely indicates the concentration of hydrogen ions in the solution. This is because the formula used to calculate pH approximates the negative of the base 10 logarithm of the molar concentration[a] of hydrogen ions in the solution. More precisely, pH is the negative of the base 10 logarithm of the activity of the H+ ion.[2] At 25 °C, solutions with a pH less than 7 are acidic, and solutions with a pH greater than 7 are basic. Solutions with a pH of 7 at this temperature are neutral (e.g. pure water).     The neutral value of the pH depends on the temperature, being lower than 7 if the temperature increases. The pH value can be less than 0 for very strong acids, or greater than 14 for very strong bases.[3] The pH scale is traceable to a set of standard solutions whose pH is established by international agreement.[4] Primary pH standard values are determined using a concentration cell with transference, by measuring the potential difference between a hydrogen electrode and a standard electrode such as the silver chloride electrode. The pH of aqueous solutions can be measured with a glass electrode and a pH meter, or a color-changing indicator. Measurements of pH are important in chemistry, agronomy, medicine, water treatment, and many other applications.
"
Pharmacology,Biology,5,"
 Pharmacology is a branch of medicine and pharmaceutical sciences which is concerned with the study of drug or medication action,[1] where a drug can be broadly or narrowly defined as any man-made, natural, or endogenous (from within the body) molecule which exerts a biochemical or physiological effect on the cell, tissue, organ, or organism (sometimes the word pharmacon is used as a term to encompass these endogenous and exogenous bioactive species). More specifically, it is the study of the interactions that occur between a living organism and chemicals that affect normal or abnormal biochemical function. If substances have medicinal properties, they are considered pharmaceuticals.
 The field encompasses drug composition and properties, synthesis and drug design, molecular and cellular mechanisms, organ/systems mechanisms, signal transduction/cellular communication, molecular diagnostics, interactions, chemical biology, therapy, and medical applications and antipathogenic capabilities. The two main areas of pharmacology are pharmacodynamics and pharmacokinetics. Pharmacodynamics studies the effects of a drug on biological systems, and pharmacokinetics studies the effects of biological systems on a drug. In broad terms, pharmacodynamics discusses the chemicals with biological receptors, and pharmacokinetics discusses the absorption, distribution, metabolism, and excretion (ADME) of chemicals from the biological systems. 
 Pharmacology is not synonymous with pharmacy and the two terms are frequently confused. Pharmacology, a biomedical science, deals with the research, discovery, and characterization of chemicals which show biological effects and the elucidation of cellular and organismal function in relation to these chemicals. In contrast, pharmacy, a health services profession, is concerned with the application of the principles learned from pharmacology in its clinical settings; whether it be in a dispensing or clinical care role. In either field, the primary contrast between the two is their distinctions between direct-patient care, pharmacy practice, and the science-oriented research field, driven by pharmacology.
"
Phenotype,Biology,5,"Phenotype (from Greek  pheno- 'showing', and  type 'type') is the term used in genetics for the composite observable characteristics or traits of an organism.[1][2] The term covers the organism's  morphology or physical form and structure, its  developmental processes, its biochemical and physiological properties, its behavior, and the products of behavior. An organism's phenotype results from two basic factors: the  expression of an organism's genetic code, or its genotype, and the influence of environmental factors. Both factors may interact, further affecting phenotype. When two or more clearly different phenotypes exist in the same population of a species, the species is called  polymorphic. A well-documented example of polymorphism is  Labrador Retriever coloring; while the coat color depends on many genes, it is clearly seen in the environment as yellow, black, and brown.  Richard Dawkins in 1978[3] and then again in his 1982 book The Extended Phenotype suggested that one can regard bird nests and other built structures such as  caddis-fly larvae cases and beaver dams as ""extended phenotypes"".
 Wilhelm Johannsen proposed the genotype-phenotype distinction in 1911 to make clear the difference between an organism's heredity and what that heredity produces.[4][5] The distinction resembles that proposed by August Weismann (1834-1914), who distinguished between germ plasm (heredity) and somatic cells (the body).
 The genotype-phenotype distinction should not be confused with Francis Crick's central dogma of molecular biology, a statement about the directionality of molecular sequential information flowing from DNA to protein, and not the reverse.
"
Pheromone,Biology,5,"A pheromone (from Ancient Greek φέρω phero ""to bear"" and hormone) is a secreted or excreted chemical factor that triggers a social response in members of the same species. Pheromones are chemicals capable of acting like hormones outside the body of the secreting individual, to impact the behavior of the receiving individuals.[1] There are alarm pheromones, food trail pheromones, sex pheromones, and many others that affect behavior or physiology. Pheromones are used from basic unicellular prokaryotes to complex multicellular eukaryotes.[2] Their use among insects has been particularly well documented. In addition, some vertebrates, plants and ciliates communicate by using pheromones.
"
Phloem,Biology,5,"Phloem (/ˈfloʊ.əm/, FLOH-əm) is the living tissue in vascular plants that transports the soluble organic compounds made during photosynthesis and known as photosynthates, in particular the sugar sucrose,[1] to parts of the plant where needed. This transport process is called translocation.[2] In trees, the phloem is the innermost layer of the bark, hence the name, derived from the Greek word φλοιός (phloios) meaning ""bark"". The term was introduced by Carl Nägeli in 1858.[3][4]"
Photosynthesis,Biology,5,"
 Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organisms' activities. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water – hence the name photosynthesis, from the Greek phōs (φῶς), ""light"", and sunthesis (σύνθεσις), ""putting together"".[1][2][3] In most cases, oxygen is also released as a waste product. Most plants, most algae, and cyanobacteria perform photosynthesis; such organisms are called photoautotrophs. Photosynthesis is largely responsible for producing and maintaining the oxygen content of the Earth's atmosphere, and supplies most of the energy necessary for life on Earth.[4] Although photosynthesis is performed differently by different species, the process always begins when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments. In plants, these proteins are held inside organelles called chloroplasts, which are most abundant in leaf cells, while in bacteria they are embedded in the plasma membrane. In these light-dependent reactions, some energy is used to strip electrons from suitable substances, such as water, producing oxygen gas. The hydrogen freed by the splitting of water is used in the creation of two further compounds that serve as short-term stores of energy, enabling its transfer to drive other reactions: these compounds are reduced nicotinamide adenine dinucleotide phosphate (NADPH) and adenosine triphosphate (ATP), the ""energy currency"" of cells.
 In plants, algae and cyanobacteria, long-term energy storage in the form of sugars is produced by a subsequent sequence of light-independent reactions called the Calvin cycle; some bacteria use different mechanisms, such as the reverse Krebs cycle, to achieve the same end. In the Calvin cycle, atmospheric carbon dioxide is incorporated into already existing organic carbon compounds, such as ribulose bisphosphate (RuBP).[5] Using the ATP and NADPH produced by the light-dependent reactions, the resulting compounds are then reduced and removed to form further carbohydrates, such as glucose.
 The first photosynthetic organisms probably evolved early in the evolutionary history of life and most likely used reducing agents such as hydrogen or hydrogen sulfide, rather than water, as sources of electrons.[6] Cyanobacteria appeared later; the excess oxygen they produced contributed directly to the oxygenation of the Earth,[7] which rendered the evolution of complex life possible. Today, the average rate of energy capture by photosynthesis globally is approximately 130 terawatts,[8][9][10] which is about eight times the current power consumption of human civilization.[11]
Photosynthetic organisms also convert around 100–115 billion tons (91–104 petagrams) of carbon into biomass per year.[12][13] The fact that plants receive some energy from light—in addition to air, soil, and water—was discovered in 1779 by Jan Ingenhousz.
"
Phylogeny,Biology,5,"A phylogenetic tree or evolutionary tree is a branching diagram or ""tree"" showing the evolutionary relationships among various biological species or other entities—their phylogeny (/faɪˈlɒdʒəni/)—based upon similarities and differences in their physical or genetic characteristics. All life on Earth is part of a single phylogenetic tree, indicating common ancestry.
 In a rooted phylogenetic tree, each node with descendants represents the inferred most recent common ancestor of those descendants, and the edge lengths in some trees may be interpreted as time estimates. Each node is called a taxonomic unit. Internal nodes are generally called hypothetical taxonomic units, as they cannot be directly observed. Trees are useful in fields of biology such as bioinformatics, systematics, and phylogenetics. Unrooted trees illustrate only the relatedness of the leaf nodes and do not require the ancestral root to be known or inferred.
"
Phylum,Biology,5,"
 In biology, a phylum (/ˈfaɪləm/; plural: phyla) is a level of classification or taxonomic rank below kingdom and above class. Traditionally, in botany the term division has been used instead of phylum, although the International Code of Nomenclature for algae, fungi, and plants accepts the terms as equivalent.[1][2][3] Depending on definitions, the animal kingdom Animalia or Metazoa contains approximately 35 phyla; the plant kingdom Plantae contains about 14, and the fungus kingdom Fungi contains about 8 phyla. Current research in phylogenetics is uncovering the relationships between phyla, which are contained in larger clades, like Ecdysozoa and Embryophyta.
"
Physiology,Biology,5,"Physiology (/ˌfɪziˈɒlədʒi/; from Ancient Greek  φύσις (physis) 'nature, origin', and  -λογία (-logia) 'study of'[1]) is the scientific study of functions and mechanisms in a living system.[2][3] As a sub-discipline of biology, physiology focuses on how organisms, organ systems, individual organs, cells, and biomolecules carry out the chemical and physical functions in a living system.[4] According to the classes of organisms, the field can be divided into medical physiology, animal physiology, plant physiology, cell physiology, and comparative physiology.[4] Central to physiological functioning are biophysical and biochemical processes, homeostatic control mechanisms, and communication between cells.[5] Physiological state is the condition of normal function, while pathological state refers to abnormal conditions, including human diseases.
 The Nobel Prize in Physiology or Medicine is awarded by the Royal Swedish Academy of Sciences for exceptional scientific achievements in physiology related to the field of medicine. 
"
Phytochemistry,Biology,5,"Phytochemistry is the study of phytochemicals, which are chemicals derived from plants. Phytochemists strive to describe the structures of the large number of secondary metabolites found in plants, the functions of these compounds in human and plant biology, and the biosynthesis of these compounds. Plants synthesize phytochemicals for many reasons, including to protect themselves against insect attacks and plant diseases. The compounds found in plants are of many kinds, but most can be grouped into four major biosynthetic classes: alkaloids, phenylpropanoids, polyketides, and terpenoids.
 Phytochemistry can be considered a subfield of botany or chemistry. Activities can be led in botanical gardens or in the wild with the aid of ethnobotany. Phytochemical studies directed toward human (i.e. drug discovery) use may fall under the discipline of pharmacognosy, whereas phytochemical studies focused on the ecological functions and evolution of phytochemicals likely fall under the discipline of chemical ecology. Phytochemistry also has relevance to the field of plant physiology.
"
Phytopathology,Biology,5,"Plant pathology (also phytopathology) is the scientific study of diseases in plants caused by pathogens (infectious organisms) and environmental conditions (physiological factors).[1] Organisms that cause infectious disease include fungi, oomycetes, bacteria, viruses, viroids, virus-like organisms, phytoplasmas, protozoa, nematodes and parasitic plants. Not included are ectoparasites like insects, mites, vertebrate, or other pests that affect plant health by eating of plant tissues. Plant pathology also involves the study of pathogen identification, disease etiology, disease cycles, economic impact, plant disease epidemiology, plant disease resistance, how plant diseases affect humans and animals, pathosystem genetics, and management of plant diseases.
"
Placebo,Biology,5,"A placebo (/pləˈsiːboʊ/ plə-SEE-boh) is a substance or treatment which is designed to have no therapeutic value.[1] Common placebos include inert tablets (like sugar pills), inert injections (like saline), sham surgery,[2] and other procedures.[3] In general, placebos can affect how patients perceive their condition and encourage the body's chemical processes for relieving pain[4] and a few other symptoms,[5] but have no impact on the disease itself.[6][4] Improvements that patients experience after being treated with a placebo can also be due to unrelated factors, such as regression to the mean (a natural recovery from the illness).[4] The use of placebos in clinical medicine raises ethical concerns, especially if they are disguised as an active treatment, as this introduces dishonesty into the doctor–patient relationship and bypasses informed consent.[7] While it was once assumed that this deception was necessary for placebos to have any effect, there is now evidence that placebos can have effects even when the patient is aware that the treatment is a placebo.[8] In drug testing and medical research, a placebo can be made to resemble an active medication or therapy so that it functions as a control; this is to prevent the recipient or others from knowing (with their consent) whether a treatment is active or inactive, as expectations about efficacy can influence results.[9][10] In a clinical trial any change in the placebo arm is known as the placebo response, and the difference between this and the result of no treatment is the placebo effect.[11] Some researchers now recommend comparing the experimental treatment with an existing treatment when possible, instead of a placebo.[12] The idea of a placebo effect—a therapeutic outcome derived from an inert treatment—was discussed in 18th century psychology[13] but became more prominent in the 20th century. An influential 1955 study entitled The Powerful Placebo firmly established the idea that placebo effects were clinically important,[14] and were a result of the brain's role in physical health. A 1997 reassessment found no evidence of any placebo effect in the source data, as the study had not accounted for regression to the mean.[15][16]"
Plant,Biology,5,"
 Plants are mainly multicellular organisms, predominantly photosynthetic eukaryotes of the kingdom Plantae. Historically, plants were treated as one of two kingdoms including all living things that were not animals, and all algae and fungi were treated as plants. However, all current definitions of Plantae exclude the fungi and some algae, as well as the prokaryotes (the archaea and bacteria). By one definition, plants form the clade Viridiplantae (Latin name for ""green plants""), a group that includes the flowering plants, conifers and other gymnosperms, ferns and their allies, hornworts, liverworts, mosses, and the green algae, but excludes the red and brown algae.
 Green plants obtain most of their energy from sunlight via photosynthesis by primary chloroplasts that are derived from endosymbiosis with cyanobacteria. Their chloroplasts contain chlorophylls a and b, which gives them their green color. Some plants are parasitic or mycotrophic and have lost the ability to produce normal amounts of chlorophyll or to photosynthesize, but still have flowers, fruits, and seeds. Plants are characterized by sexual reproduction and alternation of generations, although asexual reproduction is also common.
 There are about 320,000 species of plants, of which the great majority, some 260–290 thousand, produce seeds.[5] Green plants provide a substantial proportion of the world's molecular oxygen,[6] and are the basis of most of Earth's ecosystems. Plants that produce grain, fruit, and vegetables also form basic human foods and have been domesticated for millennia. Plants have many cultural and other uses, as ornaments, building materials, writing material and, in great variety, they have been the source of medicines and psychoactive drugs. The scientific study of plants is known as botany, a branch of biology.
"
Plasmolysis,Biology,5,"Plasmolysis is the process in which cells lose water in a hypertonic solution. The reverse process, deplasmolysis or cytolysis, can occur if the cell is in a hypotonic solution resulting in a lower external osmotic pressure and a net flow of water into the cell. Through observation of plasmolysis and deplasmolysis, it is possible to determine the tonicity of the cell's environment as well as the rate solute molecules cross the cellular membrane.
"
Pollination,Biology,5,"Pollination is the transfer of pollen from a male part of a plant to a female part of a plant, later enabling fertilisation and the production of seeds, most often by an animal or by wind.[1] Pollinating agents are animals such as insects, birds, and bats; water; wind; and even plants themselves, when self-pollination occurs within a closed flower. Pollination often occurs within a species. When pollination occurs between species it can produce hybrid offspring in nature and in plant breeding work.
 In angiosperms, after the pollen grain (gametophyte) has landed on the stigma, it germinates and develops a pollen tube which grows down the style until it reaches an ovary. Its two gametes travel down the tube to where the gametophyte(s) containing the female gametes are held within the carpel. After entering an ovum cell through the micropyle, one male nucleus fuses with the polar bodies to produce the endosperm tissues, while the other fuses with the ovule to produce the embryo.[2][3] Hence the term: ""double fertilization"". This process would result in the production of a seed made of both nutritious tissues and embryo.
 In gymnosperms, the ovule is not contained in a carpel, but exposed on the surface of a dedicated support organ, such as the scale of a cone, so that the penetration of carpel tissue is unnecessary. Details of the process vary according to the division of gymnosperms in question. Two main modes of fertilization are found in gymnosperms. Cycads and Ginkgo have motile sperm that swim directly to the egg inside the ovule, whereas conifers and gnetophytes have sperm that are unable to swim but are conveyed to the egg along a pollen tube.
 The study of pollination spans many disciplines, such as botany, horticulture, entomology, and ecology. The pollination process as an interaction between flower and pollen vector was first addressed in the 18th century by Christian Konrad Sprengel. It is important in horticulture and agriculture, because fruiting is dependent on fertilization: the result of pollination. The study of pollination by insects is known as anthecology. There are also studies in economics that look at the positive and negative benefits of pollination, focused on bees, and how the process affects the pollinators themselves.
"
Polymer,Biology,5,"
 A polymer (/ˈpɒlɪmər/;[4][5] Greek poly-, ""many"" + -mer, ""part"")
is a substance or material consisting of very large molecules, or macromolecules, composed of many repeating subunits.[6] Due to their broad spectrum of properties,[7] both synthetic and natural polymers play essential and ubiquitous roles in everyday life.[8] Polymers range from familiar synthetic plastics such as polystyrene to natural biopolymers such as DNA and proteins that are fundamental to biological structure and function. Polymers, both natural and synthetic, are created via polymerization of many small molecules, known as monomers. Their consequently large molecular mass, relative to small molecule compounds, produces unique physical properties including toughness, high elasticity, viscoelasticity, and a tendency to form amorphous and semicrystalline structures rather than crystals.
 The term ""polymer"" derives from the Greek word πολύς (polus, meaning ""many, much"") and μέρος (meros, meaning ""part""), and refers to large molecules whose structure is composed of multiple repeating units, from which originates a characteristic of high relative molecular mass and attendant properties.[3] The units composing polymers derive, actually or conceptually, from molecules of low relative molecular mass.[3] The term was coined in 1833 by Jöns Jacob Berzelius, though with a definition distinct from the modern IUPAC definition.[9][10] The modern concept of polymers as covalently bonded macromolecular structures was proposed in 1920 by Hermann Staudinger,[11] who spent the next decade finding experimental evidence for this hypothesis.[12] Polymers are studied in the fields of polymer science (which includes polymer chemistry and polymer physics), biophysics and materials science and engineering. Historically, products arising from the linkage of repeating units by covalent chemical bonds have been the primary focus of polymer science. An emerging important area now focuses on supramolecular polymers formed by non-covalent links. Polyisoprene of latex rubber is an example of a natural polymer, and the polystyrene of styrofoam is an example of a synthetic polymer. In biological contexts, essentially all biological macromolecules—i.e., proteins (polyamides), nucleic acids (polynucleotides), and polysaccharides—are purely polymeric, or are composed in large part of polymeric components.
"
Polymerase_chain_reaction,Biology,5,"
 Polymerase chain reaction (PCR) is a method widely used to rapidly make millions to billions of copies of a specific DNA sample, allowing scientists to take a very small sample of DNA and amplify it to a large enough amount to study in detail. PCR was invented in 1984 by the American biochemist Kary Mullis at Cetus Corporation. It is fundamental to much of genetic testing including analysis of ancient samples of DNA and identification of infectious agents. Using PCR, copies of very small amounts of DNA sequences are exponentially amplified in a series of cycles of temperature changes. PCR is now a common and often indispensable technique used in medical laboratory and clinical laboratory research for a broad variety of applications including biomedical research and criminal forensics.[1][2] The majority of PCR methods rely on thermal cycling. Thermal cycling exposes reactants to repeated cycles of heating and cooling to permit different temperature-dependent reactions – specifically, DNA melting and enzyme-driven DNA replication. PCR employs two main reagents – primers (which are short single strand DNA fragments known as oligonucleotides that are a complementary sequence to the target DNA region) and a DNA polymerase. In the first step of PCR, the two strands of the DNA double helix are physically separated at a high temperature in a process called nucleic acid denaturation. In the second step, the temperature is lowered and the primers bind to the complementary sequences of DNA. The two DNA strands then become templates for DNA polymerase to enzymatically assemble a new DNA strand from free nucleotides, the building blocks of DNA. As PCR progresses, the DNA generated is itself used as a template for replication, setting in motion a chain reaction in which the original DNA template is exponentially amplified.
 Almost all PCR applications employ a heat-stable DNA polymerase, such as Taq polymerase, an enzyme originally isolated from the thermophilic bacterium Thermus aquaticus. If the polymerase used was heat-susceptible, it would denature under the high temperatures of the denaturation step. Before the use of Taq polymerase, DNA polymerase had to be manually added every cycle, which was a tedious and costly process.[3] Applications of the technique include DNA cloning for sequencing, gene cloning and manipulation, gene mutagenesis; construction of DNA-based phylogenies, or functional analysis of genes; diagnosis and monitoring of hereditary diseases; amplification of ancient DNA;[4] analysis of genetic fingerprints for DNA profiling (for example, in forensic science and parentage testing); and detection of pathogens in nucleic acid tests for the diagnosis of infectious diseases.
"
Polyploidy,Biology,5,"Polyploidy is a condition in which the cells of an organism have more than two paired (homologous) sets of chromosomes. Most species whose cells have nuclei (eukaryotes) are diploid, meaning they have two sets of chromosomes—one set inherited from each parent. However, some organisms are polyploid, and polyploidy is especially common in plants. Most eukaryotes have diploid somatic cells, but produce haploid gametes (eggs and sperm) by meiosis. A monoploid has only one set of chromosomes, and the term is usually only applied to cells or organisms that are normally haploid. Males of bees and other Hymenoptera, for example, are monoploid. Unlike animals, plants and multicellular algae have life cycles with two alternating multicellular generations. The gametophyte generation is haploid, and produces gametes by mitosis, the sporophyte generation is diploid and produces spores by meiosis.
 Polyploidy may occur due to abnormal cell division, either during mitosis, or commonly during metaphase I in meiosis. In addition, it can be induced in plants and cell cultures by some chemicals: the best known is colchicine, which can result in chromosome doubling, though its use may have other less obvious consequences as well. Oryzalin will also double the existing chromosome content.
 Polyploidy occurs in highly differentiated human tissues in the  liver, heart muscle, bone marrow and the placenta.[1] It occurs in the somatic cells of some animals, such as goldfish,[2] salmon, and salamanders, but is especially common among ferns and flowering plants (see Hibiscus rosa-sinensis), including both wild and cultivated species. Wheat, for example, after millennia of hybridization and modification by humans, has strains that are diploid (two sets of chromosomes), tetraploid (four sets of chromosomes) with the common name of durum or macaroni wheat, and hexaploid (six sets of chromosomes) with the common name of bread wheat. Many agriculturally important plants of the genus Brassica are also tetraploids.
 Polyploidization can be a mechanism of sympatric speciation because polyploids are usually unable to interbreed with their diploid ancestors. An example is the plant Erythranthe peregrina. Sequencing confirmed that this species originated from E. × robertsii, a sterile triploid hybrid between E. guttata and E. lutea, both of which have been introduced and naturalised in the United Kingdom. New populations of E. peregrina arose on the Scottish mainland and the Orkney Islands via genome duplication from local populations of E. × robertsii.[3] Because of a rare genetic mutation, E. peregrina is not sterile.[4]"
Population_(biology),Biology,5,"
 In biology, a population is a number of all the organisms of the same group or species who live in a particular geographical area and are capable of interbreeding.[1][2] The area of a sexual population is the area where inter-breeding is possible between any pair within the area and more probable than cross-breeding with individuals from other areas.[3] In sociology, population refers to a collection of humans. Demography is a social science which entails the statistical study of populations. 
Population, in simpler terms, is the number of people in a city or town, region, country or world; population is usually determined by a process called census (a process of collecting, analyzing, compiling and publishing data).
"
Population_biology,Biology,5,"The term population biology has been used with different meanings.
 In 1971 Edward O. Wilson et al. used the term in the sense of applying mathematical models to population genetics, community ecology, and population dynamics.[1] Alan Hastings used the term in 1997 as the title of his book on the mathematics used in population dynamics.[2] The name was also used for a course given at UC Davis in the late 2010s, which describes it as an interdisciplinary field combining the areas of ecology and evolutionary biology. The course includes mathematics, statistics, ecology, genetics, and systematics. Numerous types of organisms are studied.[3] The journal Theoretical Population Biology is published.
"
Population_ecology,Biology,5,"Population ecology is a sub-field of ecology that deals with the dynamics of species populations and how these populations interact with the environment, such as birth and death rates, and by immigration and emigration).[2] The discipline is important in conservation biology, especially in the development of population viability analysis which makes it possible to predict the long-term probability of a species persisting in a given patch of habitat.[3] Although population ecology is a subfield of biology, it provides interesting problems for mathematicians and statisticians who work in population dynamics.[4]"
Predation,Biology,5,"
 Predation is a biological interaction where one organism, the predator, kills and eats another organism, its prey. It is one of a family of common feeding behaviours that includes parasitism and micropredation (which usually do not kill the host) and parasitoidism (which always does, eventually). It is distinct from scavenging on dead prey, though many predators also scavenge; it overlaps with herbivory, as seed predators and destructive frugivores are predators.
 Predators may actively search for or pursue prey or wait for it, often concealed. When prey is detected, the predator assesses whether to attack it. This may involve ambush or pursuit predation, sometimes after stalking the prey. If the attack is successful, the predator kills the prey, removes any inedible parts like the shell or spines, and eats it.
 Predators are adapted and often highly specialized for hunting, with acute senses such as vision, hearing, or smell. Many predatory animals, both vertebrate and invertebrate, have sharp claws or jaws to grip, kill, and cut up their prey. Other adaptations include stealth and aggressive mimicry that improve hunting efficiency.
 Predation has a powerful selective effect on prey, and the prey develop antipredator adaptations such as warning coloration, alarm calls and other signals, camouflage, mimicry of well-defended species, and defensive spines and chemicals. Sometimes predator and prey find themselves in an evolutionary arms race, a cycle of adaptations and counter-adaptations. Predation has been a major driver of evolution since at least the Cambrian period.
"
Predator,Biology,5,"
 Predation is a biological interaction where one organism, the predator, kills and eats another organism, its prey. It is one of a family of common feeding behaviours that includes parasitism and micropredation (which usually do not kill the host) and parasitoidism (which always does, eventually). It is distinct from scavenging on dead prey, though many predators also scavenge; it overlaps with herbivory, as seed predators and destructive frugivores are predators.
 Predators may actively search for or pursue prey or wait for it, often concealed. When prey is detected, the predator assesses whether to attack it. This may involve ambush or pursuit predation, sometimes after stalking the prey. If the attack is successful, the predator kills the prey, removes any inedible parts like the shell or spines, and eats it.
 Predators are adapted and often highly specialized for hunting, with acute senses such as vision, hearing, or smell. Many predatory animals, both vertebrate and invertebrate, have sharp claws or jaws to grip, kill, and cut up their prey. Other adaptations include stealth and aggressive mimicry that improve hunting efficiency.
 Predation has a powerful selective effect on prey, and the prey develop antipredator adaptations such as warning coloration, alarm calls and other signals, camouflage, mimicry of well-defended species, and defensive spines and chemicals. Sometimes predator and prey find themselves in an evolutionary arms race, a cycle of adaptations and counter-adaptations. Predation has been a major driver of evolution since at least the Cambrian period.
"
Prey,Biology,5,"
 Predation is a biological interaction where one organism, the predator, kills and eats another organism, its prey. It is one of a family of common feeding behaviours that includes parasitism and micropredation (which usually do not kill the host) and parasitoidism (which always does, eventually). It is distinct from scavenging on dead prey, though many predators also scavenge; it overlaps with herbivory, as seed predators and destructive frugivores are predators.
 Predators may actively search for or pursue prey or wait for it, often concealed. When prey is detected, the predator assesses whether to attack it. This may involve ambush or pursuit predation, sometimes after stalking the prey. If the attack is successful, the predator kills the prey, removes any inedible parts like the shell or spines, and eats it.
 Predators are adapted and often highly specialized for hunting, with acute senses such as vision, hearing, or smell. Many predatory animals, both vertebrate and invertebrate, have sharp claws or jaws to grip, kill, and cut up their prey. Other adaptations include stealth and aggressive mimicry that improve hunting efficiency.
 Predation has a powerful selective effect on prey, and the prey develop antipredator adaptations such as warning coloration, alarm calls and other signals, camouflage, mimicry of well-defended species, and defensive spines and chemicals. Sometimes predator and prey find themselves in an evolutionary arms race, a cycle of adaptations and counter-adaptations. Predation has been a major driver of evolution since at least the Cambrian period.
"
Primer_(molecular_biology),Biology,5,"A primer is a short single-stranded nucleic acid utilized by all living organisms in the initiation of DNA synthesis. The enzymes responsible for DNA replication, DNA polymerases, are only capable of adding nucleotides to the 3’-end of an existing nucleic acid, requiring a primer be bound to the template before DNA polymerase can begin a complementary strand.[1] Living organisms use solely RNA primers, while laboratory techniques in biochemistry and molecular biology that require in vitro DNA synthesis (such as DNA sequencing and polymerase chain reaction) usually use DNA primers, since they are more temperature stable.
"
Progesterone,Biology,5,"Progesterone (P4) is an endogenous steroid and progestogen sex hormone involved in the menstrual cycle, pregnancy, and embryogenesis of humans and other species.[1][13] It belongs to a group of steroid hormones called the progestogens,[13] and is the major progestogen in the body. Progesterone has a variety of important functions in the body. It is also a crucial metabolic intermediate in the production of other endogenous steroids, including the sex hormones and the corticosteroids, and plays an important role in brain function as a neurosteroid.[14] In addition to its role as a natural hormone, progesterone is also used as a medication, such as in menopausal hormone therapy and transfeminine hormone therapy.[15] It was first prescribed in 1934.[16]"
Prokaryote,Biology,5,"A prokaryote is a cellular organism that lacks an envelope-enclosed nucleus.[1] The word prokaryote comes from the Greek πρό (pro, 'before') and κάρυον (karyon, 'nut' or 'kernel').[2][3] In the two-empire system arising from the work of Édouard Chatton, prokaryotes were classified within the empire Prokaryota.[4] But in the three-domain system, based upon molecular analysis, prokaryotes are divided into two domains: Bacteria (formerly Eubacteria) and Archaea (formerly Archaebacteria). Organisms with nuclei are placed in a third domain, Eukaryota.[5] In the study of the origins of life, prokaryotes are thought to have arisen before eukaryotes. 
 Prokaryotes lack mitochondria, or any other eukaryotic membrane-bound organelles; and it was once thought that prokaryotes lacked cellular compartments, and therefore all cellular components within the cytoplasm were unenclosed, except for an outer cell membrane. But bacterial microcompartments, which are thought to be primitive organelles enclosed in protein shells, have been discovered;[6][7] and there is also evidence of prokaryotic membrane-bound organelles.[8] While typically being unicellular, some prokaryotes, such as cyanobacteria, may form large colonies. Others, such as myxobacteria, have multicellular stages in their life cycles.[9] Prokaryotes are asexual, reproducing without fusion of gametes, although horizontal gene transfer also takes place.
 Molecular studies have provided insight into the evolution and interrelationships of the three domains of life.[10] The division between prokaryotes and eukaryotes reflects the existence of two very different levels of cellular organization; only eukaryotic cells have an enveloped nucleus that contains its chromosomal DNA, and other characteristic membrane-bound organelles including mitochondria. Distinctive types of prokaryotes include extremophiles and methanogens; these are common in some extreme environments.[1]"
Protein,Biology,5,"
 Proteins are large biomolecules, or macromolecules, consisting of one or more long chains of amino acid residues. Proteins perform a vast array of functions within organisms, including catalysing metabolic reactions, DNA replication, responding to stimuli, providing structure to cells and organisms, and transporting molecules from one location to another. Proteins differ from one another primarily in their sequence of amino acids, which is dictated by the nucleotide sequence of their genes, and which usually results in protein folding into a specific 3D structure that determines its activity.
 A linear chain of amino acid residues is called a polypeptide. A protein contains at least one long polypeptide. Short polypeptides, containing less than 20–30 residues, are rarely considered to be proteins and are commonly called peptides, or sometimes oligopeptides. The individual amino acid residues are bonded together by peptide bonds and adjacent amino acid residues. The sequence of amino acid residues in a protein is defined by the sequence of a gene, which is encoded in the genetic code. In general, the genetic code specifies 20 standard amino acids; but in certain organisms the genetic code can include selenocysteine and—in certain archaea—pyrrolysine. Shortly after or even during synthesis, the residues in a protein are often chemically modified by post-translational modification, which alters the physical and chemical properties, folding, stability, activity, and ultimately, the function of the proteins. Some proteins have non-peptide groups attached, which can be called prosthetic groups or cofactors. Proteins can also work together to achieve a particular function, and they often associate to form stable protein complexes.
 Once formed, proteins only exist for a certain period and are then degraded and recycled by the cell's machinery through the process of protein turnover. A protein's lifespan is measured in terms of its half-life and covers a wide range. They can exist for minutes or years with an average lifespan of 1–2 days in mammalian cells. Abnormal or misfolded proteins are degraded more rapidly either due to being targeted for destruction or due to being unstable.
 Like other biological macromolecules such as polysaccharides and nucleic acids, proteins are essential parts of organisms and participate in virtually every process within cells. Many proteins are enzymes that catalyse biochemical reactions and are vital to metabolism. Proteins also have structural or mechanical functions, such as actin and myosin in muscle and the proteins in the cytoskeleton, which form a system of scaffolding that maintains cell shape. Other proteins are important in cell signaling, immune responses, cell adhesion, and the cell cycle. In animals, proteins are needed in the diet to provide the essential amino acids that cannot be synthesized. Digestion breaks the proteins down for use in the metabolism.
 Proteins may be purified from other cellular components using a variety of techniques such as ultracentrifugation, precipitation, electrophoresis, and chromatography; the advent of genetic engineering has made possible a number of methods to facilitate purification. Methods commonly used to study protein structure and function include immunohistochemistry, site-directed mutagenesis, X-ray crystallography, nuclear magnetic resonance and mass spectrometry.
"
Protist,Biology,5,"A protist (/ˈproʊtɪst/) is any eukaryotic organism (that is, an organism whose cells contains a cell nucleus) that is not an animal, plant, or fungus. While it is likely that protists share a common ancestor (the last eukaryotic common ancestor),[2] the exclusion of other eukaryotes means that protists do not form a natural group, or clade.[a] So some protists may be more closely related to animals, plants, or fungi than they are to other protists; however, like algae, invertebrates, or protozoans, the grouping is used for convenience. The study of protists is termed protistology.[3] Supergroups[1] and typical phyla
 Many others;classification varies
 The classification of a kingdom separate from animals and plants was first proposed by John Hogg in 1860 as the kingdom Protoctista; in 1866 Ernst Haeckel also proposed a third kingdom Protista as ""the kingdom of primitive forms"".[4]  Originally these also included prokaryotes, but with time these would be removed to a fourth kingdom Monera.[b] In the popular five-kingdom scheme proposed by Robert Whittaker in 1969, Protista was defined as eukaryotic ""organisms which are unicellular or unicellular-colonial and which form no tissues"", and the fifth kingdom Fungi was established.[5][6][c] In the five-kingdom system of Lynn Margulis, the term protist is reserved for microscopic organisms, while the more inclusive kingdom Protoctista (or protoctists) included certain large multicellular eukaryotes, such as kelp, red algae and slime molds.[9] Others use the term protist interchangeably with Margulis's protoctist, to encompass both single-celled and multicellular eukaryotes, including those that form specialized tissues but do not fit into any of the other traditional kingdoms.[10] Besides their relatively simple levels of organization, protists do not necessarily have much in common.[11] When used, the term ""protists"" is now considered to mean a paraphyletic assemblage of similar-appearing but diverse taxa (biological groups); these taxa do not have an exclusive common ancestor beyond being composed of eukaryotes, and have different life cycles, trophic levels, modes of locomotion and cellular structures.[12][13] Examples of protists include:[14] amoebas (including nucleariids and Foraminifera); choanaflagellates; ciliates; diatoms; dinoflagellates; Giardia; Plasmodium (which causes malaria); oomycetes (including Phytophthora, the cause of the Great Famine of Ireland); and slime molds. These examples are unicellular, although oomycetes can form filaments, and slime molds can aggregate.
 In cladistic systems (classifications based on common ancestry), there are no equivalents to the taxa Protista or Protoctista, as both terms refer to a paraphyletic group that spans the entire eukaryotic tree of life. In cladistic classification, the contents of Protista are mostly distributed among various supergroups: examples include the SAR supergroup (of stramenopiles or heterokonts, alveolates, and Rhizaria); Archaeplastida (or Plantae sensu lato); Excavata (which is mostly unicellular flagellates); and Opisthokonta (which commonly includes unicellular flagellates, but also animals and fungi). ""Protista"", ""Protoctista"", and ""Protozoa"" are therefore considered obsolete. However, the term ""protist"" continues to be used informally as a catch-all term for eukayotic organisms that aren't within other traditional kingdoms. For example, the word ""protist pathogen"" may be used to denote any disease-causing organism that is not plant, animal, fungal, prokaryotic, viral, or subviral.[15]"
Psychobiology,Biology,5,"Behavioral neuroscience, also known as biological psychology,[1] biopsychology, or psychobiology,[2] is the application of the principles of biology to the study of physiological, genetic, and developmental mechanisms of behavior in humans and other animals.[3]"
Regeneration_(biology),Biology,5,"In biology, regeneration is the process of renewal, restoration, and tissue growth that makes genomes, cells, organisms, and ecosystems resilient to natural fluctuations or events that cause disturbance or damage.[1] Every species is capable of regeneration, from bacteria to humans.[2][3] Regeneration can either be complete[4] where the new tissue is the same as the lost tissue,[4] or incomplete[5] where after the necrotic tissue comes fibrosis.[5] At its most elementary level, regeneration is mediated by the molecular processes of gene regulation and involves the cellular processes of cell proliferation, morphogenesis and cell differentiation.[6][7] Regeneration in biology, however, mainly refers to the morphogenic processes that characterize the phenotypic plasticity of traits allowing multi-cellular organisms to repair and maintain the integrity of their physiological and morphological states. Above the genetic level, regeneration is fundamentally regulated by asexual cellular processes.[8] Regeneration is different from reproduction. For example, hydra perform regeneration but reproduce by the method of budding.
 The hydra and the planarian flatworm have long served as model organisms for their highly adaptive regenerative capabilities.[9] Once wounded, their cells become activated and restore the organs back to their pre-existing state.[10] The Caudata (""urodeles""; salamanders and newts), an order of tailed amphibians, is possibly the most adept vertebrate group at regeneration, given their capability of regenerating limbs, tails, jaws, eyes and a variety of internal structures.[2] The regeneration of organs is a common and widespread adaptive capability among metazoan creatures.[9] In a related context, some animals are able to reproduce asexually through fragmentation, budding, or fission.[8] A planarian parent, for example, will constrict, split in the middle, and each half generates a new end to form two clones of the original.[11] Echinoderms (such as the sea star), crayfish, many reptiles, and amphibians exhibit remarkable examples of tissue regeneration. The case of autotomy, for example, serves as a defensive function as the animal detaches a limb or tail to avoid capture. After the limb or tail has been autotomized, cells move into action and the tissues will regenerate.[12][13][14] In some cases a shed limb can itself regenerate a new individual.[15] Limited regeneration of limbs occurs in most fishes and salamanders, and tail regeneration takes place in larval frogs and toads (but not adults). The whole limb of a salamander or a triton will grow again and again after amputation. In reptiles, chelonians, crocodilians and snakes are unable to regenerate lost parts, but many (not all) kinds of lizards, geckos and iguanas possess regeneration capacity in a high degree. Usually, it involves dropping a section of their tail and regenerating it as part of a defense mechanism. While escaping a predator, if the predator catches the tail, it will disconnect.[16]"
Reproduction,Biology,5,"
 Reproduction (or procreation or breeding) is the biological process by which new individual organisms – ""offspring"" – are produced from their ""parents"". Reproduction is a fundamental feature of all known life; each individual organism exists as the result of reproduction. There are two forms of reproduction: asexual and sexual.
 In asexual reproduction, an organism can reproduce without the involvement of another organism. Asexual reproduction is not limited to single-celled organisms. The cloning of an organism is a form of asexual reproduction. By asexual reproduction, an organism creates a genetically similar or identical copy of itself. The evolution of sexual reproduction is a major puzzle for biologists. The two-fold cost of sexual reproduction is that only 50% of organisms reproduce[1] and organisms only pass on 50% of their genes.[2] Sexual reproduction typically requires the sexual interaction of two specialized organisms, called gametes, which contain half the number of chromosomes of normal cells and are created by meiosis, with typically a male fertilizing a female of the same species to create a fertilized zygote. This produces offspring organisms whose genetic characteristics are derived from those of the two parental organisms.
"
Reproductive_biology,Biology,5,"Reproductive biology includes both sexual and asexual reproduction.[1][2] Reproductive biology includes a wide number of fields:
"
Ribonucleic_acid,Biology,5,"
 Ribonucleic acid (RNA) is a polymeric molecule essential in various biological roles in coding, decoding, regulation and expression of genes. RNA and DNA are nucleic acids. Along with lipids, proteins, and carbohydrates, nucleic acids constitute one of the four major macromolecules essential for all known forms of life. Like DNA, RNA is assembled as a chain of nucleotides, but unlike DNA, RNA is found in nature as a single strand folded onto itself, rather than a paired double strand. Cellular organisms use messenger RNA (mRNA) to convey genetic information (using the nitrogenous bases of guanine, uracil, adenine, and cytosine, denoted by the letters G, U, A, and C) that directs synthesis of specific proteins. Many viruses encode their genetic information using an RNA genome.
 Some RNA molecules play an active role within cells by catalyzing biological reactions, controlling gene expression, or sensing and communicating responses to cellular signals. One of these active processes is protein synthesis, a universal function in which RNA molecules direct the synthesis of proteins on ribosomes. This process uses transfer RNA (tRNA) molecules to deliver amino acids to the ribosome, where ribosomal RNA (rRNA) then links amino acids together to form coded proteins.
"
Ribosome,Biology,5,"Ribosomes (/ˈraɪbəˌsoʊm, -boʊ-/[1]) are macromolecular machines, found within all living cells, that perform biological protein synthesis (mRNA translation). Ribosomes link amino acids together in the order specified by the codons of messenger RNA (mRNA) molecules to form polypeptide chains. Ribosomes consist of two major components: the small and large ribosomal subunits. Each subunit consists of one or more ribosomal RNA (rRNA) molecules and many ribosomal proteins (RPs or r-proteins).[2][3][4] The ribosomes and associated molecules are also known as the translational apparatus.
"
RNA,Biology,5,"
 Ribonucleic acid (RNA) is a polymeric molecule essential in various biological roles in coding, decoding, regulation and expression of genes. RNA and DNA are nucleic acids. Along with lipids, proteins, and carbohydrates, nucleic acids constitute one of the four major macromolecules essential for all known forms of life. Like DNA, RNA is assembled as a chain of nucleotides, but unlike DNA, RNA is found in nature as a single strand folded onto itself, rather than a paired double strand. Cellular organisms use messenger RNA (mRNA) to convey genetic information (using the nitrogenous bases of guanine, uracil, adenine, and cytosine, denoted by the letters G, U, A, and C) that directs synthesis of specific proteins. Many viruses encode their genetic information using an RNA genome.
 Some RNA molecules play an active role within cells by catalyzing biological reactions, controlling gene expression, or sensing and communicating responses to cellular signals. One of these active processes is protein synthesis, a universal function in which RNA molecules direct the synthesis of proteins on ribosomes. This process uses transfer RNA (tRNA) molecules to deliver amino acids to the ribosome, where ribosomal RNA (rRNA) then links amino acids together to form coded proteins.
"
RNA_polymerase,Biology,5,"In molecular biology, RNA polymerase (abbreviated  RNAP or  RNApol, and officially DNA-directed RNA polymerase), is an enzyme that synthesizes RNA from a DNA template. 
 Using the enzyme helicase, RNAP locally opens the double-stranded DNA so that one strand of the exposed nucleotides can be used as a template for the synthesis of RNA, a process called transcription. A transcription factor and its associated transcription mediator complex must be attached to a DNA binding site called a promoter region before RNAP can initiate the DNA unwinding at that position. RNAP not only initiates RNA transcription, it also guides the nucleotides into position, facilitates attachment and elongation, has intrinsic proofreading and replacement capabilities, and termination recognition capability. In eukaryotes, RNAP can build chains as long as 2.4 million nucleotides.
 RNAP produces RNA that, functionally, is either for protein coding, i.e. messenger RNA (mRNA); or non-coding (so-called ""RNA genes""). At least four functional types of RNA genes exist: 
 
RNA polymerase is essential to life, and is found in all living organisms and many viruses. Depending on the organism, a RNA polymerase can be a protein complex (multi-subunit RNAP) or only consist of one subunit (single-subunit RNAP, ssRNAP), each representing an independent lineage. The former is found in bacteria, archaea, and eukaryotes alike, sharing a similar core structure and mechanism.[1] The latter is found in phages as well as eukaryotic chloroplasts and mitochondria, and is related to modern DNA polymerases.[2] Eukaryotic and archaeal RNAPs have more subunits than bacterial ones do, and are controlled differently.
 Bacteria and archaea only have one RNA polymerase. Eukaryotes have multiple types of nuclear RNAP, each responsible for synthesis of a distinct subset of RNA: 
"
Ground_tissue#Sclerenchyma,Biology,5,"The ground tissue of plants includes all tissues that are neither dermal nor vascular. It can be divided into three types based on the nature of the cell walls.
"
Seed,Biology,5,"A seed is an embryonic plant enclosed in a protective outer covering. The formation of the seed is part of the process of reproduction in seed plants, the spermatophytes, including the gymnosperm and angiosperm plants.
 Seeds are the product of the ripened ovule, after fertilization by pollen and some growth within the mother plant.  The embryo is developed from the zygote and the seed coat from the integuments of the ovule. 
 Seeds have been an important development in the reproduction and success of gymnosperm and angiosperm plants, relative to more primitive plants such as ferns, mosses and liverworts, which do not have seeds and use water-dependent means to propagate themselves.  Seed plants now dominate biological niches on land, from forests to grasslands both in hot and cold climates.
 The term ""seed"" also has a general meaning that antedates the above – anything that can be sown, e.g. ""seed"" potatoes, ""seeds"" of corn or sunflower ""seeds"". In the case of sunflower and corn ""seeds"", what is sown is the seed enclosed in a shell or husk, whereas the potato is a tuber.
 Many structures commonly referred to as ""seeds"" are actually dry fruits. Plants producing berries are called baccate. Sunflower seeds are sometimes sold commercially while still enclosed within the hard wall of the fruit, which must be split open to reach the seed. Different groups of plants have other modifications, the so-called stone fruits (such as the peach) have a hardened fruit layer (the endocarp) fused to and surrounding the actual seed. Nuts are the one-seeded, hard-shelled fruit of some plants with an indehiscent seed, such as an acorn or hazelnut.
"
Selective_breeding,Biology,5,"
 Selective breeding (also called artificial selection) is the process by which humans use animal breeding and plant breeding to selectively develop particular phenotypic traits (characteristics) by choosing which typically animal or plant males and females will sexually reproduce and have offspring together. Domesticated animals are known as breeds, normally bred by a professional breeder, while domesticated plants are known as varieties, cultigens, cultivars, or breeds.[1] Two purebred animals of different breeds produce a crossbreed, and crossbred plants are called hybrids. Flowers, vegetables and fruit-trees may be bred by amateurs and commercial or non-commercial professionals: major crops are usually the provenance of the professionals.
 In animal breeding, techniques such as inbreeding, linebreeding, and outcrossing are utilized. In plant breeding, similar methods are used. Charles Darwin discussed how selective breeding had been successful in producing change over time in his 1859 book, On the Origin of Species. Its first chapter discusses selective breeding and domestication of such animals as pigeons, cats, cattle, and dogs. Darwin used artificial selection as a springboard to introduce and support the theory of natural selection.[2] The deliberate exploitation of selective breeding to produce desired results has become very common in agriculture and experimental biology.
 Selective breeding can be unintentional, e.g., resulting from the process of human cultivation; and it may also produce unintended – desirable or undesirable – results. For example, in some grains, an increase in seed size may have resulted from certain ploughing practices rather than from the intentional selection of larger seeds. Most likely, there has been an interdependence between natural and artificial factors that have resulted in plant domestication.[3]"
Sessility_(motility),Biology,5,"Sessility is the biological property of an organism describing its lack of a means of self-locomotion. Sessile organisms for which natural motility is absent are normally immobile. This is distinct from the botanical meaning of sessility, which refers to an organism or biological structure attached directly by its base without a stalk.
 Sessile organisms can move via external forces (such as water currents), but are usually permanently attached to something. Organisms such as corals lay down their own substrate from which they grow.  Other sessile organisms grow from a solid such as a rock, dead tree trunk, or a man-made object such as a buoy or ship's hull.[1]"
Sex,Biology,5,"
 
 Organisms of many species are specialized into male and female varieties, each known as a sex.[1][2] Sexual reproduction involves the combining and mixing of genetic traits: specialized cells known as gametes combine to form offspring that inherit traits from each parent. The gametes produced by an organism define its sex: males produce small gametes (e.g. spermatozoa, or sperm, in animals) while females produce large gametes (ova, or egg cells). Individual organisms which produce both male and female gametes are termed hermaphroditic.[2][3] Gametes can be identical in form and function (known as isogamy), but, in many cases, an asymmetry has evolved such that two different types of gametes (heterogametes) exist (known as anisogamy).[4][5] Physical differences are often associated with the different sexes of an organism; these sexual dimorphisms can reflect the different reproductive pressures the sexes experience. For instance, mate choice and sexual selection can accelerate the evolution of physical differences between the sexes.
 Among humans and other mammals, males typically carry an X and a Y chromosome (XY), whereas females typically carry two X chromosomes (XX), which are a part of the XY sex-determination system. Other animals have various sex-determination systems, such as the ZW system in birds, the X0 system in insects, and various environmental systems, for example in reptiles and crustaceans. Fungi may also have more complex allelic mating systems, with sexes not accurately described as male, female, or hermaphroditic.[6]"
Sexual_reproduction,Biology,5,"Sexual reproduction is a type of reproduction that involves a complex life cycle in which a gamete (such as a sperm or egg cell) with a single set of chromosomes (haploid) combines with another to produce an organism composed of cells with two sets of chromosomes (diploid).[1] Sexual reproduction is the most common life cycle in multicellular eukaryotes, such as animals, fungi and plants. Sexual reproduction does not occur in prokaryotes (organisms without cell nuclei), but they have processes with similar effects such as bacterial conjugation, transformation and transduction, which may have been precursors to sexual reproduction in early eukaryotes.
 In the production of sex cells in eukaryotes, diploid mother cells divide to produce haploid cells known as gametes in a process called meiosis that involves genetic recombination. The homologous chromosomes pair up so that their DNA sequences are aligned with each other, and this is followed by exchange of genetic information between them. Two rounds of cell division then produce four haploid gametes, each with half the number of chromosomes from each parent cell, but with the genetic information in the parental chromosomes recombined. Two haploid gametes combine into one diploid cell known as a zygote in a process called fertilisation. The zygote incorporates genetic material from both gametes. Multiple cell divisions, without change of the number of chromosomes, then form a multicellular diploid phase or generation. 
 In human reproduction, each cell contains 46 chromosomes in 23 pairs. Meiosis in the parents' gonads produces gametes that each contain only 23 chromosomes that are genetic recombinants of the DNA sequences contained in the parental chromosomes. When the nuclei of the gametes come together to form a fertilized egg or zygote, each cell of the resulting child will have 23 chromosomes from each parent, or 46 in total.[2][3] In plants only, the diploid phase, known as the sporophyte, produces spores by meiosis that germinate and then divide by mitosis to form a haploid multicellular phase, the gametophyte, that produces gametes directly by mitosis. This type of life cycle, involving alternation between two multicellular phases, the sexual haploid gametophyte and asexual diploid sporophyte, is known as alternation of generations.
 The evolution of sexual reproduction is considered paradoxical,[3] because asexual reproduction should be able to outperform it as every young organism created can bear its own young. This implies that an asexual population has an intrinsic capacity to grow more rapidly with each generation.[4] This 50% cost is a fitness disadvantage of sexual reproduction.[5] The two-fold cost of sex includes this cost and the fact that any organism can only pass on 50% of its own genes to its offspring. One definite advantage of sexual reproduction is that it impedes the accumulation of genetic mutations.[6] Sexual selection is a mode of natural selection in which some individuals out-reproduce others of a population because they are better at securing mates for sexual reproduction.[7][8] It has been described as ""a powerful evolutionary force that does not exist in asexual populations.""[9]"
Sociality,Biology,5,"Sociality is the degree to which individuals in an animal population tend to associate in social groups (gregariousness) and form cooperative societies.
 Sociality is a survival response to evolutionary pressures.[1] For example, when a mother wasp stays near her larvae in the nest, parasites are less likely to eat the larvae.[2] Biologists suspect that pressures from parasites and other predators selected this behavior in wasps of the family Vespidae.
 This wasp behaviour evidences the most fundamental characteristic of animal sociality: parental investment. Parental investment is any expenditure of resources (time, energy, social capital) to benefit one's offspring. Parental investment detracts from a parent's capacity to invest in future reproduction and aid to kin (including other offspring). An animal that cares for its young but shows no other sociality traits is said to be subsocial.
 An animal that exhibits a high degree of sociality is called a social animal. The highest degree of sociality recognized by sociobiologists is eusociality. A eusocial taxon is one that exhibits overlapping adult generations, reproductive division of labor, cooperative care of young, and—in the most refined cases—a biological caste system.
"
Sociobiology,Biology,5,"Sociobiology is a field of biology that aims to examine and explain social behavior in terms of evolution. It draws from disciplines including psychology, ethology, anthropology, evolution, zoology, archaeology, and population genetics. Within the study of human societies, sociobiology is closely allied to evolutionary anthropology, human behavioral ecology and evolutionary psychology.[1] Sociobiology investigates social behaviors such as mating patterns, territorial fights, pack hunting, and the hive society of social insects. It argues that just as selection pressure led to animals evolving useful ways of interacting with the natural environment, so also it led to the genetic evolution of advantageous social behavior.[2] While the term ""sociobiology"" originated at least as early as the 1940s, the concept did not gain major recognition until the publication of E. O. Wilson's book Sociobiology: The New Synthesis in 1975. The new field quickly became the subject of controversy.  Critics, led by Richard Lewontin and Stephen Jay Gould, argued that genes played a role in human behavior, but that traits such as aggressiveness  could be explained by social environment rather than by biology. Sociobiologists responded by pointing to the complex relationship between nature and nurture.
"
Soil_biology,Biology,5,"Soil biology is the study of microbial and faunal activity and ecology in soil.
Soil life, soil biota, soil fauna, or edaphon is a collective term that encompasses all organisms that spend a significant portion of their life cycle within a soil profile, or at the  soil-litter interface.
These organisms include earthworms, nematodes, protozoa, fungi, bacteria, different arthropods, as well as some reptiles (such as snakes), and species of burrowing mammals like gophers, moles and prairie dogs. Soil biology plays a vital role in determining many soil characteristics. The decomposition of organic matter by soil organisms has an immense influence on soil fertility, plant growth, soil structure, and carbon storage. As a relatively new science, much remains unknown about soil biology and its effect on soil ecosystems.  
"
Species,Biology,5,"
 In biology, a species is the basic unit of classification and a taxonomic rank of an organism, as well as a unit of biodiversity. A species is often defined as the largest group of organisms in which any two individuals of the appropriate sexes or mating types can produce fertile offspring, typically by sexual reproduction. Other ways of defining species include their karyotype, DNA sequence, morphology, behaviour or ecological niche. In addition, paleontologists use the concept of the chronospecies since fossil reproduction cannot be examined.
 The total number of species is estimated to be between 8 and 8.7 million.[1][2] However the vast majority of them are not studied or documented and it may take over 1000 years to fully catalogue them all.[3] All species (except viruses) are given a two-part name, a ""binomial"". The first part of a binomial is the genus to which the species belongs. The second part is called the specific name or the specific epithet (in botanical nomenclature, also sometimes in zoological nomenclature). For example, Boa constrictor is one of four species of the genus Boa, with constrictor being the species’ epithet.
 While the definitions given above may seem adequate at first glance, when looked at more closely they represent problematic species concepts. For example, the boundaries between closely related species become unclear with hybridisation, in a species complex of hundreds of similar microspecies, and in a ring species. Also, among organisms that reproduce only asexually, the concept of a reproductive species breaks down, and each clone is potentially a microspecies. Although none of these are entirely satisfactory definitions, and while the concept of species may not be a perfect model of life, it is still an incredibly useful tool to scientists and conservationists for studying life on Earth, regardless of the theoretical difficulties. If species were fixed and clearly distinct from one another, there would be no problem, but evolutionary processes cause species to change continually, and to gradate into one another.
 Species were seen from the time of Aristotle until the 18th century as fixed categories that could be arranged in a hierarchy, the great chain of being. In the 19th century, biologists grasped that species could evolve given sufficient time. Charles Darwin's 1859 book On the Origin of Species explained how species could arise by natural selection. That understanding was greatly extended in the 20th century through genetics and population ecology. Genetic variability arises from mutations and recombination, while organisms themselves are mobile, leading to geographical isolation and genetic drift with varying selection pressures. Genes can sometimes be exchanged between species by horizontal gene transfer; new species can arise rapidly through hybridisation and polyploidy; and species may become extinct for a variety of reasons. Viruses are a special case, driven by a balance of mutation and selection, and can be treated as quasispecies.
"
Speciation,Biology,5,"Speciation is the evolutionary process by which populations evolve to become distinct species. The biologist Orator F. Cook coined the term in 1906 for cladogenesis, the splitting of lineages, as opposed to anagenesis, phyletic evolution within lineages.[1][2][3] Charles Darwin was the first to describe the role of natural selection in speciation in his 1859 book On the Origin of Species.[4] He also identified sexual selection as a likely mechanism, but found it problematic.
 There are four geographic modes of speciation in nature, based on the extent to which speciating populations are isolated from one another: allopatric, peripatric, parapatric, and sympatric. Speciation may also be induced artificially, through animal husbandry, agriculture, or laboratory experiments. Whether genetic drift is a minor or major contributor to speciation is the subject matter of much ongoing discussion.
 Rapid sympatric speciation can take place through polyploidy, such as by doubling of chromosome number; the result is progeny which are immediately reproductively isolated from the parent population. New species can also be created through hybridisation followed, if the hybrid is favoured by natural selection, by reproductive isolation.
"
Sperm,Biology,5,"
 Sperm is the male reproductive cell, or gamete, in anisogamous forms of sexual reproduction (forms in which there is a larger, ""female"" reproductive cell and a smaller, ""male"" one). Animals produce motile sperm with a tail known as a flagellum, which are known as spermatozoa, while some red algae and fungi produce non-motile sperm cells, known as spermatia.[1] Flowering plants contain non-motile sperm inside pollen, while some more basal plants like ferns and some gymnosperms have motile sperm.[2] Sperm cells form during the process known as spermatogenesis, which in amniotes (reptiles and mammals) takes place in the seminiferous tubules of the testes.[3] This process involves the production of several successive sperm cell precursors, starting with spermatogonia, which differentiate into spermatocytes. The spermatocytes then undergo meiosis, reducing their chromosome number by half, which produces spermatids. The spermatids then mature and, in animals, construct a tail, or flagellum, which gives rise to the mature, motile sperm cell. This whole process occurs constantly and takes around 3 months from start to finish.
 Sperm cells cannot divide and have a limited lifespan, but after fusion with egg cells during fertilisation, a new organism begins developing, starting as a totipotent zygote. The human sperm cell is haploid, so that its 23 chromosomes can join the 23 chromosomes of the female egg to form a diploid cell. In mammals, sperm is stored in the epididymis and is released from the penis during ejaculation in a fluid known as semen.
 The word sperm is derived from the Greek word σπέρμα, sperma, meaning ""seed"".
"
Spore,Biology,5,"
 In biology, a spore is a unit of sexual or asexual reproduction that may be adapted for dispersal and for survival, often for extended periods of time, in unfavourable conditions. Spores form part of the life cycles of many plants, algae, fungi and protozoa.[1] Bacterial spores are not part of a sexual cycle but are resistant structures used for survival under unfavourable conditions. Myxozoan spores release amoebulae into their hosts for parasitic infection, but also reproduce within the hosts through the pairing of two nuclei within the plasmodium, which develops from the amoebula.[2] Spores are usually haploid and unicellular and are produced by meiosis in the sporangium of a diploid sporophyte. Under favourable conditions the spore can develop into a new organism using mitotic division, producing a multicellular gametophyte, which eventually goes on to produce gametes. Two gametes fuse to form a zygote which develops into a new sporophyte. This cycle is known as alternation of generations.
 The spores of seed plants are produced internally, and the megaspores (formed within the ovules) and the microspores are involved in the formation of more complex structures that form the dispersal units, the seeds and pollen grains.
"
Stem_cell,Biology,5,"In multicellular organisms, stem cells are undifferentiated or partially differentiated cells that can differentiate into various types of cells and proliferate indefinitely to produce more of the same stem cell. They are the earliest type of cell in a cell lineage.[1] They are found in both embryonic and adult organisms, but they have slightly different properties in each. They are usually distinguished from progenitor cells, which cannot divide indefinitely, and precursor or blast cells, which are usually committed to differentiating into one cell type.
 In mammals, roughly 50–150 cells make up the inner cell mass during the blastocyst stage of embryonic development, around days 5–14. These have stem-cell capability. In vivo, they eventually differentiate into all of the body's cell types (making them pluripotent). This process starts with the differentiation into the three germ layers – the ectoderm, mesoderm and endoderm – at the gastrulation stage. However, when they are isolated and cultured in vitro, they can be kept in the stem-cell stage and are known as embryonic stem cells (ESCs).
 Adult stem cells are found in a few select locations in the body, known as niches, such as those in the bone marrow or gonads. They exist to replenish rapidly lost cell types and are multipotent or unipotent, meaning they only differentiate into a few cell types or one cell type. In mammals, they include, among others, hematopoietic stem cells, which replenish blood and immune cells, basal cells, which maintain the skin epithelium, and mesenchymal stem cells, which maintain bone, cartilage, muscle and fat cells. Adult stem cells are a small minority of cells; they are vastly outnumbered by the progenitor cells and terminally differentiated cells that they differentiate into.[1] Research into stem cells grew out of findings by Canadian biologists Ernest A. McCulloch, James E. Till and Andrew J. Becker at the University of Toronto in the 1960s.[2][3] As of 2016[update], the only established medical therapy using stem cells is hematopoietic stem cell transplantation,[4] first performed in 1958 by French oncologist Georges Mathé. Since 1998 however, it has been possible to culture and differentiate human embryonic stem cells (in stem-cell lines). The process of isolating these cells has been controversial, because it typically results in the destruction of the embryo. Sources for isolating ESCs have been restricted in some European countries and Canada, but others such as the UK and China have promoted the research.[5] Somatic cell nuclear transfer is a cloning method that can be used to create a cloned embryo for the use of its embryonic stem cells in stem cell therapy.[6] In 2006, a Japanese team led by Shinya Yamanaka discovered a method to convert mature body cells back into stem cells. These were termed induced pluripotent stem cells (iPSCs).[7]"
Steroid,Biology,5,"
 A steroid is a biologically active organic compound with four rings arranged in a specific molecular configuration. Steroids have two principal biological functions: as important components of cell membranes which alter membrane fluidity; and as signaling molecules. Hundreds of steroids are found in plants, animals and fungi. All steroids are manufactured in cells from the sterols lanosterol (opisthokonts) or cycloartenol (plants). Lanosterol and cycloartenol are derived from the cyclization of the triterpene squalene.[2] The steroid core structure is typically composed of seventeen carbon atoms, bonded in four ""fused"" rings: three six-member cyclohexane rings (rings A, B and C in the first illustration) and one five-member cyclopentane ring (the D ring). Steroids vary by the functional groups attached to this four-ring core and by the oxidation state of the rings. Sterols are forms of steroids with a hydroxy group at position three and a skeleton derived from cholestane.[1]:1785f[3] Steroids can also be more radically modified, such as by changes to the ring structure, for example, cutting one of the rings. Cutting Ring B produces secosteroids one of which is vitamin D3.
 Examples include the lipid cholesterol, the sex hormones estradiol and testosterone,[4]:10–19 and the anti-inflammatory drug dexamethasone.[5]"
Strain_(biology),Biology,5,"In biology, a strain is a genetic variant, a subtype or a culture within a biological species. Strains are often seen as inherently artificial concepts, characterized by a specific intent for genetic isolation.[1] This is most easily observed in microbiology where strains are derived from a single cell colony and are typically quarantined by the physical constraints of a Petri dish. Strains are also commonly referred to within virology, botany, and with rodents used in experimental studies.
"
Structural_biology,Biology,5,"Structural biology is a branch of molecular biology, biochemistry, and biophysics concerned with the molecular structure of biological macromolecules (especially proteins, made up of amino acids, RNA or DNA, made up of nucleotides, membranes, made up of lipids) how they acquire the structures they have, and how alterations in their structures affect their function.[1] This subject is of great interest to biologists because macromolecules carry out most of the functions of cells, and it is only by coiling into specific three-dimensional shapes that they are able to perform these functions. This architecture, the ""tertiary structure"" of molecules, depends in a complicated way on each molecule's basic composition, or ""primary structure.""
 Biomolecules are too small to see in detail even with the most advanced light microscopes. The methods that structural biologists use to determine their structures generally involve measurements on vast numbers of identical molecules at the same time. These methods include:
 Most often researchers use them to study the ""native states"" of macromolecules. But variations on these methods are also used to watch nascent or denatured molecules assume or reassume their native states. See protein folding.
 A third approach that structural biologists take to understanding structure is bioinformatics to look for patterns among the diverse sequences that give rise to particular shapes. Researchers often can deduce aspects of the structure of integral membrane proteins based on the membrane topology predicted by hydrophobicity analysis. See protein structure prediction.
 In the past few years it has become possible for highly accurate physical molecular models to complement the in silico study of biological structures.  Examples of these models can be found in the Protein Data Bank.
 Computational techniques like Molecular Dynamics simulations can be used in conjunction with empirical structure determination strategies to extend and study protein structure, conformation and function.[2]"
Symbiogenesis,Biology,5,"Symbiogenesis, or endosymbiotic theory, is the leading evolutionary theory of the origin of eukaryotic cells from prokaryotic organisms.[1] The theory holds that mitochondria, plastids such as chloroplasts, and possibly other organelles of eukaryotic cells are descended from formerly free-living prokaryotes (more closely related to bacteria than archaea) taken one inside the other in endosymbiosis.
 Mitochondria appear to be phylogenetically related to Rickettsiales proteobacteria, and chloroplasts to nitrogen-fixing filamentous cyanobacteria. The theory was first articulated in 1905 and 1910 by the Russian botanist Konstantin Mereschkowski, and advanced and substantiated with microbiological evidence by Lynn Margulis in 1967. Among the many lines of evidence supporting symbiogenesis are that new mitochondria and plastids are formed only through binary fission, and that cells cannot create new ones otherwise; that the transport proteins called porins are found in the outer membranes of mitochondria, chloroplasts and bacterial cell membranes; that cardiolipin is found only in the inner mitochondrial membrane and bacterial cell membranes; and that some mitochondria and plastids contain single circular DNA molecules similar to the chromosomes of bacteria.
"
Symbiont,Biology,5,"Symbiosis (from Greek συμβίωσις, sumbíōsis, ""living together"", from σύν, sún, ""together"", and βίωσις, bíōsis, ""living"")[2] is any type of a close and long-term biological interaction between two different biological organisms, be it mutualistic, commensalistic, or parasitic. The organisms, each termed a symbiont, may be of the same or of different species. In 1879, Heinrich Anton de Bary defined it as ""the living together of unlike organisms"". The term was subject to a century-long debate about whether it should specifically denote mutualism, as in lichens. Biologists have now abandoned that restriction.
 Symbiosis can be obligatory, which means that one or more of the symbionts entirely depend on each other for survival, or facultative (optional), when they can generally live independently.
 Symbiosis is also classified by physical attachment.  When symbionts form a single body it is called conjunctive symbiosis, while all other arrangements are called disjunctive symbiosis.[3] When one organism lives on the surface of another, such as head lice on humans, it is called ectosymbiosis; when one partner lives inside the tissues of another, such as Symbiodinium within coral, it is termed endosymbiosis.[4][5]"
Symbiosis,Biology,5,"Symbiosis (from Greek συμβίωσις, sumbíōsis, ""living together"", from σύν, sún, ""together"", and βίωσις, bíōsis, ""living"")[2] is any type of a close and long-term biological interaction between two different biological organisms, be it mutualistic, commensalistic, or parasitic. The organisms, each termed a symbiont, may be of the same or of different species. In 1879, Heinrich Anton de Bary defined it as ""the living together of unlike organisms"". The term was subject to a century-long debate about whether it should specifically denote mutualism, as in lichens. Biologists have now abandoned that restriction.
 Symbiosis can be obligatory, which means that one or more of the symbionts entirely depend on each other for survival, or facultative (optional), when they can generally live independently.
 Symbiosis is also classified by physical attachment.  When symbionts form a single body it is called conjunctive symbiosis, while all other arrangements are called disjunctive symbiosis.[3] When one organism lives on the surface of another, such as head lice on humans, it is called ectosymbiosis; when one partner lives inside the tissues of another, such as Symbiodinium within coral, it is termed endosymbiosis.[4][5]"
Synthetic_biology,Biology,5,"Synthetic biology (SynBio) is a multidisciplinary area of research that seeks to create new biological parts, devices, and systems, or to redesign systems that are already found in nature.
 It is a branch of science that encompasses a broad range of methodologies from various disciplines, such as biotechnology, genetic engineering, molecular biology, molecular engineering, systems biology, membrane science, biophysics, chemical and biological engineering, electrical and computer engineering, control engineering and evolutionary biology.
 Due to more powerful genetic engineering capabilities and decreased DNA synthesis and sequencing costs, the field of synthetic biology is rapidly growing. In 2016, more than 350 companies across 40 countries were actively engaged in synthetic biology applications; all these companies had an estimated net worth of $3.9 billion in the global market.[1]"
Systematics,Biology,5,"Biological systematics is the study of the diversification of living forms, both past and present, and the relationships among living things through time. Relationships are visualized as evolutionary trees (synonyms: cladograms, phylogenetic trees, phylogenies).  Phylogenies have two components: branching order (showing group relationships) and branch length (showing amount of evolution).  Phylogenetic trees of species and higher taxa are used to study the evolution of traits (e.g., anatomical or molecular characteristics) and the distribution of organisms (biogeography). Systematics, in other words, is used to understand the evolutionary history of life on Earth.
 The word systematics is derived from Latin word `systema', which means systematic arrangement of organisms. Carl Linnaeus used 'Systema Naturae' as the title of his book.
"
Systems_biology,Biology,5,"Systems biology is the computational and mathematical analysis and modeling of complex biological systems. It is a biology-based interdisciplinary field of study that focuses on complex interactions within biological systems, using a holistic approach (holism instead of the more traditional reductionism) to biological research.[1] When it is crossing the field of systems theory and the applied mathematics methods, it develops into the sub-branch of complex systems biology.
 Particularly from year 2000 onwards, the concept has been used widely in biology in a variety of contexts. The Human Genome Project is an example of applied systems thinking in biology which has led to new, collaborative ways of working on problems in the biological field of genetics.[2] One of the aims of systems biology is to model and discover emergent properties, properties of cells, tissues and organisms functioning as a system whose theoretical description is only possible using techniques of systems biology.[3][1] These typically involve metabolic networks or cell signaling networks.[4][1]"
T_cell,Biology,5,"A T cell is a type of lymphocyte.  The T cell is originated from hematopoietic stem cells,[1] which are found in the bone marrow; however, the T cell matures in the thymus gland (hence the name) and plays a central role in the immune response. T cells can be distinguished from other lymphocytes by the presence of a T-cell receptor on the cell surface. These immune cells originate as precursor cells, derived from bone marrow,[2] and develop into several distinct types of T cells once they have migrated to the thymus gland. T cell differentiation continues even after they have left the thymus.
 Groups of specific, differentiated T cells have an important role in controlling and shaping the immune response by providing a variety of immune-related functions. One of these functions is immune-mediated cell death, and it is carried out by T cells in several ways: CD8+ T cells, also known as ""killer cells"", are cytotoxic – this means that they are able to directly kill virus-infected cells as well as cancer cells. CD8+ T cells are also able to utilize small signalling proteins, known as cytokines, to recruit other cells when mounting an immune response. A different population of T cells, the CD4+ T cells, function as ""helper cells"". Unlike CD8+ killer T cells, these CD4+ helper T cells function by indirectly killing cells identified as foreign: they determine if and how other parts of the immune system respond to a specific, perceived threat. Helper T cells also use cytokine signalling to influence regulatory B cells directly, and other cell populations indirectly. Regulatory T cells are yet another distinct population of these cells that provide the critical mechanism of tolerance, whereby immune cells are able to distinguish invading cells from ""self"" – thus preventing immune cells from inappropriately mounting a response against oneself (which would by definition be an ""autoimmune"" response). For this reason these regulatory T cells have also been called ""suppressor"" T cells. These same self-tolerant cells are co-opted by cancer cells to prevent the recognition of, and an immune response against, tumor cells.
"
Taxon,Biology,5,"In biology, a taxon (back-formation from taxonomy; plural taxa) is a group of one or more populations of an organism or organisms seen by taxonomists to form a unit. Although neither is required, a taxon is usually known by a particular name and given a particular ranking, especially if and when it is accepted or becomes established. It is very common, however, for taxonomists to remain at odds over what belongs to a taxon and the criteria used for inclusion. If a taxon is given a formal scientific name, its use is then governed by one of the nomenclature codes specifying which scientific name is correct for a particular grouping.
 Initial attempts at classifying and ordering organisms (plants and animals) were set forth in Linnaeus's system in Systema Naturae, 10th edition, (1758)[1] as well as an unpublished work by Bernard and Antoine Laurent de Jussieu.  The idea of a unit-based system of biological classification was first made widely available in 1805 in the introduction of Jean-Baptiste Lamarck's Flore françoise, of Augustin Pyramus de Candolle's Principes élémentaires de botanique. Lamarck set out a system for the ""natural classification"" of plants. Since then, systematists continue to construct accurate classifications encompassing the diversity of life; today, a ""good"" or ""useful"" taxon is commonly taken to be one that reflects evolutionary relationships.[note 1] Many modern systematists, such as advocates of phylogenetic nomenclature, use cladistic methods that require taxa to be monophyletic (all descendants of some ancestor). Their basic unit, therefore, is the clade rather than the taxon. Similarly, among those contemporary taxonomists working with the traditional Linnean (binomial) nomenclature, few propose taxa they know to be paraphyletic.[2] An example of a well-established taxon that is not also a clade is the class Reptilia, the reptiles; birds are descendants of reptiles but are not included in the Reptilia (birds are included in the Aves).
"
Taxonomy_(biology),Biology,5,"
 In biology, taxonomy (from Ancient Greek  τάξις (taxis) 'arrangement', and  -νομία (-nomia) 'method') is the scientific study of naming, defining (circumscribing) and classifying groups of biological organisms based on shared characteristics. Organisms are grouped into taxa (singular: taxon) and these groups are given a taxonomic rank; groups of a given rank can be aggregated to form a super-group of higher rank, thus creating a taxonomic hierarchy. The principal ranks in modern use are domain, kingdom, phylum (division is sometimes used in botany in place of phylum), class, order, family, genus, and species. The Swedish botanist Carl Linnaeus is regarded as the founder of the current system of taxonomy, as he developed a system known as Linnaean taxonomy for categorizing organisms and binomial nomenclature for naming organisms.
 With the advent of such fields of study as phylogenetics, cladistics, and systematics, the Linnaean system has progressed to a system of modern biological classification based on the evolutionary relationships between organisms, both living and extinct.
"
Telophase,Biology,5,"Telophase (from the Greek τέλος (télos), ""end"" and φάσις (phásis), ""stage"") is the final stage in both meiosis and mitosis in a eukaryotic cell. During telophase, the effects of prophase and prometaphase (the nucleolus and nuclear membrane disintegrating) are reversed. As chromosomes reach the cell poles, a nuclear envelope is re-assembled around each set of chromatids, the nucleoli reappear, and chromosomes begin to decondense back into the expanded chromatin that is present during interphase. The mitotic spindle is disassembled and remaining spindle microtubules are depolymerized. Telophase accounts for approximately 2% of the cell cycle's duration.
 Cytokinesis typically begins before late telophase[1] and, when complete, segregates the two daughter nuclei between a pair of separate daughter cells.
 Telophase is primarily driven by the dephosphorylation of mitotic cyclin-dependent kinase (Cdk) substrates.[2]"
Testosterone,Biology,5,"
 Testosterone is the primary sex hormone and anabolic steroid in males.[3] In male humans, testosterone plays a key role in the development of male reproductive tissues such as testes and prostate, as well as promoting secondary sexual characteristics such as increased muscle and bone mass, and the growth of body hair.[4] In addition, testosterone is involved in health and well-being,[5] and the prevention of osteoporosis.[6] Insufficient levels of testosterone in men may lead to abnormalities including frailty and bone loss.
 Testosterone is a steroid from the androstane class containing a keto and hydroxyl groups at positions three and seventeen respectively. It is biosynthesized in several steps from cholesterol and is converted in the liver to inactive metabolites.[7]  It exerts its action through binding to and activation of the androgen receptor.[7] In humans and most other vertebrates, testosterone is secreted primarily by the testicles of males and, to a lesser extent, the ovaries of females. On average, in adult males, levels of testosterone are about seven to eight times as great as in adult females.[8] As the metabolism of testosterone in males is more pronounced, the daily production is about 20 times greater in men.[9][10] Females are also more sensitive to the hormone.[11] In addition to its role as a natural hormone, testosterone is used as a medication in the treatment of male hypogonadism, breast cancer in women, and as part of transgender hormone therapy for transgender men.[12] Since testosterone levels decrease as men age, testosterone is sometimes used in older men to counteract this deficiency. It is also used illicitly to enhance physique and performance, for instance in athletes.[13]"
Thymine,Biology,5,"Thymine /ˈθaɪmɪn/ (T, Thy) is one of the four nucleobases in the nucleic acid of DNA that are represented by the letters G–C–A–T. The others are adenine, guanine, and cytosine.  Thymine is also known as 5-methyluracil, a pyrimidine nucleobase. In RNA, thymine is replaced by the nucleobase uracil.  Thymine was first isolated in 1893 by Albrecht Kossel and Albert Neumann from calves' thymus glands, hence its name.[1]"
Tissue_(biology),Biology,5,"In biology, tissue is a cellular organizational level between cells and a complete organ. A tissue is an ensemble of similar cells and their extracellular matrix from the same origin that together carry out a specific function. Organs are then formed by the functional grouping together of multiple tissues.
 The English word ""tissue"" derives from the French word ""tissue"", meaning that something that is ""woven"", from the verb tisse, ""to weave"".
 The study of human and animal tissues is known as histology or, in connection with disease, as histopathology. For plants, the discipline is called plant anatomy. The classical tools for studying tissues are the paraffin block in which tissue is embedded and then sectioned, the  histological stain, and the optical microscope. Developments in electron microscopy, immunofluorescence, and the use of frozen tissue-sections have enhanced the detail that can be observed in tissues. With these tools, the classical appearances of tissues can be examined in health and disease, enabling considerable refinement of medical diagnosis and prognosis.
"
Phenotypic_trait,Biology,5,"A phenotypic trait[1][2], simply trait, or character state[3][4] is a distinct variant of a phenotypic characteristic of an organism; it may be either inherited or  determined environmentally, but typically occurs as a combination of the two.[5]  For example, eye color is a character of an organism,   while blue, brown and hazel are traits.
"
Transcription_(genetics),Biology,5,"Transcription is the first of several steps of DNA based gene expression in which a particular segment of DNA is copied into RNA (especially mRNA) by the enzyme RNA polymerase.
 Both DNA and RNA are nucleic acids, which use base pairs of nucleotides as a complementary language.  During transcription, a DNA sequence is read by an RNA polymerase, which produces a complementary, antiparallel RNA strand called a primary transcript.
 Transcription proceeds in the following general steps:
 The stretch of DNA transcribed into an RNA molecule is called a transcription unit and encodes at least one gene. If the gene encodes a protein, the transcription produces messenger RNA (mRNA); the mRNA, in turn, serves as a template for the protein's synthesis through translation. Alternatively, the transcribed gene may encode for non-coding RNA such as microRNA, ribosomal RNA (rRNA), transfer RNA (tRNA), or enzymatic RNA molecules called ribozymes.[1] Overall, RNA helps synthesize, regulate, and process proteins; it therefore plays a fundamental role in performing functions within a cell.
 In virology, the term may also be used when referring to mRNA synthesis from an RNA molecule (i.e., RNA replication). For instance, the genome of a negative-sense single-stranded RNA (ssRNA -) virus may be template for a positive-sense single-stranded RNA (ssRNA +)[clarification needed]. This is because the positive-sense strand contains the information needed to translate the viral proteins for viral replication afterwards. This process is catalyzed by a viral RNA replicase.[2][clarification needed]"
Translation_(biology),Biology,5,"In molecular biology and genetics, translation is the process in which ribosomes in the cytoplasm or endoplasmic reticulum synthesize proteins after the process transcription of DNA to RNA in the cell's nucleus.   The entire process is called gene expression.
 In translation, messenger RNA (mRNA) is decoded in a ribosome, outside the nucleus, to produce a specific amino acid chain, or polypeptide. The polypeptide later folds into an active protein and performs its functions in the cell. The ribosome facilitates decoding by inducing the binding of complementary tRNA anticodon sequences to mRNA codons. The tRNAs carry specific amino acids that are chained together into a polypeptide as the mRNA passes through and is ""read"" by the ribosome.
 Translation proceeds in three phases: 
 In prokaryotes (bacteria and archaea), translation occurs in the cytoplasm, where the large and small subunits of the ribosome bind to the mRNA. In eukaryotes, translation occurs in the cytosol or across the membrane of the endoplasmic reticulum in a process called co-translational translocation.  In co-translational translocation, the entire ribosome/mRNA complex binds to the outer membrane of the rough endoplasmic reticulum (ER) and the new protein is synthesized and released into the ER; the newly created polypeptide can be stored inside the ER for future vesicle transport and secretion outside the cell, or immediately secreted.
 Many types of transcribed RNA, such as transfer RNA, ribosomal RNA, and small nuclear RNA, do not undergo translation into proteins.
 A number of antibiotics act by inhibiting translation. These include anisomycin, cycloheximide, chloramphenicol, tetracycline, streptomycin, erythromycin, and puromycin. Prokaryotic ribosomes have a different structure from that of eukaryotic ribosomes, and thus antibiotics can specifically target bacterial infections without any harm to a eukaryotic host's cells.
"
Trophic_level,Biology,5,"The trophic level of an organism is the position it occupies in a food web.  A food chain is a succession of organisms that eat other organisms and may, in turn, be eaten themselves. The trophic level of an organism is the number of steps it is from the start of the chain. A food web starts at trophic level 1 with primary producers such as plants, can move to herbivores at level 2, carnivores at level 3 or higher, and typically finish with apex predators at level 4 or 5.  The path along the chain can form either a one-way flow or a food ""web"".  Ecological communities with higher biodiversity form more complex trophic paths.
 The word trophic derives from the Greek τροφή (trophē) referring to food or nourishment.[1]"
Tumor,Biology,5,"A neoplasm (/ˈniːoʊplæzəm, ˈniə-/[1]) is a type of abnormal and excessive growth, called neoplasia, of tissue. The growth of a neoplasm is uncoordinated with that of the normal surrounding tissue, and persists in growing abnormally, even if the original trigger is removed.[2][3][4] This abnormal growth usually forms a mass, when it may be called a tumor.[5] ICD-10 classifies neoplasms into four main groups: benign neoplasms, in situ neoplasms, malignant neoplasms, and neoplasms of uncertain or unknown behavior.[6] Malignant neoplasms are also simply known as cancers and are the focus of oncology.
 Prior to the abnormal growth of tissue, as neoplasia, cells often undergo an abnormal pattern of growth, such as metaplasia or dysplasia.[7] However, metaplasia or dysplasia does not always progress to neoplasia and can occur in other conditions as well.[2] The word is from Ancient Greek νέος- neo (""new"") and πλάσμα plasma (""formation"", ""creation"").
"
Unicellular_organism,Biology,5,"A unicellular organism, also known  as a single-celled organism, is an organism that consists of a single cell, unlike a multicellular organism that consists of multiple cells. Unicellular organisms fall into two general categories: prokaryotic organisms and eukaryotic organisms. All prokaryotes are unicellular and are classified into bacteria and archaea. Many eukaryotes are multicellular, but many are unicellular such as protozoa, unicellular algae, and unicellular fungi. Unicellular organisms are thought to be the oldest form of life, with early protocells possibly emerging 3.8–4 billion years ago.[1][2] Although some prokaryotes live in colonies, they are not specialised cells with differing functions. These organisms live together, and each cell must carry out all life processes to survive. In contrast, even the simplest multicellular organisms have cells that depend on each other to survive.
 Most multicellular organisms have a unicellular life-cycle stage. Gametes, for example, are reproductive unicells for multicellular organisms.[3] Additionally, multicellularity appears to have evolved independently many times in the history of life.
 Some organisms are partially unicellular, like Dictyostelium discoideum. Additionally, unicellular organisms can be multinucleate, like Caulerpa, Plasmodium, and Myxogastria.
"
Uracil,Biology,5,"Uracil (/ˈjʊərəsɪl/; U) is one of the four nucleobases in the nucleic acid RNA that are represented by the letters A, G, C and U. The others are adenine (A), cytosine (C), and guanine (G). In RNA, uracil binds to adenine via two hydrogen bonds. In DNA, the uracil nucleobase is replaced by thymine. Uracil is a demethylated form of thymine.
 Uracil is a common and naturally occurring pyrimidine derivative.[2] The name ""uracil"" was coined in 1885 by the German chemist Robert Behrend, who was attempting to synthesize derivatives of uric acid.[3] Originally discovered in 1900 by Alberto Ascoli, it was isolated by hydrolysis of yeast nuclein;[4] it was also found in bovine thymus and spleen, herring sperm, and wheat germ.[5] It is a planar, unsaturated compound that has the ability to absorb light.[6] Based on 12C/13C isotopic ratios of organic compounds found in the Murchison meteorite, it is believed that uracil, xanthine, and related molecules can also be formed extraterrestrially.[7][8] In 2012, an analysis of data from the Cassini mission orbiting in the Saturn system showed that Titan's surface composition may include uracil.[9]"
Urea,Biology,5,"
 Urea, also known as carbamide, is an organic compound with chemical formula CO(NH2)2. This amide has two –NH2 groups joined by a carbonyl (C=O) functional group.
 50g/L ethanol
~4 g/L acetonitrile[3] Urea serves an important role in the metabolism of nitrogen-containing compounds by animals and is the main nitrogen-containing substance in the urine of mammals. It is a colorless, odorless solid, highly soluble in water, and practically non-toxic (LD50 is 15 g/kg for rats).[5]  Dissolved in water, it is neither acidic nor alkaline.  The body uses it in many processes, most notably nitrogen excretion. The liver forms it by combining two ammonia molecules (NH3) with a carbon dioxide (CO2) molecule in the urea cycle. Urea is widely used in fertilizers as a source of nitrogen (N) and is an important raw material for the chemical industry.
 Friedrich Wöhler's discovery, in 1828, that urea can be produced from inorganic starting materials, was an important conceptual milestone in chemistry. It showed, for the first time, that a substance, previously known only as a byproduct of life, could be synthesized in the laboratory, without biological starting materials, thereby contradicting the widely held doctrine of vitalism, which stated that only living things could produce the chemicals of life.
"
Urine,Biology,5,"
 Urine is a liquid by-product of metabolism in humans and in many other animals. Urine flows from the kidneys through the ureters to the urinary bladder. Urination results in urine being excreted from the body through the urethra.
 Cellular metabolism generates many by-products that are rich in nitrogen and must be cleared from the bloodstream, such as urea, uric acid, and creatinine. These by-products are expelled from the body during urination, which is the primary method for excreting water-soluble chemicals from the body. A urinalysis can detect nitrogenous wastes of the mammalian body.
 Urine has a role in the earth's nitrogen cycle. In balanced ecosystems, urine fertilizes the soil and thus helps plants to grow. Therefore, urine can be used as a fertilizer. Some animals use it to mark their territories. Historically, aged or fermented urine (known as lant) was also used for gunpowder production, household cleaning, tanning of leather and dyeing of textiles.
 Human urine and feces are collectively referred to as human waste or human excreta, and are managed via sanitation systems. Livestock urine and feces also require proper management if the livestock population density is high.
"
Uterus,Biology,5,"The uterus (from Latin ""uterus"", plural uteri) or womb (/wuːm/) is a major female hormone-responsive secondary sex organ of the reproductive system in humans and most other mammals.it has the shape of a Hyperbolic triangle in humans when not carrying. In the human, the lower end of the uterus, the cervix, opens into the vagina, while the upper end, the fundus, is connected to the fallopian tubes. It is within the uterus that the fetus develops during gestation. In the human embryo, the uterus develops from the paramesonephric ducts which fuse into the single organ known as a simplex uterus. The uterus has different forms in many other animals and in some it exists as two separate uteri known as a duplex uterus.
 In medicine, and related professions the term uterus is consistently used, while the Germanic-derived term womb is commonly used in everyday contexts.
"
Vacuole,Biology,5,"
 A vacuole (/ˈvækjuːoʊl/) is a membrane-bound organelle which is present in plant and fungal cells and some protist, animal[1] and bacterial cells.[2] Vacuoles are essentially enclosed compartments which are filled with water containing inorganic and organic molecules including enzymes in solution, though in certain cases they may contain solids which have been engulfed. Vacuoles are formed by the fusion of multiple membrane vesicles and are effectively just larger forms of these.[3] The organelle has no basic shape or size; its structure varies according to the requirements of the cell.
"
Vasodilation,Biology,5,"
Vasodilation is the widening of blood vessels.[1]  It results from relaxation of smooth muscle cells within the vessel walls, in particular in the large veins, large arteries, and smaller arterioles. The process is the opposite of vasoconstriction, which is the narrowing of blood vessels.
 When blood vessels dilate, the flow of blood is increased due to a decrease in vascular resistance and increase in cardiac output[further explanation needed]. Therefore, dilation of arterial blood vessels (mainly the arterioles[citation needed]) decreases blood pressure. The response may be intrinsic (due to local processes in the surrounding tissue) or extrinsic (due to hormones or the nervous system). In addition, the response may be localized to a specific organ (depending on the metabolic needs of a particular tissue, as during strenuous exercise), or it may be systemic (seen throughout the entire systemic circulation).
 Endogenous substances and drugs that cause vasodilation are termed vasodilators. Such vasoactivity is necessary for homeostasis (keeping the body running normally).
"
Vector_(epidemiology),Biology,5,"In epidemiology, a disease vector is any agent which carries and transmits an infectious pathogen into another living organism;[1][2] agents regarded as vectors are organisms, such as intermediate parasites or microbes. The first major discovery of a disease vector came from Ronald Ross on 20 August 1897. Sir Ronald Ross discovered the Malaria pathogen when he dissected a mosquito.[3]"
Vegetative_reproduction,Biology,5,"Vegetative reproduction (also known as vegetative propagation, vegetative multiplication or cloning) is any form of asexual reproduction occurring in plants in which a new plant grows from a fragment of the parent plant or a specialized reproductive structure.[1] Many plants naturally reproduce this way, but it can also be induced artificially. Horticulturalists have developed asexual propagation techniques that use vegetative plant parts to replicate plants. Success rates and difficulty of propagation vary greatly.  Monocotyledons typically lack a vascular cambium and therefore are harder to propagate.
"
Vertebrate,Biology,5,"
 Vertebrates (/ˈvɜːrtɪbrəts/) comprise all species of animals within the subphylum Vertebrata (/vɜːrtɪˈbreɪtə/) (chordates with backbones). Vertebrates represent the overwhelming majority of the phylum Chordata, with currently about 69,963 species described.[4] Vertebrates include such groups as the following:
 Ossea Batsch, 1788[3] Extant vertebrates range in size from the frog species Paedophryne amauensis, at as little as 7.7 mm (0.30 in), to the blue whale, at up to 33 m (108 ft). Vertebrates make up less than five percent of all described animal species; the rest are invertebrates, which lack vertebral columns.
 The vertebrates traditionally include the hagfish, which do not have proper vertebrae due to their loss in evolution,[5] though their closest living relatives, the lampreys, do.[6] Hagfish do, however, possess a cranium. For this reason, the vertebrate subphylum is sometimes referred to as ""Craniata"" when discussing morphology. Molecular analysis since 1992 has suggested that hagfish are most closely related to lampreys,[7] and so also are vertebrates in a monophyletic sense. Others consider them a sister group of vertebrates in the common taxon of craniata.[8] The populations of vertebrates have dropped in the past 50 years.[9]"
Vesicle_(biology_and_chemistry),Biology,5,"In cell biology, a vesicle is a structure within or outside a cell, consisting of liquid or cytoplasm enclosed by a lipid bilayer. Vesicles form naturally during the processes of secretion (exocytosis), uptake (endocytosis) and transport of materials within the plasma membrane. Alternatively, they may be prepared artificially, in which case they are called liposomes (not to be confused with lysosomes). If there is only one phospholipid bilayer, they are called unilamellar liposome vesicles; otherwise they are called multilamellar. The membrane enclosing the vesicle is also a lamellar phase, similar to that of the plasma membrane, and intracellular vesicles can fuse with the plasma membrane to release their contents outside the cell. Vesicles can also fuse with other organelles within the cell. A vesicle released from the cell is known as an extracellular vesicle.
 Vesicles perform a variety of functions. Because it is separated from the cytosol, the inside of the vesicle can be made to be different from the cytosolic environment.  For this reason, vesicles are a basic tool used by the cell for organizing cellular substances. Vesicles are involved in metabolism, transport, buoyancy control,[1] and temporary storage of food and enzymes. They can also act as chemical reaction chambers.
 The 2013 Nobel Prize in Physiology or Medicine was shared by James Rothman, Randy Schekman and Thomas Südhof for their roles in elucidating (building upon earlier research, some of it by their mentors) the makeup and function of cell vesicles, especially in yeasts and in humans, including information on each vesicle's parts and how they are assembled.  Vesicle dysfunction is thought to contribute to Alzheimer's disease, diabetes, some hard-to-treat cases of epilepsy, some cancers and immunological disorders and certain neurovascular conditions.[3][4]"
Vestigiality,Biology,5,"Vestigiality is the retention during the process of evolution of genetically determined structures or attributes that have lost some or all of the ancestral function in a given species.[1] Assessment of the vestigiality must generally rely on comparison with homologous features in related species. The emergence of vestigiality occurs by normal evolutionary processes, typically by loss of function of a feature that is no longer subject to positive selection pressures when it loses its value in a changing environment. The feature may be selected against more urgently when its function becomes definitively harmful, but if the lack of the feature provides no advantage, and its presence provides no disadvantage, the feature may not be phased out by natural selection and persist across species. 
 Examples of vestigial structures are the loss of functional wings in island-dwelling birds; the human appendix and vomeronasal organ; and the hindlimbs of the snake and whale.
"
Virology,Biology,5,"Virology is the study of viruses – submicroscopic, parasitic particles of genetic material contained in a protein coat[1][2] – and virus-like agents. It focuses on the following aspects of viruses: their structure, classification and evolution, their ways to infect and exploit host cells for reproduction, their interaction with host organism physiology and immunity, the diseases they cause, the techniques to isolate and culture them, and their use in research and therapy. Virology is a subfield of microbiology.
"
Virus,Biology,5,"
 
 A virus is a submicroscopic infectious agent that replicates only inside the living cells of an organism.[1] Viruses infect all types of life forms, from animals and plants to microorganisms, including bacteria and archaea.[2]
Since Dmitri Ivanovsky's 1892 article describing a non-bacterial pathogen infecting tobacco plants and the discovery of the tobacco mosaic virus by Martinus Beijerinck in 1898,[3] more than 6,000 virus species have been described in detail[4] of the millions of types of viruses in the environment.[5] Viruses are found in almost every ecosystem on Earth and are the most numerous type of biological entity.[6][7] The study of viruses is known as virology, a subspeciality of microbiology.
 When infected, a host cell is forced to rapidly produce thousands of identical copies of the original virus. When not inside an infected cell or in the process of infecting a cell, viruses exist in the form of independent particles, or virions, consisting of: (i) the genetic material, i.e., long molecules of DNA or RNA that encode the structure of the proteins by which the virus acts; (ii) a protein coat, the capsid, which surrounds and protects the genetic material; and in some cases (iii) an outside envelope of lipids. The shapes of these virus particles range from simple helical and icosahedral forms to more complex structures. Most virus species have virions too small to be seen with an optical microscope, as they are one-hundredth the size of most bacteria.
 The origins of viruses in the evolutionary history of life are unclear: some may have evolved from plasmids—pieces of DNA that can move between cells—while others may have evolved from bacteria. In evolution, viruses are an important means of horizontal gene transfer, which increases genetic diversity in a way analogous to sexual reproduction.[8] Viruses are considered by some biologists to be a life form, because they carry genetic material, reproduce, and evolve through natural selection, although they lack the key characteristics, such as cell structure, that are generally considered necessary criteria for life. Because they possess some but not all such qualities, viruses have been described as ""organisms at the edge of life"",[9] and as self-replicators.[10] Viruses spread in many ways. One transmission pathway is through disease-bearing organisms known as vectors: for example, viruses are often transmitted from plant to plant by insects that feed on plant sap, such as aphids; and viruses in animals can be carried by blood-sucking insects. Influenza viruses are spread by coughing and sneezing. Norovirus and rotavirus, common causes of viral gastroenteritis, are transmitted by the faecal–oral route, passed by hand-to-mouth contact or in food or water. The infectious dose of norovirus required to produce infection in humans is less than 100 particles.[11] HIV is one of several viruses transmitted through sexual contact and by exposure to infected blood. The variety of host cells that a virus can infect is called its ""host range"". This can be narrow, meaning a virus is capable of infecting few species, or broad, meaning it is capable of infecting many.[12] Viral infections in animals provoke an immune response that usually eliminates the infecting virus. Immune responses can also be produced by vaccines, which confer an artificially acquired immunity to the specific viral infection. Some viruses, including those that cause AIDS, HPV infection, and viral hepatitis, evade these immune responses and result in chronic infections. Several antiviral drugs have been developed.
"
White_blood_cell,Biology,5,"
 White blood cells (WBCs), also called leukocytes or leucocytes, are the cells of the immune system that are involved in protecting the body against both infectious disease and foreign invaders. All white blood cells are produced and derived from multipotent cells in the bone marrow known as hematopoietic stem cells. Leukocytes are found throughout the body, including the blood and lymphatic system.[1] All white blood cells have nuclei, which distinguishes them from the other blood cells, the anucleated red blood cells (RBCs) and platelets. The different white blood cell types are classified in standard ways; two pairs of broadest categories classify them either by structure (granulocytes or agranulocytes) or by cell lineage (myeloid cells or lymphoid cells). These broadest categories can be further divided into the five main types: neutrophils, eosinophils (acidophiles), basophils, lymphocytes, and monocytes.[2] These types are distinguished by their physical and functional characteristics. Monocytes and neutrophils are phagocytic. Further subtypes can be classified; for example, among lymphocytes, there are B cells (named from bursa or bone marrow cells), T cells (named from thymus cells), and natural killer cells.
 The number of leukocytes in the blood is often an indicator of disease, and thus the white blood cell count is an important subset of the complete blood count. The normal white cell count is usually between 4 × 109/L and 1.1 × 1010/L. In the US, this is usually expressed as 4,000 to 11,000 white blood cells per microliter of blood.[3] White blood cells make up approximately 1% of the total blood volume in a healthy adult,[4] making them substantially less numerous than the red blood cells at 40% to 45%. However, this 1% of the blood makes a large difference to health, because immunity depends on it. An increase in the number of leukocytes over the upper limits is called leukocytosis. It is normal when it is part of healthy immune responses, which happen frequently. It is occasionally abnormal, when it is neoplastic or autoimmune in origin. A decrease below the lower limit is called leukopenia. This indicates a weakened immune system.
"
Whole_genome_sequencing,Biology,5,"Whole genome sequencing is ostensibly the process of determining the complete DNA sequence of an organism's genome at a single time. This entails sequencing all of an organism's chromosomal DNA as well as DNA contained in the mitochondria and, for plants, in the chloroplast. In practice, genome sequences that are nearly complete are also called whole genome sequences.[2] Whole genome sequencing has largely been used as a research tool, but was being introduced to clinics in 2014.[3][4][5] In the future of personalized medicine, whole genome sequence data may be an important tool to guide therapeutic intervention.[6] The tool of gene sequencing at SNP level is also used to pinpoint functional variants from association studies and improve the knowledge available to researchers interested in evolutionary biology, and hence may lay the foundation for predicting disease susceptibility and drug response.
 Whole genome sequencing should not be confused with DNA profiling, which only determines the likelihood that genetic material came from a particular individual or group, and does not contain additional information on genetic relationships, origin or susceptibility to specific diseases.[7] In addition, whole genome sequencing should not be confused with methods that sequence specific subsets of the genome - such methods include whole exome sequencing (1-2% of the genome) or SNP genotyping (<0.1% of the genome). As of 2017 there were no complete genomes for any mammals, including humans. Between 4% to 9% of the human genome, mostly satellite DNA, had not been sequenced.[8]"
Wood,Biology,5,"
 Wood is a porous and fibrous structural tissue found in the stems and roots of trees and other woody plants. It is an organic material – a natural composite of cellulose fibers that are strong in tension and embedded in a matrix of lignin that resists compression. Wood is sometimes defined as only the secondary xylem in the stems of trees,[1] or it is defined more broadly to include the same type of tissue elsewhere such as in the roots of trees or shrubs.[citation needed] In a living tree it performs a support function, enabling woody plants to grow large or to stand up by themselves. It also conveys water and nutrients between the leaves, other growing tissues, and the roots. Wood may also refer to other plant materials with comparable properties, and to material engineered from wood, or wood chips or fiber.
 Wood has been used for thousands of years for fuel, as a construction material, for making tools and weapons, furniture and paper. More recently it emerged as a feedstock for the production of purified cellulose and its derivatives, such as cellophane and cellulose acetate.
 As of 2005, the growing stock of forests worldwide was about 434 billion cubic meters, 47% of which was commercial.[2] As an abundant, carbon-neutral[citation needed] renewable resource, woody materials have been of intense interest as a source of renewable energy. In 1991 approximately 3.5 billion cubic meters of wood were harvested. Dominant uses were for furniture and building construction.[3]"
Xanthophyll,Biology,5,"Xanthophylls (originally phylloxanthins) are yellow pigments that occur widely in nature and form one of two major divisions of the carotenoid group; the other division is formed by the carotenes. The name is from Greek xanthos (ξανθός, ""yellow"")[1] and phyllon (φύλλον, ""leaf""),[2] due to their formation of the yellow band seen in early chromatography of leaf pigments.
"
Xylem,Biology,5,"
 Xylem is one of the two types of transport tissue in vascular plants, phloem being the other. The basic function of xylem is to transport water from roots to stems and leaves, but it also transports  nutrients.[1][2] The word ""xylem"" is derived from the Greek word ξύλον (xylon), meaning ""wood""; the best-known xylem tissue is wood, though it is found throughout a plant.[3] The term was introduced by Carl Nägeli in 1858.[4][5]"
Yolk,Biology,5,"Among animals which produce eggs, the yolk (also known as the vitellus) is the nutrient-bearing portion of the egg whose primary function is to supply food for the development of the embryo.  Some types of egg contain no yolk, for example because they are laid in situations where the food supply is sufficient (such as in the body of the host of a parasitoid) or because the embryo develops in the parent's body, which supplies the food, usually through a placenta.  Reproductive systems in which the mother's body supplies the embryo directly are said to be matrotrophic; those in which the embryo is supplied by yolk are said to be lecithotrophic.  In many species, such as all birds, and most reptiles and insects, the yolk takes the form of a special storage organ constructed in the reproductive tract of the mother.  In many other animals, especially very small species such as some fish and invertebrates, the yolk material is not in a special organ, but inside the egg cell (ovum).
 As stored food, yolks are often rich in vitamins, minerals, lipids and proteins.  The proteins function partly as food in their own right, and partly in regulating the storage and supply of the other nutrients.  For example, in some species the amount of yolk in an egg cell affects the developmental processes that follow fertilization.
 The yolk is not living cell material like protoplasm, but largely passive material, that is to say deutoplasm.  The food material and associated control structures are supplied during oogenesis.  Some of the material is stored more or less in the form in which the maternal body supplied it, partly as processed by dedicated non-germ tissues in the egg, while part of the biosynthetic processing into its final form happens in the oocyte itself.[1] Apart from animals, other organisms, like algae, specially in the oogamous, can also accumulate resources in their female gametes.  In gymnosperms, the remains of the female gametophyte serve also as food supply, and in flowering plants, the endosperm.
"
Zoology,Biology,5,"Zoology (/zoʊˈɒlədʒi/)[note 1] is the branch of biology that studies the animal kingdom, including the structure, embryology, evolution, classification, habits, and distribution of all animals, both living and extinct, and how they interact with their ecosystems. The term is derived from Ancient Greek ζῷον, zōion, i.e. ""animal"" and λόγος, logos, i.e. ""knowledge, study"".[1]"
Zooplankton,Biology,5,"Zooplankton (/ˈzoʊ.əˌplæŋktən, ˈzuː(ə)-, ˈzoʊoʊ-/,[1] /ˌzoʊ.əˈplæŋktən, -tɒn/)[2] are heterotrophic (sometimes detritivorous) plankton (cf. phytoplankton). Plankton are organisms drifting in oceans, seas, and bodies of fresh water. The word zooplankton is derived from the Greek zoon (ζῴον), meaning ""animal"", and planktos (πλαγκτός), meaning ""wanderer"" or ""drifter"".[3] Individual zooplankton are usually microscopic, but some (such as jellyfish) are larger and visible to the naked eye.
"
Zygospore,Biology,5,"A zygospore is a diploid reproductive stage in the life cycle of many fungi and protists.  Zygospores are created by the nuclear fusion of haploid cells.  In fungi, zygospores are formed in zygosporangia after the fusion of specialized budding structures, from mycelia of the same (in homothallic fungi) or different mating types (in heterothallic fungi), and may be chlamydospores.[1] In many eukaryotic algae, including many species of the Chlorophyta, zygospores are formed by the fusion of unicellular gametes of different mating types.
 A zygospore remains dormant while it waits for environmental cues, such as light, moisture, heat, or chemicals secreted by plants. When the environment is favorable, the zygospore germinates, meiosis occurs, and haploid vegetative cells are released.
 In fungi, a sporangium is produced at the end of a sporangiophore that sheds spores. A fungus that forms zygospores is called a zygomycete, indicating that the class is characterized by this evolutionary development.
"
Zygote,Biology,5,"A zygote (from Greek ζυγωτός zygōtos ""joined"" or ""yoked"", from ζυγοῦν zygoun ""to join"" or ""to yoke"")[1] is a eukaryotic cell formed by a fertilization event between two gametes. The zygote's genome is a combination of the DNA in each gamete, and contains all of the genetic information necessary to form a new individual. In multicellular organisms, the zygote is the earliest developmental stage. In single-celled organisms, the zygote can divide asexually by mitosis to produce identical offspring.
 German zoologists Oscar and Richard Hertwig made some of the first discoveries on animal zygote formation in the late 19th century.
"
Abstract_data_type,Computer Science,4,"In computer science, an abstract data type (ADT) is a mathematical model for data types. An abstract data type is defined by its behavior (semantics) from the point of view of a user, of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This mathematical model contrasts with data structures, which are concrete representations of data, and are the point of view of an implementer, not a user.
 Formally, an ADT may be defined as a ""class of objects whose logical behavior is defined by a set of values and a set of operations"";[1] this is analogous to an algebraic structure in mathematics. What is meant by ""behavior"" varies by author, with the two main types of formal specifications for behavior being axiomatic (algebraic) specification and an abstract model;[2] these correspond to axiomatic semantics and operational semantics of an abstract machine, respectively. Some authors also include the computational complexity (""cost""), both in terms of time (for computing operations) and space (for representing values). In practice many common data types are not ADTs, as the abstraction is not perfect, and users must be aware of issues like arithmetic overflow that are due to the representation. For example, integers are often stored as fixed-width values (32-bit or 64-bit binary numbers), and thus experience integer overflow if the maximum value is exceeded.
 ADTs are a theoretical concept, in computer science, used in the design and analysis of algorithms, data structures, and software systems, and do not correspond to specific features of computer languages—mainstream computer languages do not directly support formally specified ADTs. However, various language features correspond to certain aspects of ADTs, and are easily confused with ADTs proper; these include abstract types, opaque data types, protocols, and design by contract. ADTs were first proposed by Barbara Liskov and Stephen N. Zilles in 1974, as part of the development of the CLU language.[3]"
Abstract_method,Computer Science,4,"A method in object-oriented programming (OOP) is a procedure associated with a message and an object. An object consists of data and behavior; these comprise an interface, which specifies how the object may be utilized by any of its various consumers.[1] Data is represented as properties of the object, and behaviors are represented as methods. For example, a Window object could have methods such as open and close, while its state (whether it is open or closed at any given point in time) would be a property.
 In class-based programming, methods are defined in a class, and objects are instances of a given class. One of the most important capabilities that a method provides is method overriding - the same name (e.g., area) can be used for multiple different kinds of classes. This allows the sending objects to invoke behaviors and to delegate the implementation of those behaviors to the receiving object. A method in Java programming sets the behavior of a class object. For example, an object can send an area message to another object and the appropriate formula is invoked whether the receiving object is a rectangle, circle, triangle, etc.
 Methods also provide the interface that other classes use to access and modify the properties of an object; this is known as encapsulation. Encapsulation and overriding are the two primary distinguishing features between methods and procedure calls.[2]"
Abstraction_(software_engineering),Computer Science,4,"
 – John V. Guttag[1]
 In software engineering and computer science, abstraction is:
 Abstraction, in general, is a fundamental concept in computer science and software development.[4] The process of abstraction can also be referred to as modeling and is closely related to the concepts of theory and design.[5] Models can also be considered types of abstractions per their generalization of aspects of reality.
 Abstraction in computer science is closely related to abstraction in mathematics due to their common focus on building abstractions as objects,[2] but is also related to other notions of abstraction used in other fields such as art.[3] Abstractions may also refer to real-world objects and systems, rules of computational systems or rules of programming languages that carry or utilize features of abstraction itself, such as:
"
Agent_architecture,Computer Science,4,"Agent architecture in computer science is a blueprint for software agents and intelligent control systems, depicting the arrangement of components. The architectures implemented by intelligent agents are referred to as cognitive architectures.[1] The term agent is a conceptual idea, but not defined precisely. It contains of facts, set of goals and sometimes a plan library.[2]"
Agent-based_model,Computer Science,4,"
An agent-based model (ABM) is a class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to introduce randomness.  Particularly within ecology, ABMs are also called individual-based models (IBMs),[1] and individuals within IBMs may be simpler than fully autonomous agents within ABMs. A review of recent literature on individual-based models, agent-based models, and multiagent systems shows that ABMs are used on non-computing related scientific domains including biology, ecology and social science.[2] Agent-based modeling is related to, but distinct from, the concept of multi-agent systems or multi-agent simulation in that the goal of ABM is to search for explanatory insight into the collective behavior of agents obeying simple rules, typically in natural systems, rather than in designing agents or solving specific practical or engineering problems.[2] Agent-based models are a kind of microscale model[3] that simulate the simultaneous operations and interactions of multiple agents in an attempt to re-create and predict the appearance of complex phenomena. The process is one of emergence, which some express as ""the whole is greater than the sum of its parts"". In other words, higher-level system properties emerge from the interactions of lower-level subsystems. Or, macro-scale state changes emerge from micro-scale agent behaviors. Or, simple behaviors (meaning rules followed by agents) generate complex behaviors (meaning state changes at the whole system level).
 Individual agents are typically characterized as boundedly rational, presumed to be acting in what they perceive as their own interests, such as reproduction, economic benefit, or social status,[4] using heuristics or simple decision-making rules. ABM agents may experience ""learning"", adaptation, and reproduction.[5] Most agent-based models are composed of: (1) numerous agents specified at various scales (typically referred to as agent-granularity); (2) decision-making heuristics; (3) learning rules or adaptive processes; (4) an interaction topology; and (5) an environment. ABMs are typically implemented as computer simulations, either as custom software, or via ABM toolkits, and this software can be then used to test how changes in individual behaviors will affect the system's emerging overall behavior.
"
Aggregate_function,Computer Science,4,"In database management, an aggregate function or aggregation function is a function where the values of multiple rows are grouped together to form a single summary value.
 Common aggregate functions include:
 Others include:
 Formally, an aggregate function takes as input a set, a multiset (bag), or a list from some input domain I and outputs an element of an output domain O.[1] The input and output domains may be the same, such as for SUM, or may be different, such as for COUNT.
 Aggregate functions occur commonly in numerous programming languages, in spreadsheets, and in relational algebra.
 The listagg function, as defined in the SQL:2016 standard[2]
aggregates data from multiple rows into a single concatenated string.
"
Agile_software_development,Computer Science,4,"
 In software development, agile (sometimes written Agile)[1] practices approach discovering requirements and developing solutions through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s).[2] It advocates adaptive planning, evolutionary development, early delivery, and continual improvement, and it encourages flexible responses to change.[3][4][further explanation needed] It was popularized by the Manifesto for Agile Software Development.[5] The values and principles espoused in this manifesto were derived from and underpin a broad range of software development frameworks, including Scrum and Kanban.[6][7] While there is much anecdotal evidence that adopting agile practices and values improves the agility of software professionals, teams and organizations, the empirical evidence is mixed and hard to find.[8][9]"
Algorithm,Computer Science,4,"
 In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ (listen)) is a finite sequence of well-defined, computer-implementable instructions, typically to solve a class of problems or to perform a computation.[1][2] Algorithms are always unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks.
 As an effective method, an algorithm can be expressed within a finite amount of space and time,[3] and in a well-defined formal language[4] for calculating a function.[5] Starting from an initial state and initial input (perhaps empty),[6] the instructions describe a computation that, when executed, proceeds through a finite[7] number of well-defined successive states, eventually producing ""output""[8] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[9] The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, was used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC.[10] Greek mathematicians later used algorithms in the sieve of Eratosthenes for finding prime numbers,[11] and the Euclidean algorithm for finding the greatest common divisor of two numbers.[12] Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.[13] The word algorithm itself is derived from the name of the 9th-century mathematician Muḥammad ibn Mūsā al-Khwārizmī, whose nisba (identifying him as from Khwarazm) was Latinized as Algoritmi.[14] A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define ""effective calculability""[15] or ""effective method"".[16] Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.
"
Algorithm_design,Computer Science,4,"
 In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ (listen)) is a finite sequence of well-defined, computer-implementable instructions, typically to solve a class of problems or to perform a computation.[1][2] Algorithms are always unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks.
 As an effective method, an algorithm can be expressed within a finite amount of space and time,[3] and in a well-defined formal language[4] for calculating a function.[5] Starting from an initial state and initial input (perhaps empty),[6] the instructions describe a computation that, when executed, proceeds through a finite[7] number of well-defined successive states, eventually producing ""output""[8] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[9] The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, was used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC.[10] Greek mathematicians later used algorithms in the sieve of Eratosthenes for finding prime numbers,[11] and the Euclidean algorithm for finding the greatest common divisor of two numbers.[12] Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.[13] The word algorithm itself is derived from the name of the 9th-century mathematician Muḥammad ibn Mūsā al-Khwārizmī, whose nisba (identifying him as from Khwarazm) was Latinized as Algoritmi.[14] A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define ""effective calculability""[15] or ""effective method"".[16] Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.
"
Algorithmic_efficiency,Computer Science,4,"
In computer science, algorithmic efficiency is a property of an algorithm which relates to the number of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on the usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.
 For maximum efficiency we wish to minimize resource usage. However, different resources such as time and space complexity cannot be compared directly, so which of two algorithms is considered to be more efficient often depends on which measure of efficiency is considered most important.
 For example, bubble sort and timsort are both algorithms to sort a list of items from smallest to largest. Bubble sort sorts the list in time proportional to the number of elements squared (







O



(

n

2


)





{  \scriptstyle {{\mathcal {O}}\left(n^{2}\right)}}
, see Big O notation), but only requires a small amount of extra memory which is constant with respect to the length of the list (







O



(
1
)





{\textstyle \scriptstyle {{\mathcal {O}}\left(1\right)}}
). Timsort sorts the list in time linearithmic (proportional to a quantity times its logarithm) in the list's length (






O

(

n
log
⁡
n

)






{\textstyle \scriptstyle {\mathcal {O\left(n\log n\right)}}}
), but has a space requirement linear in the length of the list (






O

(
n
)






{\textstyle \scriptstyle {\mathcal {O\left(n\right)}}}
). If large lists must be sorted at high speed for a given application, timsort is a better choice; however, if minimizing the memory footprint of the sorting is more important, bubble sort is a better choice.
"
American_Standard_Code_for_Information_Interchange,Computer Science,4,"
 ASCII (/ˈæskiː/ (listen) ASS-kee),[3]:6 abbreviated from American Standard Code for Information Interchange, is a character encoding standard for electronic communication. ASCII codes represent text in computers, telecommunications equipment, and other devices. Most modern character-encoding schemes are based on ASCII, although they support many additional characters.
 The Internet Assigned Numbers Authority (IANA) prefers the name US-ASCII for this character encoding.[2] ASCII is one of the IEEE milestones.
"
Application_programming_interface,Computer Science,4,"An application programming interface (API)  is a computing interface that defines interactions between multiple software intermediaries. It defines the kinds of calls or requests that can be made, how to make them, the data formats that should be used, the conventions to follow, etc. It can also provide extension mechanisms so that users can extend existing functionality in various ways and to varying degrees.[1] An API can be entirely custom, specific to a component, or designed based on an industry-standard to ensure interoperability. Through information hiding, APIs enable modular programming, allowing users to use the interface independently of the implementation.
"
Application_software,Computer Science,4,"Application software (app for short) is a program or group of programs designed for end-users. Examples of an application include a word processor, a spreadsheet, an accounting application, a web browser, an email client, a media player, a file viewer, simulators, a console game, or a photo editor. The collective noun application software refers to all applications collectively.[1] This contrasts with system software, which is mainly involved with running the computer.
 Applications may be bundled with the computer and its system software or published separately and may be coded as proprietary, open-source, or university projects.[2] Apps built for mobile platforms are called mobile apps.
"
Array_data_structure,Computer Science,4,"
 In computer science, an array data structure, or simply an array, is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula.[1][2][3] The simplest type of data structure is a linear array, also called one-dimensional array.
 For example, an array of 10 32-bit (4-byte) integer variables, with indices 0 through 9, may be stored as 10 words at memory addresses 2000, 2004, 2008, ..., 2036, so that the element with index i has the address 2000 + (i × 4).[4] The memory address of the first element of an array is called first address, foundation address, or base address.
 Because the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called matrices. In some cases the term ""vector"" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word table is sometimes used as a synonym of array.
 Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.
 Arrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually,[3][5] but not always,[2] fixed while the array is in use.
 The term array is often used to mean array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.
 The term is also used, especially in the description of algorithms, to mean associative array or ""abstract array"", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays.
"
Artifact_(software_development),Computer Science,4,"An artifact is one of many kinds of tangible by-products produced during the development of software. Some artifacts (e.g., use cases, class diagrams, and other Unified Modeling Language (UML) models, requirements and design documents) help describe the function, architecture, and design of software. Other artifacts are concerned with the process of development itself—such as project plans, business cases, and risk assessments.
 The term artifact in connection with software development is largely associated with specific development methods or processes e.g., Unified Process. This usage of the term may have originated with those methods[citation needed].
 Build tools often refer to source code compiled for testing as an artifact, because the executable is necessary to carrying out the testing plan.  Without the executable to test, the testing plan artifact is limited to non-execution based testing.  In non-execution based testing, the artifacts are the walkthroughs, inspections and correctness proofs.  On the other hand, execution based testing requires at minimum two artifacts: a test suite and the executable.  Artifact occasionally may refer to the released code (in the case of a code library) or released executable (in the case of a program) produced, but more commonly an artifact is the byproduct of software development rather than the product itself. Open source code libraries often contain a testing harness to allow contributors to ensure their changes do not cause regression bugs in the code library.
 Much of what are considered artifacts is software documentation.
 In end-user development an artifact is either an application or a complex data object that is created by an end-user without the need to know a general programming language. Artifacts describe automated behavior or control sequences, such as database requests or grammar rules,[1] or user-generated content.
 Artifacts vary in their maintainability.  Maintainability is primarily affected by the role the artifact fulfills.  The role can be either practical or symbolic.  In the earliest stages of software development, artifacts may be created by the design team to serve a symbolic role to show the project sponsor how serious the contractor is about meeting the project's needs.  Symbolic artifacts often convey information poorly, but are impressive-looking. Symbolic enhance understanding.  Generally speaking, Illuminated Scrolls are also considered unmaintainable due to the diligence it requires to preserve the symbolic quality.  For this reason, once Illuminated Scrolls are shown to the project sponsor and approved, they are replaced by artifacts which serve a practical role.  Practical artifacts usually need to be maintained throughout the project lifecycle, and, as such, are generally highly maintainable.
 Artifacts are significant from a project management perspective as deliverables. The deliverables of a software project are likely to be the same as its artifacts with the addition of the software itself.
 The sense of artifacts as byproducts is similar to the use of the term artifact in science to refer to something that arises from the process in hand rather than the issue itself, i.e., a result of interest that stems from the means rather than the end.
 To collect, organize and manage artifacts, a Software development folder may be utilized.
"
Artificial_intelligence,Computer Science,4,"
 Artificial intelligence (AI), is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of ""intelligent agents"": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[3] Colloquially, the term ""artificial intelligence"" is often used to describe machines (or computers) that mimic ""cognitive"" functions that humans associate with the human mind, such as ""learning"" and ""problem solving"".[4] As machines become increasingly capable, tasks considered to require ""intelligence"" are often removed from the definition of AI, a phenomenon known as the AI effect.[5] A quip in Tesler's Theorem says ""AI is whatever hasn't been done yet.""[6] For instance, optical character recognition is frequently excluded from things considered to be AI,[7] having become a routine technology.[8] Modern machine capabilities generally classified as AI include successfully understanding human speech,[9] competing at the highest level in strategic game systems (such as chess and Go),[10] autonomously operating cars, intelligent routing in content delivery networks, and military simulations.[11] Artificial intelligence was founded as an academic discipline in 1955, and in the years since has experienced several waves of optimism,[12][13] followed by disappointment and the loss of funding (known as an ""AI winter""),[14][15] followed by new approaches, success and renewed funding.[13][16] After AlphaGo successfully defeated a professional Go player in 2015, artificial intelligence once again attracted widespread global attention.[17] For most of its history, AI research has been divided into sub-fields that often fail to communicate with each other.[18] These sub-fields are based on technical considerations, such as particular goals (e.g. ""robotics"" or ""machine learning""),[19] the use of particular tools (""logic"" or artificial neural networks), or deep philosophical differences.[22][23][24] Sub-fields have also been based on social factors (particular institutions or the work of particular researchers).[18] The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects.[19] General intelligence is among the field's long-term goals.[25] Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, artificial neural networks, and methods based on statistics, probability and economics. The AI field draws upon computer science, information engineering, mathematics, psychology, linguistics, philosophy, and many other fields.
 The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"".[26] This raises philosophical arguments about the mind and the ethics of creating artificial beings endowed with human-like intelligence. These issues have been explored by myth, fiction and philosophy since antiquity.[31] Some people also consider AI to be a danger to humanity if it progresses unabated.[32][33] Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.[34] In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science, software engineering and operations research.[35][16]"
ASCII,Computer Science,4,"
 ASCII (/ˈæskiː/ (listen) ASS-kee),[3]:6 abbreviated from American Standard Code for Information Interchange, is a character encoding standard for electronic communication. ASCII codes represent text in computers, telecommunications equipment, and other devices. Most modern character-encoding schemes are based on ASCII, although they support many additional characters.
 The Internet Assigned Numbers Authority (IANA) prefers the name US-ASCII for this character encoding.[2] ASCII is one of the IEEE milestones.
"
Assertion_(software_development),Computer Science,4,"In computer programming, specifically when using the imperative programming paradigm, an assertion is a predicate (a Boolean-valued function over the state space, usually expressed as a logical proposition using the variables of a program) connected to a point in the program, that always should evaluate to true at that point in code execution. Assertions can help a programmer read the code, help a compiler compile it, or help the program detect its own defects.
 For the latter, some programs check assertions by actually evaluating the predicate as they run. Then, if it is not in fact true – an assertion failure –, the program considers itself to be broken and typically deliberately crashes or throws an assertion failure exception.
"
Associative_array,Computer Science,4,"In computer science, an associative array, map, symbol table, or dictionary is an abstract data type composed of a collection of (key, value) pairs, such that each possible key appears at most once in the collection.
 Operations associated with this data type allow:[1][2] Implementing associative arrays poses the dictionary problem, a classic computer science problem: the task of designing a data structure that maintains a set of data during 'search', 'delete', and 'insert' operations.[3]
The two major solutions to the dictionary problem are a hash table or a search tree.[1][2][4][5]
In some cases it is also possible to solve the problem using directly addressed arrays, binary search trees, or other more specialized structures.
 Many programming languages include associative arrays as primitive data types, and they are available in software libraries for many others. Content-addressable memory is a form of direct hardware-level support for associative arrays.
 Associative arrays have many applications including such fundamental programming patterns as memoization and the decorator pattern.[6] The name does not come from the associative property known in mathematics. Rather, it arises from the fact that we associate values with keys.
"
Automata_theory,Computer Science,4,"
 Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science. The word automata (the plural of automaton) comes from the Greek word αὐτόματα, which means ""self-making"".
 The figure at right illustrates a finite-state machine, which belongs to a well-known type of automaton.  This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows).  As the automaton sees a symbol of input, it makes a transition (or jump) to another state, according to its transition function, which takes the current state and the recent symbol as its inputs.
 Automata theory is closely related to formal language theory. An automaton is a finite representation of a formal language that may be an infinite set. Automata are often classified by the class of formal languages they can recognize, typically illustrated by the Chomsky hierarchy, which describes the relations between various languages and kinds of formalized logics.
 Automata play a major role in theory of computation, compiler construction, artificial intelligence, parsing and formal verification.
"
Automated_reasoning,Computer Science,4,"Automated reasoning is an area of computer science (involves knowledge representation and reasoning) and metalogic dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.
 The most developed subareas of automated reasoning are automated theorem proving (and the less automated but more pragmatic subfield of interactive theorem proving) and automated proof checking (viewed as guaranteed correct reasoning under fixed assumptions).[citation needed] Extensive work has also been done in reasoning by analogy using induction and abduction.[1] Other important topics include reasoning under uncertainty and non-monotonic reasoning. An important part of the uncertainty field is that of argumentation, where further constraints of minimality and consistency are applied on top of the more standard automated deduction. John Pollock's OSCAR system[2] is an example of an automated argumentation system that is more specific than being just an automated theorem prover.
 Tools and techniques of automated reasoning include the classical logics and calculi, fuzzy logic, Bayesian inference, reasoning with maximal entropy and many less formal ad hoc techniques.
"
Bandwidth_(computing),Computer Science,4,"In computing, bandwidth is the maximum rate of data transfer across a given path. Bandwidth may be characterized as network bandwidth,[1] data bandwidth,[2] or digital bandwidth.[3][4] This definition of bandwidth is in contrast to the field of signal processing, wireless communications, modem data transmission, digital communications, and electronics[citation needed], in which bandwidth is used to refer to analog signal bandwidth measured in hertz, meaning the frequency range between lowest and highest attainable frequency while meeting a well-defined impairment level in signal power.  The actual bit rate that can be achieved depends not only on the signal bandwidth but also on the noise on the channel.
"
Bayesian_programming,Computer Science,4,"Bayesian programming is a formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available.
 Edwin T. Jaynes proposed that probability could be considered as an alternative and an extension of logic for rational reasoning with incomplete and uncertain information. In his founding book Probability Theory: The Logic of Science[1] he developed this theory and proposed what he called “the robot,” which was not
a physical device, but an inference engine to automate probabilistic reasoning—a kind of Prolog for probability instead of logic. Bayesian programming[2] is a formal and concrete implementation of this ""robot"".
 Bayesian programming may also be seen as an algebraic formalism to specify graphical models such as, for instance, Bayesian networks, dynamic Bayesian networks, Kalman filters or hidden Markov models. Indeed, Bayesian Programming is more general than Bayesian networks and has a power of expression equivalent to probabilistic factor graphs.[3]"
Benchmark_(computing),Computer Science,4,"In computing, a benchmark is the act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it.[1]
The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves.
 Benchmarking is usually associated with assessing performance characteristics of computer hardware, for example, the floating point operation performance of a CPU, but there are circumstances when the technique is also applicable to software. Software benchmarks are, for example, run against compilers or database management systems (DBMS).
 Benchmarks provide a method of comparing the performance of various subsystems across different chip/system architectures.
 Test suites are a type of system intended to assess the correctness of software.
"
"Best,_worst_and_average_case",Computer Science,4,"In computer science, best, worst, and average cases of a given algorithm express what the resource usage is at least, at most and on average, respectively. Usually the resource being considered is running time, i.e. time complexity, but could also be memory or other resource.
Best case is the function which performs the minimum number of steps on input data of n elements.
Worst case is the function which performs the maximum number of steps on input data of size n.
Average case is the function which performs an average number of steps on input data of n elements.
 In real-time computing, the worst-case execution time is often of particular concern since it is important to know how much time might be needed in the worst case to guarantee that the algorithm will always finish on time.
 Average performance and worst-case performance are the most used in algorithm analysis. Less widely found is best-case performance, but it does have uses: for example, where the best cases of individual tasks are known, they can be used to improve the accuracy of an overall worst-case analysis.  Computer scientists use probabilistic analysis techniques, especially expected value, to determine expected running times.
 The terms are used in other contexts; for example the worst- and best-case outcome of a planned-for epidemic, worst-case temperature to which an electronic circuit element is exposed, etc. Where components of specified tolerance are used, devices must be designed to work properly with the worst-case combination of tolerances and external conditions.
"
Big_data,Computer Science,4,"
 Big data is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.[2] Big data challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. When we handle big data, we may not sample but simply observe and track what happens. Therefore, big data often includes data with sizes that exceed the capacity of traditional software to process within an acceptable time and value.
 Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from data, and seldom to a particular size of data set. ""There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.""[3]
Analysis of data sets can find new correlations to ""spot business trends, prevent diseases, combat crime and so on.""[4] Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, urban informatics, and business informatics.  Scientists encounter limitations in e-Science work, including meteorology, genomics,[5] connectomics, complex physics simulations, biology and environmental research.[6] Data sets grow rapidly, to a certain extent because they are increasingly gathered by cheap and numerous information-sensing Internet of things devices such as mobile devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks.[7][8] The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;[9] as of 2012[update], every day 2.5 exabytes (2.5×260 bytes) of data are generated.[10] Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data.[11] One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.[12] Relational database management systems, desktop statistics[clarification needed] and software packages used to visualize data often have difficulty handling big data. The work may require ""massively parallel software running on tens, hundreds, or even thousands of servers"".[13] What qualifies as being ""big data"" varies depending on the capabilities of the users and their tools, and expanding capabilities make big data a moving target. ""For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.""[14]"
Big_O_notation,Computer Science,4,"Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. Big O is a member of a family of notations invented by Paul Bachmann,[1] Edmund Landau,[2] and others, collectively called Bachmann–Landau notation or asymptotic notation.
 In computer science, big O notation is used to classify algorithms according to how their run time or space requirements grow as the input size grows.[3]  In analytic number theory, big O notation is often used to express a bound on the difference between an arithmetical function and a better understood approximation; a famous example of such a difference is the remainder term in the prime number theorem.
 Big O notation characterizes functions according to their growth rates: different functions with the same growth rate may be represented using the same O notation.
 The letter O is used because the growth rate of a function is also referred to as the order of the function.  A description of a function in terms of big O notation usually only provides an upper bound on the growth rate of the function. Associated with big O notation are several related notations, using the symbols o, Ω, ω, and Θ, to describe other kinds of bounds on asymptotic growth rates.
 Big O notation is also used in many other fields to provide similar estimates.
"
Binary_number,Computer Science,4,"
 In mathematics and digital electronics, a binary number is a number expressed in the base-2 numeral system or binary numeral system, which uses only two symbols: typically ""0"" (zero) and ""1"" (one).
 The base-2 numeral system is a positional notation with a radix of 2. Each digit is referred to as a bit. Because of its straightforward implementation in digital electronic circuitry using logic gates, the binary system is used by almost all modern computers and computer-based devices, as a preferred system of use, over various other human techniques of communication, because of the simplicity of the language.
"
Binary_search_algorithm,Computer Science,4,"
 
 In computer science, binary search, also known as half-interval search,[1] logarithmic search,[2] or binary chop,[3] is a search algorithm that finds the position of a target value within a sorted array.[4][5] Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array.
 Binary search runs in logarithmic time in the worst case, making 



O
(
log
⁡
n
)


{  O(\log n)}
 comparisons, where 



n


{  n}
 is the number of elements in the array.[a][6] Binary search is faster than linear search except for small arrays. However, the array must be sorted first to be able to apply binary search. There are specialized data structures designed for fast searching, such as hash tables, that can be searched more efficiently than binary search. However, binary search can be used to solve a wider range of problems, such as finding the next-smallest or next-largest element in the array relative to the target even if it is absent from the array.
 There are numerous variations of binary search. In particular, fractional cascading speeds up binary searches for the same value in multiple arrays. Fractional cascading efficiently solves a number of search problems in computational geometry and in numerous other fields. Exponential search extends binary search to unbounded lists. The binary search tree and B-tree data structures are based on binary search.
"
Binary_tree,Computer Science,4,"In computer science, a binary tree is a tree data structure in which each node has at most two children, which are referred to as the left child and the right child.  A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set containing the root.[1] Some authors allow the binary tree to be the empty set as well.[2] From a graph theory perspective, binary (and K-ary) trees as defined here are actually arborescences.[3] A binary tree may thus be also called a bifurcating arborescence[3]—a term which appears in some very old programming books,[4] before the modern computer science terminology prevailed. It is also possible to interpret a binary tree as an undirected, rather than a directed graph, in which case a binary tree is an ordered, rooted tree.[5] Some authors use rooted binary tree instead of binary tree to emphasize the fact that the tree is rooted, but as defined above, a binary tree is always rooted.[6] A binary tree is a special case of an ordered K-ary tree, where k is 2.
 In mathematics, what is termed binary tree can vary significantly from author to author. Some use the definition commonly used in computer science,[7] but others define it as every non-leaf having exactly two children and don't necessarily order (as left/right) the children either.[8] In computing, binary trees are used in two very different ways:
"
Bioinformatics,Computer Science,4,"
 Bioinformatics /ˌbaɪ.oʊˌɪnfərˈmætɪks/ (listen) is an interdisciplinary field that develops methods and software tools for understanding biological data, in particular when the data sets are large and complex. As an interdisciplinary field of science, bioinformatics combines biology, computer science, information engineering, mathematics and statistics to analyze and interpret the biological data. Bioinformatics has been used for in silico analyses of biological queries using mathematical and statistical techniques.[clarification needed] Bioinformatics includes biological studies that use computer programming as part of their methodology, as well as a specific analysis ""pipelines"" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidates genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organizational principles within nucleic acid and protein sequences, called proteomics.[1]"
Bit,Computer Science,4,"
 The bit is a basic unit of information in computing and digital communications. The name is a portmanteau of binary digit.[1] The bit represents a logical state with one of two possible values. These values are most commonly represented as either ""1""or""0"", but other representations such as true/false, yes/no, +/−, or on/off are common.
 The correspondence between these values and the physical states of the underlying storage or device is a matter of convention, and different assignments may be used even within the same device or program. It may be physically implemented with a two-state device.
 The symbol for the binary digit is either bit per recommendation by the IEC 80000-13:2008 standard, or the lowercase character b, as recommended by the IEEE 1541-2002 and IEEE Std 260.1-2004 standards.
 A contiguous group of binary digits is commonly called a bit string, a bit vector, or a one- or more-dimensional bit array.
A group of eight binary digits is called one byte, but historically the size of the byte is not strictly defined. Frequently, half-, full-, double- and quad-words consist of a number of bytes which is a low power of two.
 In information theory, one bit is the information entropy of a binary random variable that is 0 or 1 with equal probability,[2] or the information that is gained when the value of such a variable becomes known.[3][4] As a unit of information, the bit is also known as a shannon,[5] named after Claude E. Shannon.
"
Bit_rate,Computer Science,4,"
 In telecommunications and computing, bit rate (bitrate or as a variable R) is the number of bits that are conveyed or processed per unit of time.[1] The bit rate is quantified using the bits per second unit (symbol: ""bit/s""), often in conjunction with an SI prefix such as ""kilo"" (1 kbit/s = 1,000 bit/s), ""mega"" (1 Mbit/s = 1,000 kbit/s), ""giga"" (1 Gbit/s = 1,000 Mbit/s) or ""tera"" (1 Tbit/s = 1000 Gbit/s).[2]  The non-standard abbreviation ""bps"" is often used to replace the standard symbol ""bit/s"", so that, for example, ""1 Mbps"" is used to mean one million bits per second.
 In most environments, one byte per second (1 B/s) corresponds to 8 bit/s.
"
Blacklist_(computing),Computer Science,4,"In computing, a blacklist or a blocklist or a denylist is a basic access control mechanism that allows through all elements (email addresses, users, passwords, URLs, IP addresses, domain names, file hashes, etc.), except those explicitly mentioned. Those items on the list are denied access. The opposite is a whitelist (allowlist, passlist) which means only items on the list are let through whatever gate is being used. A greylist contains items that are temporarily blocked (or temporarily allowed) until an additional step is performed.
 Blacklists can be applied at various points in a security architecture, such as a host, web proxy, DNS servers, email server, firewall, directory servers or application authentication gateways. The type of element blocked is influenced by the access control location.[1] DNS servers may be well-suited to block domain names, for example, but not URLs. A firewall is well-suited for blocking IP addresses, but less so for blocking malicious files or passwords.
 Example uses include a company that might prevent a list of software from running on its network, a school that might prevent access to a list of web sites from its computers, or a business that wants to ensure their computer users are not choosing easily guessed, poor passwords.
"
BMP_file_format,Computer Science,4,"The BMP file format, also known as bitmap image file, device independent bitmap (DIB) file format and bitmap, is a raster graphics image file format used to store bitmap digital images, independently of the display device (such as a graphics adapter), especially on Microsoft Windows[2] and OS/2[3] operating systems.
 The BMP file format is capable of storing two-dimensional digital images both monochrome and color, in various color depths, and optionally with data compression, alpha channels, and color profiles. The Windows Metafile (WMF) specification covers the BMP file format.[4]"
Boolean_data_type,Computer Science,4,"In computer science, the Boolean data type is a data type that has one of two possible values (usually denoted true and false) which is  intended to represent the two truth values of logic and Boolean algebra. It is named after George Boole, who first defined an algebraic system of logic in the mid 19th century. The Boolean data type is primarily associated with conditional statements, which allow different actions by changing control flow depending on whether a programmer-specified Boolean condition evaluates to true or false. It is a special case of a more general logical data type (see probabilistic logic)—logic doesn't always need to be Boolean.
"
Boolean_expression,Computer Science,4,"In computer science, a Boolean expression is an expression used in programming languages that produces a Boolean value when evaluated. A Boolean value is either true or false. A Boolean expression may be composed of a combination of the Boolean constants true or false, Boolean-typed variables, Boolean-valued operators, and Boolean-valued functions.[1] Boolean expressions correspond to propositional formulas in logic and are a special case of Boolean circuits.[2]"
Boolean_algebra,Computer Science,4,"In mathematics and mathematical logic, Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0, respectively.[1] Instead of elementary algebra, where the values of the variables are numbers and the prime operations are addition and multiplication, the main operations of Boolean algebra are the conjunction (and) denoted as ∧, the disjunction (or) denoted as ∨, and the negation (not) denoted as ¬. It is thus a formalism for describing logical operations, in the same way that elementary algebra describes numerical operations.
 Boolean algebra was introduced by George Boole in his first book The Mathematical Analysis of Logic (1847), and set forth more fully in his An Investigation of the Laws of Thought (1854).[2]
According to Huntington, the term ""Boolean algebra"" was first suggested by Sheffer in 1913,[3] although Charles Sanders Peirce gave the title ""A Boolean Algebra with One Constant"" to the first chapter of his ""The Simplest Mathematics"" in 1880.[4]
Boolean algebra has been fundamental in the development of digital electronics, and is provided for in all modern programming languages. It is also used in set theory and statistics.[5]"
Byte,Computer Science,4," The byte is a unit of digital information that most commonly consists of eight bits. Historically, the byte was the number of bits used to encode a single character of text in a computer[1][2] and for this reason it is the smallest addressable unit of memory in many computer architectures.  To disambiguate arbitrarily sized bytes from the common 8-bit definition, network protocol documents such as The Internet Protocol (RFC 791)(1981) refer to an 8-bit byte as an octet.[3] 
 The size of the byte has historically been hardware dependent and no definitive standards existed that mandated the size. Sizes from 1 to 48 bits have been used.[4][5][6][7] The six-bit character code was an often used implementation in early encoding systems and computers using six-bit and nine-bit bytes were common in the 1960s. These systems often had memory words of 12, 18, 24, 30, 36, 48, or 60 bits, corresponding to 2, 3, 4, 5, 6, 8, or 10 six-bit bytes. In this era, bit groupings in the instruction stream were often referred to as syllables[a] or slab, before the term byte became common.
 The modern de facto standard of eight bits, as documented in ISO/IEC 2382-1:1993, is a convenient power of two permitting the binary-encoded values 0 through 255 for one byte—2 to the power 8 is 256.[8] The international standard IEC 80000-13 codified this common meaning. Many types of applications use information representable in eight or fewer bits and processor designers optimize for this common usage. The popularity of major commercial computing architectures has aided in the ubiquitous acceptance of the eight-bit size.[9] Modern architectures typically use 32- or 64-bit words, built of four or eight bytes.
 The unit symbol for the byte was designated as the upper-case letter B by the International Electrotechnical Commission (IEC) and Institute of Electrical and Electronics Engineers (IEEE)[10] in contrast to the bit, whose IEEE symbol is a lower-case b. Internationally, the unit octet, symbol o, explicitly defines a sequence of eight bits, eliminating the ambiguity of the byte.[11][12]"
Booting,Computer Science,4,"
In computing, booting is the process of starting a computer. It can be initiated by hardware such as a button press, or by a software command. After it is switched on, a computer's central processing unit (CPU) has no software in its main memory, so some process must load software into memory before it can be executed.  This may be done by hardware or firmware in the CPU, or by a separate processor in the computer system.
 Restarting a computer also is called rebooting, which can be ""hard"", e.g. after electrical power to the CPU is switched from off to on, or ""soft"", where the power is not cut. On some systems, a soft boot may optionally clear RAM to zero. Both hard and soft booting can be initiated by hardware such as a button press or by software command. Booting is complete when the operative runtime system, typically operating system and some applications,[nb 1] is attained.
 The process of returning a computer from a state of hibernation or sleep does not involve booting. Minimally, some embedded systems do not require a noticeable boot sequence to begin functioning and when turned on may simply run operational programs that are stored in ROM. All computing systems are state machines, and a reboot may be the only method to return to a designated zero-state from an unintended, locked state.
 In addition to loading an operating system or stand-alone utility, the boot process can also load a storage dump program for diagnosing problems in an operating system.
 Boot is short for bootstrap[1][2] or bootstrap load and derives from the phrase to pull oneself up by one's bootstraps.[3][4] The usage calls attention to the requirement that, if most software is loaded onto a computer by other software already running on the computer, some mechanism must exist to load the initial software onto the computer.[5] Early computers used a variety of ad-hoc methods to get a small program into memory to solve this problem. The invention of read-only memory (ROM) of various types solved this paradox by allowing computers to be shipped with a start up program that could not be erased. Growth in the capacity of ROM has allowed ever more elaborate start up procedures to be implemented.
"
Callback_(computer_programming),Computer Science,4,"In computer programming, a callback, also known as a ""call-after""[1] function, is any executable code that is passed as an argument to other code; that other code is expected to call back (execute) the argument at a given time. This execution may be immediate as in a synchronous callback, or it might happen at a later time as in an asynchronous callback. 
Programming languages support callbacks in different ways, often implementing them with subroutines, lambda expressions, blocks, or function pointers.
"
Central_processing_unit,Computer Science,4,"A central processing unit (CPU), also called a central processor, main processor or just processor, is the electronic circuitry within a computer that executes instructions that make up a computer program. The CPU performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program. This contrasts with external components such as main memory and I/O circuitry,[1] and specialized processors such as graphics processing units (GPUs).
 The computer industry used the term ""central processing unit"" as early as 1955.[2][3] The form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory) and execution of instructions by directing the coordinated operations of the ALU, registers and other components.
 Most modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single metal-oxide-semiconductor (MOS) IC chip. Microprocessors chips with multiple CPUs are multi-core processors. The individual physical CPUs, processor cores, can also be multithreaded to create additional virtual or logical CPUs.[4] An IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC). 
 Array processors or vector processors have multiple processors that operate in parallel, with no unit considered central. Virtual CPUs are an abstraction of dynamical aggregated computational resources.[5]"
Character_(computing),Computer Science,4,"
 In computer and machine-based telecommunications terminology, a character is a unit of information that roughly corresponds to a grapheme, grapheme-like unit, or symbol, such as in an alphabet or syllabary in the written form of a natural language.[1] Examples of characters include letters, numerical digits, common punctuation marks (such as ""."" or ""-""), and whitespace. The concept also includes control characters, which do not correspond to visible symbols but rather to instructions to format or process the text. Examples of control characters include carriage return or tab, as well as instructions to printers or other devices that display or otherwise process text.
 Characters are typically combined into strings.
 Historically, the term character was also used to just denote a specific number of contiguous bits. While a character is most commonly assumed to refer to 8 bits (one byte) today, other definitions, like 4 bits[2] or 6 bits,[3][4] have been used in the past as well.
"
Cipher,Computer Science,4,"In cryptography, a cipher (or cypher) is an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure. An alternative, less common term is encipherment. To encipher or encode is to convert information into cipher or code. In common parlance, ""cipher"" is synonymous with ""code"", as they are both a set of steps that encrypt a message; however, the concepts are distinct in cryptography, especially classical cryptography.
 Codes generally substitute different length strings of character in the output, while ciphers generally substitute the same number of characters as are input.  There are exceptions and some cipher systems may use slightly more, or fewer, characters when output versus the number that were input.
 Codes operated by substituting according to a large codebook which linked a random string of characters or numbers to a word or phrase.  For example, ""UQJHSE"" could be the code for ""Proceed to the following coordinates."" When using a cipher the original information is known as plaintext, and the encrypted form as ciphertext. The ciphertext message contains all the information of the plaintext message, but is not in a format readable by a human or computer without the proper mechanism to decrypt it.
 The operation of a cipher usually depends on a piece of auxiliary information, called a key (or, in traditional NSA parlance, a cryptovariable). The encrypting procedure is varied depending on the key, which changes the detailed operation of the algorithm. A key must be selected before using a cipher to encrypt a message. Without knowledge of the key, it should be extremely difficult, if not impossible, to decrypt the resulting ciphertext into readable plaintext.
 Most modern ciphers can be categorized in several ways
"
Class_(computer_science),Computer Science,4,"In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods).[1][2] In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated.[2] When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class.
 In some languages, classes are only a compile-time feature (new classes cannot be declared at run-time), while in other languages classes are first-class citizens, and are generally themselves objects (typically of type .mw-parser-output .monospaced{font-family:monospace,monospace}Class or similar). In these languages, a class that creates classes is called a metaclass.
"
Class-based_programming,Computer Science,4,"Class-based programming, or more commonly class-orientation, is a style of object-oriented programming (OOP) in which inheritance occurs via defining classes of objects, instead of inheritance occurring via the objects alone (compare prototype-based programming).
 The most popular and developed model of OOP is a class-based model, instead of an object-based model. In this model, objects are entities that combine state (i.e., data), behavior (i.e., procedures, or methods) and identity (unique existence among all other objects). The structure and behavior of an object are defined by a class, which is a definition, or blueprint, of all objects of a specific type. An object must be explicitly created based on a class and an object thus created is considered to be an instance of that class. An object is similar to a structure, with the addition of method pointers, member access control, and an implicit data member which locates instances of the class (i.e., objects of the class) in the class hierarchy (essential for runtime inheritance features).
 Junade Ali, Mastering PHP Design Patterns[1]
"
Class-based_programming,Computer Science,4,"Class-based programming, or more commonly class-orientation, is a style of object-oriented programming (OOP) in which inheritance occurs via defining classes of objects, instead of inheritance occurring via the objects alone (compare prototype-based programming).
 The most popular and developed model of OOP is a class-based model, instead of an object-based model. In this model, objects are entities that combine state (i.e., data), behavior (i.e., procedures, or methods) and identity (unique existence among all other objects). The structure and behavior of an object are defined by a class, which is a definition, or blueprint, of all objects of a specific type. An object must be explicitly created based on a class and an object thus created is considered to be an instance of that class. An object is similar to a structure, with the addition of method pointers, member access control, and an implicit data member which locates instances of the class (i.e., objects of the class) in the class hierarchy (essential for runtime inheritance features).
 Junade Ali, Mastering PHP Design Patterns[1]
"
Client_(computing),Computer Science,4,"In computing, a client is a piece of computer hardware or software that accesses a service made available by a server as part of the client–server model of computer networks. The server is often (but not always) on another computer system, in which case the client accesses the service by way of a network.
"
Cleanroom_software_engineering,Computer Science,4,"
 The cleanroom software engineering  process is a software development process intended to produce software with a certifiable level of reliability. The cleanroom process was originally developed by Harlan Mills and several of his colleagues including Alan Hevner at IBM.[1] The focus of the cleanroom process is on defect prevention, rather than defect removal. The name ""cleanroom"" was chosen to evoke the cleanrooms used in the electronics industry to prevent the introduction of defects during the fabrication of semiconductors. The cleanroom process first saw use in the mid to late 1980s. Demonstration projects within the military began in the early 1990s.[2] Recent work on the cleanroom process has examined fusing cleanroom with the automated verification capabilities provided by specifications expressed in CSP.[3]"
Closure_(computer_programming),Computer Science,4,"
In programming languages, a closure, also lexical closure or function closure, is a technique for implementing lexically scoped name binding in a language with first-class functions. Operationally, a closure is a record storing a function[a] together with an environment.[1] The environment is a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created.[b] Unlike a plain function, a closure allows the function to access those captured variables through the closure's copies of their values or references, even when the function is invoked outside their scope.
"
Cloud_computing,Computer Science,4,"
 Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user. The term is generally used to describe data centers available to many users over the Internet.[1] Large clouds, predominant today, often have functions distributed over multiple locations from central servers. If the connection to the user is relatively close, it may be designated an edge server.
 Clouds may be limited to a single organization (enterprise clouds[2][3]), or be available to multiple organizations (public cloud).
 Cloud computing relies on sharing of resources to achieve coherence and economies of scale.
 Advocates of public and hybrid clouds note that cloud computing allows companies to avoid or minimize up-front IT infrastructure costs. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and that it enables IT teams to more rapidly adjust resources to meet fluctuating and unpredictable demand,[3][4][5] providing the burst computing capability: high computing power at certain periods of peak demand.[6] Cloud providers typically use a ""pay-as-you-go"" model, which can lead to unexpected operating expenses if administrators are not familiarized with cloud-pricing models.[7] The availability of high-capacity networks, low-cost computers and storage devices as well as the widespread adoption of hardware virtualization, service-oriented architecture and autonomic and utility computing has led to growth in cloud computing.[8][9][10] By 2019, Linux was the most widely used operating system, including in Microsoft's offerings and is thus described as dominant.[11] The Cloud Service Provider (CSP) will screen, keep up and gather data about the firewalls, intrusion identification or/and counteractive action frameworks and information stream inside the network.[12]"
Library_(computing),Computer Science,4,"
 In computer science, a library is a collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets.
 A library is also a collection of implementations of behavior, written in terms of a language, that has a well-defined interface by which the behavior is invoked. For instance, people who want to write a higher-level program can use a library to make system calls instead of implementing those system calls over and over again. In addition, the behavior is provided for reuse by multiple independent programs. A program invokes the library-provided behavior via a mechanism of the language. For example, in a simple imperative language such as C, the behavior in a library is invoked by using C's normal function-call. What distinguishes the call as being to a library function, versus being to another function in the same program, is the way that the code is organized in the system.
 Library code is organized in such a way that it can be used by multiple programs that have no connection to each other, while code that is part of a program is organized to be used only within that one program. This distinction can gain a hierarchical notion when a program grows large, such as a multi-million-line program. In that case, there may be internal libraries that are reused by independent sub-portions of the large program. The distinguishing feature is that a library is organized for the purposes of being reused by independent programs or sub-programs, and the user only needs to know the interface and not the internal details of the library.
 The value of a library lies in the reuse of the behavior. When a program invokes a library, it gains the behavior implemented inside that library without having to implement that behavior itself. Libraries encourage the sharing of code in a modular fashion and ease the distribution of the code.
 The behavior implemented by a library can be connected to the invoking program at different program lifecycle phases. If the code of the library is accessed during the build of the invoking program, then the library is called a static library.[1] An alternative is to build the executable of the invoking program and distribute that, independently of the library implementation. The library behavior is connected after the executable has been invoked to be executed, either as part of the process of starting the execution, or in the middle of execution. In this case the library is called a dynamic library (loaded at runtime). A dynamic library can be loaded and linked when preparing a program for execution, by the linker. Alternatively, in the middle of execution, an application may explicitly request that a module be loaded.
 Most compiled languages have a standard library, although programmers can also create their own custom libraries. Most modern software systems provide libraries that implement the majority of the system services. Such libraries have organized the services which a modern application requires. As such, most code used by modern applications is provided in these system libraries.
"
Computer_programming,Computer Science,4,"
 Computer programming is the process of designing and building an executable computer program to accomplish a specific computing result or to perform a specific task. Programming involves tasks such as: analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding).[1][2] The source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit. The purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem. Proficient programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.
 Tasks accompanying and related to programming include: testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of code. Software engineering combines engineering techniques with software development practices.  Reverse engineering is a related process used by designers, analysts and programmers to understand and re-create/re-implement.[3]:3"
Coding_theory,Computer Science,4,"Coding theory is the study of the properties of codes and their respective fitness for specific applications. Codes are used for data compression, cryptography, error detection and correction, data transmission and data storage. Codes are studied by various scientific disciplines—such as information theory, electrical engineering,  mathematics, linguistics, and computer science—for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.
 There are four types of coding:[1] Data compression attempts to remove redundancy from the data from a source in order to transmit it more efficiently. For example, ZIP data compression makes data files smaller, for purposes such as to reduce Internet traffic. Data compression and error correction may be studied in combination.
 Error correction adds extra data bits to make the transmission of data more robust to disturbances present on the transmission channel. The ordinary user may not be aware of many applications using error correction. A typical music compact disc (CD) uses the Reed-Solomon code to correct for scratches and dust. In this application the transmission channel is the CD itself. Cell phones also use coding techniques to correct for the fading and noise of high frequency radio transmission. Data modems, telephone transmissions, and the NASA Deep Space Network all employ channel coding techniques to get the bits through, for example the turbo code and LDPC codes.
"
Cognitive_science,Computer Science,4,"
 Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[2] It examines the nature, the tasks, and the functions of cognition (in a broad sense). Cognitive scientists study intelligence and behavior, with a focus on how nervous systems represent, process, and transform information. Mental faculties of concern to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, neuroscience, and anthropology.[3] The typical analysis of cognitive science spans many levels of organization, from learning and decision to logic and planning; from neural circuitry to modular brain organization. One of the fundamental concepts of cognitive science is that ""thinking can best be understood in terms of representational structures in the mind and computational procedures that operate on those structures.""[3] The goal of cognitive science is to understand the principles of intelligence with the hope that this will lead to better comprehension of the mind and of learning and to develop intelligent devices.
The cognitive sciences began as an intellectual movement in the 1950s often referred to as the cognitive revolution.[4]"
Collection_(abstract_data_type),Computer Science,4,"In computer science, a collection or container is a grouping of some variable number of data items (possibly zero) that have some shared significance to the problem being solved and need to be operated upon together in some controlled fashion.  Generally, the data items will be of the same type or, in languages supporting inheritance, derived from some common ancestor type. A collection is a concept applicable to abstract data types, and does not prescribe a specific implementation as a concrete data structure, though often there is a conventional choice (see Container for type theory discussion).
 Examples of collections include lists, sets, multisets, trees and graphs.
 Fixed-size arrays (or tables) are usually not considered a collection because they hold a fixed number of data items, although they commonly play a role in the implementation of collections. Variable-size arrays are generally considered collections.[citation needed]"
Comma-separated_values,Computer Science,4,"A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. The use of the comma as a field separator is the source of the name for this file format. A CSV file typically stores tabular data (numbers and text) in plain text, in which case each line will have the same number of fields.
 The CSV file format is not fully standardized. The basic idea of separating fields with a comma is clear, but the situation gets complicated when field data also contain commas or embedded line breaks. CSV implementations may not handle such field data, or they may use quotation marks to surround the field. Quotation does not solve everything: some fields may need embedded quotation marks, so a CSV implementation may include escape characters or escape sequences.
 In addition, the term ""CSV"" also denotes[citation needed] several closely related delimiter-separated formats that use other field delimiters, for example, semicolons. These include tab-separated values and space-separated values. A delimiter such as tab that is not present in the field data allows more simple format parsing. These alternative delimiter-separated files are often[citation needed] given a .csv extension despite the use of a non-comma field separator. This loose terminology can cause problems in data exchange. Many applications that accept CSV files have options to select the delimiter character and the quotation character. Semicolons are often used instead of commas in many European locales in order to use the comma as the decimal separator and, possibly, the period as a decimal grouping character. Because of that, the term character-separated values is suggested as a wider definition of this file format.[by whom?]"
Compiler,Computer Science,4,"
 In computing, a compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name ""compiler"" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.[1][2]:p1 There are many different types of compilers. If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is a cross-compiler. A bootstrap compiler is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transcompiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler refers to tools used to create parsers that perform syntax analysis.
 A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.[3] Compilers are not the only language processor used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations.[2]:p2 The translation process influences the design of computer languages, which leads to a preference of compilation or interpretation. In practice, an interpreter can be implemented for compiled languages and compilers can be implemented for interpreted languages.
"
Computability_theory,Computer Science,4,"Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability[disambiguation needed]. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory.
 Basic questions addressed by recursion theory include:
 Although there is considerable overlap in terms of knowledge and methods, mathematical recursion theorists study the theory of relative computability, reducibility notions, and degree structures; those in the computer science field focus on the theory of subrecursive hierarchies, formal methods, and formal languages.
"
Computation,Computer Science,4,"A computation is any type of calculation[1][2] that includes both arithmetical and non-arithmetical steps and which follows a well-defined model (e.g. an algorithm).
 Mechanical or electronic devices (or, historically, people) that perform computations are known as computers. An especially well-known discipline of the study of computation is computer science.
"
Computational_biology,Computer Science,4,"Computational biology involves the development and application of data-analytical and theoretical methods, mathematical modelling and computational simulation techniques to the study of biological, ecological, behavioural, and social systems.[1] The field is broadly defined and includes foundations in biology, applied mathematics, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, computer science, and evolution.[2] Computational biology is different from biological computing, which is a subfield of computer science and computer engineering using bioengineering and biology to build computers.
"
Computational_chemistry,Computer Science,4,"Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids. It is necessary because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form.  While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.
 Examples of such properties are structure (i.e., the expected positions of the constituent atoms), absolute and relative (interaction) energies, electronic charge density distributions, dipoles and higher multipole moments, vibrational frequencies, reactivity, or other spectroscopic quantities, and cross sections for collision with other particles.
 The methods used cover both static and dynamic situations. In all cases, the computer time and other resources (such as memory and disk space) increase rapidly with the size of the system being studied.  That system can be one molecule, a group of molecules, or a solid.  Computational chemistry methods range from very approximate to highly accurate; the latter are usually feasible for small systems only. Ab initio methods are based entirely on quantum mechanics and basic physical constants. Other methods are called empirical or semi-empirical because they use additional empirical parameters.
 Both ab initio and semi-empirical approaches involve approximations.  These range from simplified forms of the first-principles equations that are easier or faster to solve, to approximations limiting the size of the system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all.  For example, most ab initio calculations make the Born–Oppenheimer approximation, which greatly simplifies the underlying Schrödinger equation by assuming that the nuclei remain in place during the calculation.  In principle, ab initio methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced.  In practice, however, it is impossible to eliminate all approximations, and residual error inevitably remains.  The goal of computational chemistry is to minimize this residual error while keeping the calculations tractable.
 In some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are used, typically with molecular mechanics force fields, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target. Other problems include predicting binding specificity, off-target effects, toxicity, and pharmacokinetic properties.
"
Computational_complexity_theory,Computer Science,4,"
Computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.
 A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millennium Prize Problems, is dedicated to the field of computational complexity.[1] Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kinds of problems can, in principle, be solved algorithmically.
"
Computational_model,Computer Science,4,"A computational model is a mathematical model in computational science that requires extensive computational resources to study the behavior of a complex system by computer simulation.[1] The system under study is often a complex nonlinear system for which simple, intuitive analytical solutions are not readily available. Rather than deriving a mathematical analytical solution to the problem, experimentation with the model is done by adjusting the parameters of the system in the computer, and studying the differences in the outcome of the experiments.  Operation theories of the model can be derived/deduced from these computational experiments.
 Examples of common computational models are weather forecasting models, earth simulator models, flight simulator models, molecular protein folding models, and neural network models.
"
Computational_neuroscience,Computer Science,4,"Computational neuroscience (also known as theoretical neuroscience or mathematical neuroscience) is a branch of neuroscience which employs mathematical models, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology and cognitive abilities of the nervous system.[1][2][3][4] In theory, computational neuroscience would be a sub-field of theoretical neuroscience which employs computational simulations to validate and solve the mathematical models. However, since the biologically plausible mathematical models formulated in neuroscience are in most cases too complex to be solved analytically, the two terms are essentially synonyms and are used interchangeably.[5] The term mathematical neuroscience is also used sometimes, to stress the quantitative nature of the field.[6] Computational neuroscience focuses on the description of biologically plausible neurons (and neural systems) and their physiology and dynamics, and it is therefore not directly concerned with biologically unrealistic models used in connectionism, control theory, cybernetics, quantitative psychology, machine learning, artificial neural networks, artificial intelligence and computational learning theory;[7][8][9][10] although mutual inspiration exists and sometimes there is no strict limit between fields,[11][12][13][14] with model abstraction in computational neuroscience depending on research scope and the granularity at which biological entities are analyzed.
 Models in theoretical neuroscience are aimed at capturing the essential features of the biological system at multiple spatial-temporal scales, from membrane currents, and chemical coupling via network oscillations, columnar and topographic architecture, nuclei, all the way up to psychological faculties like memory, learning and behavior. These computational models frame hypotheses that can be directly tested by biological or psychological experiments.
"
Computational_physics,Computer Science,4,"Computational physics is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists.[1] Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science.
 It is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics - an area of study which supplements both theory and experiment.[2]"
Computational_science,Computer Science,4,"Computational science, also known as scientific computing or scientific computation (SC), is a rapidly growing field that uses advanced computing capabilities to understand and solve complex problems. It is an area of science which spans many disciplines, but at its core, it involves the development of models and simulations to understand natural systems.
 In practical use, it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to solve problems in various scientific disciplines. The field is different from theory and laboratory experiment which are the traditional forms of science and engineering. The scientific computing approach is to gain understanding, mainly through the analysis of mathematical models implemented on computers. Scientists and engineers develop computer programs, application software, that model systems being studied and run these programs with various sets of input parameters. The essence of computational science is the application of numerical algorithms[1] and/or computational mathematics. In some cases, these models require massive amounts of calculations (usually floating-point) and are often executed on supercomputers or distributed computing platforms. Actually the science which deals with the Computer Modeling and Simulation of any physical objects and phenomena by high programming language and software and hardware is known as Computer Simulation.
"
Computational_steering,Computer Science,4,"Computational steering is the practice of manually intervening with an otherwise autonomous computational process, to change its outcome. The term is commonly used within the numerical simulation community, where it more specifically refers to the practice of interactively guiding a computational experiment into some region of interest.[citation needed]"
Computer,Computer Science,4,"
 A computer is a machine that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming. Modern computers have the ability to follow generalized sets of operations, called programs. These programs enable computers to perform an extremely wide range of tasks. A ""complete"" computer including the hardware, the operating system (main software), and peripheral equipment required and used for ""full"" operation can be referred to as a computer system. This term may as well be used for a group of computers that are connected and work together, in particular a computer network or computer cluster.
     
  Computers are used as control systems for a wide variety of industrial and consumer devices. This includes simple special purpose devices like microwave ovens and remote controls, factory devices such as industrial robots and computer-aided design, and also general purpose devices like personal computers and mobile devices such as smartphones. The Internet is run on computers and it connects hundreds of millions of other computers and their users.
 Early computers were only conceived as calculating devices. Since ancient times, simple manual devices like the abacus aided people in doing calculations. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit (IC) chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (as predicted by Moore's law), leading to the Digital Revolution during the late 20th to early 21st centuries.
 Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, along with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved.
"
Computer_architecture,Computer Science,4,"In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation.[1] In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.[2]"
Computer_data_storage,Computer Science,4,"
 Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.[1]:15–16 The central processing unit (CPU) of a computer is what manipulates data by performing computations.  In practice, almost all computers use a storage hierarchy,[1]:468–473 which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally the fast volatile technologies (which lose data when off power) are referred to as ""memory"", while slower persistent technologies are referred to as ""storage"".
 Even the first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.
"
Computer_ethics,Computer Science,4,"Computer ethics is a part of practical philosophy concerned with how computing professionals should make decisions regarding professional and social conduct.[1] 
Margaret Anne Pierce, a professor in the Department of Mathematics and Computers at Georgia Southern University has categorized the ethical decisions related to computer technology and usage into three primary influences:[2]"
Computer_graphics,Computer Science,4,"Computer graphics is the branch of computer science[1][2] that deals with generating images with the aid of computers. Today, computer graphics is a core technology in digital photography, film, video games, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI).
 Some topics in computer graphics include user interface design, sprite graphics, rendering, ray tracing, geometry processing, computer animation, vector graphics, 3D modeling, shaders, GPU design, implicit surface visualization, image processing, computational photography, scientific visualization, computational geometry and computer vision, among others. The overall methodology depends heavily on the underlying sciences of geometry, optics, physics, and perception.
 Computer graphics is responsible for displaying art and image data effectively and meaningfully to the consumer. It is also used for processing image data received from the physical world, such as photo and video content. Computer graphics development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, video games, and graphic design in general.
"
Computer_network,Computer Science,4,"
 A computer network is a group of computers that use a set of common communication protocols over digital interconnections for the purpose of sharing resources located on or provided by the network nodes. The interconnections between nodes are formed from a broad spectrum of telecommunication network technologies, based on physically wired, optical, and wireless radio-frequency methods that may be arranged in a variety of network topologies.
 The nodes of a computer network may be classified by many means as personal computers, servers, networking hardware, or general-purpose hosts. They are identified by hostnames and network addresses. Hostnames serve as memorable labels for the nodes, rarely changed after initial assignment. Network addresses serve for locating and identifying the nodes by communication protocols such as the Internet Protocol.
 Computer networks may be classified by many criteria, for example, the transmission medium used to carry signals,  bandwidth, communications protocols to organize network traffic, the network size, the topology, traffic control mechanism, and organizational intent.
 Computer networks support many applications and services, such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications.
"
Computer_program,Computer Science,4,"A computer program is a collection of instructions[1] that can be executed by a computer to perform a specific task. 
 A computer program is usually written by a computer programmer in a programming language. From the program in its human-readable form of source code, a compiler or assembler can derive machine code—a form consisting of instructions that the computer can directly execute. Alternatively, a computer program may be executed with the aid of an interpreter.
 A collection of computer programs, libraries, and related data are referred to as software. Computer programs may be categorized along functional lines, such as application software and system software.  The underlying method used for some calculation or manipulation is known as an algorithm.
"
Computer_programming,Computer Science,4,"
 Computer programming is the process of designing and building an executable computer program to accomplish a specific computing result or to perform a specific task. Programming involves tasks such as: analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding).[1][2] The source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit. The purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem. Proficient programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.
 Tasks accompanying and related to programming include: testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of code. Software engineering combines engineering techniques with software development practices.  Reverse engineering is a related process used by designers, analysts and programmers to understand and re-create/re-implement.[3]:3"
Computer_science,Computer Science,4,"
 Computer science is the study of algorithmic processes and computational machines.[1][2] As a discipline, computer science spans a range of topics from theoretical studies of algorithms, computation and information to the practical issues of implementing computing systems in hardware and software.[3][4] Computer science addresses any computational problems, especially information processes, such as control, communication, perception, learning, and intelligence.[5][6][7] Its fields can be divided into theoretical and practical disciplines. For example, the theory of computation concerns abstract models of computation and general classes of problems that can be solved using them, while computer graphics and computational geometry emphasize more specific applications. Algorithmics have been called the heart of computer science.[8] Programming language theory considers approaches to the description of computational processes, while computer programming involves the use of them to create complex systems. Computer architecture describes construction of computer components and computer-controlled equipment. Artificial intelligence aims to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. The fundamental concern of computer science is determining what can and cannot be automated.[9][5] Unlike other computing paradigms, computer scientists are focused on academic research.
"
Computer_scientist,Computer Science,4,"

A computer scientist is a person who has acquired the knowledge of computer science, the study of the theoretical foundations of information and computation and their application.[1] Computer scientists typically work on the theoretical side of computer systems, as opposed to the hardware side on which computer engineers mainly focus (although there is overlap). Although computer scientists can also focus their work and research on specific areas (such as algorithm and data structure development and design, software engineering, information theory, database theory, computational complexity theory, numerical analysis, programming language theory, computer graphics, and computer vision), their foundation is the theoretical study of computing from which these other fields derive.[2] A primary goal of computer scientists is to develop or validate models, often mathematical, to describe the properties of computer-based systems (processors, programs, computers interacting with people, computers interacting with other computers, etc.) with an overall objective of discovering designs that yield useful benefits (faster, smaller, cheaper, more precise, etc.).
"
Computer_security,Computer Science,4,"
 Computer security, cybersecurity[1] or information technology security (IT security) is the protection of computer systems and networks from the theft of or damage to their hardware, software, or electronic data, as well as from the disruption or misdirection of the services they provide.
 The field is becoming more significant due to the increased reliance on computer systems, the Internet[2] and wireless network standards such as Bluetooth and Wi-Fi, and due to the growth of ""smart"" devices, including smartphones, televisions, and the various devices that constitute the ""Internet of things"". Owing to its complexity, both in terms of politics and technology, cybersecurity is also one of the major challenges in the contemporary world.[3]"
Computer_vision,Computer Science,4,"Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.[1][2][3] Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.[4][5][6][7] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[8] The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, or medical scanning device. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.
 Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.[6]"
Computing,Computer Science,4,"Computing is any goal-oriented activity requiring, benefiting from, or creating computing machinery. It includes study of algorithmic processes and development of both  hardware and software. It has scientific, engineering, mathematical, technological and social aspects. Major computing fields include computer engineering, computer science, cybersecurity, data science, information systems, information technology and software engineering.[2]"
Concatenation,Computer Science,4,"In formal language theory and computer programming, string concatenation  is the operation of joining character strings end-to-end.  For example, the concatenation of ""snow"" and ""ball"" is ""snowball"". In certain formalisations of concatenation theory, also called string theory, string concatenation is a primitive notion.
"
Concurrency_(computer_science),Computer Science,4,"In computer science, concurrency is the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the final outcome.  This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. In more technical terms, concurrency refers to the decomposability property of a program, algorithm, or problem into order-independent or partially-ordered components or units.[1] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi, the parallel random-access machine model, the actor model and the Reo Coordination Language.
"
Conditional_(computer_programming),Computer Science,4,"In computer science, conditional statements, conditional expressions and conditional constructs are features of a programming language, which perform different computations or actions depending on whether a programmer-specified boolean condition evaluates to true or false. Apart from the case of branch predication, this is always achieved by selectively altering the control flow based on some condition.
 In imperative programming languages, the term ""conditional statement"" is usually used, whereas in functional programming, the terms ""conditional expression"" or ""conditional construct"" are preferred, because these terms all have distinct meanings.
 Although dynamic dispatch is not usually classified as a conditional construct, it is another way to select between alternatives at runtime.
"
Container_(abstract_data_type),Computer Science,4,"
 In computer science, a container is a class, a data structure,[1][2] or an abstract data type (ADT) whose instances are collections of other objects. In other words, they store objects in an organized way that follows specific access rules. The size of the container depends on the number of objects (elements) it contains. Underlying (inherited) implementations of various container types may vary in size and complexity, and provide flexibility in choosing the right implementation for any given scenario.
"
Continuation-passing_style,Computer Science,4,"In functional programming, continuation-passing style (CPS) is a style of programming in which control is passed explicitly in the form of a continuation. This is contrasted with direct style, which is the usual style of programming. Gerald Jay Sussman and Guy L. Steele, Jr. coined the phrase in AI Memo 349 (1975), which sets out the first version of the Scheme programming language.[1][2]John C. Reynolds gives a detailed account of the numerous discoveries of continuations.[3] A function written in continuation-passing style takes an extra argument: an explicit ""continuation"", i.e. a function of one argument.  When the CPS function has computed its result value, it ""returns"" it by calling the continuation function with this value as the argument. That means that when invoking a CPS function, the calling function is required to supply a procedure to be invoked with the subroutine's ""return"" value.  Expressing code in this form makes a number of things explicit which are implicit in direct style.  These include: procedure returns, which become apparent as calls to a continuation; intermediate values, which are all given names; order of argument evaluation, which is made explicit; and tail calls, which simply call a procedure with the same continuation, unmodified, that was passed to the caller.
 Programs can be automatically transformed from direct style to CPS. Functional and logic compilers often use CPS as an intermediate representation where a compiler for an imperative or procedural programming language would use static single assignment form (SSA).[4] SSA is formally equivalent to a subset of CPS (excluding non-local control flow, which does not occur when CPS is used as intermediate representation).[5] Functional compilers can also use A-normal form (ANF) (but only for languages requiring eager evaluation), rather than with 'thunks' (described in the examples below) in CPS.  CPS is used more frequently by compilers than by programmers as a local or global style.
"
Control_flow,Computer Science,4,"In computer science, control flow (or flow of control) is the order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language.
 Within an imperative programming language, a control flow statement is a statement that results in a choice being made as to which of two or more paths to follow. For non-strict functional languages, functions and language constructs exist to achieve the same result, but they are usually not termed control flow statements.
 A set of statements is in turn generally structured as a block, which in addition to grouping, also defines a lexical scope.
 Interrupts and signals are low-level mechanisms that can alter the flow of control in a way similar to a subroutine, but usually occur as a response to some external stimulus or event (that can occur asynchronously), rather than execution of an in-line control flow statement.
 At the level of machine language or assembly language, control flow instructions usually work by altering the program counter. For some central processing units (CPUs), the only control flow instructions available are conditional or unconditional branch instructions, also termed jumps.
"
Creative_Commons,Computer Science,4,"
 
 Creative Commons (CC) is an American non-profit organization and international network devoted to educational access and expanding the range of creative works available for others to build upon legally and to share.[3] The organization has released several copyright-licenses, known as Creative Commons licenses, free of charge to the public. These licenses allow authors of creative works to communicate which rights they reserve and which rights they waive for the benefit of recipients or other creators. An easy-to-understand one-page explanation of rights, with associated visual symbols, explains the specifics of each Creative Commons license. Creative Commons licenses do not replace copyright, but are based upon it. They replace individual negotiations for specific rights between copyright owner (licensor) and licensee, which are necessary under an ""all rights reserved"" copyright management, with a ""some rights reserved"" management employing standardized licenses for re-use cases where no commercial compensation is sought by the copyright owner. 
 The organization was founded in 2001 by Lawrence Lessig, Hal Abelson, and Eric Eldred[4] with the support of Center for the Public Domain. The first article in a general interest publication about Creative Commons, written by Hal Plotkin, was published in February 2002.[5] The first set of copyright licenses was released in December 2002.[6] The founding management team that developed the licenses and built the Creative Commons infrastructure as we know it today included Molly Shaffer Van Houweling, Glenn Otis Brown, Neeru Paharia, and Ben Adida.[7] In 2002, the Open Content Project, a 1998 precursor project by David A. Wiley, announced the Creative Commons as successor project and Wiley joined as CC director.[8][9] Aaron Swartz played a role in the early stages of Creative Commons,[10] as did Matthew Haughey.[11] As of May 2018, there were 1.4 billion works licensed under the various Creative Commons licenses.[12] Wikipedia uses one of these licenses.[13] As of May 2018, Flickr alone hosted over 415 million Creative Commons-licensed photos.[14][15]"
Cryptography,Computer Science,4,"
 Cryptography, or cryptology (from Ancient Greek: κρυπτός, romanized: kryptós ""hidden, secret""; and γράφειν graphein, ""to write"", or -λογία -logia, ""study"", respectively[1]), is the practice and study of techniques for secure communication in the presence of third parties called adversaries.[2] More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages;[3] various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation[4] are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.
 Cryptography prior to the modern age was effectively synonymous with encryption, the conversion of information from a readable state to apparent nonsense. The originator of an encrypted message shares the decoding technique only with intended recipients to preclude access from adversaries. The cryptography literature often uses the names Alice (""A"") for the sender, Bob (""B"") for the intended recipient, and Eve (""eavesdropper"") for the adversary.[5] Since the development of rotor cipher machines in World War I and the advent of computers in World War II, the methods used to carry out cryptology have become increasingly complex and its application more widespread.
 Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power—an example is the one-time pad—but these schemes are more difficult to use in practice than the best theoretically breakable but computationally secure mechanisms.
 The growth of cryptographic technology has raised a number of legal issues in the information age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export.[6] In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation.[7][8] Cryptography also plays a major role in digital rights management and copyright infringement of digital media.[9]"
Comma-separated_values,Computer Science,4,"A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. The use of the comma as a field separator is the source of the name for this file format. A CSV file typically stores tabular data (numbers and text) in plain text, in which case each line will have the same number of fields.
 The CSV file format is not fully standardized. The basic idea of separating fields with a comma is clear, but the situation gets complicated when field data also contain commas or embedded line breaks. CSV implementations may not handle such field data, or they may use quotation marks to surround the field. Quotation does not solve everything: some fields may need embedded quotation marks, so a CSV implementation may include escape characters or escape sequences.
 In addition, the term ""CSV"" also denotes[citation needed] several closely related delimiter-separated formats that use other field delimiters, for example, semicolons. These include tab-separated values and space-separated values. A delimiter such as tab that is not present in the field data allows more simple format parsing. These alternative delimiter-separated files are often[citation needed] given a .csv extension despite the use of a non-comma field separator. This loose terminology can cause problems in data exchange. Many applications that accept CSV files have options to select the delimiter character and the quotation character. Semicolons are often used instead of commas in many European locales in order to use the comma as the decimal separator and, possibly, the period as a decimal grouping character. Because of that, the term character-separated values is suggested as a wider definition of this file format.[by whom?]"
Cyberbullying,Computer Science,4,"
 Cyberbullying or cyberharassment is a form of bullying or harassment using electronic means. Cyberbullying and cyberharassment are also known as online bullying. It has become increasingly common, especially among teenagers, as the digital sphere has expanded and technology has advanced.[1] Cyberbullying is when someone, typically a teenager, bullies or harasses others on the internet and in other digital spaces, particularly on social media sites. Harmful bullying behavior can include posting rumors, threats, sexual remarks, a victims' personal information, or pejorative labels (i.e. hate speech).[2] Bullying or harassment can be identified by repeated behavior and an intent to harm.[3] Victims of cyberbulling may experience lower self-esteem, increased suicidal ideation, and a variety of negative emotional responses including being scared, frustrated, angry, or depressed.[4] Awareness in the United States has risen in the 2010s, due in part to high-profile cases.[5][6] Several US states and other countries have passed laws to combat cyberbullying.[7] Some are designed to specifically target teen cyberbullying, while others extend from the scope of physical harassment.[8] In cases of adult cyberharassment, these reports are usually filed beginning with local police.[9] The laws differ by area or state.
 Research has demonstrated a number of serious consequences of cyberbullying victimisation.[10] Specific statistics on the negative effects of cyberbullying differ by country and other demographics. Some researchers point out there could be some way to use modern computer techniques to determine and stopping cyberbullying.[11] Internet trolling is a common form of bullying that takes place in an online community (such as online gaming or social media) in order to elicit a reaction or disruption, or simply just for someone's own personal amusement.[12][13] Cyberstalking is another form of bullying or harassment that uses electronic communications to stalk a victim; this may pose a credible threat to the victim.[14] Not all negative interaction online or on social media can be attributed to cyberbullying. Research suggests that there are also interactions online that result in peer pressure, which can have a negative, positive, or neutral impact on those involved.[15][16][17]"
Cyberspace,Computer Science,4,"Cyberspace is a concept describing a widespread, interconnected digital technology. ""The expression dates back from the first decade of the diffusion of the internet. It refers to the online world as a world ""apart,"" as distinct from everyday reality. In cyberspace people can hide behind fake identities, as in the famous The New Yorker cartoon."" (Delfanti, Arvidsson, 150) The term entered the popular culture from science fiction and the arts but is now used by technology strategists, security professionals, government, military and industry leaders and entrepreneurs to describe the domain of the global technology environment, commonly defined as standing for the global network of interdependent information technology infrastructures, telecommunications networks and computer processing systems. Others consider cyberspace to be just a notional environment in which communication over computer networks occurs.[1] The word became popular in the 1990s when the uses of the Internet, networking, and digital communication were all growing dramatically and the term cyberspace was able to represent the many new ideas and phenomena that were emerging.[2][3] As a social experience, individuals can interact, exchange ideas, share information, provide social support, conduct business, direct actions, create artistic media, play games, engage in political discussion, and so on, using this global network. They are sometimes referred to as cybernauts. The term cyberspace has become a conventional means to describe anything associated with the Internet and the diverse Internet culture. The United States government recognizes the interconnected information technology and the interdependent network of information technology infrastructures operating across this medium as part of the US national critical infrastructure. Amongst individuals on cyberspace, there is believed to be a code of shared rules and ethics mutually beneficial for all to follow, referred to as cyberethics. Many view the right to privacy as most important to a functional code of cyberethics.[4]  Such moral responsibilities go hand in hand when working online with global networks, specifically, when opinions are involved with online social experiences.[5][6] According to Chip Morningstar and F. Randall Farmer, cyberspace is defined more by the social interactions involved rather than its technical implementation.[7] In their view, the computational medium in cyberspace is an augmentation of the communication channel between real people; the core characteristic of cyberspace is that it offers an environment that consists of many participants with the ability to affect and influence each other. They derive this concept from the observation that people seek richness, complexity, and depth within a virtual world.
"
Daemon_(computing),Computer Science,4,"In multitasking computer operating systems, a daemon (/ˈdiːmən/ or /ˈdeɪmən/)[1] is a computer program that runs as a background process, rather than being under the direct control of an interactive user. Traditionally, the process names of a daemon end with the letter d, for clarification that the process is in fact a daemon, and for differentiation between a daemon and a normal computer program. For example, .mw-parser-output .monospaced{font-family:monospace,monospace}syslogd is a daemon that implements system logging facility, and sshd is a daemon that serves incoming SSH connections.
 In a Unix environment, the parent process of a daemon is often, but not always, the init process. A daemon is usually created either by a process forking a child process and then immediately exiting, thus causing init to adopt the child process, or by the init process directly launching the daemon. In addition, a daemon launched by forking and exiting typically must perform other operations, such as dissociating the process from any controlling terminal (tty). Such procedures are often implemented in various convenience routines such as daemon(3) in Unix.
 Systems often start daemons at boot time that will respond to network requests, hardware activity, or other programs by performing some task. Daemons such as cron may also perform defined tasks at scheduled times.
"
Data_center,Computer Science,4,"
 A data center (American English)[1] or data centre (British English)[2][note 1] is a building, dedicated space within a building, or a group of buildings[3] used to house computer systems and associated components, such as telecommunications and storage systems.[4][5] Since IT operations are crucial for business continuity, it generally includes redundant or backup components and infrastructure for power supply, data communication connections, environmental controls (e.g. air conditioning, fire suppression) and various security devices. A large data center is an industrial-scale operation using as much electricity as a small town.[6][7]"
Database,Computer Science,4,"A database is an organized collection of data, generally stored and accessed electronically from a computer system. Where databases are more complex they are often developed using formal design and modeling techniques.
 The database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a ""database system"". Often the term ""database"" is also used to loosely refer to any of the DBMS, the database system or an application associated with the database.
 Computer scientists may classify database-management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, referred to as NoSQL because they use different query languages.
"
Data_mining,Computer Science,4,"Data mining is a process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
 The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
 The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10] The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
"
Data_science,Computer Science,4,"
 Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data.[1][2] Data science is related to data mining, machine learning and big data.
 Data science is a ""concept to unify statistics, data analysis and their related methods"" in order to ""understand and analyze actual phenomena"" with data.[3] It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, domain knowledge and information science. Turing award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge.[4][5]"
Data_structure,Computer Science,4,"In computer science, a data structure is a data organization, management, and storage format that enables efficient access and modification.[1][2][3] More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data.[4]"
Data_type,Computer Science,4,"In computer science and computer programming, a data type or simply type is an attribute of data which tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans. A data type constrains the values that an expression, such as a variable or a function, might take. This data type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored. A data type provides a set of values from which an expression (i.e. variable, function, etc.) may take its values.[1][2]"
Debugging,Computer Science,4,"In computer programming and software development, debugging is the process of finding and resolving bugs (defects or problems that prevent correct operation) within computer programs, software, or systems.
 Debugging tactics can involve interactive debugging, control flow analysis, unit testing, integration testing, log file analysis, monitoring at the application or system level, memory dumps, and profiling. Many programming languages and software development tools also offer programs to aid in debugging, known as debuggers.
"
Declaration_(computer_programming),Computer Science,4,"In computer programming, a declaration is a language construct that specifies properties of an identifier: it declares what a word (identifier) ""means"".[1] Declarations are most commonly used for functions, variables, constants, and classes, but can also be used for other entities such as enumerations and type definitions.[1] Beyond the name (the identifier itself) and the kind of entity (function, variable, etc.), declarations typically specify the data type (for variables and constants), or the type signature (for functions); types may also include dimensions, such as for arrays. A declaration is used to announce the existence of the entity to the compiler; this is important in those strongly typed languages that require functions, variables, and constants, and their types to be specified with a declaration before use, and is used in forward declaration.[2] The term ""declaration"" is frequently contrasted with the term ""definition"",[1] but meaning and usage varies significantly between languages; see below.
 Declarations are particularly prominent in languages in the ALGOL tradition, including the BCPL family, most prominently C and C++, and also Pascal. Java uses the term ""declaration"", though Java does not require separate declarations and definitions.
"
Digital_data,Computer Science,4,"Digital data, in information theory and information systems, is the discrete, discontinuous representation of information or works. Numbers and letters are commonly used representations.
 Digital data can be contrasted with analog signals which behave in a continuous manner, and with continuous functions such as sounds, images, and other measurements.
 The word digital comes from the same source as the words digit and digitus (the Latin word for finger), as fingers are often used for discrete counting. Mathematician George Stibitz of Bell Telephone Laboratories used the word digital in reference to the fast electric pulses emitted by a device designed to aim and fire anti-aircraft guns in 1942.[1] The term is most commonly used in computing and electronics, especially where real-world information is converted to binary numeric form as in digital audio and digital photography.
"
Digital_signal_processing,Computer Science,4,"Digital signal processing (DSP) is the use of digital processing, such as by computers or more specialized digital signal processors, to perform a wide variety of signal processing operations.  The digital signals processed in this manner are a sequence of numbers that represent samples of a continuous variable in a domain such as time, space, or frequency. In digital electronics, a digital signal is represented as a pulse train,[1][2] which is typically generated by the switching of a transistor.[3] Digital signal processing and analog signal processing are subfields of signal processing. DSP applications include audio and speech processing, sonar, radar and other sensor array processing, spectral density estimation, statistical signal processing, digital image processing, data compression, video coding, audio coding, image compression, signal processing for telecommunications, control systems, biomedical engineering, and seismology, among others.
 DSP can involve linear or nonlinear operations. Nonlinear signal processing is closely related to nonlinear system identification[4]  and can be implemented in the time, frequency, and spatio-temporal domains.
 The application of digital computation to signal processing allows for many advantages over analog processing in many applications, such as error detection and correction in transmission as well as data compression.[5] Digital signal processing is also fundamental to digital technology, such as digital telecommunication and wireless communications.[6] DSP is applicable to both streaming data and static (stored) data.
"
Discrete_event_simulation,Computer Science,4,"A discrete-event simulation (DES) models the operation of a system as a (discrete) sequence of events in time. Each event occurs at a particular instant in time and marks a change of state in the system.[1] Between consecutive events, no change in the system is assumed to occur; thus the simulation time can directly jump to the occurrence time of the next event, which is called next-event time progression.
 In addition to next-event time progression, there is also an alternative approach, called fixed-increment time progression, where time is broken up into small time slices and the system state is updated according to the set of events/activities happening in the time slice.[2] Because not every time slice has to be simulated, a next-event time simulation can typically run much faster than a corresponding fixed-increment time simulation.
 Both forms of DES contrast with continuous simulation in which the system state is changed continuously over time on the basis of a set of differential equations defining the rates of change of state variables.
"
Disk_storage,Computer Science,4,"Disk storage (also sometimes called drive storage) is a general category of storage mechanisms where data is recorded by various electronic, magnetic, optical, or mechanical changes to a surface layer of one or more rotating disks. A disk drive is a device implementing such a storage mechanism. Notable types are the hard disk drive (HDD) containing a non-removable disk, the  floppy disk drive (FDD) and its removable floppy disk, and various optical disc drives (ODD) and associated optical disc media.
 (The spelling disk and disc are used interchangeably except where trademarks preclude one usage, e.g. the Compact Disc logo. The choice of a particular form is frequently historical, as in IBM's usage of the disk form beginning in 1956 with the ""IBM 350 disk storage unit"").
"
Distributed_computing,Computer Science,4,"Distributed computing is a field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another.[1] The components interact with one another in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.[1] Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.
 A computer program that runs within a distributed system is called  a distributed program (and distributed programming is the process of writing such programs).[2] There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.[3] Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[4] which communicate with each other via message passing.[5]"
Divide_and_conquer_algorithm,Computer Science,4,"In computer science, divide and conquer is an algorithm design paradigm based on multi-branched recursion. A divide-and-conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.
 This divide-and-conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. the Karatsuba algorithm), finding the closest pair of points, syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFT).[1] Understanding and designing divide-and-conquer algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization.[clarification needed] These divide-and-conquer complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.[why?][citation needed] The correctness of a divide-and-conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.
"
Domain_Name_System,Computer Science,4,"
 
 The Domain Name System (DNS) is a hierarchical and decentralized naming system for computers, services, or other resources connected to the Internet or a private network. It associates various information with domain names assigned to each of the participating entities. Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols. By providing a worldwide, distributed directory service, the Domain Name System has been an essential component of the functionality of the Internet since 1985.
 The Domain Name System delegates the responsibility of assigning domain names and mapping those names to Internet resources by designating authoritative name servers for each domain. Network administrators may delegate authority over sub-domains of their allocated name space to other name servers. This mechanism provides distributed and fault-tolerant service and was designed to avoid a single large central database.
 The Domain Name System also specifies the technical functionality of the database service that is at its core. It defines the DNS protocol, a detailed specification of the data structures and data communication exchanges used in the DNS, as part of the Internet Protocol Suite.
 The Internet maintains two principal namespaces, the domain name hierarchy[1] and the Internet Protocol (IP) address spaces.[2] The Domain Name System maintains the domain name hierarchy and provides translation services between it and the address spaces. Internet name servers and a communication protocol implement the Domain Name System.[3] A DNS name server is a server that stores the DNS records for a domain; a DNS name server responds with answers to queries against its database.
 The most common types of records stored in the DNS database are for Start of Authority (SOA), IP addresses (A and AAAA), SMTP mail exchangers (MX), name servers (NS), pointers for reverse DNS lookups (PTR), and domain name aliases (CNAME). Although not intended to be a general purpose database, DNS has been expanded over time to store records for other types of data for either automatic lookups, such as DNSSEC records, or for human queries such as responsible person (RP) records. As a general purpose database, the DNS has also been used in combating unsolicited email (spam) by storing a real-time blackhole list (RBL).  The DNS database is traditionally stored in a structured text file, the zone file, but other database systems are common.
"
Software_documentation,Computer Science,4,"Software documentation is written text or illustration that accompanies computer software or is embedded in the source code.  The documentation either explains how the software operates or how to use it, and may mean different things to people in different roles.
 Documentation is an important part of software engineering. Types of documentation include:
"
Domain_(software_engineering),Computer Science,4,"A domain is the targeted subject area of a computer program. It is a term used in software engineering. Formally it represents the target subject of a specific programming project, whether narrowly or broadly defined.[1] For example, a particular programming project might have had as a goal the creation of a program for a particular hospital, and that hospital would be the domain. Or the project could be expanded in scope to include all hospitals as their domain.[1]:352 In a computer programming design, you define a domain by delineating a set of common requirements, terminology, and functionality for any software program constructed to solve a problem in the area of computer programming, known as domain engineering. The word domain is also taken as a synonym of application domain.[1] Domain in the realm of software engineering commonly refers to the subject area on which the application is intended to apply. In other words, during application development, the domain is the ""sphere of knowledge and activity around which the application logic revolves."" —Andrew Powell-Morse[2] Domain: A sphere of knowledge, influence, or activity. The subject area to which the user applies a program is the domain of the software. —Eric Evans[3] THERE ARE 7 TYPES OF SOFTWARE DOMAINS:-
 • System software
 • Application software
 • Engineering/scientific software
 • Embedded software
 • Product-line software
 • WebApps (Web applications)
 • AI software
"
Domain_Name_System,Computer Science,4,"
 
 The Domain Name System (DNS) is a hierarchical and decentralized naming system for computers, services, or other resources connected to the Internet or a private network. It associates various information with domain names assigned to each of the participating entities. Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols. By providing a worldwide, distributed directory service, the Domain Name System has been an essential component of the functionality of the Internet since 1985.
 The Domain Name System delegates the responsibility of assigning domain names and mapping those names to Internet resources by designating authoritative name servers for each domain. Network administrators may delegate authority over sub-domains of their allocated name space to other name servers. This mechanism provides distributed and fault-tolerant service and was designed to avoid a single large central database.
 The Domain Name System also specifies the technical functionality of the database service that is at its core. It defines the DNS protocol, a detailed specification of the data structures and data communication exchanges used in the DNS, as part of the Internet Protocol Suite.
 The Internet maintains two principal namespaces, the domain name hierarchy[1] and the Internet Protocol (IP) address spaces.[2] The Domain Name System maintains the domain name hierarchy and provides translation services between it and the address spaces. Internet name servers and a communication protocol implement the Domain Name System.[3] A DNS name server is a server that stores the DNS records for a domain; a DNS name server responds with answers to queries against its database.
 The most common types of records stored in the DNS database are for Start of Authority (SOA), IP addresses (A and AAAA), SMTP mail exchangers (MX), name servers (NS), pointers for reverse DNS lookups (PTR), and domain name aliases (CNAME). Although not intended to be a general purpose database, DNS has been expanded over time to store records for other types of data for either automatic lookups, such as DNSSEC records, or for human queries such as responsible person (RP) records. As a general purpose database, the DNS has also been used in combating unsolicited email (spam) by storing a real-time blackhole list (RBL).  The DNS database is traditionally stored in a structured text file, the zone file, but other database systems are common.
"
Double-precision_floating-point_format,Computer Science,4,"Double-precision floating-point format (sometimes called FP64 or float64) is a computer number format, usually occupying 64 bits in computer memory; it represents a wide dynamic range of numeric values by using a floating radix point.
 Floating point is used to represent fractional values, or when a wider range is needed than is provided by fixed point (of the same bit width), even if at the cost of precision. Double precision may be chosen when the range or precision of single precision would be insufficient.
 In the IEEE 754-2008 standard, the 64-bit base-2 format is officially referred to as binary64; it was called double in IEEE 754-1985. IEEE 754 specifies additional floating-point formats, including 32-bit base-2 single precision and, more recently, base-10 representations.
 One of the first programming languages to provide single- and double-precision floating-point data types was Fortran. Before the widespread adoption of IEEE 754-1985, the representation and properties of floating-point data types depended on the computer manufacturer and computer model, and upon decisions made by programming-language implementers. E.g., GW-BASIC's double-precision data type was the 64-bit MBF floating-point format.
"
Download,Computer Science,4,"In computer networks, download means to receive data from a remote system, typically a server[1] such as a web server, an FTP server, an email server, or other similar system. This contrasts with uploading, where data is sent to a remote server.
A download is a file offered for downloading or that has been downloaded, or the process of receiving such a file.
"
Edge_device,Computer Science,4,"An edge device is a device which provides an entry point into enterprise or service provider core networks. Examples include routers, routing switches, integrated access devices (IADs), multiplexers, and a variety of metropolitan area network (MAN) and wide area network (WAN) access devices.  Edge devices also provide connections into carrier and service provider networks. An edge device that connects a local area network to a high speed switch or backbone (such as an ATM switch) may be called an edge concentrator.
"
Encryption,Computer Science,4,"In cryptography, encryption is the process of encoding information. This process converts the original representation of the information, known as plaintext, into an alternative form known as ciphertext. Ideally, only authorized parties can decipher a ciphertext back to plaintext and access the original information. Encryption does not itself prevent interference but denies the intelligible content to a would-be interceptor. For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is possible to decrypt the message without possessing the key, but, for a well-designed encryption scheme, considerable computational resources and skills are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients but not to unauthorized users. Historically, various forms of encryption have been used to aid in cryptography. Early encryption techniques were often utilized in military messaging. Since then, new techniques have emerged and become commonplace in all areas of modern computing.[1] Modern encryption schemes utilize the concepts of public-key and symmetric-key.[1] Modern encryption techniques ensure security because modern computers are inefficient at cracking the encryption.
"
Event_(computing),Computer Science,4,"In programming and software design, an event is an action or occurrence recognized by software, often originating asynchronously from the external environment, that may be handled by the software.  Computer events can be generated or triggered by the system, by the user, or in other ways. Typically, events are handled synchronously with the program flow; that is, the software may have one or more dedicated places where events are handled, frequently an event loop. A source of events includes the user, who may interact with the software through the computer's peripherals - for example, by typing on the keyboard. Another source is a hardware device such as a timer. Software can also trigger its own set of events into the event loop, e.g. to communicate the completion of a task. Software that changes its behavior in response to events is said to be event-driven, often with the goal of being interactive.
"
Event-driven_programming,Computer Science,4,"In computer programming, event-driven programming is a programming paradigm in which the flow of the program is determined by events such as user actions (mouse clicks, key presses), sensor outputs, or messages from other programs or threads. Event-driven programming is the dominant paradigm used in graphical user interfaces and other applications (e.g., JavaScript web applications) that are centered on performing certain actions in response to user input. This is also true of programming for device drivers (e.g., P in USB device driver stacks[1]).
 In an event-driven application, there is generally a main loop that listens for events and then triggers a callback function when one of those events is detected. In embedded systems, the same may be achieved using hardware interrupts instead of a constantly running main loop. Event-driven programs can be written in any programming language, although the task is easier in languages that provide high-level abstractions, such as await and closures.
"
Evolutionary_computing,Computer Science,4,"
In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.
 In evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes. In biological terminology, a population of solutions is subjected to natural selection (or artificial selection) and mutation. As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm.
 Evolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures. Evolutionary computation is also sometimes used in evolutionary biology as an in silico experimental procedure to study common aspects of general evolutionary processes.
"
Executable,Computer Science,4,"In computing, executable code, an executable file, or an executable program, sometimes simply referred to as an executable or binary, causes a computer ""to perform indicated tasks according to encoded instructions"",[1] as opposed to a data file that must be interpreted (parsed} by a program to be meaningful. 
 The exact interpretation depends upon the use. ""Instructions"" is traditionally taken to mean machine code instructions for a physical CPU.[2] In some contexts, a file containing scripting instructions (such as bytecode) may also be considered executable.
"
Execution_(computing),Computer Science,4,"Execution in computer and software engineering is the process by which a computer or  virtual machine executes the instructions of a computer program. Each instruction of a program is a description of a particular 
action which to be carried out in order for a specific problem to be solved; as instructions of a program and therefore the actions they describe are being carried out by an executing machine, specific effects are produced in accordance to the semantics of the instructions being executed. 
 Programs for a computer may be executed in a batch process without human interaction or a user may type commands in an interactive session of an interpreter. In this case, the ""commands"" are simply program instructions, whose execution is chained together. 
 The term run is used almost synonymously. A related meaning of both ""to run"" and ""to execute"" refers to the specific action of a user starting (or launching or invoking) a program, as in ""Please run the application.""
"
Exception_handling,Computer Science,4,"In computing and computer programming, exception handling is the process of responding to the occurrence of exceptions – anomalous or exceptional conditions requiring special processing - during the execution of a program. In general, an exception breaks the normal flow of execution and executes a pre-registered exception handler; the details of how this is done depend on whether it is a hardware or software exception and how the software exception is implemented. It is provided by specialized programming language constructs, hardware mechanisms like interrupts, or operating system (OS) inter-process communication (IPC) facilities like signals. Some exceptions, especially hardware ones, may be handled so gracefully that execution can resume where it was interrupted.
 An alternative approach to exception handling in software is error checking, which maintains normal program flow with later explicit checks for contingencies reported using special return values, an auxiliary global variable such as C's errno, or floating point status flags. Input validation, which preemptively filters exceptional cases, is also an approach.
"
Expression_(computer_science),Computer Science,4,"In computer science, an expression is a syntactic entity in a programming language that may be evaluated to determine its value.[1] It is a combination of one or more constants, variables, functions, and operators that the programming language interprets (according to its particular rules of precedence and of association) and computes to produce (""to return"", in a stateful environment) another value. This process, for mathematical expressions, is called evaluation. 
 In simple settings, the resulting value is usually one of various primitive types, such as numerical, string, boolean, complex data type or other types. 
 Expression is often contrasted with statement - a syntactic entity, which has no value (an instruction.)
"
Fault-tolerant_computer_system,Computer Science,4,"
 Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of (or one or more faults within) some of its components. If its operating quality decreases at all, the decrease is proportional to the severity of the failure, as compared to a naively designed system, in which even a small failure can cause total breakdown. Fault tolerance is particularly sought after in high-availability or life-critical systems. The ability of maintaining functionality when portions of a system break down is referred to as graceful degradation.[1] A fault-tolerant design enables a system to continue its intended operation, possibly at a reduced level, rather than failing completely, when some part of the system fails.[2] The term is most commonly used to describe computer systems designed to continue more or less fully operational with, perhaps, a reduction in throughput or an increase in response time in the event of some partial failure. That is, the system as a whole is not stopped due to problems either in the hardware or the software. An example in another field is a motor vehicle designed so it will continue to be drivable if one of the tires is punctured, or a structure that is able to retain its integrity in the presence of damage due to causes such as fatigue, corrosion, manufacturing flaws, or impact.
 Within the scope of an individual system, fault tolerance can be achieved by anticipating exceptional conditions and building the system to cope with them, and, in general, aiming for self-stabilization so that the system converges towards an error-free state. However, if the consequences of a system failure are catastrophic, or the cost of making it sufficiently reliable is very high, a better solution may be to use some form of duplication. In any case, if the consequence of a system failure is so catastrophic, the system must be able to use reversion to fall back to a safe mode. This is similar to roll-back recovery but can be a human action if humans are present in the loop.
"
Feasibility_study,Computer Science,4,"A feasibility study is an assessment of the practicality of a proposed project or system. A feasibility study aims to objectively and rationally uncover the strengths and weaknesses of an existing business or proposed venture, opportunities and threats present in the natural environment, the resources required to carry through, and ultimately the prospects for success.[1][2] In its simplest terms, the two criteria to judge feasibility are cost required and value to be attained.[3] A well-designed feasibility study should provide a historical background of the business or project, a description of the product or service, accounting statements, details of the operations and management, marketing research and policies, financial data, legal requirements and tax obligations.[1] Generally, feasibility studies precede technical development and project implementation. A feasibility study evaluates the project's potential for success; therefore, perceived objectivity is an important factor in the credibility of the study for potential investors and lending institutions.[citation needed][4] It must therefore be conducted with an objective, unbiased approach to provide information upon which decisions can be based.[citation needed]"
Field_(computer_science),Computer Science,4,"In computer science, data that has several parts, known as a record, can be divided into fields. Relational databases arrange data as sets of database records, so called rows. Each record consists of several fields; the fields of all records form the columns.
Examples of fields: name, gender, hair colour. 
 In object-oriented programming, a field (also called data member or member variable) is a particular piece of data encapsulated within a class or object. In the case of a regular field (also called instance variable), for each instance of the object there is an instance variable: for example, an Employee class has a Name field and there is one distinct name per employee. A static field (also called class variable) is one variable, which is shared by all instances.[1] Fields are abstracted by properties, which allow them to be read and written as if they were fields, but these can be translated to getter and setter method calls.
"
Filename_extension,Computer Science,4,"A filename extension, file extension or file type is an identifier specified as a suffix to the name of a computer file. The extension indicates a characteristic of the file contents or its intended use. A filename extension is typically delimited from the filename with a full stop (period), but in some systems[1] it is separated with spaces.
 Some file systems implement filename extensions as a feature of the file system itself and may limit the length and format of the extension, while others treat filename extensions as part of the filename without special distinction.
"
Filter_(software),Computer Science,4,"A filter is a computer program or subroutine to process a stream, producing another stream. While a single filter can be used individually, they are frequently strung together to form a pipeline.
 Some operating systems such as Unix are rich with filter programs. Windows 7 and later are also rich with filters, as they include Windows PowerShell. In comparison, however, few filters are built into cmd.exe (the original command-line interface of Windows), most of which have significant enhancements relative to the similar filter commands that were available in MS-DOS. OS X includes filters from its underlying Unix base but also has Automator, which allows filters (known as ""Actions"") to be strung together to form a pipeline.
"
Floating_point_arithmetic,Computer Science,4,"
 In computing, floating-point arithmetic (FP) is arithmetic using formulaic representation of real numbers as an approximation to support a trade-off between range and precision. For this reason, floating-point computation is often found in systems which include very small and very large real numbers, which require fast processing times. A number is, in general, represented approximately to a fixed number of significant digits (the significand) and scaled using an exponent in some fixed base; the base for the scaling is normally two, ten, or sixteen. A number that can be represented exactly is of the following form:
 where significand is an integer, base is an integer greater than or equal to two, and exponent is also an integer.
For example:
 The term floating point refers to the fact that a number's radix point (decimal point, or, more commonly in computers, binary point) can ""float""; that is, it can be placed anywhere relative to the significant digits of the number. This position is indicated as the exponent component, and thus the floating-point representation can be thought of as a kind of scientific notation.
 A floating-point system can be used to represent, with a fixed number of digits, numbers of different orders of magnitude: e.g. the distance between galaxies or the diameter of an atomic nucleus can be expressed with the same unit of length. The result of this dynamic range is that the numbers that can be represented are not uniformly spaced; the difference between two consecutive representable numbers varies with the chosen scale.[1] Over the years, a variety of floating-point representations have been used in computers.  In 1985, the IEEE 754 Standard for Floating-Point Arithmetic was established, and since the 1990s, the most commonly encountered representations are those defined by the IEEE.
 The speed of floating-point operations, commonly measured in terms of FLOPS, is an important characteristic of a computer system, especially for applications that involve intensive mathematical calculations.
 A floating-point unit (FPU, colloquially a math coprocessor) is a part of a computer system specially designed to carry out operations on floating-point numbers.
"
For_loop,Computer Science,4,"In computer science, a for-loop (or simply for loop)  is a control flow statement for specifying iteration, which allows code to be executed repeatedly. Various keywords are used to specify this statement: descendants of ALGOL use ""for"", while descendants of Fortran use ""do"". There are other possibilities, for example COBOL which uses ""PERFORM VARYING"".
 A for-loop has two parts: a header specifying the iteration, and a body which is executed once per iteration. The header often declares an explicit loop counter or loop variable, which allows the body to know which iteration is being executed. For-loops are typically used when the number of iterations is known before entering the loop. For-loops can be thought of as shorthands for while-loops which increment and test a loop variable.
 The name for-loop comes from the word for, which is used as the keyword in many programming languages to introduce a for-loop. The term in English dates to ALGOL 58 and was popularized in the influential later ALGOL 60; it is the direct translation of the earlier German für, used in Superplan (1949–1951) by Heinz Rutishauser, who also was involved in defining ALGOL 58 and ALGOL 60. The loop body is executed ""for"" the given values of the loop variable, though this is more explicit in the ALGOL version of the statement, in which a list of possible values and/or increments can be specified.
 In FORTRAN and PL/I, the keyword DO is used for the same thing and it is called a do-loop; this is different from a do-while loop.
"
Formal_methods,Computer Science,4,"In computer science, specifically software engineering and hardware engineering, formal methods are a particular kind of mathematically rigorous techniques for the specification, development and verification of software and hardware systems.[1] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.[2] Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, discrete event dynamic system and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.[3]"
Formal_verification,Computer Science,4,"
 In the context of hardware and software systems, formal verification is the act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics.[1] Formal verification can be helpful in proving the correctness of systems such as: cryptographic protocols, combinational circuits, digital circuits with internal memory, and software expressed as source code.
 The verification of these systems is done by providing a formal proof on an abstract mathematical model of the system, the correspondence between the mathematical model and the nature of the system being otherwise known by construction.  Examples of mathematical objects often used to model systems are: finite state machines, labelled transition systems, Petri nets, vector addition systems, timed automata, hybrid automata, process algebra, formal semantics of programming languages such as operational semantics, denotational semantics, axiomatic semantics and Hoare logic.[2]"
Functional_programming,Computer Science,4,"In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that each return a value, rather than a sequence of imperative statements which change the state of the program.
 In functional programming, functions are treated as first-class citizens, meaning that they can be bound to names (including local identifiers), passed as arguments, and returned from other functions, just as any other data type can. This allows programs to be written in a declarative and composable style, where small functions are combined in a modular manner.
 Functional programming is sometimes treated as synonymous with purely functional programming, a subset of functional programming which treats all functions as deterministic mathematical functions, or pure functions. When a pure function is called with some given arguments, it will always return the same result, and cannot be affected by any mutable state or other side effects. This is in contrast with impure procedures, common in imperative programming, which can have side effects (such as modifying the program's state or taking input from a user). Proponents of purely functional programming claim that by restricting side effects, programs can have fewer bugs, be easier to debug and test, and be more suited to formal verification.[1][2] Functional programming has its roots in academia, evolving from the lambda calculus, a formal system of computation based only on functions. Functional programming has historically been less popular than imperative programming, but many functional languages are seeing use today in industry and education, including Common Lisp, Scheme,[3][4][5][6] Clojure, Wolfram Language,[7][8] Racket,[9] Erlang,[10][11][12] OCaml,[13][14] Haskell,[15][16] and F#.[17][18] Functional programming is also key to some languages that have found success in specific domains, like R in statistics,[19][20] J, K and Q in financial analysis, and XQuery/XSLT for XML.[21][22] Domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, such as not allowing mutable values.[23] In addition, many other programming languages support programming in a functional style or have implemented features from functional programming, such as C++11, Kotlin,[24] Perl,[25] PHP,[26] Python,[27] Raku,[28] and Scala.[29]"
Game_theory,Computer Science,4,"
 Game theory is the study of mathematical models of strategic interaction among rational decision-makers.[1] It has applications in all fields of social science, as well as in logic, systems science and computer science. Originally, it addressed zero-sum games, in which each participant's gains or losses are exactly balanced by those of the other participants. In the 21st century, game theory applies to a wide range of behavioral relations, and is now an umbrella term for the science of logical decision making in humans, animals, and computers.
 Modern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum games and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by the 1944 book Theory of Games and Economic Behavior, co-written with Oskar Morgenstern, which considered cooperative games of several players. The second edition of this book provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.
 Game theory was developed extensively in the 1950s by many scholars. It was explicitly applied to biology in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. As of 2014[update], with the Nobel Memorial Prize in Economic Sciences going to game theorist Jean Tirole, eleven game theorists have won the economics Nobel Prize. John Maynard Smith was awarded the Crafoord Prize for his application of game theory to biology.
"
"Garbage_in,_garbage_out",Computer Science,4,"In computer science, garbage in, garbage out (GIGO) is the concept that flawed, or nonsense input data produces nonsense output or ""garbage"". Sometimes the term rubbish in, rubbish out (RIRO) is used.[1][2][3] The principle also applies more generally to all analysis and logic, in that arguments are unsound if their premises are flawed.
"
Graphics_Interchange_Format,Computer Science,4,"
 The Graphics Interchange Format (GIF; /dʒɪf/ JIF or /ɡɪf/ GHIF) is a bitmap image format that was developed by a team at the online services provider CompuServe led by American computer scientist Steve Wilhite on 15 June 1987.[1] It has since come into widespread usage on the World Wide Web due to its wide support and portability between applications and operating systems.
 The format supports up to 8 bits per pixel for each image, allowing a single image to reference its own palette of up to 256 different colors chosen from the 24-bit RGB color space. It also supports animations and allows a separate palette of up to 256 colors for each frame. These palette limitations make GIF less suitable for reproducing color photographs and other images with color gradients, but well-suited for simpler images such as graphics or logos with solid areas of color. Unlike video, the GIF file format does not support audio.
 GIF images are compressed using the Lempel–Ziv–Welch (LZW) lossless data compression technique to reduce the file size without degrading the visual quality. This compression technique was patented in 1985. Controversy over the licensing agreement between the software patent holder, Unisys, and CompuServe in 1994 spurred the development of the Portable Network Graphics (PNG) standard. By 2004 all the relevant patents had expired.
"
Gigabyte,Computer Science,4,"The gigabyte (/ˈɡɪɡəbaɪt, ˈdʒɪɡə-/; symbol: GB)[1] is a multiple of the unit byte for digital information. The prefix giga means 109 in the International System of Units (SI). Therefore, one gigabyte is one billion bytes.
 This definition is used in all contexts of science, engineering, business, and many areas of computing, including hard drive, solid state drive, and tape capacities, as well as data transmission speeds. However, the term is also used in some fields of computer science and information technology to denote 1073741824 (10243 or 230) bytes, particularly for sizes of RAM. The use of gigabyte may thus be ambiguous. Hard disk capacities as described and marketed by drive manufacturers using the standard metric definition of the gigabyte, but when a 400 GB drive's capacity is displayed by, for example, Microsoft Windows, it is reported as 372 GB, using a binary interpretation. To address this ambiguity, the International System of Quantities standardizes the binary prefixes which denote a series of integer powers of 1024. With these prefixes, a memory module that is labeled as having the size ""1GB"" has one gibibyte (1GiB) of storage capacity. Using the ISQ definitions, the ""372 GB"" reported for the hard drive is actually 372 GiB (400 GB).
"
Global_variable,Computer Science,4,"In computer programming, a global variable is a variable with global scope, meaning that it is visible (hence accessible) throughout the program, unless shadowed. The set of all global variables is known as the global environment or global state. In compiled languages, global variables are generally static variables, whose extent (lifetime) is the entire runtime of the program, though in interpreted languages (including command-line interpreters), global variables are generally dynamically allocated when declared, since they are not known ahead of time.
 In some languages, all variables are global, or global by default, while in most modern languages variables have limited scope, generally lexical scope, though global variables are often available by declaring a variable at the top level of the program. In other languages, however, global variables do not exist; these are generally modular programming languages that enforce a module structure, or class-based object-oriented programming languages that enforce a class structure.
"
Graph_theory,Computer Science,4,"In mathematics, graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines). A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically; see Graph (discrete mathematics) for more detailed definitions and for other variations in the types of graph that are commonly considered. Graphs are one of the prime objects of study in discrete mathematics.
 Refer to the glossary of graph theory for basic definitions in graph theory.
"
Handle_(computing),Computer Science,4,"In computer programming, a handle is an abstract reference to a resource that is used when application software references blocks of memory or objects that are managed by another system like a database or an operating system.
 A resource handle can be an opaque identifier, in which case it is often an integer number (often an array index in an array or ""table"" that is used to manage that type of resource), or it can be a pointer that allows access to further information. Common resource handles include file descriptors, network sockets, database connections, process identifiers (PIDs), and job IDs. PIDs and job IDs are explicitly visible integers; while file descriptors and sockets (which are often implemented as a form of file descriptor) are represented as integers, they are typically considered opaque. In traditional implementations, file descriptors are indices into a (per-process) file descriptor table, thence a (system-wide) file table.
"
Computational_complexity_theory,Computer Science,4,"
Computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.
 A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millennium Prize Problems, is dedicated to the field of computational complexity.[1] Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kinds of problems can, in principle, be solved algorithmically.
"
Hash_function,Computer Science,4,"A hash function is any function that can be used to map data of arbitrary size to fixed-size values. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes.  The values are used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter storage addressing.
 Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval, and storage space only fractionally greater than the total space required for the data or records themselves.  Hashing is a computationally and storage space efficient form of data access which avoids the non-linear access time of ordered and unordered lists and structured trees, and the often exponential storage requirements of direct access of state spaces of large or variable-length keys.
 Use of hash functions relies on statistical properties of key and function interaction: worst case behavior is intolerably bad with a vanishingly small probability, and average case behavior can be nearly optimal (minimal collisions).[1] Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash functions differ from the concepts numbered mainly in terms of data integrity. 
"
Hash_table,Computer Science,4,"
 In computing, a hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values. A hash table uses a hash function to compute an index, also called a hash code, into an array of buckets or slots, from which the desired value can be found.
During lookup, the key is hashed and the resulting hash indicates where the corresponding value is stored.
 Ideally, the hash function will assign each key to a unique bucket, but most hash table designs employ an imperfect hash function, which might cause hash collisions where the hash function generates the same index for more than one key. Such collisions are typically accommodated in some way.
 In a well-dimensioned hash table, the average cost (number of instructions) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at (amortized[2]) constant average cost per operation.[3][4] In many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.
"
Heap_(data_structure),Computer Science,4,"In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C.[2] The node at the ""top"" of the heap (with no parents) is called the root node.
 The heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact, priority queues are often referred to as ""heaps"", regardless of how they may be implemented. In a heap, the highest (or lowest) priority element is always stored at the root. However, a heap is not a sorted structure; it can be regarded as being partially ordered. A heap is a useful data structure when it is necessary to repeatedly remove the object with the highest (or lowest) priority.
 A common implementation of a heap is the binary heap, in which the tree is a binary tree (see figure). The heap data structure, specifically the binary heap, was introduced by J. W. J. Williams in 1964, as a data structure for the heapsort sorting algorithm.[3] Heaps are also crucial in several efficient graph algorithms such as Dijkstra's algorithm. When a heap is a complete binary tree, it has a smallest possible height—a heap with N nodes and for each node a branches always has loga N height.
 Note that, as shown in the graphic, there is no implied ordering between siblings or cousins and no implied sequence for an in-order traversal (as there would be in, e.g., a binary search tree). The heap relation mentioned above applies only between nodes and their parents, grandparents, etc.  The maximum number of children each node can have depends on the type of heap.
"
Heapsort,Computer Science,4,"
 In computer science, heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like selection sort, heapsort divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element from it and inserting it into the sorted region. Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; rather, heap sort maintains the unsorted region in a heap data structure to more quickly find the largest element in each step.[1] Although somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime.  Heapsort is an in-place algorithm, but it is not a stable sort.
 Heapsort was invented by J. W. J. Williams in 1964.[2] This was also the birth of the heap, presented already by Williams as a useful data structure in its own right.[3] In the same year, R. W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.[3]"
Human-computer_interaction,Computer Science,4,"Human–computer interaction (HCI) studies the design and use of computer technology, focused on the interfaces between people (users) and computers. Researchers in the field of HCI observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways. 
 As a field of research, human-computer interaction is situated at the intersection of computer science, behavioural sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their seminal 1983 book, The Psychology of Human–Computer Interaction, although the authors first used the term in 1980[1] and the first known use was in 1975.[2] The term connotes that, unlike other tools with only limited uses (such as a wooden mallet, useful for hitting things, but not much else), a computer has many uses and this takes place as an open-ended dialog between the user and the computer. The notion of dialog likens human–computer interaction to human-to-human interaction, an analogy which is crucial to theoretical considerations in the field.[3][4]"
Identifier_(computer_science),Computer Science,4,"An identifier is a name that identifies (that is, labels the identity of) either a unique object or a unique class of objects, where the ""object"" or class may be an idea, physical countable object (or class thereof), or physical noncountable substance (or class thereof). The abbreviation ID often refers to identity, identification (the process of identifying), or an identifier (that is, an instance of identification). An identifier may be a word, number, letter, symbol, or any combination of those.
 The words, numbers, letters, or symbols may follow an encoding system (wherein letters, digits, words, or symbols stand for (represent) ideas or longer names) or they may simply be arbitrary. When an identifier follows an encoding system, it is often referred to as a code or ID code.  For instance the ISO/IEC 11179 metadata registry standard defines a code as system of valid symbols that substitute for longer values in contrast to identifiers without symbolic meaning. Identifiers that do not follow any encoding scheme are often said to be arbitrary IDs; they are arbitrarily assigned and have no greater meaning. (Sometimes identifiers are called ""codes"" even when they are actually arbitrary, whether because the speaker believes that they have deeper meaning or simply because they are speaking casually and imprecisely.)
 The unique identifier (UID) is an identifier that refers to only one instance—only one particular object in the universe. A part number is an identifier, but it is not a unique identifier—for that, a serial number is needed, to identify each instance of the part design. Thus the identifier ""Model T"" identifies the class (model) of automobiles that Ford's Model T comprises; whereas the unique identifier ""Model T Serial Number 159,862"" identifies one specific member of that class—that is, one particular Model T car, owned by one specific person.
 The concepts of name and identifier are denotatively equal, and the terms are thus denotatively synonymous; but they are not always connotatively synonymous, because code names and ID numbers are often connotatively distinguished from names in the sense of traditional natural language naming. For example, both ""Jamie Zawinski"" and ""Netscape employee number 20"" are identifiers for the same specific human being; but normal English-language connotation may consider ""Jamie Zawinski"" a ""name"" and not an ""identifier"", whereas it considers ""Netscape employee number 20"" an ""identifier"" but not a ""name"". This is an emic indistinction rather than an etic one.
"
Integrated_development_environment,Computer Science,4,"An integrated development environment (IDE) is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of at least a source code editor, build automation tools and a debugger. Some IDEs, such as NetBeans and Eclipse, contain the necessary compiler, interpreter, or both; others, such as SharpDevelop and Lazarus, do not.
 The boundary between an IDE and other parts of the broader software development environment is not well-defined; sometimes a version control system or various tools to simplify the construction of a graphical user interface (GUI) are integrated. Many modern IDEs also have a class browser, an object browser, and a class hierarchy diagram for use in object-oriented software development.
"
Image_processing,Computer Science,4,"Digital image processing is the use of a digital computer to process digital images through an algorithm.[1][2] As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems. The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased.
"
Imperative_programming,Computer Science,4,"In computer science, imperative programming is a  programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.
 The term is often used in contrast to declarative programming, which focuses on what the program should accomplish without specifying how the program should achieve the result.
"
Incremental_build_model,Computer Science,4,"The incremental build model is a method of software development where the product is designed, implemented and tested incrementally (a little more is added each time) until the product is finished. It involves both development and maintenance. The product is defined as finished when it satisfies all of its requirements. This model combines the elements of the waterfall model with the iterative philosophy of prototyping.
 The product is decomposed into a number of components, each of which is designed and built separately (termed as builds).
Each component is delivered to the client when it is complete. This allows partial utilization of the product and avoids a long
development time. It also avoids a large initial capital outlay and subsequent long waiting period. This model of development also helps ease the traumatic effect of introducing a completely new system all at once.
"
Information_space_analysis,Computer Science,4,"Information space analysis is a deterministic method, enhanced by machine intelligence, for locating and assessing resources for team-centric efforts.
 Organizations need to be able to quickly assemble teams backed by the support services, information, and material to do the job. To do so, these teams need to find and assess sources of services that are potential participants in the team effort. To support this initial team and resource development, information needs to be developed via analysis tools that help make sense of sets of data sources in an Intranet or Internet. Part of the process is to characterize them, partition them, and sort and filter them.
 These tools focus on three key issues in forming a collaborative team: 
 Information space analysis tools combine multiple methods to assist in this task. This causes the tools to be particularly well-suited to integrating additional technologies in order to create specialized systems.
"
Information_visualization,Computer Science,4,"Information visualization or information visualisation is the study of (interactive) visual representations of abstract data to reinforce human cognition. The abstract data include both numerical and non-numerical data, such as text and geographic information. The naming of subfields is sometimes confusing. One accepted definition is that it's  information visualization when the spatial representation is chosen, whereas it's scientific visualization when the spatial representation is given.[1]"
Inheritance_(computer_science),Computer Science,4,"In object-oriented programming, inheritance is the mechanism of basing an object or class upon another object (prototype-based inheritance) or class (class-based inheritance), retaining similar implementation. Also defined as deriving new classes (sub classes) from existing ones such as super class or base class and then forming them into a hierarchy of classes. In most class-based object-oriented languages, an object created through inheritance, a ""child object"", acquires all the properties and behaviors of the ""parent object"" , with the exception of: constructors, destructor, overloaded operators and friend functions of the base class. Inheritance allows programmers to create classes that are built upon existing classes,[1] to specify a new implementation while maintaining the same behaviors (realizing an interface), to reuse code and to independently extend original software via public classes and interfaces. The relationships of objects or classes through inheritance give rise to a directed graph.
 Inheritance was invented in 1969 for Simula[2] and is now used throughout many object-oriented programming languages such as Java, C++ or Python.
 An inherited class is called a subclass of its parent class or super class. The term ""inheritance"" is loosely used for both class-based and prototype-based programming, but in narrow use the term is reserved for class-based programming (one class inherits from another), with the corresponding technique in prototype-based programming being instead called delegation (one object delegates to another).
 Inheritance should not be confused with subtyping.[3][4] In some languages inheritance and subtyping agree,[a] whereas in others they differ; in general, subtyping establishes an is-a relationship, whereas inheritance only reuses implementation and establishes a syntactic relationship, not necessarily a semantic relationship (inheritance does not ensure behavioral subtyping). To distinguish these concepts, subtyping is also known as interface inheritance, whereas inheritance as defined here is known as implementation inheritance or code inheritance.[5] Still, inheritance is a commonly used mechanism for establishing subtype relationships.[6] Inheritance is contrasted with object composition, where one object contains another object (or objects of one class contain objects of another class); see composition over inheritance. Composition implements a has-a relationship, in contrast to the is-a relationship of subtyping.
"
Input/output,Computer Science,4,"In computing, input/output or I/O (or, informally, io or IO) is the communication between an information processing system, such as a computer, and the outside world, possibly a human or another information processing system. Inputs are the signals or data received by the system and outputs are the signals or data sent from it. The term can also be used as part of an action; to ""perform I/O"" is to perform an input or output operation.
 I/O devices are the pieces of hardware used by a human (or other system) to communicate with a computer. For instance, a keyboard or computer mouse is an input device for a computer, while monitors and printers are output devices. Devices for communication between computers, such as modems and network cards, typically perform both input and output operations.
 The designation of a device as either input or output depends on perspective. Mice and keyboards take physical movements that the human user outputs and convert them into input signals that a computer can understand; the output from these devices is the computer's input. Similarly, printers and monitors take signals that computers output as input, and they convert these signals into a representation that human users can understand. From the human user's perspective, the process of reading or seeing these representations is receiving output; this type of interaction between computers and humans is studied in the field of human–computer interaction. A further complication is that a device traditionally considered an input device, e.g., card reader, keyboard, may accept control commands to, e.g., select stacker, display keyboard lights, while a device traditionally considered as an output device may provide status data, e.g., low toner, out of paper, paper jam.
 In computer architecture, the combination of the CPU and main memory, to which the CPU can read or write directly using individual instructions, is considered the brain of a computer.  Any transfer of information to or from the CPU/memory combo, for example by reading data from a disk drive, is considered I/O.[1]  The CPU and its supporting circuitry may provide memory-mapped I/O that is used in low-level computer programming, such as in the implementation of device drivers, or may provide access to I/O channels.  An I/O algorithm is one designed to exploit locality and perform efficiently when exchanging data with a secondary storage device, such as a disk drive.
"
Insertion_sort,Computer Science,4,"Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:
 When people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.[2]"
Instruction_cycle,Computer Science,4,"The instruction cycle (also known as the fetch–decode–execute cycle, or simply the fetch-execute cycle) is the cycle that the central processing unit (CPU) follows from boot-up until the computer has shut down in order to process instructions. It is composed of three main stages: the fetch stage, the decode stage, and the execute stage. 
 In simpler CPUs, the instruction cycle is executed sequentially, each instruction being processed before the next one is started. In most modern CPUs, the instruction cycles are instead executed concurrently, and often in parallel, through an instruction pipeline: the next instruction starts being processed before the previous instruction has finished, which is possible because the cycle is broken up into separate steps.[1]"
Integer_(computer_science),Computer Science,4,"In computer science, an integer is a datum of integral data type, a data type that represents some range of mathematical integers. Integral data types may be of different sizes and may or may not be allowed to contain negative values. Integers are commonly represented in a computer as a group of binary digits (bits). The size of the grouping varies so the set of integer sizes available varies between different types of computers. Computer hardware, including virtual machines, nearly always provide a way to represent a processor register or memory address as an integer.
"
Integrated_development_environment,Computer Science,4,"An integrated development environment (IDE) is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of at least a source code editor, build automation tools and a debugger. Some IDEs, such as NetBeans and Eclipse, contain the necessary compiler, interpreter, or both; others, such as SharpDevelop and Lazarus, do not.
 The boundary between an IDE and other parts of the broader software development environment is not well-defined; sometimes a version control system or various tools to simplify the construction of a graphical user interface (GUI) are integrated. Many modern IDEs also have a class browser, an object browser, and a class hierarchy diagram for use in object-oriented software development.
"
Integration_testing,Computer Science,4,"Integration testing (sometimes called integration and testing, abbreviated I&T) is the phase in software testing in which individual software modules are combined and tested as a group. Integration testing is conducted to evaluate the compliance of a system or component with specified functional requirements.[1] It occurs after unit testing and before validation testing. Integration testing takes as its input modules that have been unit tested, groups them in larger aggregates, applies tests defined in an integration test plan to those aggregates, and delivers as its output the integrated system ready for system testing.[2]"
Intellectual_property,Computer Science,4,"Intellectual property (IP) is a category of property that includes intangible creations of the human intellect.[1][2] There are many types of intellectual property, and some countries recognize more than others.[3][4][5][6][7] The most well-known types are copyrights, patents, trademarks, and trade secrets. The modern concept of intellectual property developed in England in the 17th and 18th centuries. The term ""intellectual property"" began to be used in the 19th century, though it was not until the late 20th century that intellectual property became commonplace in the majority of the world's legal systems.[8] The main purpose of intellectual property law is to encourage the creation of a wide variety of intellectual goods.[9] To achieve this, the law gives people and businesses property rights to the information and intellectual goods they create, usually for a limited period of time. This gives economic incentive for their creation, because it allows people to profit from the information and intellectual goods they create.[9] These economic incentives are expected to stimulate innovation and contribute to the technological progress of countries, which depends on the extent of protection granted to innovators.[10] The intangible nature of intellectual property presents difficulties when compared with traditional property like land or goods.  Unlike traditional property, intellectual property is ""indivisible"", since an unlimited number of people can ""consume"" an intellectual good without it being depleted.  Additionally, investments in intellectual goods suffer from problems of appropriation: a landowner can surround their land with a robust fence and hire armed guards to protect it, but a producer of information or literature can usually do very little to stop their first buyer from replicating it and selling it at a lower price. Balancing rights so that they are strong enough to encourage the creation of intellectual goods but not so strong that they prevent the goods' wide use is the primary focus of modern intellectual property law.[11]"
Intelligent_agent,Computer Science,4,"In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent).[1] Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex. A reflex machine, such as a thermostat, is considered an example of an intelligent agent.[2] Intelligent agents are often described schematically as an abstract functional system similar to a computer program. Researchers such as Russell & Norvig (2003) harvtxt error: no target: CITEREFRussellNorvig2003 (help) consider goal-directed behavior to be the essence of intelligence; a normative agent can be labeled with a term borrowed from economics, ""rational agent"". In this rational-action paradigm, an AI possesses an internal ""model"" of its environment. This model encapsulates all the agent's beliefs about the world. The agent also has an ""objective function"" that encapsulates all the AI's goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function.[3] A reinforcement learning agent can have a ""reward function"" that allows the programmers to shape the AI's desired behavior,[4] and an evolutionary algorithm's behavior is shaped by a ""fitness function"".[5] Abstract descriptions of intelligent agents are sometimes called abstract intelligent agents (AIA) to distinguish them from their real world implementations as computer systems, biological systems, or organizations. Some autonomous intelligent agents are designed to function in the absence of human intervention. As intelligent agents become more popular, there are increasing legal risks involved.[6]:815 Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.
 Intelligent agents are also closely related to software agents (an autonomous computer program that carries out tasks on behalf of users). In computer science, an intelligent agent is a software agent that has some intelligence, for example, autonomous programs used for operator assistance or data mining (sometimes referred to as bots) are also called ""intelligent agents"".[citation needed]"
Interface_(computing),Computer Science,4,"In computing, an interface is a shared boundary across which two or more separate components of a computer system exchange information. The exchange can be between software, computer hardware, peripheral devices, humans, and combinations of these.[1] Some computer hardware devices, such as a touchscreen, can both send and receive data through the interface, while others such as a mouse or microphone may only provide an interface to send data to a given system.[2]"
Internal_documentation,Computer Science,4,"Computer software is said to have Internal Documentation if the notes on how and why various parts of code operate is included within the source code as comments.  It is often combined with meaningful variable names with the intention of providing potential future programmers a means of understanding the workings of the code.
 This contrasts with external documentation, where programmers keep their notes and explanations in a separate document.
 Internal documentation has become increasingly popular as it cannot be lost, and any programmer working on the code is immediately made aware of its existence and has it readily available.
"
Internet,Computer Science,4,"
 
 The Internet (or internet)  is the global system of interconnected computer networks that uses the Internet protocol suite (TCP/IP) to communicate between networks and devices. It is a network of networks that consists of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries a vast range of information resources and services, such as the inter-linked hypertext documents and applications of the World Wide Web (WWW), electronic mail, telephony, and file sharing.
 The origins of the Internet date back to the development of packet switching and research commissioned by the United States Department of Defense in the 1960s to enable time-sharing of computers.[1] The primary precursor network, the ARPANET, initially served as a backbone for interconnection of regional academic and military networks in the 1970s. The funding of the National Science Foundation Network as a new backbone in the 1980s, as well as private funding for other commercial extensions, led to worldwide participation in the development of new networking technologies, and the merger of many networks.[2] The linking of commercial networks and enterprises by the early 1990s marked the beginning of the transition to the modern Internet,[3] and generated a sustained exponential growth as generations of institutional, personal, and mobile computers were connected to the network. Although the Internet was widely used by academia in the 1980s, commercialization incorporated its services and technologies into virtually every aspect of modern life.
 Most traditional communication media, including telephony, radio, television, paper mail and newspapers are reshaped, redefined, or even bypassed by the Internet, giving birth to new services such as email, Internet telephony, Internet television, online music, digital newspapers, and video streaming websites. Newspaper, book, and other print publishing are adapting to website technology, or are reshaped into blogging, web feeds and online news aggregators. The Internet has enabled and accelerated new forms of personal interactions through instant messaging, Internet forums, and social networking services. Online shopping has grown exponentially for major retailers, small businesses, and entrepreneurs, as it enables firms to extend their ""brick and mortar"" presence to serve a larger market or even sell goods and services entirely online. Business-to-business and financial services on the Internet affect supply chains across entire industries.
 The Internet has no single centralized governance in either technological implementation or policies for access and usage; each constituent network sets its own policies.[4] The overreaching definitions of the two principal name spaces in the Internet, the Internet Protocol address (IP address) space and the Domain Name System (DNS), are directed by a maintainer organization, the Internet Corporation for Assigned Names and Numbers (ICANN). The technical underpinning and standardization of the core protocols is an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise.[5] In November 2006, the Internet was included on USA Today's list of New Seven Wonders.[6]"
Internet_bot,Computer Science,4,"An Internet bot, web robot, robot or simply bot, is a software application that runs automated tasks (scripts) over the Internet.[1] Typically, bots perform tasks that are simple and repetitive, much faster than a person could. The most extensive use of bots is for web crawling, in which an automated script fetches, analyzes and files information from web servers. More than half of all web traffic is generated by bots.[2] Efforts by web servers to restrict bots vary. Some servers have a robots.txt file which contains the rules governing bot behavior on that server. Any bot that does not follow the rules could, in theory, be denied access to, or removed from, the affected website. If the posted text file has no associated program/software/app, then adhering to the rules is entirely voluntary. There would be no way to enforce the rules, or to ensure that a bot's creator or implementer reads or acknowledges the robots.txt file. Some bots are ""good"" – e.g. search engine spiders – while others are used to launch malicious attacks, for example on political campaigns.[2]"
Interpreter_(computing),Computer Science,4,"In computer science, an interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program. An interpreter generally uses one of the following strategies for program execution:
 Early versions of Lisp programming language and minicomputer and microcomputer BASIC dialects would be examples of the first type. Perl, Python, MATLAB, and Ruby are examples of the second, while UCSD Pascal is an example of the third type. Source programs are compiled ahead of time and stored as machine independent code, which is then linked at run-time and executed by an interpreter and/or compiler (for JIT systems). Some systems, such as Smalltalk and contemporary versions of BASIC and Java may also combine two and three.[2] Interpreters of various types have also been constructed for many languages traditionally associated with compilation, such as Algol, Fortran, Cobol, C and C++.
 While interpretation and compilation are the two main means by which programming languages are implemented, they are not mutually exclusive, as most interpreting systems also perform some translation work, just like compilers. The terms ""interpreted language"" or ""compiled language"" signify that the canonical implementation of that language is an interpreter or a compiler, respectively. A high level language is ideally an abstraction independent of particular implementations.
"
Invariant_(computer_science),Computer Science,4,"In mathematics, an invariant is a property of a mathematical object (or a class of mathematical objects) which remains unchanged, after operations or transformations of a certain type are applied to the objects.[1][2][3] The particular class of objects and type of transformations are usually indicated by the context in which the term is used. For example, the area of a triangle is an invariant with respect to isometries of the Euclidean plane. The phrases ""invariant under"" and ""invariant to"" a transformation are both used.[1] More generally, an invariant with respect to an equivalence relation is a property that is constant on each equivalence class.[4] Invariants are used in diverse areas of mathematics such as geometry, topology, algebra and discrete mathematics. Some important classes of transformations are defined by an invariant they leave unchanged. For example, conformal maps are defined as transformations of the plane that preserve angles. The discovery of invariants is an important step in the process of classifying mathematical objects.[3][4]"
Iteration,Computer Science,4,"
 Iteration is the repetition of a process in order to generate an outcome. The sequence will approach some end point or end value. Each repetition of the process is a single iteration, and the outcome of each iteration is then the starting point of the next iteration. 
 In mathematics and computer science, iteration (along with the related technique of recursion) is a standard element of algorithms.
"
Java_(programming_language),Computer Science,4,"
 Java is a class-based, object-oriented programming language that is designed to have as few implementation dependencies as possible. It is a general-purpose programming language intended to let application developers write once, run anywhere (WORA),[17] meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.[18] Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of the underlying computer architecture. The syntax of Java is similar to C and C++, but has fewer low-level facilities than either of them. The Java runtime provides dynamic capabilities (such as reflection and runtime code modification) that are typically not available in traditional compiled languages. As of 2019[update], Java was one of the most popular programming languages in use according to GitHub,[19][20] particularly for client-server web applications, with a reported 9 million developers.[21] Java was originally developed by James Gosling at Sun Microsystems (which has since been acquired by Oracle) and released in 1995 as a core component of Sun Microsystems' Java platform. The original and reference implementation Java compilers, virtual machines, and class libraries were originally released by Sun under proprietary licenses. As of May 2007, in compliance with the specifications of the Java Community Process, Sun had relicensed most of its Java technologies under the GNU General Public License. Oracle offers its own HotSpot Java Virtual Machine, however the official reference implementation is the OpenJDK JVM which is free open source software and used by most developers including the Eclipse IDE and is the default JVM for almost all Linux distributions. 
 The latest version is Java 15, released in September 2020, with Java 11, a currently supported long-term support (LTS) version, released on September 25, 2018; Oracle released for the legacy Java 8 LTS the last zero-cost public update in January 2019 for commercial use, although it will otherwise still support Java 8 with public updates for personal use up to at least December 2020. Other vendors have begun to offer zero-cost builds of OpenJDK 8 and 11 that are still receiving security and other upgrades.
 Oracle (and others) highly recommend uninstalling older versions of Java because of serious risks due to unresolved security issues.[22] Since Java 9, 10, 12 and 13 are no longer supported, Oracle advises its users to immediately transition to the latest version (currently Java 15) or an LTS release.
"
Kernel_(operating_system),Computer Science,4,"The kernel is a computer program at the core of a computer's operating system that has complete control over everything in the system.[1] It is the ""portion of the operating system code that is always resident in memory"",[2] and facilitates interactions between hardware and software components. On most systems, the kernel is one of the first programs loaded on startup (after the bootloader). It handles the rest of startup as well as memory, peripherals, and input/output (I/O) requests from software, translating them into data-processing instructions for the central processing unit.
 The critical code of the kernel is usually loaded into a separate area of memory, which is protected from access by application programs or other, less critical parts of the operating system. The kernel performs its tasks, such as running processes, managing hardware devices such as the hard disk, and handling interrupts, in this protected kernel space. In contrast, application programs like browsers, word processors, or audio or video players  use a separate area of memory, user space. This separation prevents user data and kernel data from interfering with each other and causing instability and slowness,[1] as well as preventing malfunctioning application programs from crashing the entire operating system.
 The kernel's interface is a low-level abstraction layer. When a process requests a service to the kernel, it must invoke a system call, usually through a wrapper function that is exposed to userspace applications by system libraries which embed the assembly code for entering the kernel after loading the CPU registers with the syscall number and its parameters (e.g., UNIX-like operating systems accomplish this task using the C standard library).
 There are different kernel architecture designs. Monolithic kernels run entirely in a single address space with the CPU executing in supervisor mode, mainly for speed. Microkernels run most but not all of their services in user space,[3] like user processes do, mainly for resilience and modularity.[4] MINIX 3 is a notable example of microkernel design. Instead, the Linux kernel is monolithic, although it is also modular, for it can insert and remove loadable kernel modules at runtime.
 This central component of a computer system is responsible for 'running' or 'executing' programs. The kernel takes responsibility for deciding at any time which of the many running programs should be allocated to the processor or processors.
"
Library_(computing),Computer Science,4,"
 In computer science, a library is a collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets.
 A library is also a collection of implementations of behavior, written in terms of a language, that has a well-defined interface by which the behavior is invoked. For instance, people who want to write a higher-level program can use a library to make system calls instead of implementing those system calls over and over again. In addition, the behavior is provided for reuse by multiple independent programs. A program invokes the library-provided behavior via a mechanism of the language. For example, in a simple imperative language such as C, the behavior in a library is invoked by using C's normal function-call. What distinguishes the call as being to a library function, versus being to another function in the same program, is the way that the code is organized in the system.
 Library code is organized in such a way that it can be used by multiple programs that have no connection to each other, while code that is part of a program is organized to be used only within that one program. This distinction can gain a hierarchical notion when a program grows large, such as a multi-million-line program. In that case, there may be internal libraries that are reused by independent sub-portions of the large program. The distinguishing feature is that a library is organized for the purposes of being reused by independent programs or sub-programs, and the user only needs to know the interface and not the internal details of the library.
 The value of a library lies in the reuse of the behavior. When a program invokes a library, it gains the behavior implemented inside that library without having to implement that behavior itself. Libraries encourage the sharing of code in a modular fashion and ease the distribution of the code.
 The behavior implemented by a library can be connected to the invoking program at different program lifecycle phases. If the code of the library is accessed during the build of the invoking program, then the library is called a static library.[1] An alternative is to build the executable of the invoking program and distribute that, independently of the library implementation. The library behavior is connected after the executable has been invoked to be executed, either as part of the process of starting the execution, or in the middle of execution. In this case the library is called a dynamic library (loaded at runtime). A dynamic library can be loaded and linked when preparing a program for execution, by the linker. Alternatively, in the middle of execution, an application may explicitly request that a module be loaded.
 Most compiled languages have a standard library, although programmers can also create their own custom libraries. Most modern software systems provide libraries that implement the majority of the system services. Such libraries have organized the services which a modern application requires. As such, most code used by modern applications is provided in these system libraries.
"
Linear_search,Computer Science,4,"In computer science, a linear search or sequential search is a method for finding an element within a list. It sequentially checks each element of the list until a match is found or the whole list has been searched.[1] A linear search runs in at worst linear time and makes at most n comparisons, where n is the length of the list. If each element is equally likely to be searched, then linear search has an average case of n+1/2 comparisons, but the average case can be affected if the search probabilities for each element vary. Linear search is rarely practical because other search algorithms and schemes, such as the binary search algorithm and hash tables, allow significantly faster searching for all but short lists.[2]"
Linked_list,Computer Science,4,"In computer science, a linked list is a linear collection of data elements whose order is not given by their physical placement in memory. Instead, each element points to the next. It is a data structure consisting of a collection of nodes which together represent a sequence. In its most basic form, each node contains: data, and a reference (in other words, a link) to the next node in the sequence. This structure allows for efficient insertion or removal of elements from any position in the sequence during iteration. More complex variants add additional links, allowing more efficient insertion or removal of nodes at arbitrary positions. A drawback of linked lists is that access time is linear (and difficult to pipeline). Faster access, such as random access, is not feasible. Arrays have better cache locality compared to linked lists.
 Linked lists are among the simplest and most common data structures. They can be used to implement several other common abstract data types, including lists, stacks, queues, associative arrays, and S-expressions, though it is not uncommon to implement those data structures directly without using a linked list as the basis.
 The principal benefit of a linked list over a conventional array is that the list elements can be easily inserted or removed without reallocation or reorganization of the entire structure because the data items need not be stored contiguously in memory or on disk, while restructuring an array at run-time is a much more expensive operation. Linked lists allow insertion and removal of nodes at any point in the list, and allow doing so with a constant number of operations by keeping the link previous to the link being added or removed in memory during list traversal.
 On the other hand, since simple linked lists by themselves do not allow random access to the data or any form of efficient indexing, many basic operations—such as obtaining the last node of the list, finding a node that contains a given datum, or locating the place where a new node should be inserted—may require iterating through most or all of the list elements. The advantages and disadvantages of using linked lists are given below. Linked list are dynamic, so the length of list can increase or decrease as necessary. Each node does not necessarily follow the previous one physically in the memory.
"
Linker_(computing),Computer Science,4,"
 In computing, a linker or link editor is a computer system program that takes one or more object files (generated by a compiler or an assembler) and combines them into a single executable file, library file, or another ""object"" file.
 A simpler version that writes its output directly to memory is called the loader, though loading is typically considered a separate process.[1][2]"
List_(abstract_data_type),Computer Science,4,"In computer science, a list or sequence is an abstract data type that represents a countable number of ordered values, where the same value may occur more than once. An instance of a list is a computer representation of the mathematical concept of a tuple or finite sequence; the (potentially) infinite analog of a list is a stream.[1]:§3.5 Lists are a basic example of containers, as they contain other values. If the same value occurs multiple times, each occurrence is considered a distinct item.
 The name list is also used for several concrete data structures that can be used to implement abstract lists, especially linked lists and arrays. In some contexts, such as in Lisp programming, the term list may refer specifically to a linked list rather than an array. In class-based programming, lists are usually provided as instances of subclasses of a generic ""list"" class, and traversed via separate iterators.
 Many programming languages provide support for list data types, and have special syntax and semantics for lists and list operations. A list can often be constructed by writing the items in sequence, separated by commas, semicolons, and/or spaces, within a pair of delimiters such as parentheses '()', brackets '[]', braces '{}', or angle brackets '<>'. Some languages may allow list types to be indexed or sliced like array types, in which case the data type is more accurately described as an array.
 In type theory and functional programming, abstract lists are usually defined inductively by two operations: nil that yields the empty list, and cons, which adds an item at the beginning of a list.[2]"
Loader_(computing),Computer Science,4,"In computer systems a loader is the part of an operating system that is responsible for loading programs and libraries. It is one of the essential stages in the process of starting a program, as it places programs into memory and prepares them for execution. Loading a program involves reading the contents of the executable file containing the program instructions into memory, and then carrying out other required preparatory tasks to prepare the executable for running. Once loading is complete, the operating system starts the program by passing control to the loaded program code.
 All operating systems that support program loading have loaders, apart from highly specialized computer systems that only have a fixed set of specialized programs. Embedded systems typically do not have loaders, and instead, the code executes directly from ROM. In order to load the operating system itself, as part of booting, a specialized boot loader is used. In many operating systems, the loader resides permanently in memory, though some operating systems that support virtual memory may allow the loader to be located in a region of memory that is pageable.
 In the case of operating systems that support virtual memory, the loader may not actually copy the contents of executable files into memory, but rather may simply declare to the virtual memory subsystem that there is a mapping between a region of memory allocated to contain the running program's code and the contents of the associated executable file. (See memory-mapped file.) The virtual memory subsystem is then made aware that pages with that region of memory need to be filled on demand if and when program execution actually hits those areas of unfilled memory. This may mean parts of a program's code are not actually copied into memory until they are actually used, and unused code may never be loaded into memory at all.
"
Logic_error,Computer Science,4,"In computer programming, a logic error  is a bug in a program that causes it to operate incorrectly, but not to terminate abnormally (or crash). A logic error produces unintended or undesired output or other behaviour, although it may not immediately be recognized as such.
 Logic errors occur in both compiled and interpreted languages. Unlike a program with a syntax error, a program with a logic error is a valid program in the language, though it does not behave as intended. Often the only clue to the existence of logic errors is the production of wrong solutions, though static analysis may sometimes spot them.
"
Logic_programming,Computer Science,4,"Logic programming is a programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:
 and are read declaratively as logical implications:
 H is called the head of the rule and B1, ..., Bn is called the body. Facts are rules that have no body, and are written in the simplified form:
 In the simplest case in which H, B1, ..., Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there are many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulas. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.
 In ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be controlled by the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:
 Consider the following clause as an example:
 based on an example used by Terry Winograd[1] to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X which is fallible by finding an X which is human. Even facts have a procedural interpretation. For example, the clause:
 can be used both as a procedure to show that socrates is human, and as a procedure to find an X that is human by ""assigning"" socrates to X.
 The declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.
"
Machine_learning,Computer Science,4,"Machine learning (ML) is the study of computer algorithms that improve automatically through experience.[1] It is seen as a subset of artificial intelligence. Machine learning algorithms build a model based on sample data, known as ""training data"", in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.
 A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[4][5] In its application across business problems, machine learning is also referred to as predictive analytics.
"
Machine_vision,Computer Science,4,"Machine vision (MV) is the technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection,  process control, and robot guidance, usually in industry. Machine vision refers to many technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of computer science. It attempts to integrate existing technologies in new ways and apply them to solve real world problems. The term is the prevalent one for these functions in industrial automation environments but is also used for these functions in other environments such as security and vehicle guidance.
 The overall machine vision process includes planning the details of the requirements and project, and then creating a solution. During run-time, the process starts with imaging, followed by  automated analysis of the image and extraction of the required information.
"
Mathematical_logic,Computer Science,4,"
 Mathematical logic is a subfield of mathematics exploring the applications of formal logic to mathematics.  It bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science.[1]  The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.
 Mathematical logic is often divided into the fields of set theory, model theory, recursion theory, and proof theory.  These areas share basic results on logic, particularly first-order logic, and definability. In computer science (particularly in the ACM Classification) mathematical logic encompasses additional topics not detailed in this article; see Logic in computer science for those.
 Since its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.
"
Matrix_(mathematics),Computer Science,4,"In mathematics, a matrix (plural matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns.[1][2] For example, the dimension of the matrix below is 2 × 3 (read ""two by three""), because there are two rows and three columns:
 Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for an (m×n)-matrix times an (n×p)-matrix, resulting in an (m×p)-matrix). There is no product the other way round—a first hint that matrix multiplication is not commutative. Any matrix can be multiplied element-wise by a scalar from its associated field. Matrices are often denoted using capital roman letters such as 



A


{  A}
, 



B


{  B}
 and 



C


{  C}
.[3] The individual items in an m×n matrix A, often denoted by ai,j, where i and j usually vary from 1 to m and n, respectively, are called its elements or entries.[4][5] For conveniently expressing an element of the results of matrix operations, the indices of the element are often attached to the parenthesized or bracketed matrix expression (e.g., (AB)i,j refers to an element of a matrix product). In the context of abstract index notation, this ambiguously refers also to the whole matrix product.
 A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. 
 If the matrix is square, then it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.
 Applications of matrices are found in most scientific fields.[6] In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. 
 In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[7] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.
 A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.
"
Computer_data_storage,Computer Science,4,"
 Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.[1]:15–16 The central processing unit (CPU) of a computer is what manipulates data by performing computations.  In practice, almost all computers use a storage hierarchy,[1]:468–473 which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally the fast volatile technologies (which lose data when off power) are referred to as ""memory"", while slower persistent technologies are referred to as ""storage"".
 Even the first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.
"
Merge_sort,Computer Science,4,"In computer science, merge sort (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945.[2] A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and von Neumann as early as 1948.[3]"
Method_(computer_programming),Computer Science,4,"A method in object-oriented programming (OOP) is a procedure associated with a message and an object. An object consists of data and behavior; these comprise an interface, which specifies how the object may be utilized by any of its various consumers.[1] Data is represented as properties of the object, and behaviors are represented as methods. For example, a Window object could have methods such as open and close, while its state (whether it is open or closed at any given point in time) would be a property.
 In class-based programming, methods are defined in a class, and objects are instances of a given class. One of the most important capabilities that a method provides is method overriding - the same name (e.g., area) can be used for multiple different kinds of classes. This allows the sending objects to invoke behaviors and to delegate the implementation of those behaviors to the receiving object. A method in Java programming sets the behavior of a class object. For example, an object can send an area message to another object and the appropriate formula is invoked whether the receiving object is a rectangle, circle, triangle, etc.
 Methods also provide the interface that other classes use to access and modify the properties of an object; this is known as encapsulation. Encapsulation and overriding are the two primary distinguishing features between methods and procedure calls.[2]"
Software_development_process,Computer Science,4,"In software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management.  It is also known as a software development life cycle (SDLC).  The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.[1] Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.
 A life-cycle ""model"" is sometimes considered a more general term for a category of methodologies and a software development ""process"" a more specific term to refer to a specific process chosen by a specific organization.[citation needed] For example, there are many specific software development processes that fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle.
"
Modem,Computer Science,4,"A modem – a portmanteau of ""modulator-demodulator"" – is a hardware device that converts data from a digital format, intended for communication directly between devices with specialized wiring, into one suitable for a transmission medium such as telephone lines or radio. A modem modulates one or more carrier wave signals to encode digital information for transmission, and demodulates signals to decode the transmitted information. The goal is to produce a signal that can be transmitted easily and decoded reliably to reproduce the original digital data.
 Modems can be used with almost any means of transmitting analog signals, from light-emitting diodes to radio. A common type of modem is one that turns the digital data of a computer into a modulated electrical signal for transmission over telephone lines, to be demodulated by another modem at the receiver side to recover the digital data.
"
Natural_language_processing,Computer Science,4,"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.
 Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation.
"
Node_(computer_science),Computer Science,4,"A node is a basic unit of a data structure, such as a linked list or tree data structure. Nodes contain data and also may link to other nodes. Links between nodes are often implemented by pointers.
"
Number_theory,Computer Science,4,"Number theory (or arithmetic or higher arithmetic in older usage) is a branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions. German mathematician Carl Friedrich Gauss (1777–1855) said, ""Mathematics is the queen of the sciences—and number theory is the queen of mathematics.""[1] Number theorists study prime numbers as well as the properties of mathematical objects made out of integers (for example, rational numbers) or defined as generalizations of the integers (for example, algebraic integers). 
 Integers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory are often best understood through the study of analytical objects (for example, the Riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers, for example, as approximated by the latter (Diophantine approximation).
 The older term for number theory is arithmetic. By the early twentieth century, it had been superseded by ""number theory"".[note 1] (The word ""arithmetic"" is used by the general public to mean ""elementary calculations""; it has also acquired other meanings in mathematical logic, as in Peano arithmetic, and computer science, as in floating point arithmetic.) The use of the term arithmetic for number theory regained some ground in the second half of the 20th century, arguably in part due to French influence.[note 2] In particular, arithmetical is preferred as an adjective to number-theoretic.[by whom?]"
Numerical_analysis,Computer Science,4,"
 Numerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). Numerical analysis naturally finds application in all fields of engineering and the physical sciences, but in the 21st century also the life sciences, social sciences, medicine, business and even the arts have adopted elements of scientific computations. The growth in computing power has revolutionized the use of realistic mathematical models in science and engineering, and subtle numerical analysis is required to implement these detailed models of the world. For example, ordinary differential equations appear in celestial mechanics (predicting the motions of planets, stars and galaxies); numerical linear algebra is important for data analysis;[2][3][4] stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.
 Before the advent of modern computers, numerical methods often depended on hand interpolation formulas applied to data from large printed tables. Since the mid 20th century, computers calculate the required functions instead, but many of the same formulas nevertheless continue to be used as part of the software algorithms.[5] The numerical point of view goes back to the earliest mathematical writings. A tablet from the Yale Babylonian Collection (YBC 7289), gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square.
 Numerical analysis continues this long tradition: rather than exact symbolic answers, which can only be applied to real-world measurements by translation into digits, it gives approximate solutions within specified error bounds.
"
Numerical_method,Computer Science,4,"In numerical analysis, a numerical method is a mathematical tool designed to solve numerical problems. The implementation of a numerical method with an appropriate convergence check in a programming language is called a numerical algorithm
"
Object_(computer_science),Computer Science,4,"In computer science, an object can be a variable, a data structure, a function, or a method, and as such, is a value in memory referenced by an identifier.
 In the object-oriented programming paradigm object can be a combination of variables, functions, and data structures; in particular in class-based flavour of the paradigm it refers to a particular instance of a class.
 In the relational model of database management, an object can be a table or column, or an association between data and a database entity (such as relating a person's age to a specific person).[1]"
Object_code,Computer Science,4,"
 In computing, object code or object module is the product of a compiler.[1] In a general sense object code is a sequence of statements or instructions in a computer language,[2] usually a machine code language (i.e., binary) or an intermediate language such as register transfer language (RTL). The term indicates that the code is the goal or result of the compiling process, with some early sources referring to source code as a ""subject program"".
"
Object-oriented_analysis_and_design,Computer Science,4,"Object-oriented analysis and design (OOAD) is a technical approach for analyzing and designing an application, system, or business by applying object-oriented programming, as well as using visual modeling throughout the software development process to guide stakeholder communication and product quality.
 OOAD in modern software engineering is typically conducted in an iterative and incremental way. The outputs of OOAD activities are analysis models (for OOA) and design models (for OOD) respectively. The intention is for these to be continuously refined and evolved, driven by key factors like risks and business value.
"
Object-oriented_programming,Computer Science,4,"
 Object-oriented programming (OOP) is a programming paradigm based on the concept of ""objects"", which can contain data and code: data in the form of fields (often known as attributes or properties), and code, in the form of procedures (often known as methods). 
 A feature of objects is that an object's own procedures can access and often modify the data fields of itself (objects have a notion of this or self). In OOP, computer programs are designed by making them out of objects that interact with one another.[1][2] OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types.
 Many of the most widely used programming languages (such as C++, Java, Python, etc.) are multi-paradigm and they support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. Significant object-oriented languages include: (list order based on TIOBE index)
Java,
C++,
C#,
Python,
R,
PHP,
Visual Basic.NET,
JavaScript,
Ruby,
Perl,
Object Pascal,
Objective-C,
Dart,
Swift,
Scala,
Kotlin,
Common Lisp,
MATLAB,
and
Smalltalk.
"
Open-source_software,Computer Science,4,"
 
 Open-source software (OSS) is a type of computer software in which source code is released under a license in which the copyright holder grants users the rights to use, study, change, and distribute the software to anyone and for any purpose.[1] Open-source software may be developed in a collaborative public manner. Open-source software is a prominent example of open collaboration.[2] Open-source software development can bring in diverse perspectives beyond those of a single company. A 2008 report by the Standish Group stated that adoption of open-source software models has resulted in savings of about $60 billion (£48 billion) per year for consumers.[3][4]"
Operating_system,Computer Science,4,"
 An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs.
 Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.
 For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,[1][2] although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer –  from cellular phones and video game consoles to web servers and supercomputers.
 The dominant desktop operating system is Microsoft Windows with a market share of around 76.45%. macOS by Apple Inc. is in second place (17.72%), and the varieties of Linux are collectively in third place (1.73%).[3] In the mobile sector (including smartphones and tablets), Android's share is up to 72% in the year 2020.[4] According to third quarter 2016 data, Android's share on smartphones is dominant with 87.5 percent with also a growth rate of 10.3 percent per year, followed by Apple's iOS with 12.1 percent with per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent.[5] Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems, such as embedded and real-time systems, exist for many applications.
"
Optical_fiber,Computer Science,4,"
 An optical fiber is a flexible, transparent fiber made by drawing glass (silica) or plastic to a diameter slightly thicker than that of a human hair.[1] Optical fibers are used most often as a means to transmit light[a] between the two ends of the fiber and find wide usage in fiber-optic communications, where they permit transmission over longer distances and at higher bandwidths (data transfer rates) than electrical cables. Fibers are used instead of metal wires because signals travel along them with less loss; in addition, fibers are immune to electromagnetic interference, a problem from which metal wires suffer.[2] Fibers are also used for illumination and imaging, and are often wrapped in bundles so they may be used to carry light into, or images out of confined spaces, as in the case of a fiberscope.[3] Specially designed fibers are also used for a variety of other applications, some of them being fiber optic sensors and fiber lasers.[4] Optical fibers typically include a core surrounded by a transparent cladding material with a lower index of refraction. Light is kept in the core by the phenomenon of total internal reflection which causes the fiber to act as a waveguide.[5] Fibers that support many propagation paths or transverse modes are called multi-mode fibers, while those that support a single mode are called single-mode fibers (SMF). Multi-mode fibers generally have a wider core diameter[6] and are used for short-distance communication links and for applications where high power must be transmitted.[7] Single-mode fibers are used for most communication links longer than 1,000 meters (3,300 ft).[citation needed] Being able to join optical fibers with low loss is important in fiber optic communication.[8] This is more complex than joining electrical wire or cable and involves careful cleaving of the fibers, precise alignment of the fiber cores, and the coupling of these aligned cores. For applications that demand a permanent connection a fusion splice is common. In this technique, an electric arc is used to  melt the ends of the fibers together. Another common technique is a mechanical splice, where the ends of the fibers are held in contact by mechanical force. Temporary or semi-permanent connections are made by means of specialized optical fiber connectors.[9] The field of applied science and engineering concerned with the design and application of optical fibers is known as fiber optics. The term was coined by Indian-American physicist Narinder Singh Kapany, who is widely acknowledged as the father of fiber optics.[10]"
Pair_programming,Computer Science,4,"Pair programming is an agile software development technique in which two programmers work together at one workstation. One, the driver, writes code while the other, the observer or navigator,[1] reviews each line of code as it is typed in. The two programmers switch roles frequently.
 While reviewing, the observer also considers the ""strategic"" direction of the work, coming up with ideas for improvements and likely future problems to address. This is intended to free the driver to focus all of their attention on the ""tactical"" aspects of completing the current task, using the observer as a safety net and guide.
"
Parallel_computing,Computer Science,4,"Parallel computing is a type of computation where many calculations or the execution of processes are carried out simultaneously.[1] Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling.[2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.[4] Parallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU).[5][6] In parallel computing, a computational task is typically broken down into several, often many, very similar sub-tasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.
 Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.
 In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones,[7] because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting optimal parallel program performance.
 A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law.
"
Parameter_(computer_programming),Computer Science,4,"In computer programming, a parameter or a  formal argument, is a special kind of variable, used in a subroutine to refer to one of the pieces of data provided as input to the subroutine.[a] These pieces of data are the values[1][2][3] of the arguments (often called actual arguments or actual parameters) with which the subroutine is going to be called/invoked. An ordered list of parameters is usually included in the definition of a subroutine, so that, each time the subroutine is called, its arguments for that call are evaluated, and the resulting values can be assigned to the corresponding parameters.
 Unlike argument in usual mathematical usage, the argument in computer science is thus the actual input expression passed/supplied to a function, procedure, or routine in the invocation/call statement, whereas the parameter is the variable inside the implementation of the subroutine. For example, if one defines the add subroutine as def add(x, y): return x + y, then x, y are parameters, while if this is called as add(2, 3), then 2, 3 are the arguments. Note that variables (and expressions thereof) from the calling context can be arguments: if the subroutine is called as a = 2; b = 3; add(a, b) then the variables a, b are the arguments, not the values 2, 3. See the Parameters and arguments section for more information.
 In the most common case, call by value, a parameter acts within the subroutine as a new local variable initialized to the value of the argument (a local (isolated) copy of the argument if the argument is a variable), but in other cases, e.g. call by reference, the argument variable supplied by the caller can be affected by actions within the called subroutine (as discussed in evaluation strategy). 
 The semantics for how parameters can be declared and how the (value of) arguments are passed to the parameters of subroutines are defined by the language, but the details of how this is represented in any particular computer system depend on the calling conventions of that system.
"
Peripheral,Computer Science,4,"A peripheral or peripheral device is an auxillary device used to put information into and get information out of the computer.[1] Several categories of peripheral devices may be identified, based on their relationship with the computer:
 Many modern electronic devices, such as Internet-enabled digital watches, keyboards, and tablet computers, have interfaces for use as computer peripheral devices.
"
Pointer_(computer_programming),Computer Science,4,"Donald Knuth, Structured Programming, with go to Statements[1]
 In computer science, a pointer is an object in many programming languages that stores a memory address. This can be that of another value located in computer memory, or in some cases, that of memory-mapped computer hardware. A pointer references a location in memory, and obtaining the value stored at that location is known as dereferencing the pointer. As an analogy, a page number in a book's index could be considered a pointer to the corresponding page; dereferencing such a pointer would be done by flipping to the page with the given page number and reading the text found on that page. The actual format and content of a pointer variable is dependent on the underlying computer architecture.
 Using pointers significantly improves performance for repetitive operations, like traversing iterable data structures (e.g. strings, lookup tables, control tables and tree structures). In particular, it is often much cheaper in time and space to copy and dereference pointers than it is to copy and access the data to which the pointers point.
 Pointers are also used to hold the addresses of entry points for called subroutines in procedural programming and for run-time linking to dynamic link libraries (DLLs). In object-oriented programming, pointers to functions are used for binding methods, often using virtual method tables.
 A pointer is a simple, more concrete implementation of the more abstract reference data type. Several languages, especially low-level languages, support some type of pointer, although some have more restrictions on their use than others. While ""pointer"" has been used to refer to references in general, it more properly applies to data structures whose interface explicitly allows the pointer to be manipulated (arithmetically via pointer arithmetic) as a memory address, as opposed to a magic cookie or capability which does not allow such.[citation needed] Because pointers allow both protected and unprotected access to memory addresses, there are risks associated with using them, particularly in the latter case. Primitive pointers are often stored in a format similar to an integer; however, attempting to dereference or ""look up"" such a pointer whose value is not a valid memory address will cause a program to crash. To alleviate this potential problem, as a matter of type safety, pointers are considered a separate type parameterized by the type of data they point to, even if the underlying representation is an integer. Other measures may also be taken (such as validation & bounds checking), to verify that the pointer variable contains a value that is both a valid memory address and within the numerical range that the processor is capable of addressing.
"
Postcondition,Computer Science,4,"In computer programming, a postcondition is a condition or predicate that must always be true just after the execution of some section of code or after an operation in a formal specification. Postconditions are sometimes tested using assertions within the code itself. Often, postconditions are simply included in the documentation of the affected section of code.
 For example: The result of a factorial is always an integer and greater than or equal to 1. So a program that calculates the factorial of an input number would have postconditions that the result after the calculation be an integer and that it be greater than or equal to 1.  Another example: a program that calculates the square root of an input number might have the postconditions that the result be a number and that its square be equal to the input.
"
Precondition,Computer Science,4,"In computer programming, a precondition is a condition or predicate that must always be true just prior to the execution of some section of code or before an operation in a formal specification.
 If a precondition is violated, the effect of the section of code becomes undefined and thus may or may not carry out its intended work.  Security problems can arise due to incorrect preconditions.
 Often, preconditions are simply included in the documentation of the affected section of code.  Preconditions are sometimes tested using guards or assertions within the code itself, and some languages have specific syntactic constructions for doing so.
 For example: the factorial is only defined for integers greater than or equal to zero.  So a program that calculates the factorial of an input number would have preconditions that the number be an integer and that it be greater than or equal to zero.
"
Primary_storage,Computer Science,4,"
 Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.[1]:15–16 The central processing unit (CPU) of a computer is what manipulates data by performing computations.  In practice, almost all computers use a storage hierarchy,[1]:468–473 which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally the fast volatile technologies (which lose data when off power) are referred to as ""memory"", while slower persistent technologies are referred to as ""storage"".
 Even the first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.
"
Primitive_data_type,Computer Science,4,"In computer science, primitive data type is either of the following:[citation needed] In most programming languages, all basic data types are built-in. In addition, many languages also provide a set of composite data types.
 Depending on the language and its implementation, primitive data types may or may not have a one-to-one correspondence with objects in the computer's memory. However, one usually expects operations on basic primitive data types to be the fastest language constructs there are.[citation needed] Integer addition, for example, can be performed as a single machine instruction, and some processors offer specific instructions to process sequences of characters with a single instruction.[citation needed] In particular, the C standard mentions that ""a 'plain' int object has the natural size suggested by the architecture of the execution environment.""[citation needed] This means that int is likely to be 32 bits long on a 32-bit architecture. Basic primitive types are almost always value types.
 Most languages do not allow the behavior or capabilities of primitive (either built-in or basic) data types to be modified by programs. Exceptions include Smalltalk, which permits all data types to be extended within a program, adding to the operations that can be performed on them or even redefining the built-in operations.[citation needed]"
Priority_queue,Computer Science,4,"In computer science, a priority queue is an abstract data type similar to a regular queue or stack data structure in which each element additionally has a ""priority"" associated with it. In a priority queue, an element with high priority is served before an element with low priority. In some implementations, if two elements have the same priority, they are served according to the order in which they were enqueued, while in other implementations, ordering of elements with the same priority is undefined.
 While priority queues are often implemented with heaps, they are conceptually distinct from heaps. A priority queue is a concept like ""a list"" or ""a map""; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with a heap or a variety of other methods such as an unordered array.
"
Procedural_programming,Computer Science,4,"Procedural programming is a programming paradigm, derived from structured programming,[citation needed] based on the concept of the procedure call.  Procedures (a type of routine or subroutine) simply contain a series of computational steps to be carried out.  Any given procedure might be called at any point during a program's execution, including by other procedures or itself. The first major procedural programming languages appeared circa 1957–1964, including Fortran, ALGOL, COBOL, PL/I and BASIC.[1] Pascal and C were published circa 1970–1972.
 Computer processors provide hardware support for procedural programming through a stack register and instructions for calling procedures and returning from them. Hardware support for other types of programming is possible, but no attempt was commercially successful (for example Lisp machines or Java processors).[contradictory]"
Procedure_(computer_science),Computer Science,4,"In computer programming, a subroutine is a sequence of program instructions that performs a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed.
 Subroutines may be defined within programs, or separately in libraries that can be used by many programs.  In different programming languages, a subroutine may be called a routine, subprogram, function, method, or procedure. Technically, these terms all have different definitions. The generic, umbrella term callable unit is sometimes used.[1] The name subprogram suggests a subroutine behaves in much the same way as a computer program that is used as one step in a larger program or another subprogram. A subroutine is often coded so that it can be started several times and from several places during one execution of the program, including from other subroutines, and then branch back (return) to the next instruction after the call, once the subroutine's task is done. The idea of a subroutine was initially conceived by John Mauchly during his work on ENIAC,[2] and recorded in a January 1947 Harvard symposium on ""Preparation of Problems for EDVAC-type Machines"".[3] Maurice Wilkes, David Wheeler, and Stanley Gill are generally credited with the formal invention of this concept, which they termed a closed subroutine,[4][5] contrasted with an open subroutine or macro.[6]
However, Turing had discussed subroutines in a paper of 1945 on design proposals for the NPL ACE, going so far as to invent the concept of a return address stack.[7] Subroutines are a powerful programming tool,[8] and the syntax of many programming languages includes support for writing and using them. Judicious use of subroutines (for example, through the structured programming approach) will often substantially reduce the cost of developing and maintaining a large program, while increasing its quality and reliability.[9] Subroutines, often collected into libraries, are an important mechanism for sharing and trading software. The discipline of object-oriented programming is based on objects and methods (which are subroutines attached to these objects or object classes).
 In the compiling method called threaded code, the executable program is basically a sequence of subroutine calls.
"
Program_lifecycle_phase,Computer Science,4,"Program lifecycle phases are the stages a computer program undergoes, from initial creation to deployment and execution. The phases are edit time, compile time, link time, distribution time, installation time, load time, and run time.
 Lifecycle phases do not necessarily happen in a linear order, and they can be intertwined in various ways.  For example, when modifying a program, software developers may need to repeatedly edit, compile, install, and execute it on their own computers to ensure sufficient quality before it can be distributed to users; copies of the modified program are then downloaded, installed, and executed by users on their computers.
"
Programming_language,Computer Science,4,"
 A programming language is a formal language comprising a set of instructions that produce various kinds of output. Programming languages are used in computer programming to implement algorithms.
 Most programming languages consist of instructions for computers. There are programmable machines that use a set of specific instructions, rather than general programming languages. Early ones preceded the invention of the digital computer, the first probably being the automatic flute player described in the 9th century by the brothers Musa in Baghdad, during the Islamic Golden Age.[1] Since the early 1800s, programs have been used to direct the behavior of machines such as Jacquard looms, music boxes and player pianos.[2] The programs for these machines (such as a player piano's scrolls) did not produce different behavior in response to different inputs or conditions.
 Thousands of different programming languages have been created, and more are being created every year. Many programming languages are written in an imperative form (i.e., as a sequence of operations to perform) while other languages use the declarative form (i.e. the desired result is specified, not how to achieve it).
 The description of a programming language is usually split into the two components of syntax (form) and semantics (meaning). Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common.
"
Programming_language_implementation,Computer Science,4,"A programming language implementation is a system for executing computer programs. There are two general approaches to programming language implementation: interpretation and compilation.[1] Interpretation is a method of executing a program. The program is read as input by an interpreter, which performs the actions written in the program.[2] Compilation is a different process, where a compiler reads in a program, but instead of running the program, the compiler translates it into some other language, such as bytecode or machine code. The translated code may either be directly executed by hardware, or serve as input to another interpreter or another compiler.[2]"
Programming_language_theory,Computer Science,4,"Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and of their individual features.  It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, linguistics and even cognitive science.  It has become a well-recognized branch of computer science, and an active research area, with results published in numerous  journals dedicated to PLT, as well as in general computer science and engineering publications.
"
Prolog,Computer Science,4,"Prolog is a logic programming language associated with artificial intelligence and computational linguistics.[1][2][3] Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is intended primarily as a declarative programming language: the program logic is expressed in terms of relations, represented as facts and rules.  A computation is initiated by running a query over these relations.[4] The language was developed and implemented in Marseille, France, in 1972 by Alain Colmerauer with Philippe Roussel, based on Robert Kowalski's procedural interpretation of Horn clauses.[5][6] Prolog was one of the first logic programming languages[7] and remains the most popular such language today, with several free and commercial implementations available. The language has been used for theorem proving,[8] expert systems,[9] term rewriting,[10] type systems,[11] and automated planning,[12] as well as its original intended field of use, natural language processing.[13][14]  Modern Prolog environments support the creation of graphical user interfaces, as well as administrative and networked applications.
 Prolog is well-suited for specific tasks that benefit from rule-based logical queries such as searching databases, voice control systems, and filling templates.
"
Python_(programming_language),Computer Science,4,"
 Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.[28] Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented, and functional programming. Python is often described as a ""batteries included"" language due to its comprehensive standard library.[29] Python was created in the late 1980s, and first released in 1991, by Guido van Rossum as a successor to the ABC programming language. Python 2.0, released in 2000, introduced new features, such as list comprehensions, and a garbage collection system with reference counting, and was discontinued with version 2.7 in 2020.[30] Python 3.0, released in 2008, was a major revision of the language that is not completely backward-compatible and much Python 2 code does not run unmodified on Python 3.
 Python interpreters are available for many operating systems. A global community of programmers develops and maintains CPython, a free and open-source[31] reference implementation. A non-profit organization, the Python Software Foundation, manages and directs resources for Python and CPython development. It currently ties with Java as the second most popular programming language in the world. [32][33]"
Quantum_computing,Computer Science,4,"Quantum computing is the use of quantum phenomena such as superposition and entanglement to perform computation. Computers that perform quantum computations are known as quantum computers.[1]:I-5 Quantum computers are believed to be able to solve certain computational problems, such as integer factorization (which underlies RSA encryption), substantially faster than classical computers. The study of quantum computing is a subfield of quantum information science.
 Quantum computing began in the early 1980s, when physicist Paul Benioff proposed a quantum mechanical model of the Turing machine.[2] Richard Feynman and Yuri Manin later suggested that a quantum computer had the potential to simulate things that a classical computer could not.[3][4]  In 1994, Peter Shor developed a quantum algorithm for factoring integers that had the potential to decrypt RSA-encrypted communications.[5] Despite ongoing experimental progress since the late 1990s, most researchers believe that ""fault-tolerant quantum computing [is] still a rather distant dream.""[6] In recent years, investment into quantum computing research has increased in both the public and private sector.[7][8] On 23 October 2019, Google AI, in partnership with the U.S. National Aeronautics and Space Administration (NASA), claimed to have performed a quantum computation that is infeasible on any classical computer.[9] There are several models of quantum computers (or rather, quantum computing systems), including the quantum circuit model, quantum Turing machine, adiabatic quantum computer, one-way quantum computer, and various quantum cellular automata. The most widely used model is the quantum circuit. Quantum circuits are based on the quantum bit, or ""qubit"", which is somewhat analogous to the bit in classical computation. Qubits can be in a 1 or 0 quantum state, or they can be in a superposition of the 1 and 0 states. However, when qubits are measured the result of the measurement is always either a 0 or a 1; the probabilities of these two outcomes depend on the quantum state that the qubits were in immediately prior to the measurement.
 Progress towards building a physical quantum computer focuses on technologies such as transmons, ion traps and topological quantum computers, which aim to create high-quality qubits.[1]:2–13 These qubits may be designed differently, depending on the full quantum computer's computing model, whether quantum logic gates, quantum annealing, or adiabatic quantum computation. There are currently a number of significant obstacles in the way of constructing useful quantum computers. In particular, it is difficult to maintain the quantum states of qubits as they suffer from quantum decoherence and state fidelity. Quantum computers therefore require error correction.[10][11] Any computational problem that can be solved by a classical computer can also be solved by a quantum computer. Conversely, any problem that can be solved by a quantum computer can also be solved by a classical computer, at least in principle given enough time. In other words, quantum computers obey the Church–Turing thesis. While this means that quantum computers provide no additional advantages over classical computers in terms of computability, quantum algorithms for certain problems have significantly lower time complexities than corresponding known classical algorithms. Notably, quantum computers are believed to be able to quickly solve certain problems that no classical computer could solve in any feasible amount of time—a feat known as ""quantum supremacy."" The study of the computational complexity of problems with respect to quantum computers is known as quantum complexity theory.
"
Queue_(abstract_data_type),Computer Science,4,"In computer science, a queue is a collection of entities that are maintained in a sequence and can be modified by the addition of entities at one end of the sequence and the removal of entities from the other end of the sequence. By convention, the end of the sequence at which elements are added is called the back, tail, or rear of the queue, and the end at which elements are removed is called the head or front of the queue, analogously to the words used when people line up to wait for goods or services.
 The operation of adding an element to the rear of the queue is known as enqueue, and the operation of removing an element from the front is known as dequeue.  Other operations may also be allowed, often including a peek or front operation that returns the value of the next element to be dequeued without dequeuing it.
 The operations of a queue make it a first-in-first-out (FIFO) data structure. In a FIFO data structure, the first element added to the queue will be the first one to be removed. This is equivalent to the requirement that once a new element is added, all elements that were added before have to be removed before the new element can be removed.  A queue is an example of a linear data structure, or more abstractly a sequential collection.
Queues are common in computer programs, where they are implemented as data structures coupled with access routines, as an abstract data structure or in object-oriented languages as classes. Common implementations are circular buffers and linked lists.
 Queues provide services in computer science, transport, and operations research where various entities such as data, objects, persons, or events are stored and held to be processed later. In these contexts, the queue performs the function of a buffer.
Another usage of queues is in the implementation of breadth-first search.
"
Quicksort,Computer Science,4,"
 Quicksort (sometimes called partition-exchange sort) is an efficient sorting algorithm. Developed by British computer scientist Tony Hoare in 1959[1] and published in 1961,[2] it is still a commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort.[3][contradictory] Quicksort is a divide-and-conquer algorithm. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then sorted recursively. This can be done in-place, requiring small additional amounts of memory to perform the sorting.
 Quicksort is a comparison sort, meaning that it can sort items of any type for which a ""less-than"" relation (formally, a total order) is defined. Efficient implementations of Quicksort are not a stable sort, meaning that the relative order of equal sort items is not preserved.
 Mathematical analysis of quicksort shows that, on average, the algorithm takes O(n log n) comparisons to sort n items. In the worst case, it makes O(n2) comparisons, though this behavior is rare.
"
R_(programming_language),Computer Science,4,"
 R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing.[6] The R language is widely used among statisticians and data miners for developing statistical software[7] and data analysis.[8] Polls, data mining surveys, and studies of scholarly literature databases show substantial increases in popularity;[9] as of September 2020,[update] R ranks 9th in the TIOBE index, a measure of popularity of programming languages.[10] A GNU package,[11] the official R software environment is written primarily in C, Fortran, and R itself[12] (thus, it is partially self-hosting) and is freely available under the GNU General Public License. Pre-compiled executables are provided for various operating systems. Although R has a command line interface, there are several third-party graphical user interfaces, such as RStudio, an integrated development environment, and Jupyter, a notebook interface.[13][14]"
Radix,Computer Science,4,"In a positional numeral system, the radix or base is the number of unique digits, including the digit zero, used to represent numbers. For example, for the decimal/denary system (the most common system in use today) the radix (base number) is ten, because it uses the ten digits from 0 through 9.
 In any standard positional numeral system, a number is conventionally written as (x)y with x as the string of digits and y as its base, although for base ten the subscript is usually assumed (and omitted, together with the pair of parentheses), as it is the most common way to express value. For example, (100)10 is equivalent to  100 (the decimal system is implied in the latter) and represents the number one hundred, while (100)2 (in the binary system with base 2) represents the number four.[1]"
Record_(computer_science),Computer Science,4,"In computer science, a record (also called a structure,  struct, or compound data) is a basic data structure. Records in a database or spreadsheet are usually called ""rows"".[1][2][3][4] A record is a collection of fields, possibly of different data types, typically in a fixed number and sequence.[5] The fields of a record may also be called members, particularly in object-oriented programming; fields may also be called elements, though this risks confusion with the elements of a collection.
 For example, a date could be stored as a record containing a numeric year field, a month field represented as a string, and a numeric day-of-month field. A personnel record might contain a name, a salary, and a rank. A Circle record might contain a center and a radius—in this instance, the center itself might be represented as a point record containing x and y coordinates.
 Records are distinguished from arrays by the fact that their number of fields is typically fixed, each field has a name, and that each field may have a different type.
 A record type is a data type that describes such values and variables. Most modern computer languages allow the programmer to define new record types. The definition includes specifying the data type of each field and an identifier (name or label) by which it can be accessed.  In type theory, product types (with no field names) are generally preferred due to their simplicity, but proper record types are studied in languages such as System F-sub.  Since type-theoretical records may contain first-class function-typed fields in addition to data, they can express many features of object-oriented programming.
 Records can exist in any storage medium, including main memory and mass storage devices such as magnetic tapes or hard disks.  Records are a fundamental component of most data structures, especially linked data structures.  Many computer files are organized as arrays of logical records, often grouped into larger physical records or blocks for efficiency.
 The parameters of a function or procedure can often be viewed as the fields of a record variable; and the arguments passed to that function can be viewed as a record value that gets assigned to that variable at the time of the call. Also, in the call stack that is often used to implement procedure calls, each entry is an activation record or call frame, containing the procedure parameters and local variables, the return address, and other internal fields.
 An object in object-oriented language is essentially a record that contains procedures specialized to handle that record; and object types are an elaboration of record types. Indeed, in most object-oriented languages, records are just special cases of objects, and are known as plain old data structures (PODSs), to contrast with objects that use OO features.
 A record can be viewed as the computer analog of a mathematical tuple, although a tuple may or may not be considered a record, and vice versa, depending on conventions and the specific programming language.  In the same vein, a record type can be viewed as the computer language analog of the Cartesian product of two or more mathematical sets, or the implementation of an abstract product type in a specific language.
"
Recursion,Computer Science,4,"
 Recursion (adjective: recursive) occurs when a thing is defined in terms of itself or of its type. Recursion is used in a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no infinite loop or infinite chain of references can occur.
"
Reference_(computer_science),Computer Science,4,"In computer science, a reference is a value that enables a program to indirectly access a particular datum, such as a variable's value or a record, in the computer's memory or in some other storage device.  The reference is said to refer to the datum, and accessing the datum is called dereferencing the reference.
 A reference is distinct from the datum itself. Typically, for references to data stored in memory on a given system, a reference is implemented as the physical address of where the data is stored in memory or in the storage device. For this reason, a reference is often erroneously confused with a pointer or address, and is said to ""point to"" the data. However, a reference may also be implemented in other ways, such as the offset (difference) between the datum's address and some fixed ""base"" address, as an index into an array, or more abstractly as a handle. More broadly, in networking, references may be network addresses, such as URLs.
 The concept of reference must not be confused with other values (keys or identifiers) that uniquely identify the data item, but give access to it only through a non-trivial lookup operation in some table data structure.
 References are widely used in programming, especially to efficiently pass large or mutable data as arguments to procedures, or to share such data among various uses.  In particular, a reference may point to a variable or record that contains references to other data.  This idea is the basis of indirect addressing and of many linked data structures, such as linked lists. References can cause significant complexity in a program, partially due to the possibility of dangling and wild references and partially because the topology of data with references is a directed graph, whose analysis can be quite complicated.
"
Reference_counting,Computer Science,4,"
 In computer science, reference counting is a programming technique of storing the number of references, pointers, or handles to a resource, such as an object, a block of memory, disk space, and others.
 In garbage collection algorithms, reference counts may be used to deallocate objects which are no longer needed.
"
Relational_database,Computer Science,4,"A relational database is a digital database based on the relational model of data, as proposed by E. F. Codd in 1970.[1]
A software system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems have an option of using the SQL (Structured Query Language) for querying and maintaining the database.[2]"
Reliability_engineering,Computer Science,4,"
Reliability engineering is a sub-discipline of systems engineering that emphasizes the ability of equipment to function without failure. Reliability describes the ability of a system or component to function under stated conditions for a specified period of time.[1] Reliability is closely related to availability, which is typically described as the ability of a component or system to function at a specified moment or interval of time.
 The Reliability function is theoretically defined as the probability of success at time t, which is denoted R(t). This probability is estimated from previous data sets or through reliability testing. Availability, Testability, maintainability and maintenance are often defined as a part of ""reliability engineering"" in reliability programs. Reliability can  play a key role in the cost-effectiveness of systems; for example, a consumer product in many cases will have a higher resale value, if it fails less often. 
 Reliability and quality are closely related. Normally quality focuses on the prevention of defects during the warranty phase whereas reliability looks at preventing failures during the useful lifetime of the product or system from commissioning, through operation, to decommissioning [2].
 Reliability engineering deals with the prediction, prevention and management of high levels of ""lifetime"" engineering uncertainty and risks of failure. Although stochastic parameters define and affect reliability, reliability is not only achieved by mathematics and statistics.[3][4] Reliability engineering can be achieved through process and reliability testing. ""Nearly all teaching and literature on the subject emphasize these aspects, and ignore the reality that the ranges of uncertainty involved largely invalidate quantitative methods for prediction and measurement.""[5] For example, it is easy to represent ""probability of failure"" as a symbol or value in an equation, but it is almost impossible to predict its true magnitude in practice, which is massively multivariate, so having the equation for reliability does not begin to equal having an accurate predictive measurement of reliability.
 Reliability engineering relates closely to Quality Engineering, safety engineering and system safety, in that they use common methods for their analysis and may require input from each other. It can be said that a system must be reliably safe.
 Reliability engineering focuses on costs of failure caused by system downtime, cost of spares, repair equipment, personnel, and cost of warranty claims.
"
Regression_testing,Computer Science,4,"Regression testing (rarely non-regression testing[1]) is re-running functional and non-functional tests to ensure that previously developed and tested software still performs after a change.[2] If not, that would be called a regression. Changes that may require regression testing include bug fixes, software enhancements, configuration changes, and even substitution of electronic components.[3] As regression test suites tend to grow with each found defect, test automation is frequently involved. Sometimes a change impact analysis is performed to determine an appropriate subset of tests (non-regression analysis[4]).
"
Requirements_analysis,Computer Science,4,"
 In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements.[2] Requirements analysis is critical to the success or failure of a systems or software project.[3] The requirements should be documented, actionable, measurable, testable, traceable, related to identified business needs or opportunities, and defined to a level of detail sufficient for system design.
"
Robotics,Computer Science,4,"Robotics is an interdisciplinary research area at the interface of computer science and engineering.[1] Robotics involves design, construction, operation, and use of robots. The goal of robotics is to design intelligent machines that can help and assist humans in their day-to-day lives and keep everyone safe. Robotics draws on the achievement of information engineering, computer engineering, mechanical engineering, electronic engineering and others.
 Robotics develops machines that can substitute for humans and replicate human actions. Robots can be used in many situations and for many purposes, but today many are used in dangerous environments (including inspection of radioactive materials, bomb detection and deactivation), manufacturing processes, or where humans cannot survive (e.g. in space, underwater, in high heat, and clean up and containment of hazardous materials and radiation). Robots can take on any form but some are made to resemble humans in appearance. This is said to help in the acceptance of a robot in certain replicative behaviors usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, or any other human activity. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics.
 The concept of creating robots that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed by various scholars, inventors, engineers, and technicians that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes, whether domestically, commercially, or militarily. Many robots are built to do jobs that are hazardous to people, such as defusing bombs, finding survivors in unstable ruins, and exploring mines and shipwrecks. Robotics is also used in STEM (science, technology, engineering, and mathematics) as a teaching aid.[2] Robotics is a branch of engineering that involves the conception, design, manufacture, and operation of robots. This field overlaps with computer engineering, computer science (especially artificial intelligence), electronics, mechatronics, mechanical, nanotechnology and bioengineering.[3]"
Round-off_error,Computer Science,4,"
A roundoff error,[1] also called rounding error,[2] is the difference between the result produced by a given algorithm using exact arithmetic and the result produced by the same algorithm using finite-precision, rounded arithmetic.[3] Rounding errors are due to inexactness in the representation of real numbers and the arithmetic operations done with them. This is a form of quantization error.[4] When using approximation equations or algorithms, especially when using finitely many digits to represent real numbers (which in theory have infinitely many digits), one of the goals of numerical analysis is to estimate computation errors.[5] Computation errors, also called numerical errors, include both truncation errors and roundoff errors. 
 When a sequence of calculations with an input involving roundoff error are made, errors may accumulate, sometimes dominating the calculation. In ill-conditioned problems, significant error may accumulate.[6] In short, there are two major facets of roundoff errors involved in numerical calculations:[7]"
Router_(computing),Computer Science,4,"
 A router[a] is a networking device that forwards data packets between computer networks. Routers perform the traffic directing functions on the Internet.  Data sent through the internet, such as a web page or email, is in the form of data packets.  A packet is typically forwarded from one router to another router through the networks that constitute an internetwork (e.g. the Internet) until it reaches its destination node.[2] A router is connected to two or more data lines from different IP networks.[b] When a data packet comes in on one of the lines, the router reads the network address information in the packet header to determine the ultimate destination.  Then, using information in its routing table or routing policy, it directs the packet to the next network on its journey.
 The most familiar type of IP routers are home and small office routers that simply forward IP packets between the home computers and the Internet. More sophisticated routers, such as enterprise routers, connect large business or ISP networks up to the powerful core routers that forward data at high speed along the optical fiber lines of the Internet backbone.
"
Routing_table,Computer Science,4,"In computer networking a routing table, or routing information base (RIB), is a data table stored in a router or a network host that lists the routes to particular network destinations, and in some cases, metrics (distances) associated with those routes. The routing table contains information about the topology of the network immediately around it. 
 The construction of routing tables is the primary goal of routing protocols. Static routes are entries made in a routing table by non-automatic means and which are fixed rather than being the result of routing protocols and associated network topology discovery procedures.
"
Run_time_(program_lifecycle_phase),Computer Science,4,"In computer science, runtime, run time, or execution time is the final phase of a computer program's life cycle, in which the code is being executed on the computer's central processing unit (CPU) as machine code. In other words, ""runtime"" is the running phase of a program.
 A runtime error is detected after or during the execution (running state) of a program, whereas a compile-time error is detected by the compiler before the program is ever executed. Type checking, register allocation, code generation, and code optimization are typically done at compile time, but may be done at runtime depending on the particular language and compiler. Many other runtime errors exist and are handled differently by different programming languages, such as division by zero errors, domain errors, array subscript out of bounds errors, arithmetic underflow errors, several types of underflow and overflow errors, and many other runtime errors generally considered as software bugs which may or may not be caught and handled by any particular computer language. 
"
Run_time_(program_lifecycle_phase),Computer Science,4,"In computer science, runtime, run time, or execution time is the final phase of a computer program's life cycle, in which the code is being executed on the computer's central processing unit (CPU) as machine code. In other words, ""runtime"" is the running phase of a program.
 A runtime error is detected after or during the execution (running state) of a program, whereas a compile-time error is detected by the compiler before the program is ever executed. Type checking, register allocation, code generation, and code optimization are typically done at compile time, but may be done at runtime depending on the particular language and compiler. Many other runtime errors exist and are handled differently by different programming languages, such as division by zero errors, domain errors, array subscript out of bounds errors, arithmetic underflow errors, several types of underflow and overflow errors, and many other runtime errors generally considered as software bugs which may or may not be caught and handled by any particular computer language. 
"
Search_algorithm,Computer Science,4,"In computer science, a search algorithm is any algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain, either with discrete or continuous values. Specific applications of search algorithms include:
 The classic search problems described above and web search are both problems in information retrieval, but are generally studied as separate subfields and are solved and evaluated differently. Web search problems are generally focused on filtering and finding documents that are most relevant to human queries. Classic search algorithms are typically evaluated on how fast they can find a solution, and whether that solution is guaranteed to be optimal. Though information retrieval algorithms must be fast, the quality of ranking is more important, as is whether good results have been left out and bad results included.
 The appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. Some database structures are specially constructed to make search algorithms faster or more efficient, such as a search tree, hash map, or a database index. [1][2] Search algorithms can be classified based on their mechanism of searching. Linear search algorithms check every record for the one associated with a target key in a linear fashion.[3] Binary, or half interval searches, repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order.[4] Digital search algorithms work based on the properties of digits in data structures that use numerical keys.[5] Finally, hashing directly maps keys to records based on a hash function.[6] Searches outside a linear search require that the data be sorted in some way.
 Algorithms are often evaluated by their computational complexity, or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of O(log n), or logarithmic time. This means that the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space.
"
Auxiliary_memory,Computer Science,4,"
 Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.[1]:15–16 The central processing unit (CPU) of a computer is what manipulates data by performing computations.  In practice, almost all computers use a storage hierarchy,[1]:468–473 which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally the fast volatile technologies (which lose data when off power) are referred to as ""memory"", while slower persistent technologies are referred to as ""storage"".
 Even the first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.
"
Selection_sort,Computer Science,4,"In computer science, selection sort is an in-place comparison sorting algorithm. It has an O(n2) time complexity, which makes it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.
 The algorithm divides the input list into two parts: a sorted sublist of items which is built up from left to right at the front (left) of the list and a sublist of the remaining unsorted items that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right. 
 The time efficiency of selection sort is quadratic, so there are a number of sorting techniques which have better time complexity than selection sort.  One thing which distinguishes selection sort from other sorting algorithms is that it makes the minimum possible number of swaps, n − 1 in the worst case.
"
Semantics_(computer_science),Computer Science,4,"In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.
 Formal semantics, for instance, helps to write compilers, better understand what a program is doing, and to prove, e.g., that the following if statement
 has the same effect as S1 alone.
"
Sequence,Computer Science,4,"In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed and order matters. Like a set, it contains members (also called elements, or terms). The number of elements (possibly infinite) is called the length of the sequence. Unlike a set, the same elements can appear multiple times at different positions in a sequence, and unlike a set, the order does matter. Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences), or the set of the first n natural numbers (for a sequence of finite length n).
 For example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last. This sequence differs from (A, R, M, Y). Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence. Sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers (2, 4, 6, ...).
 The position of an element in a sequence is its rank or index; it is the natural number for which the element is the image. The first element has index 0 or 1, depending on the context or a specific convention. In mathematical analysis, a sequence is often denoted by letters in the form of 




a

n




{  a_{n}}
, 




b

n




{  b_{n}}
 and 




c

n




{  c_{n}}
, where the subscript n refers to the nth element of the sequence;[1] for example, the nth element of the Fibonacci sequence 



F


{  F}
 is generally denoted as 




F

n




{  F_{n}}
.
 
In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams. The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context."
Serializability,Computer Science,4,"In concurrency control of databases,[1][2] transaction processing (transaction management), and various transactional applications (e.g., transactional memory[3] and software transactional memory), both centralized and distributed, a transaction schedule is serializable if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e. without overlapping in time. Transactions are normally executed concurrently (they overlap), since this is the most efficient way. Serializability is the major correctness criterion for concurrent transactions' executions[citation needed]. It is considered the highest level of isolation between transactions, and plays an essential role in concurrency control. As such it is supported in all general purpose database systems. Strong strict two-phase locking (SS2PL) is a popular serializability mechanism utilized in most of the database systems (in various variants) since their early days in the 1970s.
 Serializability theory provides the formal framework to reason about and analyze serializability and its techniques. Though it is mathematical in nature, its fundamentals are informally (without mathematics notation) introduced below.
"
Serialization,Computer Science,4,"In computing, serialization (US spelling) or serialisation (UK spelling) is the process of translating a data structure or object state into a format that can be stored (for example, in a file or memory data buffer) or transmitted (for example, across a computer network) and reconstructed later (possibly in a different computer environment).[1] When the resulting series of bits is reread according to the serialization format, it can be used to create a semantically identical clone of the original object. For many complex objects, such as those that make extensive use of references, this process is not straightforward. Serialization of object-oriented objects does not include any of their associated methods with which they were previously linked.
 This process of serializing an object is also called marshalling an object in some situations.[1][2] The opposite operation, extracting a data structure from a series of bytes, is deserialization, (also called unserialization or unmarshalling).
"
Service_level_agreement,Computer Science,4,"A service-level agreement (SLA) is a commitment between a service provider and a client. Particular aspects of the service – quality, availability, responsibilities – are agreed between the service provider and the service user.[1] The most common component of an SLA is that the services should be provided to the customer as agreed upon in the contract. As an example, Internet service providers and telcos will commonly include service level agreements within the terms of their contracts with customers to define the level(s) of service being sold in plain language terms. In this case the SLA will typically have a technical definition in  mean time between failures (MTBF), mean time to repair or mean time to recovery (MTTR); identifying which party is responsible for reporting faults or paying fees; responsibility for various data rates; throughput; jitter; or similar measurable details.
"
Set_(abstract_data_type),Computer Science,4,"In computer science, a set is an abstract data type that can store unique values, without any particular order. It is a computer implementation of the mathematical concept of a finite set. Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set.
 Some set data structures are designed for static or frozen sets that do not change after they are constructed. Static sets allow only query operations on their elements — such as checking whether a given value is in the set, or enumerating the values in some arbitrary order.  Other variants, called dynamic or mutable sets, allow also the insertion and deletion of elements from the set.
 A multiset is a special kind of set in which an element can figure several times.
"
Software,Computer Science,4,"
 Software is a collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.
 At the lowest programming level,[clarification needed] executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). A machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example displaying some text on a computer screen; causing state changes which should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to ""jump"" to a different instruction, or is interrupted by the operating system. As of 2015[update], most personal computers, smartphone devices and servers have processors with multiple execution units or multiple processors performing computation together, and computing has become a much more concurrent activity than in the past.
 The majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages.[1] High-level languages are translated into machine language using a compiler or an interpreter or a combination of the two. Software may also be written in a low-level assembly language, which has strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler.
"
Software_agent,Computer Science,4,"In computer science, a software agent  is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf. Such ""action on behalf of"" implies the authority to decide which, if any, action is appropriate.[1][2] Agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or  as software such as a chatbot
executing on a phone (e.g. Siri)  or other computing device.  Software agents may be autonomous or work together with other agents or people.  Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).
 Related and derived concepts include intelligent agents (in particular exhibiting some aspects of artificial intelligence, such as reasoning), autonomous agents (capable of modifying the methods of achieving their objectives), distributed agents (being executed on physically distinct computers), multi-agent systems (distributed agents that work together to achieve an objective that could not be accomplished by a single agent acting alone), and mobile agents (agents that can relocate their execution onto different processors).
"
Software_construction,Computer Science,4,"Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.[1]"
Software_deployment,Computer Science,4,"
 Software deployment is all of the activities that make a software system available for use.[1] The general deployment process consists of several interrelated activities with possible transitions between them. These activities can occur at the producer side or at the consumer side or both. Because every software system is unique, the precise processes or procedures within each activity can hardly be defined. Therefore, ""deployment"" should be interpreted as a general process that has to be customized according to specific requirements or characteristics.[2]"
Software_design,Computer Science,4,"Software design is the process by which an agent creates a specification of a software artifact intended to accomplish goals, using a set of primitive components and subject to constraints.[1] Software design may refer to either ""all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex systems"" or ""the activity following requirements specification and before programming, as ... [in] a stylized software engineering process.""[2] Software design usually involves problem-solving and planning a software solution. This includes both a low-level component and algorithm design and a high-level, architecture design.
"
Software_development,Computer Science,4,"Software development is the process of conceiving, specifying, designing, programming, documenting, testing, and bug fixing involved in creating and maintaining applications, frameworks, or other software components. Software development is a process of writing and maintaining the source code, but in a broader sense, it includes all that is involved between the conception of the desired software through to the final manifestation of the software, sometimes in a planned and structured process.[1] Therefore, software development may include research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products.[2] The software can be developed for a variety of purposes, the three most common being to meet specific needs of a specific client/business (the case with custom software), to meet a perceived need of some set of potential users (the case with commercial and open source software), or for personal use (e.g. a scientist may write software to automate a mundane task). Embedded software development, that is, the development of embedded software, such as used for controlling consumer products, requires the development process to be integrated with the development of the controlled physical product. System software underlies applications and the programming process itself, and is often developed separately.
 The need for better quality control of the software development process has given rise to the discipline of software engineering, which aims to apply the systematic approach exemplified in the engineering paradigm to the process of software development.
 There are many approaches to software project management, known as software development life cycle models, methodologies, processes, or models. The waterfall model is a traditional version, contrasted with the more recent innovation of agile software development.
"
Software_development_process,Computer Science,4,"In software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management.  It is also known as a software development life cycle (SDLC).  The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.[1] Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.
 A life-cycle ""model"" is sometimes considered a more general term for a category of methodologies and a software development ""process"" a more specific term to refer to a specific process chosen by a specific organization.[citation needed] For example, there are many specific software development processes that fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle.
"
Software_engineering,Computer Science,4,Software engineering is the systematic application of engineering approaches to the development of software.[1][2][3] Software engineering is a computing discipline.[4]
Software_maintenance,Computer Science,4,"Software maintenance in software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.[1] A common perception of maintenance is that it merely involves fixing defects. However, one study indicated that over 80% of maintenance effort is used for non-corrective actions.[2] This perception is perpetuated by users submitting problem reports that in reality are functionality enhancements to the system.[citation needed] More recent studies put the bug-fixing proportion closer to 21%.[3]"
Software_prototyping,Computer Science,4,"Software prototyping is the activity of creating prototypes of software applications, i.e., incomplete versions of the software program being developed. It is an activity that can occur in software development and is comparable to prototyping as known from other fields, such as mechanical engineering or manufacturing.
 A prototype typically simulates only a few aspects of, and may be completely different from, the final product. 
 Prototyping has several benefits: the software designer and implementer can get valuable feedback from the users early in the project. The client and the contractor can compare if the software made matches the software specification, according to which the software program is built. It also allows the software engineer some insight into the accuracy of initial project estimates and whether the deadlines and milestones proposed can be successfully met. The degree of completeness and the techniques used in prototyping have been in development and debate since its proposal in the early 1970s.[6]"
Software_requirements_specification,Computer Science,4,"A software requirements specification (SRS) is a description of a software system to be  developed. It is modeled after business requirements specification (CONOPS), also known as a stakeholder requirements specification (StRS).[citation needed] The software requirements specification lays out functional and non-functional requirements, and it may include a set of use cases that describe user interactions that the software must provide to the user for perfect interaction.
 Software requirements specification establishes the basis for an agreement between customers and contractors or suppliers on how the software product should function (in a market-driven project, these roles may be played by the marketing and development divisions). Software requirements specification is a rigorous assessment of requirements before the more specific system design stages, and its goal is to reduce later redesign. It should also provide a realistic basis for estimating product costs, risks, and schedules.[1] Used appropriately, software  requirements specifications can help prevent software project failure.[2] The software requirements specification document lists sufficient and necessary requirements for the project development.[3] To derive the requirements, the developer needs to have clear and thorough understanding of the products under development. This is achieved through detailed and continuous communications with the project team and customer throughout the software development process.
 The SRS may be one of a contract's deliverable data item descriptions[4] or have other forms of organizationally-mandated content.
 Typically a SRS is written by a technical writer, a systems architect, or a software programmer.[5]"
Software_testing,Computer Science,4,"
 Software testing is an investigation conducted to provide stakeholders with information about the quality of the software product or service under test.[1] Software testing can also provide an objective, independent view of the software to allow the business to appreciate and understand the risks of software implementation. Test techniques include the process of executing a program or application with the intent of finding software bugs (errors or other defects), and verifying that the software product is fit for use.
 Software testing involves the execution of a software component or system component to evaluate one or more properties of interest. In general, these properties indicate the extent to which the component or system under test:
 As the number of possible tests for even simple software components is practically infinite, all software testing uses some strategy to select tests that are feasible for the available time and resources. As a result, software testing typically (but not exclusively) attempts to execute a program or application with the intent of finding software bugs (errors or other defects). The job of testing is an iterative process as when one bug is fixed, it can illuminate other, deeper bugs, or can even create new ones.
 Software testing can provide objective, independent information about the quality of software and risk of its failure to users or sponsors.[1] Software testing can be conducted as soon as executable software (even if partially complete) exists. The overall approach to software development often determines when and how testing is conducted. For example, in a phased process, most testing occurs after system requirements have been defined and then implemented in testable programs. In contrast, under an agile approach, requirements, programming, and testing are often done concurrently.
"
Sorting_algorithm,Computer Science,4,"In computer science, a sorting algorithm is an algorithm that puts elements of a list in a certain order. The most frequently used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the efficiency of other algorithms (such as search and merge algorithms) that require input data to be in sorted lists. Sorting is also often useful for canonicalizing data and for producing human-readable output. More formally, the output of any sorting algorithm must satisfy two conditions:
 Further, the input data is often stored in an array, which allows random access, rather than a list, which only allows sequential access; though many algorithms can be applied to either type of data after suitable modification.
 Sorting algorithms are often referred to as a word followed by the word ""sort"" and grammatically are used in English as noun phrases, for example in the sentence, ""it is inefficient to use insertion sort on large lists"" the phrase insertion sort refers to the insertion sort sorting algorithm.
"
Source_code,Computer Science,4,"
 In computing, source code is any collection of code, with or without comments, written using[1] a human-readable programming language, usually as plain text. The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code. The source code is often transformed by an assembler or compiler into binary machine code that can be executed by the computer. The machine code might then be stored for execution at a later time. Alternatively, source code may be interpreted and thus immediately executed.
 Most application software is distributed in a form that includes only executable files. If the source code were included it would be useful to a user, programmer or a system administrator, any of whom might wish to study or modify the program.
"
Spiral_model,Computer Science,4,"The spiral model is a risk-driven software development process model. Based on the unique risk patterns of a given project, the spiral model guides a team to adopt elements of one or more process models, such as incremental, waterfall, or evolutionary prototyping.
"
Stack_(abstract_data_type),Computer Science,4,"In computer science, a stack is an abstract data type that serves as a collection of elements, with two main principal operations:
 The order in which elements come off a stack gives rise to its alternative name, LIFO (last in, first out). Additionally, a peek operation may give access to the top without modifying the stack.[1] The name ""stack"" for this type of structure comes from the analogy to a set of physical items stacked on top of each other. This structure makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first.[2] Considered as a linear data structure, or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack.  This data structure makes it possible to implement a stack as a singly linked list and a pointer to the top element. A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack.
 A stack is needed to implement depth-first search.
"
State_(computer_science),Computer Science,4,"In information technology and computer science, a system is described as stateful if it is designed to remember preceding events or user interactions;[1] the remembered information is called the state of the system.
 The set of states a system can occupy is known as its state space. In a discrete system, the state space is countable and often finite. The system's internal behaviour or interaction with its environment consists of separately occurring individual actions or events, such as accepting input or producing output, that may or may not cause the system to change its state. Examples of such systems are digital logic circuits and components, automata and formal language, computer programs, and computers.  
 The output of a digital circuit or deterministic computer program at any time is completely determined by its current inputs and its state.[2]"
Statement_(computer_science),Computer Science,4,"In computer programming, a statement is a syntactic unit of an imperative programming language that expresses some action to be carried out.[1] A program written in such a language is formed by a sequence of one or more statements. A statement may have internal components (e.g., expressions).
 Many imperative languages (e.g. C) make a distinction between statements and definitions, with a statement only containing executable code and a definition instantiating an identifier, while an expression evaluates to a value only. A distinction can also be made between simple and compound statements; the latter may contain statements as components.
"
Computer_data_storage,Computer Science,4,"
 Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.[1]:15–16 The central processing unit (CPU) of a computer is what manipulates data by performing computations.  In practice, almost all computers use a storage hierarchy,[1]:468–473 which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally the fast volatile technologies (which lose data when off power) are referred to as ""memory"", while slower persistent technologies are referred to as ""storage"".
 Even the first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.
"
Stream_(computing),Computer Science,4,"In computer science, a stream is a sequence of data elements made available over time. A stream can be thought of as items on a conveyor belt being processed one at a time rather than in large batches.
 Streams are processed differently from batch data – normal functions cannot operate on streams as a whole, as they have potentially unlimited data, and formally, streams are codata (potentially unlimited), not data (which is finite). Functions that operate on a stream, producing another stream, are known as filters, and can be connected in pipelines, analogously to function composition. Filters may operate on one item of a stream at a time, or may base an item of output on multiple items of input, such as a moving average.
"
String_(computer_science),Computer Science,4,"In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally considered as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. String may also denote more general arrays or other sequence (or list) data types and structures.
 Depending on the programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold a variable number of elements.
 When a string appears literally in source code, it is known as a string literal or an anonymous string.[1] In formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.
"
Structured_storage,Computer Science,4,"A NoSQL (originally referring to ""non-SQL"" or ""non-relational"")[1] database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name ""NoSQL"" was only coined in the early 21st century,[2] triggered by the needs of Web 2.0 companies.[3][4] NoSQL databases are increasingly used in big data and real-time web applications.[5]  NoSQL systems are also sometimes called ""Not only SQL"" to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.[6][7] Motivations for this approach include: simplicity of design, simpler ""horizontal"" scaling to clusters of machines (which is a problem for relational databases),[2] finer control over availability and limiting the object-relational impedance mismatch.[8] The data structures used by NoSQL databases (e.g. key–value pair, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve.  Sometimes the data structures used by NoSQL databases are also viewed as ""more flexible"" than relational database tables.[9] Many NoSQL stores compromise consistency (in the sense of the CAP theorem) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance), lack of ability to perform ad-hoc joins across tables, lack of standardized interfaces, and huge previous investments in existing relational databases.[10] Most NoSQL stores lack true ACID transactions, although a few databases have made them central to their designs.
 Instead, most NoSQL databases offer a concept of ""eventual consistency"", in which database changes are propagated to all nodes ""eventually"" (typically within milliseconds), so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale reads.[11]  Additionally, some NoSQL systems may exhibit lost writes and other forms of data loss.[12] Some NoSQL systems provide concepts such as write-ahead logging to avoid data loss.[13] For distributed transaction processing across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Relational databases ""do not allow referential integrity constraints to span databases"".[14] Few systems maintain both ACID transactions and X/Open XA standards for distributed transaction processing.[15] Interactive relational databases share conformational relay analysis techniques as a common feature.[16] Limitations within the interface environment are overcome using semantic virtualization protocols, such that NoSQL services are accessible to most operating systems.[17]"
Subroutine,Computer Science,4,"In computer programming, a subroutine is a sequence of program instructions that performs a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed.
 Subroutines may be defined within programs, or separately in libraries that can be used by many programs.  In different programming languages, a subroutine may be called a routine, subprogram, function, method, or procedure. Technically, these terms all have different definitions. The generic, umbrella term callable unit is sometimes used.[1] The name subprogram suggests a subroutine behaves in much the same way as a computer program that is used as one step in a larger program or another subprogram. A subroutine is often coded so that it can be started several times and from several places during one execution of the program, including from other subroutines, and then branch back (return) to the next instruction after the call, once the subroutine's task is done. The idea of a subroutine was initially conceived by John Mauchly during his work on ENIAC,[2] and recorded in a January 1947 Harvard symposium on ""Preparation of Problems for EDVAC-type Machines"".[3] Maurice Wilkes, David Wheeler, and Stanley Gill are generally credited with the formal invention of this concept, which they termed a closed subroutine,[4][5] contrasted with an open subroutine or macro.[6]
However, Turing had discussed subroutines in a paper of 1945 on design proposals for the NPL ACE, going so far as to invent the concept of a return address stack.[7] Subroutines are a powerful programming tool,[8] and the syntax of many programming languages includes support for writing and using them. Judicious use of subroutines (for example, through the structured programming approach) will often substantially reduce the cost of developing and maintaining a large program, while increasing its quality and reliability.[9] Subroutines, often collected into libraries, are an important mechanism for sharing and trading software. The discipline of object-oriented programming is based on objects and methods (which are subroutines attached to these objects or object classes).
 In the compiling method called threaded code, the executable program is basically a sequence of subroutine calls.
"
Symbolic_computation,Computer Science,4,"In mathematics and computer science,[1] computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols.
 Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.
 Computer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, as in public key cryptography, or for some non-linear problems.
"
Syntax_(programming_languages),Computer Science,4,"In computer science, the syntax of a computer language is the set of rules that defines the combinations of symbols that are considered to be correctly structured statements or expressions in that language. This applies both to programming languages, where the document represents source code, and to markup languages, where the document represents data.
 The syntax of a language defines its surface form.[1] Text-based computer languages are based on sequences of characters, while visual programming languages are based on the spatial layout and connections between symbols (which may be textual or graphical). Documents that are syntactically invalid are said to have a syntax error. When designing the syntax of a language, a designer might start by writing down examples of both legal and illegal strings, before trying to figure out the general rules from these examples.[2] Syntax therefore refers to the form of the code, and is contrasted with semantics – the meaning. In processing computer languages, semantic processing generally comes after syntactic processing; however, in some cases, semantic processing is necessary for complete syntactic analysis, and these are done together or concurrently. In a compiler, the syntactic analysis comprises the frontend, while the semantic analysis comprises the backend (and middle end, if this phase is distinguished).
"
Syntax_error,Computer Science,4,"In computer science, a syntax error is an error in the syntax of a sequence of characters or tokens that is intended to be written in compile-time. A program will not compile until all syntax errors are corrected. For interpreted languages, however, a syntax error may be detected during program execution, and an interpreter's error messages might not differentiate syntax errors from errors of other kinds.
 There is some disagreement as to just what errors are ""syntax errors"". For example, some would say that the use of an uninitialized variable's value in Java code is a syntax error, but many others would disagree[1][2] and would classify this as a (static) semantic error.
 In 8-bit home computers that used BASIC interpreter as their primary user interface, the .mw-parser-output .monospaced{font-family:monospace,monospace}SYNTAX ERROR error message became somewhat notorious, as this was the response to any command or user input the interpreter could not parse.
 A syntax error may also occur when an invalid equation is entered into a calculator. This can be caused, for instance, by opening brackets without closing them, or less commonly, entering several decimal points in one number.
 In Java the following is a syntactically correct statement:
 while the following is not:
 The second example would theoretically print the variable Hello World instead of the words Hello World. However, a variable in Java cannot have a space in between, so the syntactically correct line would be System.out.println(Hello_World).
 A compiler will flag a syntax error when given source code that does not meet the requirements of the language grammar.
 Type errors (such as an attempt to apply the ++ increment operator to a boolean variable in Java) and undeclared variable errors are sometimes considered to be syntax errors when they are detected at compile-time. However, it is common to classify such errors as (static) semantic errors instead.[2][3][4]"
System_console,Computer Science,4,"The system console, computer console, root console, operator's console, or simply console is the text entry and display device for system administration messages, particularly those from the BIOS or boot loader, the kernel, from the init system and from the system logger. It is a physical device consisting of a keyboard and a screen, and traditionally is a text terminal, but may also be a graphical terminal. System consoles are generalized to computer terminals, which are abstracted respectively by virtual consoles and terminal emulators. Today communication with system consoles is generally done abstractly, via the standard streams (stdin, stdout, and stderr), but there may be system-specific interfaces, for example those used by the system kernel.
"
Technical_documentation,Computer Science,4,"In engineering, technical documentation refers to any type of documentation that describes handling, functionality and architecture of a technical product or a product under development or use.[1][2][3] The intended recipient for product technical documentation is both the (proficient) end user as well as the administrator / service or maintenance technician. In contrast to a mere ""cookbook"" manual, technical documentation aims at providing enough information for a user to understand inner and outer dependencies of the product at hand.
 If technical writers are employed by the technology company, their task is to translate the usually highly formalized or abbreviated technical documentation produced during the development phase into more readable, ""user-friendly"" prose.
 The documentation accompanying a piece of technology is often the only means by which the user can fully understand said technology; regardless, technical documentation is often considered a ""necessary evil"" by software developers. Consequently, the genre has suffered from what some industry experts lament as a lack of attention and precision.[4] Writing and maintaining documentation involves many technical and non-technical skills, and this work is often not enjoyed or rewarded as much as writing and maintaining code.[5]"
Third-generation_programming_language,Computer Science,4,"A third-generation programming language (3GL) is a high-level computer programming language that tends to be more machine-independent and programmer-friendly than the machine code of the first-generation and assembly languages of the second-generation, while having a less specific focus to the fourth and fifth generations.[1] Examples of common and historical third-generation programming languages are ALGOL, BASIC, C, COBOL, Fortran, Java, and Pascal.
"
Top-down_and_bottom-up_design,Computer Science,4,"
 Top-down and bottom-up are both strategies of information processing and knowledge ordering, used in a variety of fields including software, humanistic and scientific theories (see systemics), and management and organization. In practice, they can be seen as a style of thinking, teaching, or leadership.
 A top-down approach (also known as stepwise design and stepwise refinement and in some cases used as a synonym of decomposition) is essentially the breaking down of a system to gain insight into its compositional sub-systems in a reverse engineering fashion. In a top-down approach an overview of the system is formulated, specifying, but not detailing, any first-level subsystems. Each subsystem is then refined in yet greater detail, sometimes in many additional subsystem levels, until the entire specification is reduced to base elements. A top-down model is often specified with the assistance of ""black boxes"", which makes it easier to manipulate. However, black boxes may fail to clarify elementary mechanisms or be detailed enough to realistically validate the model. Top down approach starts with the big picture. It breaks down from there into smaller segments.[1] A bottom-up approach is the piecing together of systems to give rise to more complex systems, thus making the original systems sub-systems of the emergent system. Bottom-up processing is a type of information processing based on incoming data from the environment to form a perception. From a cognitive psychology perspective, information enters the eyes in one direction (sensory input, or the ""bottom""), and is then turned into an image by the brain that can be interpreted and recognized as a perception (output that is ""built up"" from processing to final cognition). In a bottom-up approach the individual base elements of the system are first specified in great detail. These elements are then linked together to form larger subsystems, which then in turn are linked, sometimes in many levels, until a complete top-level system is formed. This strategy often resembles a ""seed"" model, by which the beginnings are small but eventually grow in complexity and completeness. However, ""organic strategies"" may result in a tangle of elements and subsystems, developed in isolation and subject to local optimization as opposed to meeting a global purpose.
"
Tree_(data_structure),Computer Science,4,"In computer science, a tree is a widely used abstract data type that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes.
 A tree data structure can be defined recursively as a collection of nodes (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the ""children""), with the constraints that no reference is duplicated, and none points to the root.
 Alternatively, a tree can be defined abstractly as a whole (globally) as an ordered tree, with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node (rather than as a set of nodes and an adjacency list of edges between nodes, as one may represent a digraph, for instance). For example, looking at a tree as a whole, one can talk about ""the parent node"" of a given node, but in general, as a data structure, a given node only contains the list of its children but does not contain a reference to its parent (if any).
"
Type_theory,Computer Science,4,"In mathematics, logic, and computer science, a type system is a formal system in which every term has a ""type"" which defines its meaning and the operations that may be performed on it. Type theory is the academic study of type systems.
 Some type theories serve as alternatives to set theory as a foundation of mathematics. Two well-known such theories are Alonzo Church's typed λ-calculus and Per Martin-Löf's intuitionistic type theory.  
 Type theory was created to avoid paradoxes in previous foundations such as naive set theory, formal logics and rewrite systems.
 Type theory is closely related to, and in some cases overlaps with, computational type systems, which are a programming language feature used to reduce bugs.
"
Upload,Computer Science,4,"Uploading refers to transmitting data from one computer system to another through means of a network.[1] Common methods of uploading include: uploading via web browsers, FTP clients, and terminals (SCP/SFTP). Uploading can be used in the context of (potentially many) clients that send files to a central server. While uploading can also be defined in the context of sending files between distributed clients, such as with a peer-to-peer (P2P) file-sharing protocol like BitTorrent, the term file sharing is more often used in this case. Moving files within a computer system, as opposed to over a network, is called file copying. 
 Uploading directly contrasts with downloading, where data is received over a network. In the case of users uploading files over the internet, uploading is often slower than downloading as many internet service providers (ISPs) offer asymmetric connections, which offer more network bandwidth for downloading than uploading. 
"
URL,Computer Science,4,"
 
 A Uniform Resource Locator (URL), colloquially termed a web address,[1] is a reference to a web resource that specifies its location on a computer network and a mechanism for retrieving it. A URL is a specific type of Uniform Resource Identifier (URI),[2][3] although many people use the two terms interchangeably.[4][a] URLs occur most commonly to reference web pages (http), but are also used for file transfer (ftp), email (mailto), database access (JDBC), and many other applications.
 RFC 4248. – The telnet URI Scheme. RFC 4266. – The gopher URI Scheme. RFC 6068. – The ‘mailto’ URI Scheme. RFC 6196. – Moving mailserver: URI Scheme to Historic.  Most web browsers display the URL of a web page above the page in an address bar. A typical URL could have the form http://www.example.com/index.html, which indicates a protocol (http), a hostname (www.example.com), and a file name (index.html).
"
User_(computing),Computer Science,4,"A user is a person who utilizes a computer or network service. Users of computer systems and software products generally lack the technical expertise required to fully understand how they work.[1] Power users use advanced features of programs, though they are not necessarily capable of computer programming and system administration.[2][3] A user often has a user account and is identified to the system by a username (or user name). Other terms for username include login name, screenname (or screen name), account name, nickname (or nick) and handle, which is derived from the identical citizens band radio term.
 Some software products provide services to other systems and have no direct end users.
"
User_agent,Computer Science,4,"
In computing, a user agent is a software (a software agent) acting on behalf of a user, such as a web browser that ""retrieves, renders and facilitates end-user interaction with Web content.""[1] An email reader is a mail user agent. 
 In many cases, a user agent acts as a client in a network protocol used in communications within a client–server distributed computing system. In particular, the Hypertext Transfer Protocol (HTTP) identifies the client software originating the request, using a user-agent header, even when a user does not operate the client.  The Session Initiation Protocol (SIP) protocol (based on HTTP) followed this usage. In the SIP, the term user agent refers to both end points of a communications session.[2]"
User_interface,Computer Science,4,"In the industrial design field of human-computer interaction, a user interface (UI) is the space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, whilst the machine simultaneously feeds back information that aids the operators' decision-making process. Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls, and process controls. The design considerations applicable when creating user interfaces are related to, or involve such disciplines as, ergonomics and psychology.
 Generally, the goal of user interface design is to produce a user interface which makes it easy, efficient, and enjoyable (user-friendly) to operate a machine in the way which produces the desired result (i.e. maximum usability). This generally means that the operator needs to provide minimal input to achieve the desired output, and also that the machine minimizes undesired outputs to the user.
 User interfaces are composed of one or more layers, including a human-machine interface (HMI) that interfaces machines with physical input hardware such as keyboards, mice, or game pads, and output hardware such as computer monitors, speakers, and printers. A device that implements an HMI is called a human interface device (HID). Other terms for human-machine interfaces are man-machine interface (MMI) and, when the machine in question is a computer, human-computer interface. Additional UI layers may interact with one or more human senses, including: tactile UI (touch), visual UI (sight), auditory UI (sound), olfactory UI (smell), equilibrial UI (balance), and gustatory UI (taste).
 Composite user interfaces (CUIs) are UIs that interact with two or more senses. The most common CUI is a graphical user interface (GUI), which is composed of a tactile UI and a visual UI capable of displaying graphics. When sound is added to a GUI, it becomes a multimedia user interface (MUI). There are three broad categories of CUI: standard, virtual and augmented. Standard composite user interfaces use standard human interface devices like keyboards, mice, and computer monitors. When the CUI blocks out the real world to create a virtual reality, the CUI is virtual and uses a virtual reality interface. When the CUI does not block out the real world and creates augmented reality, the CUI is augmented and uses an augmented reality interface. When a UI interacts with all human senses, it is called a qualia interface, named after the theory of qualia. CUI may also be classified by how many senses they interact with as either an X-sense virtual reality interface or X-sense augmented reality interface, where X is the number of senses interfaced with. For example, a Smell-O-Vision is a 3-sense (3S) Standard CUI with visual display, sound and smells; when virtual reality interfaces interface with smells and touch it is said to be a 4-sense (4S) virtual reality interface; and when augmented reality interfaces interface with smells and touch it is said to be a 4-sense (4S) augmented reality interface.
"
User_interface_design,Computer Science,4,"User interface design (UI) or user interface engineering is the design of user interfaces for machines and software, such as computers, home appliances, mobile devices, and other electronic devices, with the focus on maximizing usability and the user experience. The goal of user interface design is to make the user's interaction as simple and efficient as possible, in terms of accomplishing user goals (user-centered design).
 Good user interface design facilitates finishing the task at hand without drawing unnecessary attention to itself. Graphic design and typography are utilized to support its usability, influencing how the user performs certain interactions and improving the aesthetic appeal of the design; design aesthetics may enhance or detract from the ability of users to use the functions of the interface.[1] The design process must balance technical functionality and visual elements (e.g., mental model) to create a system that is not only operational but also usable and adaptable to changing user needs.
 Interface design is involved in a wide range of projects, from computer systems, to cars, to commercial planes; all of these projects involve much of the same basic human interactions yet also require some unique skills and knowledge. As a result, designers tend to specialize in certain types of projects and have skills centered on their expertise, whether it is a software design, user research, web design, or industrial design.
"
Variable_(computer_science),Computer Science,4,"In computer programming, a variable or scalar is a storage location (identified by a memory address) paired with an associated symbolic name, which contains some known or unknown quantity of information referred to as a value. The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may thus change during the course of program execution.[1][2][3][4] Variables in programming may not directly correspond to the concept of variables in mathematics. The latter is abstract, having no reference to a physical object such as storage location. The value of a computing variable is not necessarily part of an equation or formula as in mathematics. Variables in computer programming are frequently given long names to make them relatively descriptive of their use, whereas variables in mathematics often have terse, one- or two-character names for brevity in transcription and manipulation.
 A variable's storage location may be referenced by several different identifiers, a situation known as aliasing. Assigning a value to the variable using one of the identifiers will change the value that can be accessed through the other identifiers.
 Compilers have to replace variables' symbolic names with the actual locations of the data. While a variable's name, type, and location often remain fixed, the data stored in the location may be changed during program execution.
"
Virtual_machine,Computer Science,4,"
 In computing, a virtual machine (VM) is an emulation of a computer system.  Virtual machines are based on computer architectures and provide functionality of a physical computer. Their implementations may involve specialized hardware, software, or a combination.
 There are different kinds of virtual machines, each with different functions:
 Some virtual machines, such as QEMU, are designed to also emulate different architectures and allow execution of software applications and operating systems written for another CPU or architecture.  Operating-system-level virtualization allows the resources of a computer to be partitioned via the kernel. The terms are not universally interchangeable.
"
V-Model_(software_development),Computer Science,4,"In software development, the V-model[2] represents a development process that may be considered an extension of the waterfall model, and is an example of the more general V-model. Instead of moving down in a linear way, the process steps are bent upwards after the coding phase, to form the typical V shape. The V-Model demonstrates the relationships between each phase of the development life cycle and its associated phase of testing. The horizontal and vertical axes represents time or project completeness (left-to-right) and level of abstraction (coarsest-grain abstraction uppermost), respectively.
"
Waterfall_model,Computer Science,4,"
 The waterfall model is a breakdown of project activities into linear sequential phases, where each phase depends on the deliverables of the previous one and corresponds to a specialization of tasks.  The approach is typical for certain areas of engineering design. In software development, it tends to be among the less iterative and flexible approaches, as progress flows in largely one direction (""downwards"" like a waterfall) through the phases of conception, initiation, analysis, design, construction, testing, deployment and maintenance.
 The waterfall development model originated in the manufacturing and construction industries; where the highly structured physical environments meant that design changes became prohibitively expensive much sooner in the development process. When first adopted for software development, there were no recognised alternatives for knowledge-based creative work.[1]"
WAV,Computer Science,4,"Waveform Audio File Format (WAVE, or WAV due to its filename extension; pronounced ""wave"" or /ˈwæv/ WAV[6])[3][7][8][9] is an audio file format standard, developed by IBM and Microsoft, for storing an audio bitstream on PCs. It is an application of the Resource Interchange File Format (RIFF) bitstream format method for storing data in ""chunks"", and thus is also close to the 8SVX and the AIFF format used on Amiga and Macintosh computers, respectively. It is the main format used on Microsoft Windows systems for raw and typically uncompressed audio. The usual bitstream encoding is the linear pulse-code modulation (LPCM) format.
"
Web_crawler,Computer Science,4,"
 A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the World Wide Web, typically for the purpose of Web indexing (web spidering).
 Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently.
 Crawlers consume resources on visited systems and often visit sites without approval. Issues of schedule, load, and ""politeness"" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all.
 The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly.
 Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping (see also data-driven programming).
"
Wi-Fi,Computer Science,4,"
 Wi-Fi (/ˈwaɪfaɪ/)[1] is a family of wireless network protocols, based on the IEEE 802.11 family of standards, which are commonly used for local area networking of devices and Internet access. Wi‑Fi is a trademark of the non-profit Wi-Fi Alliance, which restricts the use of the term Wi-Fi Certified to products that successfully complete interoperability certification testing.[2][3][4] As of 2017[update], the Wi-Fi Alliance consisted of more than 800 companies from around the world.[5] As of 2019[update], over 3.05 billion Wi-Fi enabled devices are shipped globally each year.[6] Devices that can use Wi-Fi technologies include personal computer desktops and laptops, smartphones and tablets, smart TVs, printers, smart speakers, cars, and drones.
 Wi-Fi uses multiple parts of the IEEE 802 protocol family and is designed to interwork seamlessly with its wired sibling Ethernet. Compatible devices can network through wireless access points to each other as well as to wired devices and the Internet. The different versions of Wi-Fi are specified by various IEEE 802.11 protocol standards, with the different radio technologies determining radio bands, and the maximum ranges, and speeds that may be achieved. Wi-Fi most commonly uses the 2.4 gigahertz (120 mm) UHF and 5 gigahertz (60 mm) SHF ISM radio bands; these bands are subdivided into multiple channels. Channels can be shared between networks but only one transmitter can locally transmit on a channel at any moment in time.
 Wi-Fi's wavebands have relatively high absorption and work best for line-of-sight use. Many common obstructions such as walls, pillars, home appliances, etc. may greatly reduce range, but this also helps minimize interference between different networks in crowded environments. An access point (or hotspot) often has a range of about 20 metres (66 feet) indoors while some modern access points claim up to a 150-metre (490-foot) range outdoors. Hotspot coverage can be as small as a single room with walls that block radio waves, or as large as many square kilometres using many overlapping access points with roaming permitted between them. Over time the speed and spectral efficiency of Wi-Fi have increased. As of 2019, at close range, some versions of Wi-Fi, running on suitable hardware, can achieve speeds of over 1 Gbit/s (gigabit per second).
 Wi-Fi is potentially more vulnerable to attack than wired networks because anyone within range of a network with a wireless network interface controller can attempt access. To connect to a Wi-Fi network, a user typically needs the network name (the SSID) and a password. The password is used to encrypt Wi-Fi packets to block eavesdroppers. Wi-Fi Protected Access (WPA) is intended to protect information moving across Wi-Fi networks and includes versions for personal and enterprise networks. Developing security features of WPA have included stronger protections and new security practices. A QR code can be used to automatically configure a mobile phone's Wi-Fi. Modern phones automatically detect a QR code when taking a picture through application software.
"
XHTML,Computer Science,4,"Extensible HyperText Markup Language (XHTML) is part of the family of XML markup languages. It mirrors or extends versions of the widely used HyperText Markup Language (HTML), the language in which Web pages are formulated.
 While HTML, prior to HTML5, was defined as an application of Standard Generalized Markup Language (SGML), a flexible markup language framework, XHTML is an application of XML, a more restrictive subset of SGML. XHTML documents are well-formed and may therefore be parsed using standard XML parsers, unlike HTML, which requires a lenient HTML-specific parser.[1] XHTML 1.0 became a World Wide Web Consortium (W3C) recommendation on January 26, 2000. XHTML 1.1 became a W3C recommendation on May 31, 2001. The standard known as XHTML5 is being developed as an XML adaptation of the HTML5 specification.[2][3]"
Absolute_advantage,Economics,6,"In economics, the principle of absolute advantage refers to the ability of a party (an individual, or firm, or country) to produce a good or service more efficiently than its competitors.[1] Adam Smith first described the principle of absolute advantage in the context of international trade, using labor as the only input. Since absolute advantage is determined by a simple comparison of labor productiveness, it is possible for a party to have no absolute advantage in anything.[2]"
Adaptive_expectations,Economics,6,"In economics, adaptive expectations is a hypothesized process by which people form their expectations about what will happen in the future based on what has happened in the past.  For example, if inflation has been higher than expected in the past, people would revise expectations for the future.
 One simple version of adaptive expectations is stated in the following equation, where 




p

e




{  p^{e}}
 is the next year's rate of inflation that is currently expected; 




p

−
1


e




{  p_{-1}^{e}}
is this year's rate of inflation that was expected last year; and 



p


{  p}
 is this year's actual rate of inflation:
 where 



λ


{  \lambda }
 is between 0 and 1. This says that current expectations of future inflation reflect past expectations and an ""error-adjustment"" term, in which current expectations are raised (or lowered) according to the gap between actual inflation and previous expectations.  This error-adjustment is also called ""partial adjustment.""
 The theory of adaptive expectations can be applied to all previous periods so that current inflationary expectations equal:
 where 




p

j




{  p_{j}}
 equals actual inflation 



j


{  j}
 years in the past. Thus, current expected inflation reflects a weighted average of all past inflation, where the weights get smaller and smaller as we move further in the past.
 Once a forecasting error is made by agents, due to a stochastic shock, they will be unable to correctly forecast the price level again even if the price level experiences no further shocks since they only ever incorporate part of their errors. The backward nature of expectation formulation and the resultant systematic errors made by agents (see Cobweb model) was unsatisfactory to economists such as John Muth, who was pivotal in the development of an alternative model of how expectations are formed, called rational expectations. This has largely replaced adaptive expectations in macroeconomic theory since its assumption of optimality of expectations is consistent with economic theory. However, it must be stressed that confronting adaptivity and rationality is not necessarily justified, in other words, there are situations in which following the adaptive scheme is a rational response.
 Adaptive expectations were instrumental in the Phillips curve outlined by Milton Friedman. For Friedman, workers form adaptive expectations, so the government can easily surprise them through unexpected monetary policy changes. As agents are trapped by the money illusion, they are unable to correctly perceive price and wage dynamics, so, for Friedman, unemployment can always be reduced through monetary expansions. The result is an increasing level of inflation if the government chooses to fix unemployment at a low rate for an extended period of time. However, in this framework, it is clear why and how adaptive expectations are problematic. Agents are arbitrarily supposed to ignore sources of information which, otherwise, would affect their expectations. For example, government announcements are such sources: agents are expected to modify their expectations and break with the former trends when changes in economic policy necessitate it. This is the reason why the theory of adaptive expectations is often regarded as a deviation from the rational tradition of economics.[1]"
Aggregate_demand,Economics,6,"In macroeconomics, aggregate demand (AD) or domestic final demand (DFD) is the total demand for final goods and services in an economy at a given time.[1] It is often called effective demand, though at other times this term is distinguished. This is the demand for the gross domestic product of a country. It specifies the amount of goods and services that will be purchased at all possible price levels.[2] The aggregate demand curve is plotted with real output on the horizontal axis and the price level on the vertical axis. While it is theorized to be downward sloping, the Sonnenschein–Mantel–Debreu results show that the slope of the curve cannot be mathematically derived from assumptions about individual rational behavior.[3][4] Instead, the downward sloping aggregate demand curve is derived with the help of three macroeconomic assumptions about the functioning of markets: Pigou's wealth effect, Keynes' interest rate effect and the Mundell–Fleming exchange-rate effect. The Pigou effect states that a higher price level implies lower real wealth and therefore lower consumption spending, giving a lower quantity of goods demanded in the aggregate. The Keynes effect states that a higher price level implies a lower real money supply and therefore higher interest rates resulting from financial market equilibrium, in turn resulting in lower investment spending on new physical capital and hence a lower quantity of goods being demanded in the aggregate.
 The Mundell–Fleming exchange-rate effect is an extension of the IS–LM model. Whereas the traditional IS-LM Model deals with a closed economy, Mundell–Fleming describes a small open economy. The Mundell–Fleming model portrays the short-run relationship between an economy's nominal exchange rate, interest rate, and output (in contrast to the closed-economy IS–LM model, which focuses only on the relationship between the interest rate and output).
 The aggregate demand curve illustrates the relationship between two factors: the quantity of output that is demanded and the aggregate price level. Aggregate demand is expressed contingent upon a fixed level of the nominal money supply. There are many factors that can shift the AD curve. Rightward shifts result from increases in the money supply, in government expenditure, or in autonomous components of investment or consumption spending, or from decreases in taxes.
 According to the aggregate demand-aggregate supply model, when aggregate demand increases, there is movement up along the aggregate supply curve, giving a higher level of prices.[5]"
Aggregate_supply,Economics,6,"In economics, aggregate supply (AS) or domestic final supply (DFS) is the total supply of goods and services that firms in a national economy plan on selling during a specific time period. It is the total amount of goods and services that firms are willing and able to sell at a given price level in an economy.[citation needed]"
Aggregation_problem,Economics,6,"An aggregate in economics is a summary measure. The aggregation problem is the difficult problem of finding a valid way to treat an empirical or theoretical aggregate as if it reacted like a less-aggregated measure, say, about behavior of an individual agent as described in general microeconomic theory.[1]  Examples of aggregates in micro- and macroeconomics relative to less aggregated counterparts are:
 Standard theory uses simple assumptions to derive general, and commonly accepted, results such as the law of demand to explain market behavior. An example is the abstraction of a composite good. It considers the price of one good changing proportionately to the composite good, that is, all other goods. If this assumption is violated and the agents are subject to aggregated utility functions, restrictions on the latter are necessary to yield the law of demand. The aggregation problem emphasizes:
 Franklin Fisher notes that this has not dissuaded macroeconomists from continuing to use such concepts.[2]"
Agent_(economics),Economics,6,"In economics, an agent is an actor (more specifically, a decision maker) in a model of some aspect of the economy. Typically, every agent makes decisions by solving a well- or ill-defined optimization or choice problem.
 For example, buyers (consumers) and sellers (producers) are two common types of agents in partial equilibrium models of a single market. Macroeconomic models, especially dynamic stochastic general equilibrium models that are explicitly based on microfoundations, often distinguish households, firms, and governments or central banks as the main types of agents in the economy. Each of these agents may play multiple roles in the economy; households, for example, might act as consumers, as workers, and as voters in the model. Some macroeconomic models distinguish even more types of agents, such as workers and shoppers[1] or commercial banks.[2] The term agent is also used in relation to principal–agent models; in this case, it refers specifically to someone delegated to act on behalf of a principal.[3] In agent-based computational economics, corresponding agents are ""computational objects modeled as interacting according to rules"" over space and time, not real people. The rules are formulated to model behavior and social interactions based on stipulated incentives and information.[4] The concept of an agent may be broadly interpreted to be any persistent individual, social, biological, or physical entity interacting with other such entities in the context of a dynamic multi-agent economic system.
"
Agricultural_economics,Economics,6,"Agricultural economics is an applied field of economics concerned with the application of economic theory in optimizing the production and distribution of food and fiber.  Agricultural economics began as a branch of economics that specifically dealt with land usage, it focused on maximizing the crop yield while maintaining a good soil ecosystem.  Throughout the 20th century the discipline expanded and the current scope of the discipline is much broader.  Agricultural economics today includes a variety of applied areas, having considerable overlap with conventional economics.[1][2][3][4] Agricultural economists have made substantial contributions to research in economics, econometrics, development economics, and environmental economics.  Agricultural economics influences food policy, agricultural policy, and environmental policy.
"
Allocative_efficiency,Economics,6,"Allocative efficiency is a state of the economy in which production represents consumer preferences; in particular, every good or service is produced up to the point where the last unit provides a marginal benefit to consumers equal to the marginal cost of producing.
 In contract theory, allocative efficiency is achieved in a contract in which the skill demanded by the offering party and the skill of the agreeing party are the same.
 Although there are different standards of evaluation for the concept of allocative efficiency, the basic principle asserts that in any economic system, choices in resource allocation produce both ""winners"" and ""losers"" relative to the choice being evaluated. The principles of rational choice, individual maximization, utilitarianism and market theory further suppose that the outcomes for winners and losers can be identified, compared and measured. Under these basic premises, the goal of attaining allocative efficiency can be defined according to some principle where some allocations are subjectively better than others. For example, an economist might say that a change in policy is an allocative improvement as long as those who benefit from the change (winners) gain more than the losers lose (see Kaldor–Hicks efficiency).
 An allocatively efficient economy produces an ""optimal mix"" of commodities.[1]:9 A firm is allocatively efficient when its price is equal to its marginal costs (that is, P = MC) in a perfect market. The demand curve coincides with the marginal utility curve, which measures the (private) benefit of the additional unit, while the supply curve coincides with the marginal cost curve, which measures the (private) cost of the additional unit. In a perfect market, there are no externalities, implying that the demand curve is also equal to the social benefit of the additional unit, while the supply curve measures the social cost of the additional unit. Therefore, the market equilibrium, where demand meets supply, is also where the marginal social benefit equals the marginal social costs. At this point, net social benefit is maximized, meaning this is the allocatively efficient outcome. When a market fails to allocate resources efficiently, there is said to be market failure. Market failure may occur because of imperfect knowledge, differentiated goods, concentrated market power (e.g., monopoly or oligopoly), or externalities.
 In the single-price model, at the point of allocative efficiency price is equal to marginal cost.[2][3]   At this point the social surplus is maximized with no deadweight loss (the latter being the value society puts on that level of output produced minus the value of resources used to achieve that level). Allocative efficiency is the main tool of welfare analysis to measure the impact of markets and public policy upon society and subgroups being made better or worse off.
 It is possible to have Pareto efficiency without allocative efficiency: in such a situation, it is impossible to reallocate resources in such a way that someone gains and no one loses (hence we have Pareto efficiency), yet it would be possible to reallocate in such a way that gainers gain more than losers lose (hence with such a reallocation, we do not have allocative efficiency).[4]:397 Also, for an extensive discussion of various types of allocative efficiency in production context and their estimations see Sickles and Zelenyuk (2019, Chapter 3, etc).[5]"
Competition_law,Economics,6,"
 Competition law is a law that promotes or seeks to maintain market competition by regulating anti-competitive conduct by companies.[1][2] Competition law is implemented through public and private enforcement.[3] Competition law is known as antitrust law in the United States for historical reasons, and as anti-monopoly law in China[1] and Russia. In previous years it has been known as trade practices law in the United Kingdom and Australia. In the European Union, it is referred to as both antitrust[4] and competition law.[5][6] The history of competition law reaches back to the Roman Empire. The business practices of market traders, guilds and governments have always been subject to scrutiny, and sometimes severe sanctions. Since the 20th century, competition law has become global.[7] The two largest and most influential systems of competition regulation are United States antitrust law and European Union competition law. National and regional competition authorities across the world have formed international support and enforcement networks.
 Modern competition law has historically evolved on a national level to promote and maintain fair competition in markets principally within the territorial boundaries of nation-states. National competition law usually does not cover activity beyond territorial borders unless it has significant effects at nation-state level.[2] Countries may allow for extraterritorial jurisdiction in competition cases based on so-called ""effects doctrine"".[2][8] The protection of international competition is governed by international competition agreements. In 1945, during the negotiations preceding the adoption of the General Agreement on Tariffs and Trade (GATT) in 1947, limited international competition obligations were proposed within the Charter for an International Trade Organisation. These obligations were not included in GATT, but in 1994, with the conclusion of the Uruguay Round of GATT multilateral negotiations, the World Trade Organization (WTO) was created. The Agreement Establishing the WTO included a range of limited provisions on various cross-border competition issues on a sector specific basis.[9]"
Applied_economics,Economics,6,"Applied economics is the application of economic theory and econometrics in specific settings. As one of the two sets of fields of economics (the other set being the core),[1] it is typically characterized by the application of the core, i.e. economic theory and econometrics to address practical issues in a range of fields including  demographic economics, labour economics, business economics, industrial organization, agricultural economics, development economics, education economics, engineering economics, financial economics, health economics, monetary economics, public economics, and economic history. From the perspective of economic development, the purpose of applied economics is to enhance the quality of business practices and national policy making.[2] The process often involves a reduction in the level of abstraction of this core theory. There are a variety of approaches including not only empirical estimation using econometrics, input-output analysis or simulations but also case studies, historical analogy and so-called common sense or the ""vernacular"".[3]  This range of approaches is indicative of what Roger Backhouse and Jeff Biddle argue is the ambiguous nature of the concept of applied economics. It is a concept with multiple meanings.[4] Among broad methodological distinctions, one source places it in neither positive nor normative economics but the art of economics, glossed as ""what most economists do"".[5]"
Appropriate_technology,Economics,6,"Appropriate technology is a movement (and its manifestations) encompassing technological choice and application that is small-scale, affordable by locals, decentralized, labor-intensive, energy-efficient, environmentally sound, and locally autonomous.[1] It was originally articulated as intermediate technology by the economist Ernst Friedrich ""Fritz"" Schumacher in his work Small Is Beautiful. Both Schumacher and many modern-day proponents of appropriate technology also emphasize the technology as people-centered.[2] Appropriate technology has been used to address issues in a wide range of fields. Well-known examples of appropriate technology applications include: bike- and hand-powered water pumps (and other self-powered equipment), the universal nut sheller, self-contained solar lamps and streetlights, and passive solar building designs. Today appropriate technology is often developed using open source principles, which have led to open-source appropriate technology (OSAT) and thus many of the plans of the technology can be freely found on the Internet.[3][4] OSAT has been proposed as a new model of enabling innovation for sustainable development.[5][6] Appropriate technology is most commonly discussed in its relationship to economic development and as an alternative to technology transfer of more capital-intensive technology from industrialized nations to developing countries.[2][7] However, appropriate technology movements can be found in both developing and developed countries. In developed countries, the appropriate technology movement grew out of the energy crisis of the 1970s and focuses mainly on environmental and sustainability issues.[8] Today the idea is multifaceted; in some contexts, appropriate technology can be described as the simplest level of technology that can achieve the intended purpose, whereas in others, it can refer to engineering that takes adequate consideration of social and environmental ramifications. The facets are connected through robustness and sustainable living.
"
Arbitrage,Economics,6,"In economics and finance, arbitrage (/ˈɑːrbɪtrɑːʒ/, UK also /-trɪdʒ/) is the practice of taking advantage of a price difference between two or more markets: striking a combination of matching deals that capitalize upon the imbalance, the profit being the difference between the market prices at which the unit is traded. When used by academics, an arbitrage is a transaction that involves no negative cash flow at any probabilistic or temporal state and a positive cash flow in at least one state; in simple terms, it is the possibility of a risk-free profit after transaction costs. For example, an arbitrage opportunity is present when there is the possibility to instantaneously buy something for a low price and sell it for a higher price.
 In principle and in academic use, an arbitrage is risk-free; in common use, as in statistical arbitrage, it may refer to expected profit, though losses may occur, and in practice, there are always risks in arbitrage, some minor (such as fluctuation of prices decreasing profit margins), some major (such as devaluation of a currency or derivative). In academic use, an arbitrage involves taking advantage of differences in price of a single asset or identical cash-flows; in common use, it is also used to refer to differences between similar assets (relative value or convergence trades), as in merger arbitrage.
 The term is mainly applied to trading in financial instruments, such as bonds, stocks, derivatives, commodities, and currencies. People who engage in arbitrage are called arbitrageurs /ˌɑːrbɪtrɑːˈʒɜːr/. 
 Arbitrage has the effect of causing prices of the same or very similar assets in different markets to converge.
"
Arrow%27s_impossibility_theorem,Economics,6,"In social choice theory, Arrow's impossibility theorem, the general possibility theorem or Arrow's paradox is an impossibility theorem stating that when voters have three or more distinct alternatives (options), no ranked voting electoral system can convert the ranked preferences of individuals into a community-wide (complete and transitive) ranking while also meeting a specified set of criteria: unrestricted domain, non-dictatorship, Pareto efficiency, and independence of irrelevant alternatives. The theorem is often cited in discussions of voting theory as it is further interpreted by the Gibbard–Satterthwaite theorem. The theorem is named after economist and Nobel laureate Kenneth Arrow, who demonstrated the theorem in his doctoral thesis and popularized it in his 1951 book Social Choice and Individual Values.  The original paper was titled ""A Difficulty in the Concept of Social Welfare"".[1] In short, the theorem states that no rank-order electoral system can be designed that always satisfies these three ""fairness"" criteria:
 Cardinal voting electoral systems are not covered by the theorem, as they convey more information than rank orders.[2][3] However, Gibbard's theorem extends Arrow's theorem for that case. The theorem can also be sidestepped by weakening the notion of independence.[citation needed] The axiomatic approach Arrow adopted can treat all conceivable rules (that are based on preferences) within one unified framework. In that sense, the approach is qualitatively different from the earlier one in voting theory, in which rules were investigated one by one. One can therefore say that the contemporary paradigm of social choice theory started from this theorem.[4] The practical consequences of the theorem are debatable: Arrow has said ""Most systems are not going to work badly all of the time. All I proved is that all can work badly at times.""[5]"
Austrian_School,Economics,6,"The Austrian School is a heterodox[1][2] school of economic thought that is based on methodological individualism—the concept that social phenomena result exclusively from the motivations and actions of individuals.[3][4][5] The Austrian School originated in late-19th and early-20th century Vienna with the work of Carl Menger, Eugen Böhm von Bawerk, Friedrich von Wieser and others.[6] It was methodologically opposed to the younger Historical School (based in Germany), in a dispute known as Methodenstreit, or methodology struggle. Current-day economists working in this tradition are located in many different countries, but their work is still referred to as Austrian economics. Among the theoretical contributions of the early years of the Austrian School are the subjective theory of value, marginalism in price theory and the formulation of the economic calculation problem, each of which has become an accepted part of mainstream economics.[7] Since the mid-20th century, mainstream economists have been critical of the modern day Austrian School and consider its rejection of mathematical modelling, econometrics and macroeconomic analysis to be outside mainstream economics, or ""heterodox"". In the 1970s, the Austrian School attracted some renewed interest after Friedrich Hayek shared the 1974 Nobel Memorial Prize in Economic Sciences.[8]"
Autarky,Economics,6,"Autarky is the characteristic of self-sufficiency, usually applied to societies, communities, states and their economic systems.[1] Autarky as an ideal or method has been embraced by a wide range of political ideologies and  movements, especially left-wing creeds like African socialism, mutualism, war communism,[2] council communism, Communalism, Swadeshi, syndicalism (especially anarcho-syndicalism) and leftist populism, generally in an effort to build alternative economic structures or to control resources against structures a particular movement views as hostile.
 Conservative, centrist and nationalist movements have also adopted autarky — usually in a fairly limited fashion — in an attempt to preserve part of an existing social order or to develop a particular industry. Some fascist and far-right movements occasionally espoused autarky as a goal in their propaganda but in practice they crushed[3] existing movements[4] towards self-sufficiency and established extensive capital connections to serve as a basis for war and genocide[5] while allying with traditional business elites.[6] Autarky may be a policy of a state or some other type of entity when it seeks to be self-sufficient as a whole, but it also can be limited to a narrow field such as possession of a key raw material. 
 Some countries have a policy of autarky with respect to foodstuffs[7] and water for  national-security reasons.
 Autarky can result from economic isolation or from external circumstances in which a state or other entity reverts to localized production when it lacks currency or excess production to trade with the outside world.[8][9] A military autarky would be a state that could defend itself without help from another country, or could manufacture all of its weapons.
"
Automatic_stabilizer,Economics,6,"In macroeconomics, automatic stabilizers are features of the structure of modern government budgets, particularly income taxes and welfare spending, that act to dampen fluctuations in real GDP.[1] The size of the government budget deficit tends to increase when a country enters a recession, which tends to keep national income higher by maintaining aggregate demand. There may also be a multiplier effect. This effect happens automatically depending on GDP and household income, without any explicit policy action by the government, and acts to reduce the severity of recessions.[2] Similarly, the budget deficit tends to decrease during booms, which pulls back on aggregate demand. Therefore, automatic stabilizers tend to reduce the size of the fluctuations in a country's GDP.
"
Autonomous_consumption,Economics,6,"Autonomous consumption (also exogenous consumption) is the consumption expenditure that occurs when income levels are zero. Such consumption is considered autonomous of income only when expenditure on these consumables does not vary with changes in income; generally, it may be required to fund necessities and debt obligations. If income levels are actually zero, this consumption counts as dissaving, because it is financed by borrowing or using up savings.  Autonomous consumption contrasts with induced consumption, in that it does not systematically fluctuate with income, whereas induced consumption does.[1] The two are related, for all households, through the consumption function:
 where
"
Average_cost,Economics,6,"In economics, average cost or unit cost is equal to total cost (TC) divided by the number of units of a good produced (the output Q):
 



A
C
=



T
C

Q


.


{  AC={\frac {TC}{Q}}.}

 It is also equal to the sum of average variable costs (total variable costs divided by Q) and average fixed costs (total fixed costs divided by Q). Average costs may be dependent on the time period considered (increasing production may be expensive or impossible number in the short term, for example). Average costs affect the supply curve and are a fundamental component of supply and demand.
"
Average_fixed_cost,Economics,6,"In economics, average fixed cost (AFC) is the fixed costs of production (FC) divided by the quantity (Q) of output produced. Fixed costs are those costs that must be incurred in fixed quantity regardless of the level of output produced.
 Average fixed cost is fixed cost per unit of output.  As the total number of units of the good produced increases, the average fixed cost decreases because the same amount of fixed costs is being spread over a larger number of units of output.
 Average variable cost plus average fixed cost equals average total cost:
"
Average_variable_cost,Economics,6,"In economics, average variable cost (AVC) is a firm's variable costs (labour, electricity, etc.) divided by the quantity of output produced. Variable costs are those costs which vary with the output level:
 where 




VC



{  {\text{VC}}}
 = variable cost, 




AVC



{  {\text{AVC}}}
 = average variable cost, and 




Q



{  {\text{Q}}}
 = quantity of output produced.
 Average variable cost plus average fixed cost equals average total cost:
 A firm would choose to shut down if average revenue is below average variable cost at the profit-maximizing positive level of output. Producing anything would not generate revenue significant enough to offset the associated variable costs; producing some output would add losses (additional costs in excess of revenues) to the costs inevitably being incurred (the fixed costs). By not producing, the firm loses only the fixed costs.
"
Average_tax_rate,Economics,6,"In a tax system, the tax rate is the ratio (usually expressed as a percentage) at which a business or person is taxed.  There are several methods used to present a tax rate: statutory, average, marginal, and effective.  These rates can also be presented using different definitions applied to a tax base: inclusive and exclusive.
"
Backward_induction,Economics,6,"Backward induction is the process of reasoning backwards in time, from the end of a problem or situation, to determine a sequence of optimal actions. It proceeds by first considering the last time a decision might be made and choosing what to do in any situation at that time. Using this information, one can then determine what to do at the second-to-last time of decision. This process continues backwards until one has determined the best action for every possible situation (i.e. for every possible information set) at every point in time. It was first used by Zermelo in 1913, to prove that chess has pure optimal strategies.[1][2] In the mathematical optimization method of dynamic programming, backward induction is one of the main methods for solving the Bellman equation.[3][4] In game theory, backward induction is a method used to compute subgame perfect equilibria in sequential games.[5] The only difference is that optimization involves just one decision maker, who chooses what to do at each point of time, whereas game theory analyzes how the decisions of several players interact. That is, by anticipating what the last player will do in each situation, it is possible to determine what the second-to-last player will do, and so on.  In the related fields of automated planning and scheduling and automated theorem proving, the method is called backward search or backward chaining. In chess it is called retrograde analysis.
 Backward induction has been used to solve games as long as the field of game theory has existed. John von Neumann and Oskar Morgenstern suggested solving zero-sum, two-person games by backward induction in their Theory of Games and Economic Behavior (1944), the book which established game theory as a field of study.[2][6]"
Balance_of_payments,Economics,6,"
 
 The balance of payments (also known as balance of international payments and abbreviated B.O.P. or BoP) of a country is the difference between all money flowing into the country in a particular period of time (e.g., a quarter or a year) and the outflow of money to the rest of the world. These financial transactions are made by individuals, firms and government bodies to compare receipts and payments arising out of trade of goods and services. 
 The balance of payments consists of three components: the current account, the capital account and the financial account. The current account reflects a country's net income, while the capital account reflects the net change in ownership of national assets.
"
Balance_of_trade,Economics,6,"
 The balance of trade, commercial balance, or net exports (sometimes symbolized as NX), is the difference between the monetary value of a nation's exports and imports over a certain time period.[1] Sometimes a distinction is made between a balance of trade for goods versus one for services. The balance of trade measures a flow of exports and imports over a given period of time. The notion of the balance of trade does not mean that exports and imports are ""in balance"" with each other.
 If a country exports a greater value than it imports, it has a trade surplus or positive trade balance, and conversely, if a country imports a greater value than it exports, it has a trade deficit or negative trade balance. As of 2016, about 60 out of 200 countries have a trade surplus. The notion that bilateral trade deficits are bad in and of themselves is overwhelmingly rejected by trade experts and economists.[2][3][4][5][6]"
Balanced_budget,Economics,6,"A balanced budget (particularly that of a government) is a budget in which revenues are equal to expenditures. Thus, neither a budget deficit nor a budget surplus exists (the accounts ""balance""). More generally, it is a budget that has no budget deficit, but could possibly have a budget surplus.[1] A cyclically balanced budget is a budget that is not necessarily balanced year-to-year, but is balanced over the economic cycle, running a surplus in boom years and running a deficit in lean years, with these offsetting over time.
 Balanced budgets and the associated topic of budget deficits are a contentious point within academic economics and within politics. Many economists argue that moving from a budget deficit to a balanced budget decreases interest rates,[2] increases investment,[2] shrinks trade deficits and helps the economy grow faster in the longer term.[2]"
Bank,Economics,6,"
 A bank is a financial institution that accepts deposits from the public and creates a demand deposit while simultaneously making loans.[1] Lending activities can be performed either directly or indirectly through capital markets.
 Due to the importance of banks in the financial stability of a country, most jurisdictions exercise a high degree of regulation over banks. Most countries have institutionalized a system known as fractional reserve banking, under which banks hold liquid assets equal to only a portion of their current liabilities. In addition to other regulations intended to ensure liquidity, banks are generally subject to minimum capital requirements based on an international set of capital standards, the Basel Accords.
 Banking in its modern sense evolved in the fourteenth century in the prosperous cities of Renaissance Italy but in many ways functioned as a continuation of ideas and concepts of credit and lending that had their roots in the ancient world. In the history of banking, a number of banking dynasties — notably, the  Medicis, the Fuggers, the Welsers, the  Berenbergs, and the  Rothschilds — have played a central role over many centuries. The oldest existing retail bank is Banca Monte dei Paschi di Siena (founded in 1472), while the oldest existing merchant bank is Berenberg Bank (founded in 1590).
"
Bankruptcy,Economics,6,"Bankruptcy is a legal process through which people or other entities who cannot repay debts to creditors may seek relief from some or all of their debts. In most jurisdictions, bankruptcy is imposed by a court order, often initiated by the debtor.
 Bankrupt is not the only legal status that an insolvent person may have, and the term bankruptcy is therefore not a synonym for insolvency.
"
Barriers_to_entry,Economics,6,"In theories of competition in economics, a barrier to entry, or an economic barrier to entry, is a fixed cost that must be incurred by a new entrant, regardless of production or sales activities, into a market that incumbents do not have or have not had to incur.[1][2]
Because barriers to entry protect incumbent firms and restrict competition in a market, they can contribute to distortionary prices and are therefore most important when discussing antitrust policy. Barriers to entry often cause or aid the existence of monopolies and oligopolies, or give companies market power.
"
Barter,Economics,6,"In trade, barter (derived from baretor[1]) is a system of exchange where participants in a transaction directly exchange goods or services for other goods or services without using a medium of exchange, such as money.[2] Economists distinguish barter from gift economies in many ways; barter, for example, features immediate reciprocal exchange, not delayed in time. Barter usually takes place on a bilateral basis, but may be multilateral (i.e., mediated through a trade exchange). In most developed countries, barter usually only exists parallel to monetary systems to a very limited extent. Market actors use barter as a replacement for money as the method of exchange in times of monetary crisis, such as when currency becomes unstable (e.g., hyperinflation or a deflationary spiral) or simply unavailable for conducting commerce.
 No ethnographic studies have shown that any present or past society has used barter without any other medium of exchange or measurement, nor have anthropologists found evidence that money emerged from barter, instead finding that gift-giving (credit extended on a personal basis with an inter-personal balance maintained over the long term) was the most usual means of exchange of goods and services. Nevertheless, economists since the times of Adam Smith (1723–1790), taking non-specific, often wholly or inaccurately imagined pre-modern societies as examples, have used the inefficiency of barter to explain the emergence of money, of ""the"" economy, and hence of the discipline of economics itself.[3][4][5]"
Behavioral_economics,Economics,6,"Behavioral economics (also, behavioural economics) studies the effects of psychological, cognitive, emotional, cultural and social factors on the decisions of individuals and institutions and how those decisions vary from those implied by classical economic theory.[1][2] 
Behavioral economics is primarily concerned with the bounds of rationality of economic agents. Behavioral models typically integrate insights from psychology, neuroscience and microeconomic theory.[3][4] The study of behavioral economics includes how market decisions are made and the mechanisms that drive public choice. .mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}"
Bellman_equation,Economics,6,"A Bellman equation, named after Richard E. Bellman, is a necessary condition for optimality associated with the mathematical optimization method known as dynamic programming.[1] It writes the ""value"" of a decision problem at a certain point in time in terms of the payoff from some initial choices and the ""value"" of the remaining decision problem that results from those initial choices.[citation needed] This breaks a dynamic optimization problem into a sequence of simpler subproblems, as Bellman's “principle of optimality” prescribes.[2] The Bellman equation was first applied to engineering control theory and to other topics in applied mathematics, and subsequently became an important tool in economic theory; though the basic concepts of dynamic programming are prefigured in John von Neumann and Oskar Morgenstern's Theory of Games and Economic Behavior and Abraham Wald's sequential analysis.[citation needed] Almost any problem that can be solved using optimal control theory can also be solved by analyzing the appropriate Bellman equation.[why?][further explanation needed] However, the term 'Bellman equation' usually refers to the dynamic programming equation associated with discrete-time optimization problems.[3] In continuous-time optimization problems, the analogous equation is a partial differential equation that is called the Hamilton–Jacobi–Bellman equation.[4][5]"
Bequest_motive,Economics,6,"A bequest motive seeks to provide an economic justification for the phenomenon of intergenerational transfers of wealth.  In other words, to explain why people leave money behind when they die.
 
Which bequest motive theory most realistically represents the intentions of estate planners is unclear.  Attempts to test the theories empirically are mired by poor availability of data about wealth holdings.
"
Bertrand%E2%80%93Edgeworth_model,Economics,6,"
In microeconomics, the Bertrand–Edgeworth model of price-setting oligopoly looks at what happens when there is a homogeneous product (i.e. consumers want to buy from the cheapest seller) where there is a limit to the output of firms which they are willing and able to sell at a particular price. This differs from the Bertrand competition model where it is assumed that firms are willing and able to meet all demand. The limit to output can be considered as a physical capacity constraint which is the same at all prices (as in Edgeworth's work), or to vary with price under other assumptions.
"
Black%E2%80%93Scholes_model,Economics,6,"The Black–Scholes /ˌblæk ˈʃoʊlz/[1] or Black–Scholes–Merton model is a mathematical model for the dynamics of a financial market containing derivative investment instruments. From the partial differential equation in the model, known as the Black–Scholes equation, one can deduce the Black–Scholes formula, which gives a theoretical estimate of the price of European-style options and shows that the option has a unique price regardless of the risk of the security and its expected return (instead replacing the security's expected return with the risk-neutral rate). The formula led to a boom in options trading and provided mathematical legitimacy to the activities of the Chicago Board Options Exchange and other options markets around the world.[2] It is widely used, although often with some adjustments, by options market participants.[3]:751 Based on works previously developed by market researchers and practitioners, such as Louis Bachelier, Sheen Kassouf and Ed Thorp among others, Fischer Black and Myron Scholes demonstrated in 1968 that a dynamic revision of a portfolio removes the expected return of the security, thus inventing the risk neutral argument.[4][5] In 1970, after they attempted to apply the formula to the markets and incurred financial losses due to lack of risk management in their trades, they decided to focus in their domain area, the academic environment.[6] After three years of efforts, the formula—named in honor of them for making it public—was finally published in 1973 in an article entitled ""The Pricing of Options and Corporate Liabilities"", in the Journal of Political Economy.[7][8][9] Robert C. Merton was the first to publish a paper expanding the mathematical understanding of the options pricing model, and coined the term ""Black–Scholes options pricing model"". Merton and Scholes received the 1997 Nobel Memorial Prize in Economic Sciences for their work, the committee citing their discovery of the risk neutral dynamic revision as a breakthrough that separates the option from the risk of the underlying security.[10] Although ineligible for the prize because of his death in 1995, Black was mentioned as a contributor by the Swedish Academy.[11] The key idea behind the model is to hedge the option by buying and selling the underlying asset in just the right way and, as a consequence, to eliminate risk. This type of hedging is called ""continuously revised delta hedging"" and is the basis of more complicated hedging strategies such as those engaged in by investment banks and hedge funds.
 The model's assumptions have been relaxed and generalized in many directions, leading to a plethora of models that are currently used in derivative pricing and risk management. It is the insights of the model, as exemplified in the Black–Scholes formula, that are frequently used by market participants, as distinguished from the actual prices. These insights include no-arbitrage bounds and risk-neutral pricing (thanks to continuous revision). Further, the Black–Scholes equation, a partial differential equation that governs the price of the option, enables pricing using numerical methods when an explicit formula is not possible.
 The Black–Scholes formula has only one parameter that cannot be directly observed in the market: the average future volatility of the underlying asset, though it can be found from the price of other options. Since the option value (whether put or call) is increasing in this parameter, it can be inverted to produce a ""volatility surface"" that is then used to calibrate other models, e.g. for OTC derivatives.
"
Board_of_governors,Economics,6,"A board of directors is a group of people who jointly supervise the activities of an organization, which can be either a for-profit or a nonprofit organization such as a business, nonprofit organization, or a government agency. 
 
 The powers, duties, and responsibilities of a board of directors are determined by government regulations (including the jurisdiction's corporate law) and the organization's own constitution and bylaws. These authorities may specify the number of members of the board, how they are to be chosen, and how often they are to meet.
 In an organization with voting members, the board is accountable to, and may be subordinate to, the organization's full membership, which usually elect the members of the board. In a stock corporation, non-executive directors are elected by the shareholders, and the board has ultimate responsibility for the management of the corporation. In nations with codetermination (such as Germany and Sweden), the workers of a corporation elect a set fraction of the board's members. 
 The board of directors appoints the chief executive officer of the corporation and sets out the overall strategic direction. In corporations with dispersed ownership, the identification and nomination of directors (that shareholders vote for or against) are often done by the board itself, leading to a high degree of self-perpetuation. In a non-stock corporation with no general voting membership, the board is the supreme governing body of the institution, and its members are sometimes chosen by the board itself.[1][2][3]"
Bond_(finance),Economics,6,"In finance, a bond is an instrument of indebtedness of the bond issuer to the holders. The most common types of bonds include municipal bonds and corporate bonds. Bonds can be in mutual funds or can be in private investing where a person would give a loan to a company or the government.
 The bond is a debt security, under which the issuer owes the holders a debt and (depending on the terms of the bond) is obliged to pay them interest (the coupon) or to repay the principal at a later date, termed the maturity date.[1] Interest is usually payable at fixed intervals (semiannual, annual, sometimes monthly). Very often the bond is negotiable, that is, the ownership of the instrument can be transferred in the secondary market. This means that once the transfer agents at the bank medallion stamp the bond, it is highly liquid on the secondary market.[2] Thus a bond is a form of loan or IOU: the holder of the bond is the lender (creditor), the issuer of the bond is the borrower (debtor), and the coupon is the interest. Bonds provide the borrower with external funds to finance long-term investments, or, in the case of government bonds, to finance current expenditure. Certificates of deposit (CDs) or short-term commercial paper are considered[by whom?] to be money market instruments and not bonds: the main difference is the length of the term of the instrument.
 Bonds and stocks are both securities, but the major difference between the two is that (capital) stockholders have an equity stake in a company (that is, they are owners), whereas bondholders have a creditor stake in the company (that is, they are lenders). Being a creditor, bondholders have priority over stockholders. This means they will be repaid in advance of stockholders, but will rank behind secured creditors, in the event of bankruptcy.[3]
Another difference is that bonds usually have a defined term, or maturity, after which the bond is redeemed, whereas stocks typically remain outstanding indefinitely. An exception is an irredeemable bond, such as a consol, which is a perpetuity, that is, a bond with no maturity.
"
Borrower,Economics,6,"A debtor or debitor is a legal entity (legal person) that owes a debt to another entity. The entity may be an individual, a firm, a government, a company or other legal person. The counterparty is called a creditor. When the counterpart of this debt arrangement is a bank, the debtor is more often referred to as a borrower.
 If X borrowed money from his/her bank, X is the debtor and the bank is the creditor. If X puts money in the bank, X is the creditor and the bank is the debtor.
 It is not a crime to fail to pay a debt. Except in certain bankruptcy situations, debtors can choose to pay debts in any priority they choose. But if one fails to pay a debt, they have broken a contract or agreement between them and a creditor. Generally, most oral and written agreements for the repayment of consumer debt - debts for personal, family or household purposes secured primarily by a person's residence - are enforceable.[1] For the most part, debts that are business related must be made in writing to be enforceable by law. If the written agreement requires the debtor to pay a specific amount of money, then the creditor does not have to accept any lesser amount, and should be paid in full.
 Also, if there was no actual agreement but the creditor has proven to have loaned an amount of money, undertaken services or given the debtor a product, the debtor must then pay the creditor.
"
Break-even_(economics),Economics,6,"The break-even point (BEP) in economics, business—and specifically cost accounting—is the point at which total cost and total revenue are equal, i.e. ""even"". There is no net loss or gain, and one has ""broken even"", though opportunity costs have been paid and capital has received the risk-adjusted, expected return. In short, all costs that must be paid are paid, and there is neither profit or loss.[1][2]"
Bretton_Woods_system,Economics,6,"
 
The Bretton Woods system of monetary management established the rules for commercial and financial relations among the United States, Canada, Western European countries, Australia, and Japan after the 1944 Bretton Woods Agreement. The Bretton Woods system was the first example of a fully negotiated monetary order intended to govern monetary relations among independent states. The chief features of the Bretton Woods system were an obligation for each country to adopt a monetary policy that maintained its external exchange rates within 1 percent by tying its currency to gold and the ability of the International Monetary Fund (IMF) to bridge temporary imbalances of payments. Also, there was a need to address the lack of cooperation among other countries and to prevent competitive devaluation of the currencies as well.
 Preparing to rebuild the international economic system while World War II was still raging, 730 delegates from all 44 Allied nations gathered at the Mount Washington Hotel in Bretton Woods, New Hampshire, United States, for the United Nations Monetary and Financial Conference, also known as the Bretton Woods Conference. The delegates deliberated during 1–22 July 1944, and signed the Bretton Woods agreement on its final day. Setting up a system of rules, institutions, and procedures to regulate the international monetary system, these accords established the IMF and the International Bank for Reconstruction and Development (IBRD), which today is part of the World Bank Group. The United States, which controlled two thirds of the world's gold, insisted that the Bretton Woods system rest on both gold and the US dollar. Soviet representatives attended the conference but later declined to ratify the final agreements, charging that the institutions they had created were ""branches of Wall Street"".[1] These organizations became operational in 1945 after a sufficient number of countries had ratified the agreement.
 On 15 August 1971, the United States unilaterally terminated convertibility of the US dollar to gold, effectively bringing the Bretton Woods system to an end and rendering the dollar a fiat currency.[2]  At the same time, many fixed currencies (such as the pound sterling) also became free-floating.
"
Deficit_spending,Economics,6,"Deficit spending is the amount by which spending exceeds revenue over a particular period of time, also called simply deficit, or budget deficit; the opposite of budget surplus. The term may be applied to the budget of a government, private company, or individual. Government deficit spending is a central point of controversy in economics, as discussed below.
"
Budget_set,Economics,6,"A budget set or opportunity set includes all possible consumption bundles that someone can afford given the prices of goods and the person's income level. The budget set is bounded above by the budget line. In set notation, for consumption goods 




x

=

[


x

1


,

x

2


,
…
,

x

k



]



{  \mathbf {x} =\left[x_{1},x_{2},\ldots ,x_{k}\right]}
 with associated prices 




p

=

[


p

1


,

p

2


,
…
,

p

k



]



{  \mathbf {p} =\left[p_{1},p_{2},\ldots ,p_{k}\right]}
, the budget set is
 where 



m


{  m}
 is income, and the consumption set 



X


{  X}
 is assumed to be the nonnegative orthant in 





R


k




{  \mathbb {R} ^{k}}
.
 Graphically speaking, all the consumption bundles that lie inside the budget constraint and on the budget constraint form the budget set or opportunity set.
 By most definitions, budget sets must be compact and convex.
"
Balanced_budget,Economics,6,"A balanced budget (particularly that of a government) is a budget in which revenues are equal to expenditures. Thus, neither a budget deficit nor a budget surplus exists (the accounts ""balance""). More generally, it is a budget that has no budget deficit, but could possibly have a budget surplus.[1] A cyclically balanced budget is a budget that is not necessarily balanced year-to-year, but is balanced over the economic cycle, running a surplus in boom years and running a deficit in lean years, with these offsetting over time.
 Balanced budgets and the associated topic of budget deficits are a contentious point within academic economics and within politics. Many economists argue that moving from a budget deficit to a balanced budget decreases interest rates,[2] increases investment,[2] shrinks trade deficits and helps the economy grow faster in the longer term.[2]"
Big_push_model,Economics,6,"The big push model is a concept in development economics or welfare economics that emphasizes that a firm's decision whether to industrialize or not depends on its expectation of what other firms will do. It assumes economies of scale and oligopolistic market structure and explains when industrialization would happen.
 The originator of this theory was Paul Rosenstein-Rodan in 1943. Further contributions were made later on by Murphy, Shleifer and Robert W. Vishny in 1989. Analysis of this economic model ordinarily involves using game theory.
 The theory of the model emphasizes that underdeveloped countries require large amounts of investments to embark on the path of economic development from their present state of backwardness. This theory proposes that a 'bit by bit' investment programme will not impact the process of growth as much as is required for developing countries. In fact, injections of small quantities of investments will merely lead to a wastage of resources.
Paul Rosenstein-Rodan approvingly quotes a Massachusetts Institute of Technology study in this regard, ""There is a minimum level of resources that must be devoted to... a development programme if it is to have any chance of success. Launching a country into self-sustaining growth is a little like getting an airplane off the ground. There is a critical ground speed which must be passed before the craft can become airborne....""[1] Rosenstein-Rodan argued that the entire industry which is intended to be created should be treated and planned as a massive entity (a firm or trust). He supports this argument by stating that the social marginal product of an investment is always different from its private marginal product, so when a group of industries are planned together according to their social marginal products, the rate of growth of the economy is greater than it would have otherwise been.[2]"
Business_cycle,Economics,6,"The business cycle, also known as the economic cycle or trade cycle, are the fluctuations of gross domestic product (GDP) around its long-term growth trend.[1] The length of a business cycle is the period of time containing a single boom and contraction in sequence. These fluctuations typically involve shifts over time between periods of relatively rapid economic growth (expansions or booms) and periods of relative stagnation or decline (contractions or recessions).
 Business cycles are usually measured by considering the growth rate of real gross domestic product. Despite the often-applied term cycles, these fluctuations in economic activity do not exhibit uniform or predictable periodicity. The common or popular usage boom-and-bust cycle refers to fluctuations in which the expansion is rapid and the contraction severe.[2] The current view of mainstream economics is that business cycles are essentially the summation of purely random shocks to the economy and thus are not, in fact, cycles, despite appearing to be so. However, certain heterodox schools propose alternative theories suggesting that cycles do in fact exist due to endogenous causes.[3]"
Business_economics,Economics,6,"Business economics is a field in applied economics which uses economic theory and quantitative methods to analyze business enterprises and the factors contributing to the diversity of organizational structures and the relationships of firms with labour, capital and product markets.[1] A professional focus of the journal Business Economics has been expressed as providing ""practical information for people who apply economics in their jobs.""[2] Business economics is an integral part of traditional economics and is an extension of economic concepts to the real business situations. It is an applied science in the sense of a tool of managerial decision-making and forward planning by management. In other words, business economics is concerned with the application of economic theory to business management. Business economics is based on microeconomics in two categories: positive and normative.
 Business economics focuses on the economic issues and problems related to business organization, management, and strategy. Issues and problems include: an explanation of why corporate firms emerge and exist; why they expand: horizontally, vertically and spacially; the role of entrepreneurs and entrepreneurship; the significance of organizational structure; the relationship of firms with employees, providers of capital,  customers, and government; and interactions between firms and the business environment.[1]"
Business_sector,Economics,6,"In business, the business sector or corporate sector - sometimes popularly called simply ""business"" - is ""the part of the economy made up by companies"".[1][need quotation to verify][2]
It is a subset of the domestic economy,[3]
excluding the economic activities of general government, of private households, and of non-profit organizations serving individuals.[4]
An alternative analysis of economies, the three-sector theory, subdivides them  into:[5] In the United States the business sector accounted for about 78 percent of the value of gross domestic product (GDP) as of 2000[update].[4] Kuwait and Tuvalu each had business sectors accounting for less than 40% of GDP as of 2015[update].[6] The Oxford English Dictionary records the phrase ""business sector"" in the general sense from 1934.[7]
Word usage suggests that the concept of a ""business sector"" came into wider use after 1940.[8]
Related terms in previous times included ""merchant class"" and ""merchant caste"".
"
Capacity_utilization,Economics,6,"Capacity utilization or capacity utilisation is the extent to which a firm or nation employs its installed productive capacity. It is the relationship between output that is produced with the installed equipment, and the potential output which could be produced with it, if capacity was fully used.The Formula is the actual output per period all over full capacity per period expressed as a percentage.
"
Capital_(economics),Economics,6,"In economics, capital consists of human-created assets that can enhance one's power to perform economically useful work.[citation needed] For example, a stone arrowhead is capital for a hunter-gatherer who can use it as a hunting instrument; similarly, roads are capital for inhabitants of a city. Capital is distinct from land and other non-renewable resources in that it can be increased by human labor, and does not include certain durable goods like homes and personal automobiles that are not used in the production of saleable goods and services. Adam Smith defined capital as ""that part of man's stock which he expects to afford him revenue"". In economic models, capital is an input in the production function. 
 The total physical capital at any given moment in time is referred to as the capital stock (not to be confused with the capital stock of a business entity). Capital goods, real capital, or capital assets are already-produced, durable goods or any non-financial asset that is used in production of goods or services.[1] In Marxian economics,[2] capital is money used to buy something only in order to sell it again to realize a profit. For Marx, capital only exists within the process of the economic circuit (represented by M-C-M')—it is wealth that grows out of the process of circulation itself, and for Marx it formed the basis of the economic system of capitalism. In more contemporary schools of economics, this form of capital is generally referred to as ""financial capital"" and is distinguished from ""capital goods"".
"
Capital_cost,Economics,6,"Capital costs are fixed, one-time expenses incurred on the purchase of land, buildings, construction, and equipment used in the production of goods or in the rendering of services. In other words, it is the total cost needed to bring a project to a commercially operable status. Whether a particular cost is capital or not depend on many factors such as accounting, tax laws, and materiality.
"
Capital_flight,Economics,6,"Capital flight, in economics, occurs when assets or money rapidly flow out of a country, due to an event of economic consequence or as the result of economic globalization. Such events could be an increase in taxes on capital or capital holders or the government of the country defaulting on its debt that disturbs investors and causes them to lower their valuation of the assets in that country, or otherwise to lose confidence in its economic strength.
 This leads to a disappearance of wealth, and is usually accompanied by a sharp drop in the exchange rate of the affected country—depreciation in a variable exchange rate regime, or a forced devaluation in a fixed exchange rate regime.
 This fall is particularly damaging when the capital belongs to the people of the affected country, because not only are the citizens now burdened by the loss in the economy and devaluation of their currency, but probably also, their assets have lost much of their nominal value. This leads to dramatic decreases in the purchasing power of the country's assets and makes it increasingly expensive to import goods and acquire any form of foreign facilities, e.g. medical facilities.
"
Capital_good,Economics,6,"A capital good (also called complex products and systems (CoPS), and means of production) is a durable good that is used in the production of goods or services. Capital goods are one of the three types of producer goods, the other two being land and labour. The three are also known collectively as ""primary factors of production""
 This classification originated during the classical economics period and has remained the dominant method for classification.
 Many definitions and descriptions of capital goods production have been proposed in the literature. Capital goods are generally considered one-of-a-kind, capital intensive products that consist of many components. They are often used as manufacturing systems or services themselves.
 Examples include hand tools, machine tools, data centers, oil rigs, semiconductor fabrication plants, and wind turbines. Their production is often organized in projects, with several parties cooperating in networks (Hicks et al. 2000; Hicks and McGovern 2009; Hobday 1998).
 A capital good lifecycle typically consists of tendering, engineering and procurement, manufacturing, commissioning, maintenance and (sometimes) decommissioning.[1][2] In terms of economics, capital goods are tangible property. A society acquires capital goods by saving wealth that can be invested in the means of production. People use them to produce other goods or services within a certain period. Machinery, tools, buildings, computers, or other kinds of equipment that are involved in production of other things for sale are capital goods. The owners of the capital good can be individuals, households, corporations or governments. Any material used to produce capital goods is also considered a capital good.
"
Cartel,Economics,6,"A cartel is a group of independent market participants who collude with each other in order to improve their profits and dominate the market. Cartels are usually associations in the same sphere of business, and thus an alliance of rivals. Most jurisdictions consider it anti-competitive behavior. Cartel behavior includes price fixing, bid rigging, and reductions in output. The doctrine in economics that analyzes cartels is cartel theory. Cartels are distinguished from other forms of collusion or anti-competitive organization such as corporate mergers.
"
Central_bank,Economics,6,"
 A central bank, reserve bank, or monetary authority is an institution that manages the currency and monetary policy of a state or formal monetary union,[1]
and oversees their commercial banking system. In contrast to a commercial bank, a central bank possesses a monopoly on increasing the monetary base. Most central banks also have supervisory and regulatory powers to ensure the stability of member institutions, to prevent bank runs, and to discourage reckless or fraudulent behavior by member banks.
 Central banks in most developed nations are institutionally independent from political interference.[2][3][4] Still, limited control by the executive and legislative bodies exists.[5][6]"
Certificate_of_Deposit,Economics,6,"A certificate of deposit (CD) is a time deposit, a financial product commonly sold by banks, thrift institutions, and credit unions. CDs differ from savings accounts in that the CD has a specific, fixed term (often one, three, or six months, or one to five years) and usually, a fixed interest rate. The bank expects CD to be held until maturity, at which time they can be withdrawn and interest paid.
 Like savings accounts, CDs are insured ""money in the bank"" (in the US up to $250,000) and thus, up to the local insured deposit limit, virtually risk free. In the US, CDs are insured by the Federal Deposit Insurance Corporation (FDIC) for banks and by the National Credit Union Administration (NCUA) for credit unions.
 In exchange for the customer depositing the money for an agreed term, institutions usually offer higher interest rates than they do on accounts that customers can withdraw from on demand—though this may not be the case in an inverted yield curve situation. Fixed rates are common, but some institutions offer CDs with various forms of variable rates. For example, in mid-2004, interest rates were expected to rise—and many banks and credit unions began to offer CDs with a ""bump-up"" feature. These allow for a single readjustment of the interest rate, at a time of the consumer's choosing, during the term of the CD. Sometimes, financial institutions introduce CDs indexed to the stock market, bond market, or other indices.
 Some features of CDs are:
 CDs typically require a minimum deposit, and may offer higher rates for larger deposits. The best rates are generally offered on ""Jumbo CDs"" with minimum deposits of $100,000. Jumbo CDs are commonly bought by large institutional investors, such as banks and pension funds, that are interested in low-risk and stable investment options. Jumbo CDs are also known as negotiable certificates of deposits and come in bearer form. These work like conventional certificate of deposits that lock in the principal amount for a set timeframe and are payable upon maturity. [1] The consumer who opens a CD may receive a paper certificate, but it is now common for a CD to consist simply of a book entry and an item shown in the consumer's periodic bank statements. That is, there is often no ""certificate"" as such. Consumers who want a hard copy that verifies their CD purchase may request a paper statement from the bank, or print out their own from the financial institution's online banking service.
"
Circular_flow_of_income,Economics,6,"The circular flow of income or circular flow is a model of the economy in which the major exchanges are represented as flows of money, goods and services, etc. between economic agents. The flows of money and goods exchanged in a closed circuit correspond in value, but run in the opposite direction. The circular flow analysis is the basis of national accounts and hence of macroeconomics.
 The idea of the circular flow was already present in the work of Richard Cantillon.[3] François Quesnay developed and visualized this concept in the so-called Tableau économique.[4] Important developments of Quesnay's tableau were Karl Marx' reproduction schemes in the second volume of Capital: Critique of Political Economy, and John Maynard Keynes' General Theory of Employment, Interest and Money. Richard Stone further developed the concept for the United Nations (UN) and the Organisation for Economic Co-operation and Development to the system, which is now used internationally.
"
Circulation_(currency),Economics,6,"In monetary economics, the currency in circulation in a country is the value of 
currency or cash (banknotes and coins) that has ever been issued by the country’s monetary authority less the amount that has been removed. More broadly, money in circulation is the total money supply of a country, which can be defined in various ways, but always includes currency and also some types of bank deposits, such as deposits at call.
 The published amount of currency in circulation tends to be overstated by an unknown amount because it does not take into account money that has been destroyed, or held by individuals as a form of security (the proverbial “money under the mattress”), or by coin collectors, domestic or foreign, or which is held in reserve within the banking system, including currency held by foreign central banks as a foreign exchange reserve asset.
"
Classical_economics,Economics,6,"Classical economics or classical political economy is a school of thought in economics that flourished, primarily in Britain, in the late 18th and early-to-mid 19th century. Its main thinkers are held to be Adam Smith, Jean-Baptiste Say, David Ricardo, Thomas Robert Malthus, and John Stuart Mill. These economists produced a theory of market economies as largely self-regulating systems, governed by natural laws of production and exchange (famously captured by Adam Smith's metaphor of the invisible hand).
 Adam Smith's The Wealth of Nations in 1776 is usually considered to mark the beginning of classical economics.[1] The fundamental message in Smith's book was that the wealth of any nation was determined not by the gold in the monarch's coffers, but by its national income. This income was in turn based on the labor of its inhabitants, organized efficiently by the division of labour and the use of accumulated capital, which became one of classical economics' central concepts.[2] In terms of economic policy, the classical economists were pragmatic liberals, advocating the freedom of the market, though they saw a role for the state in providing for the common good. Smith acknowledged that there were areas where the market is not the best way to serve the common interest, and he took it as a given that the greater proportion of the costs supporting the common good should be borne by those best able to afford them. He warned repeatedly of the dangers of monopoly, and stressed the importance of competition.[1] In terms of international trade, the classical economists were advocates of free trade, which distinguishes them from their mercantilist predecessors, who advocated protectionism.
 The designation of Smith, Ricardo and some earlier economists as ""classical"" is due to Karl Marx, to distinguish the ""greats"" of economic theory from their ""vulgar"" successors. There is some debate about what is covered by the term classical economics, particularly when dealing with the period from 1830–75, and how classical economics relates to neoclassical economics.
"
Command_economy,Economics,6,"A planned economy is a type of economic system where investment, production and the allocation of capital goods take place according to economy-wide economic plans and production plans. A planned economy may use centralized, decentralized, participatory or Soviet-type forms of economic planning.[1][2] The level of centralization or decentralization in decision-making and participation depends on the specific type of planning mechanism employed.[3] Socialist states based on the Soviet model have used central planning, although a minority such as the Socialist Federal Republic of Yugoslavia have adopted some degree of market socialism. Market abolitionist socialism replaces factor markets with direct calculation as the means to coordinate the activities of the various socially-owned economic enterprises that make up the economy.[4][5][6] More recent approaches to socialist planning and allocation have come from some economists and computer scientists proposing planning mechanisms based on advances in computer science and information technology.[7] Planned economies contrast with unplanned economies, specifically market economies, where autonomous firms operating in markets make decisions about production, distribution, pricing and investment. Market economies that use indicative planning are variously referred to as planned market economies, mixed economies and mixed market economies. A command economy follows an administrative-command system and uses Soviet-type economic planning which was characteristic of the former Soviet Union and Eastern Bloc before most of these countries converted to market economies. This highlights the central role of hierarchical administration and public ownership of production in guiding the allocation of resources in these economic systems.[8][9][10] In command economies, important allocation decisions are made by government authorities and are imposed by law.[11] This goes against the Marxist understanding of conscious planning.[12][13] Decentralized planning has been proposed as a basis for socialism and has been variously advocated by anarchists, council communists, libertarian Marxists and other democratic and libertarian socialists who advocate a non-market form of socialism, in total rejection of the type of planning adopted in the economy of the Soviet Union.[14]"
Commerce,Economics,6,"Commerce is the exchange of goods and services, especially on a large scale.[1]"
Commodity,Economics,6,"In economics, a commodity is an economic good that has full or substantial fungibility: that is, the market treats instances of the good as equivalent or nearly so with no regard to who produced them.[1][2][3] The price of a commodity good is typically determined as a function of its market as a whole: well-established physical commodities have actively traded spot and derivative markets. The wide availability of commodities typically leads to smaller profit margins and diminishes the importance of factors (such as brand name) other than price.
 Most commodities are raw materials, basic resources, agricultural, or mining products, such as iron ore, sugar, or grains like rice and wheat. Commodities can also be mass-produced unspecialized products such as chemicals and computer memory.
"
Comparative_advantage,Economics,6,"The law of comparative advantage describes how, under free trade, an agent will produce more of and consume less of a good for which they have a comparative advantage.[1] In an economic model, agents have a comparative advantage over others in producing a particular good if they can produce that good at a lower relative opportunity cost or autarky price, i.e. at a lower relative marginal cost prior to trade.[2] Comparative advantage describes the economic reality of the work gains from trade for individuals, firms, or nations, which arise from differences in their factor endowments or technological progress.[3] (One should not compare the monetary costs of production or even the resource costs (labor needed per unit of output) of production. Instead, one must compare the opportunity costs of producing goods across countries[4]).
 David Ricardo developed the classical theory of comparative advantage in 1817 to explain why countries engage in international trade even when one country's workers are more efficient at producing every single good than workers in other countries. He demonstrated that if two countries capable of producing two commodities engage in the free market, then each country will increase its overall consumption by exporting the good for which it has a comparative advantage while importing the other good, provided that there exist differences in labor productivity between both countries.[5][6] Widely regarded as one of the most powerful[7] yet counter-intuitive[8] insights in economics, Ricardo's theory implies that comparative advantage rather than absolute advantage is responsible for much of international trade.
"
Competition_law,Economics,6,"
 Competition law is a law that promotes or seeks to maintain market competition by regulating anti-competitive conduct by companies.[1][2] Competition law is implemented through public and private enforcement.[3] Competition law is known as antitrust law in the United States for historical reasons, and as anti-monopoly law in China[1] and Russia. In previous years it has been known as trade practices law in the United Kingdom and Australia. In the European Union, it is referred to as both antitrust[4] and competition law.[5][6] The history of competition law reaches back to the Roman Empire. The business practices of market traders, guilds and governments have always been subject to scrutiny, and sometimes severe sanctions. Since the 20th century, competition law has become global.[7] The two largest and most influential systems of competition regulation are United States antitrust law and European Union competition law. National and regional competition authorities across the world have formed international support and enforcement networks.
 Modern competition law has historically evolved on a national level to promote and maintain fair competition in markets principally within the territorial boundaries of nation-states. National competition law usually does not cover activity beyond territorial borders unless it has significant effects at nation-state level.[2] Countries may allow for extraterritorial jurisdiction in competition cases based on so-called ""effects doctrine"".[2][8] The protection of international competition is governed by international competition agreements. In 1945, during the negotiations preceding the adoption of the General Agreement on Tariffs and Trade (GATT) in 1947, limited international competition obligations were proposed within the Charter for an International Trade Organisation. These obligations were not included in GATT, but in 1994, with the conclusion of the Uruguay Round of GATT multilateral negotiations, the World Trade Organization (WTO) was created. The Agreement Establishing the WTO included a range of limited provisions on various cross-border competition issues on a sector specific basis.[9]"
Competitive_market,Economics,6,"In economics, competition is a scenario where different economic firms[Note 1] are in contention to obtain goods that are limited by varying the elements of the marketing mix: price, product, promotion and place. In classical economic thought, competition causes commercial firms to develop new products, services and technologies, which would give consumers greater selection and better products. The greater the selection of a good is in the market, prices are typically lower for the products, compared to what the price would be if there was no competition (monopoly) or little competition (oligopoly). This is because there is now no rivalry between firms to obtain the product as there is enough for everyone. The level of competition that exists within the market is dependant on a variety of factors both on the firm/ seller side; the number of firms, barriers to entry, information availability, availability/ accessibility of resources. The number of buyers within the market also factors into competition with each buyer having a willingness to pay, influencing overall demand for the product in the market. 
 The extent of the competition present within a particular market can measured by; the number of rivals, their similarity of size, and in particular the smaller the share of industry output possessed by the largest firm, the more vigorous competition is likely to be.[1] Early economic research focused on the difference between price- and non-price-based competition, while modern economic theory has focused on the many-seller limit of general equilibrium.
"
Complementary_goods,Economics,6,"In economics, a complementary good is a good whose appeal increases with the popularity of its complement. Technically, it displays a negative cross elasticity of demand and that demand for it increases when the price of another good decreases.[1] If A is a complement to B, an increase in the price of A will result in a negative movement along the demand curve of A and cause the demand curve for B to shift inward; less of each good will be demanded. Conversely, a decrease in the price of A will result in a positive movement along the demand curve of A and cause the demand curve of B to shift outward; more of each good will be demanded. This is in contrast to a substitute good, whose demand decreases when its substitute's price decreases.[2] When two goods are complements, they experience joint demand - the demand of one good is linked to the demand for another good. Therefore, if a higher quantity is demanded of one good, a higher quantity will also be demanded of the other, and vice versa. For example, the demand for razor blades may depend on the number of razors in use; this is why razors have sometimes been sold as loss leaders, to increase demand for the associated blades.[3] Another example is that sometimes a toothbrush is packaged free with toothpaste. The toothbrush is a complement to the toothpaste; the cost of producing a toothbrush may be higher than toothpaste, but its sales depends on the demand of toothpaste.
 All non-complementary goods can be considered substitutes.[4] If x and y are rough complements in an everyday sense, then consumers are willing to pay more for each marginal unit of good x as they accumulate more y. The opposite is true for substitutes: the consumer is willing to pay less for each marginal unit of good ""z"" as it accumulates more of good ""y"".
 Complementarity may be driven by psychological processes in which the consumption of one good (e.g., cola) stimulates demand for its complements (e.g., a cheeseburger). Consumption of a food or beverage activates a goal to consume its complements: foods that consumers believe would taste better together. Drinking cola increases consumers' willingness to pay for a cheeseburger. This effect appears to be contingent on consumer perceptions of these relationships rather than their sensory properties.[5]"
Compound_interest,Economics,6,"Compound interest is the addition of interest to the principal sum of a loan or deposit, or in other words, interest on interest. It is the result of reinvesting interest, rather than paying it out, so that interest in the next period is then earned on the principal sum plus previously accumulated interest. Compound interest is standard in finance and economics.
 Compound interest is contrasted with simple interest, where previously accumulated  interest is not added to the principal amount of the current period, so there is no compounding. The simple annual interest rate is the interest amount per period, multiplied by the number of periods per year. The simple annual interest rate is also known as the nominal interest rate (not to be confused with the interest rate not adjusted for inflation, which goes by the same name).
"
Computational_economics,Economics,6,"Computational economics is a research discipline at the interface of computer science, economics, and management science.[1] This subject encompasses  computational modeling of economic systems, whether agent-based,[2] general-equilibrium,[3] macroeconomic,[4] or rational-expectations,[5] computational econometrics and statistics,[6] computational finance, computational tools for the design
of automated internet markets, programming tool specifically designed for computational economics and the teaching of computational economics.  Some of these areas are unique, while others extend traditional areas of economics by solving problems that are tedious to study without computers and associated numerical methods.[7] Computational economics uses computer-based economic modelling for the solution of analytically and statistically- formulated economic problems. A research program, to that end, is agent-based computational economics (ACE), the computational study of economic processes, including whole economies, as dynamic systems of interacting agents.[8] As such, it is an economic adaptation of the complex adaptive systems paradigm.[9]  Here the ""agent"" refers to ""computational objects modeled as interacting according to rules,"" not real people.[2] Agents can represent social, biological, and/or physical entities. The theoretical assumption of mathematical optimisation by agents in equilibrium is replaced by the less restrictive postulate of agents with bounded rationality adapting to market forces,[10] including game-theoretical contexts.[11] Starting from initial conditions determined by the modeler, an ACE model develops forward through time driven solely by agent interactions. The scientific objective of the method is ""to ... test theoretical findings against real-world data in ways that permit empirically supported theories to cumulate over time, with each research building on the work  before.""[12] Computational solution tools include for example software for carrying out various matrix operations (e.g. matrix inversion) and for solving systems of linear and non-linear equations. For a repository of public-domain computational solutions, visit  here.
 The following journals specialise in computational economics: ACM Transactions on Economics and Computation,[13] Computational Economics,[1] Journal of Applied Econometrics,[14] Journal of Economic Dynamics and Control[15] and the Journal of Economic Interaction and Coordination.[16]"
Consumer,Economics,6,"A consumer is a person or a group who intends to order, orders, or uses purchased goods, products, or services primarily for personal, social, family, household and similar needs, not directly related to entrepreneurial or business activities.
"
Consumer_choice,Economics,6,"The theory of consumer choice is the branch of microeconomics that relates preferences to consumption expenditures and to consumer demand curves. It analyzes how consumers maximize the 
desirability of their consumption as measured by their preferences subject to limitations on their expenditures, by maximizing utility subject to a consumer budget constraint.[1] Consumption is separated from production, logically, because two different economic agents are involved. In the first case consumption is by the primary individual; in the second case, a producer might make something that he would not consume himself. Therefore, different motivations and abilities are involved. The models that make up consumer theory are used to represent prospectively observable demand patterns for an individual buyer on the hypothesis of constrained optimization.  Prominent variables used to explain the rate at which the good is purchased (demanded) are the price per unit of that good, prices of related goods, and wealth of the consumer.
 The law of demand states that the rate of consumption falls as the price of the good rises, even when the consumer is monetarily compensated for the effect of the higher price; this is called the substitution effect. As the price of a good rises, consumers will substitute away from that good, choosing more of other alternatives. If no compensation for the price rise occurs, as is usual, then the decline in overall purchasing power due to the price rise leads, for most goods, to a further decline in the quantity demanded; this is called the income effect.
 In addition, as the wealth of the individual rises, demand for most products increases, shifting the demand curve higher at all possible prices.
 The basic problem of consumer theory takes the following inputs:
"
Consumer_confidence,Economics,6,"Consumer confidence is an economic indicator that measures the degree of optimism that consumers feel about the overall state of the economy and their personal financial situation. If the consumer has confidence in  the immediate and near future economy and his/her personal finance, then the consumer will spend more than save.
 When consumer confidence is high, consumers make more purchases. When confidence is low, consumers tend to save more and spend less. A month-to-month trend in consumer confidence reflects the outlook of consumers with respect to their ability to find and retain good jobs according to their perception of the current state of the economy and their personal financial situation.
 Consumer confidence typically increases when the economy expands, and decreases when the economy contracts. In the United States, there is evidence that the measure is a lagging indicator of stock market performance.
"
Consumer_price_index,Economics,6,"
 A consumer price index measures changes in the price level of a weighted average market basket of consumer goods and services purchased by households.[1] A CPI is a statistical estimate constructed using the prices of a sample of representative items whose prices are collected periodically. Sub-indices and sub-sub-indices can be computed for different categories and sub-categories of goods and services, being combined to produce the overall index with weights reflecting their shares in the total of the consumer expenditures covered by the index. It is one of several price indices calculated by most national statistical agencies. The annual percentage change in a CPI is used as a measure of inflation. A CPI can be used to index (i.e. adjust for the effect of inflation) the real value of wages, salaries, and pensions; to regulate prices; and to deflate monetary magnitudes to show changes in real values. In most countries, the CPI, along with the population census, is one of the most closely watched national economic statistics.
 The index is usually computed monthly, or quarterly in some countries, as a weighted average of sub-indices for different components of consumer expenditure, such as food, housing, shoes, clothing, each of which is, in turn, a weighted average of sub-sub-indices. At the most detailed level, the elementary aggregate level, (for example, men's shirts sold in department stores in San Francisco), detailed weighting information is unavailable, so indices are computed using an unweighted arithmetic or geometric mean of the prices of the sampled product offers. (However, the growing use of scanner data is gradually making weighting information available even at the most detailed level.) These indices compare prices each month with prices in the price-reference month. The weights used to combine them into the higher-level aggregates, and then into the overall index, relate to the estimated expenditures during a preceding whole year of the consumers covered by the index on the products within its scope in the area covered. Thus the index is a fixed-weight index, but rarely a true Laspeyres index, since the weight-reference period of a year and the price-reference period, usually a more recent single month, do not coincide. 
 Ideally, the weights would relate to the composition of expenditure during the time between the price-reference month and the current month. There is a large technical economics literature on index formulas which would approximate this and which can be shown to approximate what economic theorists call a true cost-of-living index. Such an index would show how consumer expenditure would have to move to compensate for price changes so as to allow consumers to maintain a constant standard of living. Approximations can only be computed retrospectively, whereas the index has to appear monthly and, preferably, quite soon. Nevertheless, in some countries, notably in the United States and Sweden, the philosophy of the index is that it is inspired by and approximates the notion of a true cost of living (constant utility) index, whereas in most of Europe it is regarded more pragmatically.
 The coverage of the index may be limited. Consumers' expenditure abroad is usually excluded; visitors' expenditure within the country may be excluded in principle if not in practice; the rural population may or may not be included; certain groups such as the very rich or the very poor may be excluded. Saving and investment are always excluded, though the prices paid for financial services provided by financial intermediaries may be included along with insurance.
 The index reference period, usually called the base year, often differs both from the weight-reference period and the price-reference period. This is just a matter of rescaling the whole time-series to make the value for the index reference-period equal to 100. Annually revised weights are a desirable but expensive feature of an index, for the older the weights the greater is the divergence between the current expenditure pattern and that of the weight reference-period.
 It is calculated and reported on a per region or country basis on a monthly and annual basis. International organizations like the Organisation for Economic Co-operation and Development (OECD) report statistical figures like the consumer price index for many of its member countries.[2] In the US the CPI is usually reported by the Bureau of Economic Analysis.[3][4][5][6]"
Consumer_surplus,Economics,6,"In mainstream economics, economic surplus, also known as total welfare or Marshallian surplus (after Alfred Marshall), refers to two related quantities: 
"
Consumerism,Economics,6,"
 Consumerism is a social and economic order that encourages the acquisition of goods and services in ever-increasing amounts. With the industrial revolution, but particularly in the 20th century, mass production led to overproduction—the supply of goods would grow beyond consumer demand, and so manufacturers turned to planned obsolescence and advertising to manipulate consumer spending.[1] In 1899, a book on consumerism published by Thorstein Veblen, called The Theory of the Leisure Class, examined the widespread values and economic institutions emerging along with the widespread ""leisure time"" in the beginning of the 20th century.[2] In it, Veblen ""views the activities and spending habits of this leisure class in terms of conspicuous and vicarious consumption and waste. Both are related to the display of status and not to functionality or usefulness.""[3] In economics, consumerism may refer to economic policies which emphasise consumption. In an abstract sense, it is the consideration that the free choice of consumers should strongly orient the choice by manufacturers of what is produced and how, and therefore orient the economic organization of a society (compare producerism, especially in the British sense of the term).[4] In this sense, consumerism expresses the idea not of ""one man, one voice"", but of ""one pound, one voice"", which may or may not reflect the contribution of people to society.
 In the almost complete absence of other sustained macro-political and social narratives, concern about global climate change notwithstanding, the pursuit of the 'good life' through practices of what is known as 'consumerism' has become one of the dominant global social forces, cutting across differences of religion, class, gender, ethnicity and nationality. It is the other side of the dominant ideology of market globalism and is central to what Manfred Steger calls the 'global imaginary'.[5]"
Consumption_(economics),Economics,6,"Consumption, defined as spending for acquisition of utility, is a major concept in economics and is also studied in many other social sciences. It is seen in contrast to investing, which is spending for acquisition of future income.[1] Different schools of economists define consumption differently. According to mainstream economists, only the final purchase of newly produced goods and services by individuals for immediate use constitutes consumption, while other types of expenditure — in particular, fixed investment, intermediate consumption, and government spending — are placed in separate categories (see Consumer choice). Other economists define consumption much more broadly, as the aggregate of all economic activity that does not entail the design, production and marketing of goods and services (e.g. the selection, adoption, use, disposal and recycling of goods and services).[citation needed] Economists are particularly interested in the relationship between consumption and income, as modeled with the consumption function.
 "
Consumption_function,Economics,6,"In economics, the consumption function describes a relationship between consumption and disposable income.[1][2] The concept is believed to have been introduced into macroeconomics by John Maynard Keynes in 1936, who used it to develop the notion of a government spending multiplier.[3]"
Contract_curve,Economics,6,"In microeconomics, the contract curve is the set of points representing final allocations of two goods between two people that could occur as a result of mutually beneficial trading between those people given their initial allocations of the goods. All the points on this locus are Pareto efficient allocations, meaning that from any one of these points there is no reallocation that could make one of the people more satisfied with his or her allocation without making the other person less satisfied. The contract curve is the subset of the Pareto efficient points that could be reached by trading from the people's initial holdings of the two goods. It is drawn in the Edgeworth box diagram shown here, in which each person's allocation is measured vertically for one good and horizontally for the other good from that person's origin (point of zero allocation of both goods); one person's origin is the lower left corner of the Edgeworth box, and the other person's origin is the upper right corner of the box. The people's initial endowments (starting allocations of the two goods) are represented by a point in the diagram; the two people will trade goods with each other until no further mutually beneficial trades are possible. The set of points that it is conceptually possible for them to stop at are the points on the contract curve. However, some authors[1] identify the contract curve as the entire Pareto efficient locus from one origin to the other.
 Any Walrasian equilibrium lies on the contract curve. As with all points that are Pareto efficient, each point on the contract curve is a point of tangency between an indifference curve of one person and an indifference curve of the other person. Thus, on the contract curve the marginal rate of substitution is the same for both people.
"
Contract_theory,Economics,6,"In economics, contract theory studies how economic actors can and do construct contractual arrangements, generally in the presence of information asymmetry. Because of its connections with both agency and incentives, contract theory is often categorized within a field known as Law and economics.  One prominent application of it is the design of optimal schemes of managerial compensation. In the field of economics, the first formal treatment of this topic was given by Kenneth Arrow in the 1960s. In 2016, Oliver Hart and Bengt R. Holmström both received the Nobel Memorial Prize in Economic Sciences for their work on contract theory, covering many topics from CEO pay to privatizations. Holmström (MIT) focused more on the connection between incentives and risk, while Hart (Harvard) on the unpredictability of the future that creates holes in contracts.[1] A standard practice in the microeconomics of contract theory is to represent the behaviour of a decision maker under certain numerical utility structures, and then apply an optimization algorithm to identify optimal decisions. Such a procedure has been used in the contract theory framework to several typical situations, labeled moral hazard, adverse selection and signalling. The spirit of these models lies in finding theoretical ways to motivate agents to take appropriate actions, even under an insurance contract. The main results achieved through this family of models involve: mathematical properties of the utility structure of the principal and the agent, relaxation of assumptions, and variations of the time structure of the contract relationship, among others. It is customary to model people as maximizers of some von Neumann–Morgenstern utility functions, as stated by expected utility theory.
"
Convexity_in_economics,Economics,6,"Convexity is an important topic in economics.[1] In the Arrow–Debreu model of general economic equilibrium, agents have convex budget sets and convex preferences: At equilibrium prices, the budget hyperplane supports the best attainable indifference curve.[2] The profit function is the convex conjugate of the cost function.[1][2] Convex analysis is the standard tool for analyzing textbook economics.[1] Non‑convex phenomena in economics have been studied with nonsmooth analysis, which generalizes convex analysis.[3]"
Corporation,Economics,6,"
 A corporation is an organization—usually a group of people or a company—authorized by the state to act as a single entity (a legal entity recognized by private and public law 'born out of statute""; a legal person in legal context) and recognized as such in law for certain purposes.[1]:10 Early incorporated entities were established by charter (i.e. by an ad hoc act granted by a monarch or passed by a parliament or legislature). Most jurisdictions now allow the creation of new corporations through registration. Corporations come in many different types but are usually divided by the law of the jurisdiction where they are chartered based on two aspects: by whether they can issue stock, or by whether they are formed to make a profit.[2] Depending on the number of owners, a corporation can be classified as aggregate (the subject of this article) or sole (a legal entity consisting of a single incorporated office occupied by a single natural person).
 One of the most attractive early advantages business corporations offered to their investors, compared to earlier business entities like sole proprietorships and general partnerships, was limited liability.  Limited liability means that a passive shareholder in a corporation will not be personally liable either for contractually agreed obligations of the corporation, or for torts (involuntary harms) committed by the corporation against a third party.  Limited liability in contract is uncontroversial because the parties to the contract could have agreed to it and could agree to waive it by contract.  However, limited liability in tort remains controversial because third parties do not agree to waive the right to pursue shareholders.  There is significant concern that limited liability in tort may lead to excessive corporate risk taking and more harm by corporations to third parties.[3][4] Where local law distinguishes corporations by their ability to issue stock, corporations allowed to do so are referred to as stock corporations; one type of investment in the corporation is through stock, and owners of stock are referred to as stockholders or shareholders. Corporations not allowed to issue stock are referred to as non-stock corporations; i.e. those who are considered the owners of a non-stock corporation are persons (or other entities) who have obtained membership in the corporation and are referred to as a member of the corporation. Corporations chartered in regions where they are distinguished by whether they are allowed to be for-profit are referred to as for-profit and not-for-profit corporations, respectively.
 There is some overlap between stock/non-stock and for-profit/not-for-profit in that not-for-profit corporations are always non-stock as well. A for-profit corporation is almost always a stock corporation, but some for-profit corporations may choose to be non-stock. To simplify the explanation, whenever ""stockholder"" or ""shareholder"" is used in the rest of this article to refer to a stock corporation, it is presumed to mean the same as ""member"" for a non-profit corporation or for a profit, non-stock corporation. Registered corporations have legal personality and their shares are owned by shareholders[5][6] whose liability is generally limited to their investment.  Shareholders do not typically actively manage a corporation; shareholders instead elect or appoint a board of directors to control the corporation in a fiduciary capacity. In most circumstances, a shareholder may also serve as a director or officer of a corporation.
 In American English, the word corporation is most often used to describe large business corporations.[7] In British English and in the Commonwealth countries, the term company is more widely used to describe the same sort of entity while the word corporation encompasses all incorporated entities. In American English, the word company can include entities such as partnerships that would not be referred to as companies in British English as they are not a separate legal entity. Late in the 19th century, a new form of the company having the limited liability protections of a corporation, and the more favorable tax treatment of either a sole proprietorship or partnership was developed. While not a corporation, this new type of entity became very attractive as an alternative for corporations not needing to issue stock. In Germany, the organization was referred to as Gesellschaft mit beschränkter Haftung or GmbH. In the last quarter of the 20th century, this new form of non-corporate organization became available in the United States and other countries, and was known as the limited liability company or LLC. Since the GmbH and LLC forms of organization are technically not corporations (even though they have many of the same features), they will not be discussed in this article.
"
Cost,Economics,6,"In production, research, retail, and accounting, a cost is the value of money that has been used up to produce something or deliver a service, and hence is not available for use anymore. In [Business, the cost may be one of acquisition, in which case the amount of money expended to acquire it is counted as cost. In this case, money is the input that is gone in order to acquire the thing. This acquisition cost may be the sum of the cost of production as incurred by the original producer, and further costs of transaction as incurred by the acquirer over and above the price paid to the producer. Usually, the price also includes a mark-up for profit over the cost of production.
 More generalized in the field of economics, cost is a metric that is totaling up as a result of a process or as a differential for the result of a decision.[1]  Hence cost is the metric used in the standard modeling paradigm applied to economic processes. 
 Costs (pl.) are often further described based on their timing or their applicability.
"
Cost_curve,Economics,6,"In economics, a cost curve is a graph of the costs of production as a function of total quantity produced. In a free market economy, productively efficient firms optimize their production process by minimizing cost consistent with each possible level of production, and the result is a cost curve. Profit-maximizing firms use cost curves to decide output quantities. There are various types of cost curves, all related to each other, including total and average cost curves; marginal (""for each additional unit"") cost curves, which are equal to the differential of the total cost curves; and variable cost curves. Some are applicable to the short run, others to the long run.
"
Cost_of_living,Economics,6,"Cost of living is the cost of maintaining a certain standard of living. Changes in the cost of living over time are often operationalized in a cost-of-living index. Cost of living calculations are also used to compare the cost of maintaining a certain standard of living in different geographic areas.  Differences in cost of living between locations can also be measured in terms of purchasing power parity rates.
"
Cost_overrun,Economics,6,"A cost overrun, also known as a cost increase or budget overrun, involves unexpected incurred costs. When these costs are in excess of budgeted amounts due to an underestimation of the actual cost during budgeting, they are known by these terms.  
 Cost overruns are common in infrastructure, building, and technology projects. For IT projects, a 2004 industry study by the Standish Group found an average cost overrun of 43 percent; 71 percent of projects came in over budget, exceeded time estimates, and had estimated too narrow a scope; and total waste was estimated at $55 billion per year in the US alone.[1] Many major construction projects have incurred cost overruns; cost estimates used to decide whether important transportation infrastructure should be built can mislead grossly and systematically.[2] Cost overrun is distinguished from cost escalation, which is an anticipated growth in a budgeted cost due to factors such as inflation.
"
Cost-benefit_analysis,Economics,6,"Cost–benefit analysis (CBA), sometimes also called benefit–cost analysis, is a systematic approach to estimating the strengths and weaknesses of alternatives used to determine options which provide the best approach to achieving benefits while preserving savings (for example, in transactions, activities, and functional business requirements).[1] A CBA may be used to compare completed or  potential courses of actions, or to estimate (or evaluate) the value against the cost of a decision, project, or policy. It is commonly used in commercial transactions, business or policy decisions (particularly public policy), and project investments. For example, the U.S. Securities and Exchange Commission has to conduct cost-benefit analysis before instituting regulations or de-regulations.[2]:6
 CBA has two main applications:[3] CBA is related to cost-effectiveness analysis. Benefits and costs in CBA are expressed in monetary terms and are adjusted for the time value of money; all flows of benefits and costs over time are expressed on a common basis in terms of their net present value, regardless of whether they are incurred at different times. Other related techniques include cost–utility analysis, risk–benefit analysis, economic impact analysis, fiscal impact analysis, and social return on investment (SROI) analysis.
 Cost–benefit analysis is often used by organizations to appraise the desirability of a given policy. It is an analysis of the expected balance of benefits and costs, including an account of any alternatives and the status quo. CBA helps predict whether the benefits of a policy outweigh its costs (and by how much), relative to other alternatives. This allows the ranking of alternative policies in terms of a cost–benefit ratio.[4] Generally, accurate cost–benefit analysis identifies choices which increase welfare from a utilitarian perspective. Assuming an accurate CBA, changing the status quo by implementing the alternative with the lowest cost–benefit ratio can improve Pareto efficiency.[5] Although CBA can offer an informed estimate of the best alternative, a perfect appraisal of all present and future costs and benefits is difficult; perfection, in economic efficiency and social welfare, is not guaranteed.[6] The value of a cost–benefit analysis depends on the accuracy of the individual cost and benefit estimates. Comparative studies indicate that such estimates are often flawed, preventing improvements in Pareto and Kaldor–Hicks efficiency.[7] Interest groups may attempt to include (or exclude) significant costs in an analysis to influence its outcome.[8]"
Cost-of-production_theory_of_value,Economics,6,"In economics, the cost-of-production theory of value is the theory that the price of an object or condition is determined by the sum of the cost of the resources that went into making it. The cost can comprise any of the factors of production (including labor, capital, or land) and taxation.
 The theory makes the most sense under assumptions of constant returns to scale and the existence of just one non-produced factor of production. These are the assumptions of the so-called non-substitution theorem[clarification needed]. Under these assumptions, the long-run price of a commodity is equal to the sum of the cost of the inputs into that commodity, including interest charges
"
Credit_bureau,Economics,6,"A credit bureau is a data collection agency that gathers account information from various creditors and provides that information to a consumer reporting agency in the United States, a credit reference agency in the United Kingdom, a credit reporting body in Australia, a credit information company (CIC) in India, Special Accessing Entity in the Philippines, and also to private lenders.[1] It is not the same as a credit rating agency.
"
Credit_card,Economics,6,"
 A credit card is a payment card issued to users (cardholders) to enable the cardholder to pay a merchant for goods and services based on the cardholder's promise to the card issuer to pay them for the amounts plus the other agreed charges.[1] The card issuer (usually a bank) creates a revolving account and grants a line of credit to the cardholder, from which the cardholder can borrow money for payment to a merchant or as a cash advance.
 A credit card is different from a charge card, which requires the balance to be repaid in full each month or at the end of each statement cycle.[2] In contrast, credit cards allow the consumers to build a continuing balance of debt, subject to interest being charged. A credit card also differs from a cash card, which can be used like currency by the owner of the card. A credit card differs from a charge card also in that a credit card typically involves a third-party entity that pays the seller and is reimbursed by the buyer, whereas a charge card simply defers payment by the buyer until a later date. In 2018, there were 1.122 billion credit cards in circulation in the U.S.[3]"
Credit_score,Economics,6,"
 A credit score is a numerical expression based on a level analysis of a person's credit files, to represent the creditworthiness of an individual. A credit score is primarily based on a credit report, information typically sourced from credit bureaus.
 Lenders, such as banks and credit card companies, use credit scores to evaluate the potential risk posed by lending money to consumers and to mitigate losses due to bad debt.  Lenders use credit scores to determine who qualifies for a loan, at what interest rate, and what credit limits.  Lenders also use credit scores to determine which customers are likely to bring in the most revenue. The use of credit or identity scoring prior to authorizing access or granting credit is an implementation of a trusted system.
 Credit scoring is not limited to banks. Other organizations, such as mobile phone companies, insurance companies, landlords, and government departments employ the same techniques. Digital finance companies such as online lenders also use alternative data sources to calculate the creditworthiness of borrowers.
"
Credit_rating,Economics,6,"A credit rating is an evaluation of the credit risk of a prospective debtor (an individual, a business, company or a government), predicting their ability to pay back the debt, and  an implicit forecast of the likelihood of the debtor defaulting.[1]
The credit rating represents an evaluation of a credit rating agency of the qualitative and quantitative information for the prospective debtor, including information provided by the prospective debtor and other non-public information obtained by the credit rating agency's analysts.
 Credit reporting (or  credit score) – is a subset of credit rating – it is a numeric evaluation of an individual's credit worthiness, which is done by a credit bureau or consumer credit reporting agency.
"
Credit_union,Economics,6,"A credit union is a member-owned financial cooperative, controlled by its members and operated on the principle of people helping people, providing its members credit at competitive rates as well as other financial services.[1][2] Worldwide, credit union systems vary significantly in terms of total assets and average institution asset size, ranging from volunteer operations with a handful of members to institutions with assets worth several billion U.S. dollars and hundreds of thousands of members.[3] In 2018 the number of members in credit unions worldwide was 274 million, with nearly 40 million members being added since 2016.[4] Commercial banks engaged in approximately five times more subprime lending relative to credit unions leading up to the financial crisis and were two and a half times more likely to fail during the crisis.[5] American credit unions more than doubled lending to small businesses between 2008 and 2016, from $30 billion to $60 billion, while lending to small businesses overall during the same period declined by around $100 billion.[6] In the US, public trust in credit unions stands at 60%, compared to 30% for big banks.[7] Furthermore, small businesses are eighty percent less likely to be dissatisfied with a credit union than with a big bank.[8] ""Natural-person credit unions"" (also called ""retail credit unions"" or ""consumer credit unions"") serve individuals, as distinguished from ""corporate credit unions"", which serve other credit unions.[9][10][11]"
Creditor,Economics,6,"A creditor or lender is a party (e.g., person, organization, company, or government) that has a claim on the services of a second party. It is a person or institution to whom money is owed.[1] The first party, in general, has provided some property or service to the second party under the assumption (usually enforced by contract) that the second party will return an equivalent property and service. The second party is frequently called a debtor or borrower. The first party is called the creditor, which is the lender of property, service, or money.
 Creditors can be broadly divided into two categories: secured and unsecured.
 The term creditor is frequently used in the financial world, especially in reference to short-term loans, long-term bonds, and mortgage loans. In law, a person who has a money judgment entered in their favor by a court is called a judgment creditor.
 The term creditor derives from the notion of credit. Also, in modern America, credit refers to a rating which indicates the likelihood a borrower will pay back their loan. In earlier times, credit also referred to reputation or trustworthiness.
"
Crowding_out_(economics),Economics,6,"In economics, crowding out is a phenomenon that occurs when increased government involvement in a sector of the market economy substantially affects the remainder of the market, either on the supply or demand side of the market.
 One type frequently discussed is when expansionary fiscal policy reduces investment spending by the private sector. The government spending is ""crowding out"" investment because it is demanding more loanable funds and thus causing increased interest rates and therefore reducing investment spending. This basic analysis has been broadened to multiple channels that might leave total output little changed or even smaller.[1] Other economists use ""crowding out"" to refer to government providing a service or good that would otherwise be a business opportunity for private industry, and be subject only to the economic forces seen in voluntary exchange.
 Behavioral economists and other social scientists also use ""crowding out"" to describe a downside of solutions based on private exchange: the crowding out of intrinsic motivation and prosocial norms in response to the financial incentives of voluntary market exchange.
"
Cultural_economics,Economics,6,"Cultural economics is the branch of economics that studies the relation of culture to economic outcomes. Here, 'culture' is defined by shared beliefs and preferences of respective groups.  Programmatic issues include whether and how much culture matters as to economic outcomes and what its relation is to institutions.[1] As a growing field in behavioral economics, the role of culture in economic behavior is increasingly being demonstrate to cause significant differentials in decision-making and the management and valuation of assets.
 Applications include the study of religion,[2] social capital,[3] social norms,[4] social identity,[5] fertility,[6] beliefs in redistributive justice,[7] ideology,[8] hatred,[9] terrorism,[10] trust,[11] family ties,[12] long-term orientation,[13][14] and the culture of economics.[15][16]  A general analytical theme is how ideas and behaviors are spread among individuals through the formation of social capital,[17] social networks[18] and processes such as social learning, as in the theory of social evolution[19]  and information cascades.[20] Methods include case studies and theoretical and empirical modeling of cultural transmission within and across social groups.[21] In 2013 Said E. Dawlabani added the value systems approach to the cultural emergence aspect of macroeconomics.[22]"
Currency,Economics,6,"
 A currency[a] in the most specific sense is money in any form when in use or  circulation as a medium of exchange, especially circulating banknotes and coins.[1][2]
A more general definition is that a currency is a system of money (monetary units) in common use, especially for people in a nation.[3] Under this definition, U.S. dollars (US$), euros (€), Japanese yen (¥), and pounds sterling (£) are examples of currencies. These various currencies are recognized as stores of value and are traded between nations in foreign exchange markets, which determine the relative values of the different currencies.[4] Currencies in this sense are defined by governments, and each type has limited boundaries of acceptance.
 Other definitions of the term ""currency"" appear in the respective synonymous articles: banknote, coin, and money. This article uses the definition which focuses on the currency systems of countries.
 One can classify currencies into three monetary systems: fiat money, commodity money, and representative money, depending on what guarantees a currency's value (the economy at large vs. the government's physical metal  reserves). Some currencies function as legal tender in certain political jurisdictions. Others simply get traded for their economic value.
 Digital currency has arisen with the popularity of computers and the Internet. Whether digital notes and coins will be successfully developed remains dubious.[5] Decentralized digital currencies, such as cryptocurrencies are not legal currency, strictly speaking, since they are not issued by a government monetary authority and are not legal tender. Many warnings issued by various countries note the opportunities that cryptocurrencies create for illegal activities, such as money laundering and terrorism.[6]
In 2014 the United States IRS issued a statement explaining that virtual currency is treated as property for Federal  income-tax purposes and providing examples of how longstanding tax principles applicable to transactions involving property apply to virtual currency.[7]"
Current_account_(balance_of_payments),Economics,6,"
 In economics, a country's current account records the value of exports and imports of both goods and services. It is one of the three components of its balance of payments, the others being the capital account and the financial account. Current account measures the nation's earnings and spendings abroad and it consists of the balance of trade, net primary income or factor income (earnings on foreign investments minus payments made to foreign investors) and net unilateral transfers, that have taken place over a given period of time. The current account balance is one of two major measures of a country's foreign trade (the other being the net capital outflow). A current account surplus indicates that the value of a country's net foreign assets (i.e. assets less liabilities) grew over the period in question, and a current account deficit indicates that it shrank. Both government and private payments are included in the calculation. It is called the current account because goods and services are generally consumed in the current period.[1][2]"
Cyclical_unemployment,Economics,6,"Unemployment, according to the OECD (Organisation for Economic Co-operation and Development), is persons above a specified age (usually 15)[2] not being in paid employment or self-employment but currently available for work during the reference period.[3] Unemployment is measured by the unemployment rate, which is the number of people who are unemployed as a percentage of the labour force (the total number of people employed added to those unemployed).[4] Unemployment can have many sources, such as the following:
 Unemployment and the status of the economy can be influenced by a country through, for example, fiscal policy. Furthermore, the monetary authority of a country, such as the central bank, can influence the availability and cost for money through its monetary policy.
 In addition to theories of unemployment, a few categorisations of unemployment are used for more precisely modelling the effects of unemployment within the economic system. Some of the main types of unemployment include structural unemployment, frictional unemployment, cyclical unemployment, involuntary unemployment and classical unemployment. Structural unemployment focuses on foundational problems in the economy and inefficiencies inherent in labor markets, including a mismatch between the supply and demand of laborers with necessary skill sets. Structural arguments emphasize causes and solutions related to disruptive technologies and globalization. Discussions of frictional unemployment focus on voluntary decisions to work based on individuals' valuation of their own work and how that compares to current wage rates added to the time and effort required to find a job. Causes and solutions for frictional unemployment often address job entry threshold and wage rates.
 According to the UN's International Labour Organization (ILO), there were 172 million people worldwide (or 5% of the reported global workforce) without work in 2018.[5] Because of the difficulty in measuring the unemployment rate by, for example, using surveys (as in the United States) or through registered unemployed citizens (as in some European countries), statistical figures such as the employment-to-population ratio might be more suitable for evaluating the status of the workforce and the economy if they were based on people who are registered, for example, as taxpayers.[6]"
Deadweight_loss,Economics,6,"Deadweight loss, also known as excess burden, is a measure of lost economic efficiency when the socially optimal quantity of a good or a service is not produced. Non-optimal production can be caused by monopoly pricing in the case of artificial scarcity, a positive or negative externality, a tax or subsidy, or a binding price ceiling or price floor such as a minimum wage.
"
Debt,Economics,6,"
 Debt is an obligation that requires one party, the debtor, to pay money or other agreed-upon value to another party, the creditor. Debt is a deferred payment, or series of payments, which differentiates it from an immediate purchase. The debt may be owed by sovereign state or country, local government, company, or an individual. Commercial debt is generally subject to contractual terms regarding the amount and timing of repayments of principal and interest.[1] Loans, bonds, notes, and mortgages are all types of debt.  The term can also be used metaphorically to cover moral obligations and other interactions not based on economic value.[2] For example, in Western cultures, a person who has been helped by a second person is sometimes said to owe a ""debt of gratitude"" to the second person.
"
Debtor,Economics,6,"A debtor or debitor is a legal entity (legal person) that owes a debt to another entity. The entity may be an individual, a firm, a government, a company or other legal person. The counterparty is called a creditor. When the counterpart of this debt arrangement is a bank, the debtor is more often referred to as a borrower.
 If X borrowed money from his/her bank, X is the debtor and the bank is the creditor. If X puts money in the bank, X is the creditor and the bank is the debtor.
 It is not a crime to fail to pay a debt. Except in certain bankruptcy situations, debtors can choose to pay debts in any priority they choose. But if one fails to pay a debt, they have broken a contract or agreement between them and a creditor. Generally, most oral and written agreements for the repayment of consumer debt - debts for personal, family or household purposes secured primarily by a person's residence - are enforceable.[1] For the most part, debts that are business related must be made in writing to be enforceable by law. If the written agreement requires the debtor to pay a specific amount of money, then the creditor does not have to accept any lesser amount, and should be paid in full.
 Also, if there was no actual agreement but the creditor has proven to have loaned an amount of money, undertaken services or given the debtor a product, the debtor must then pay the creditor.
"
Deficit_spending,Economics,6,"Deficit spending is the amount by which spending exceeds revenue over a particular period of time, also called simply deficit, or budget deficit; the opposite of budget surplus. The term may be applied to the budget of a government, private company, or individual. Government deficit spending is a central point of controversy in economics, as discussed below.
"
Deflation,Economics,6,"In economics, deflation is a decrease in the general price level of goods and services.[1] Deflation occurs when the inflation rate falls below 0% (a negative inflation rate). Inflation reduces the value of currency over time, but sudden deflation increases it. This allows more goods and services to be bought than before with the same amount of currency. Deflation is distinct from disinflation, a slow-down in the inflation rate, i.e. when inflation declines to a lower rate but is still positive.[2] Economists generally believe that a sudden deflationary shock is a problem in a modern economy because it increases the real value of debt, especially if the deflation is unexpected. Deflation may also aggravate recessions and lead to a deflationary spiral.[3][4][5][6][7][8][9] Deflation usually happens when supply is high (when excess production occurs), when demand is low (when consumption decreases), or when the money supply decreases (sometimes in response to a contraction created from careless investment or a credit crunch) or because of a net capital outflow from the economy.[10] It can also occur due to too much competition and too little market concentration.[11][citation needed]"
Deflator,Economics,6,"In statistics, a deflator is a value that allows data to be measured over time in terms of some base period, usually through a price index, in order to distinguish between changes in the money value of a gross national product (GNP) that come from a change in prices, and changes from a change in physical output. It is the measure of the price level for some quantity. A deflator serves as a price index in which the effects of inflation are nulled.[1][2][3] It is the difference between real and nominal GDP.[4][5] In the United States, the import and export price indexes produced by the International Price Program are used as deflators in national accounts. For example, the gross domestic product (GDP) equals consumption expenditures plus net investment plus government expenditures plus exports minus imports. Various price indexes are used to ""deflate"" each component of the GDP to make the GDP figures comparable over time. Import price indexes are used to deflate the import component (i.e., import volume is divided by the Import Price index) and the export price indexes are used to deflate the export component (i.e., export volume is divided by the Export Price index).[1] It is generally used as a statistical tool to convert dollars purchasing power into ""inflation-adjusted"" purchasing power, thus enabling the comparison of prices while accounting for inflation in various time periods.[6][7][8]"
Demand,Economics,6,"In economics, demand is the quantity of a good that consumers are willing and able to purchase at various prices during a given period of time.[1] The relationship between price and quantity demanded is also called the demand curve. Demand for a specific item is a function of an item's perceived necessity, price, perceived quality, convenience; available alternatives; purchasers' disposable income and tastes; and many other factors.
"
Demand_curve,Economics,6,"In economics, a demand curve is a graph depicting the relationship between the price of a certain commodity (the y-axis) and the quantity of that commodity that is demanded at that price (the x-axis). Demand curves may be used to model the price-quantity relationship for an individual consumer (an individual demand curve), or more commonly for all consumers in a particular market (a market demand curve). It is generally assumed that demand curves are downward-sloping, as shown in the adjacent image. This is because of the law of demand: for most goods, the quantity demanded will decrease in response to an increase in price, and will increase in response to a decrease in price.[1] Demand curves are used to estimate behaviors in competitive markets, and are often combined with supply curves to estimate the equilibrium price (the price at which sellers together are willing to sell the same amount as buyers together are willing to buy, also known as market clearing price) and the equilibrium quantity (the amount of that good or service that will be produced and bought without surplus/excess supply or shortage/excess demand) of that market.[1]:57 In a monopolistic market, the demand curve facing the monopolist is simply the market demand curve.
 Movement along the Demand Curve is when the commodity experience change in both the quantity demanded and price, causing the curve to move in a specific direction. The shift in the demand curve is when, the price of the commodity remains constant, but there is a change in quantity demanded due to some other factors, causing the curve to shift to a particular side. [2]  Demand curves are usually considered as theoretical structures that are expected to exist in the real world, but real world measurements of actual demand curves are difficult and rare.[3]"
Demand_deposit,Economics,6,"
 Demand deposits or non-confidential money are funds held in demand accounts in commercial banks.[1] These account balances are usually considered money and form the greater part of the narrowly defined money supply of a country.[2] Simply put, these are deposits in the bank that can be withdrawn on demand, without any prior notice.
"
Demand_shock,Economics,6,"In economics, a demand shock is a sudden event that increases or decreases demand for goods or services temporarily.
 A positive demand shock increases aggregate demand (AD) and a negative demand shock decreases aggregate demand. Prices of goods and services are affected in both cases. When demand for goods or services increases, its price (or price levels) increases because of a shift in the demand curve to the right. When demand decreases, its price decreases because of a shift in the demand curve to the left. Demand shocks can originate from changes in things such as tax rates, money supply, and government spending. For example, taxpayers owe the government less money after a tax cut, thereby freeing up more money available for personal spending. When the taxpayers use the money to purchase goods and services, their prices go up.[1] In the midst of a poor economic situation in the United Kingdom in November 2002, the Bank of England's deputy governor, Mervyn King, warned that the domestic economy was sufficiently imbalanced that it ran the risk of causing a ""large negative demand shock"" in the near future. At the London School of Economics, he elaborated by saying, ""Beneath the surface of overall stability in the UK economy lies a remarkable imbalance between a buoyant consumer and housing sector, on the one hand, and weak external demand on the other.""[2] During the global financial crisis of 2008, a negative demand shock in the United States economy was caused by several factors that included falling house prices, the subprime mortgage crisis, and lost household wealth, which led to a drop in consumer spending. To counter this negative demand shock, the Federal Reserve System lowered interest rates.[3] Before the crisis occurred, the world's economy experienced a positive global supply shock. Immediately afterward, however, a positive global demand shock led to global overheating and rising inflationary pressures.[4]"
Demographic_economics,Economics,6,"Demographic economics or population economics is the application of economic analysis to demography, the study of human populations, including size, growth, density, distribution, and vital statistics.[1][2]"
Depreciation,Economics,6,"In accountancy, depreciation refers to two aspects of the same concept: first, the actual decrease of fair value of an asset, such as the decrease in value of factory equipment each year as it is used and wears, and second, the allocation in accounting statements of the original cost of the assets to periods in which the assets are used (depreciation with the matching principle).[1] Depreciation is thus the decrease in the value of assets and the method used to reallocate, or ""write down"" the cost of a tangible asset (such as equipment) over its useful life span. Businesses depreciate long-term assets for both accounting and tax purposes. The decrease in value of the asset affects the balance sheet of a business or entity, and the method of depreciating the asset, accounting-wise, affects the net income, and thus the income statement that they report. Generally, the cost is allocated as depreciation expense among the periods in which the asset is expected to be used.
 Methods of computing depreciation, and the periods over which assets are depreciated, may vary between asset types within the same business and may vary for tax purposes. These may be specified by law or accounting standards, which may vary by country. There are several standard methods of computing depreciation expense, including fixed percentage, straight line, and declining balance methods. Depreciation expense generally begins when the asset is placed in service. For example, a depreciation expense of 100 per year for five years may be recognized for an asset costing 500.
Depreciation has been defined as the diminution in the utility or value of an asset and is a non-cash expense. It does not result in any cash outflow; it just means that the asset is not worth as much as it used to be. Causes of depreciation are natural wear and tear[citation needed].
"
Depression_(economics),Economics,6,"Economic depression is a sustained, long-term downturn in economic activity in one or more economies. It is a more severe economic downturn than a recession, which is a slowdown in economic activity over the course of a normal business cycle.
 Economic depressions are characterized by their length, by abnormally large increases in unemployment, falls in the availability of credit (often due to some form of banking or financial crisis), shrinking output as buyers dry up and suppliers cut back on production and investment, more bankruptcies including sovereign debt defaults, significantly reduced amounts of trade and commerce (especially international trade), as well as highly volatile relative currency value fluctuations (often due to currency devaluations). Price deflation, financial crises, stock market crash, and bank failures are also common elements of a depression that do not normally occur during a recession.
"
Deregulation,Economics,6,"Deregulation is the process of removing or reducing state regulations, typically in the economic sphere.  It is the repeal of governmental regulation of the economy.  It became common in advanced industrial economies in the 1970s and 1980s, as a result of new trends in economic thinking about the inefficiencies of government regulation, and the risk that regulatory agencies would be controlled by the regulated industry to its benefit, and thereby hurt consumers and the wider economy.
 Economic regulations were promoted during the Gilded Age, in which progressive reforms were touted as necessary to limit externalities like corporate abuse, unsafe child labor, monopolization, pollution, and to mitigate boom and bust cycles. Around the late 1970s, such reforms were deemed as burdensome on economic growth and many politicians espousing neoliberalism started promoting deregulation.
 The stated rationale for deregulation is often that fewer and simpler regulations will lead to raised levels of competitiveness, therefore higher productivity, more efficiency and lower prices overall. Opposition to deregulation may usually involve apprehension regarding environmental pollution[1] and environmental quality standards (such as the removal of regulations on hazardous materials), financial uncertainty, and constraining monopolies.
 Regulatory reform is a parallel development alongside deregulation. Regulatory reform refers to organized and ongoing programs to review regulations with a view to minimizing, simplifying, and making them more cost effective.  Such efforts, given impetus by the Regulatory Flexibility Act of 1980, are embodied in the United States Office of Management and Budget's Office of Information and Regulatory Affairs, and the United Kingdom's Better Regulation Commission. Cost–benefit analysis is frequently used in such reviews. In addition, there have been regulatory innovations, usually suggested by economists, such as emissions trading.
 Deregulation can be distinguished from privatization, where privatization can be seen as taking state-owned service providers into the private sector.
"
Diminishing_marginal_utility,Economics,6,"In economics, utility is the satisfaction or benefit derived by consuming a product; thus the marginal utility of a good or service is the change in the utility from an increase in the consumption of that good or service.
 In the context of cardinal utility, economists sometimes speak of a law of diminishing marginal utility, meaning that the first unit of consumption of a good or service yields more utility than the second and subsequent units, with a continuing reduction for greater amounts. Therefore, the fall in marginal utility as consumption increases is known as diminishing marginal utility. This concept is used by economists to determine how much of a good a consumer is willing to purchase.
"
Diminishing_returns,Economics,6,"In economics, diminishing returns is the decrease in the marginal (incremental) output a production process as the amount of a single factor of production is incrementally increased, while the amounts of all other factors of production stay constant.
 The law of diminishing returns states that in productive processes, increasing a factor of production by one, while holding all others constant (""ceteris paribus""), will at some point return lower output per incremental input unit.[1] The law of diminishing returns does not  decrease the total production, a condition known as negative returns. Under diminishing returns, returns remain positive, though they approach zero.
 The modern understanding of the law adds the dimension of holding other outputs equal, since a given process is understood to be able to produce co-products.[2] An example would be a factory increasing its saleable product, but also increasing its CO2 production, for the same input increase.
 
The law of diminishing returns is a fundamental principle of economics.[1] It plays a central role in production theory.[3]"
Discretionary_income,Economics,6,"Disposable income is total personal income minus personal current taxes.[1] In national accounts definitions, personal income minus personal current taxes equals disposable personal income.[2] Subtracting personal outlays (which includes the major category of personal [or private] consumption expenditure) yields personal (or, private) savings, hence the income left after paying away all the taxes is referred to as disposable income.
 Restated, consumption expenditure plus savings equals disposable income[3] after accounting for transfers such as payments to children in school or elderly parents’ living and care arrangements.[4] The marginal propensity to consume (MPC) is the fraction of a change in disposable income that is consumed. For example, if disposable income rises by $100, and $65 of that $100 is consumed, the MPC is 65%. Restated, the marginal propensity to save is 35%.
 For the purposes of calculating the amount of income subject to garnishments, United States' federal law defines disposable income as an individual's compensation (including salary, overtime, bonuses, commission, and paid leave) after the deduction of health insurance premiums and any amounts required to be deducted by law. Amounts required to be deducted by law include federal, state, and local taxes, state unemployment and disability taxes, social security taxes, and other garnishments or levies, but does not include such deductions as voluntary retirement contributions and transportation deductions. Those deductions would be made only after calculating the amount of the garnishment or levy.[5] The definition of disposable income varies for the purpose of state and local garnishments and levies.
 According to the Better Life Index study conducted by the Organisation for Economic Co-operation and Development (OECD), the United States of America has the highest average household disposable income of all of the OECD member countries in the world.[6] Discretionary income is disposable income (after-tax income), minus all payments that are necessary to meet current bills. It is total personal income after subtracting taxes and minimal survival expenses (such as food, medicine, rent or mortgage, utilities, insurance, transportation, property maintenance, child support, etc.) to maintain a certain standard of living.[7] It is the amount of an individual's income available for spending after the essentials have been taken care of:
 Despite the definitions above, disposable income is often incorrectly used to denote discretionary income. For example, people commonly refer to disposable income as the amount of ""play money"" left to spend or save. The Consumer Leverage Ratio is the expression of the ratio of total household debt to disposable income.
"
Disinflation,Economics,6,"Disinflation is a decrease in the rate of inflation – a slowdown in the rate of increase of the general price level of goods and services in a nation's gross domestic product over time. It is the opposite of reflation. Disinflation occurs when the increase in the “consumer price level” slows down from the previous period when the prices were rising.
 If the inflation rate is not very high to start with, disinflation can lead to deflation – decreases in the overall money supply, as well as decreases in the general price level of goods and services, typically as a resulting effect. For example, if the annual inflation rate for the month of January is 5% and it is 4% in the month of February, the prices disinflated by 1% but are still increasing at a 4% annual rate. Again if the current rate is 1% and it is -2% for the following month, prices disinflated by 3% i.e. [1%-(-2)%] and are decreasing at a 2% annual rate.
"
Disposable_and_discretionary_income,Economics,6,"Disposable income is total personal income minus personal current taxes.[1] In national accounts definitions, personal income minus personal current taxes equals disposable personal income.[2] Subtracting personal outlays (which includes the major category of personal [or private] consumption expenditure) yields personal (or, private) savings, hence the income left after paying away all the taxes is referred to as disposable income.
 Restated, consumption expenditure plus savings equals disposable income[3] after accounting for transfers such as payments to children in school or elderly parents’ living and care arrangements.[4] The marginal propensity to consume (MPC) is the fraction of a change in disposable income that is consumed. For example, if disposable income rises by $100, and $65 of that $100 is consumed, the MPC is 65%. Restated, the marginal propensity to save is 35%.
 For the purposes of calculating the amount of income subject to garnishments, United States' federal law defines disposable income as an individual's compensation (including salary, overtime, bonuses, commission, and paid leave) after the deduction of health insurance premiums and any amounts required to be deducted by law. Amounts required to be deducted by law include federal, state, and local taxes, state unemployment and disability taxes, social security taxes, and other garnishments or levies, but does not include such deductions as voluntary retirement contributions and transportation deductions. Those deductions would be made only after calculating the amount of the garnishment or levy.[5] The definition of disposable income varies for the purpose of state and local garnishments and levies.
 According to the Better Life Index study conducted by the Organisation for Economic Co-operation and Development (OECD), the United States of America has the highest average household disposable income of all of the OECD member countries in the world.[6] Discretionary income is disposable income (after-tax income), minus all payments that are necessary to meet current bills. It is total personal income after subtracting taxes and minimal survival expenses (such as food, medicine, rent or mortgage, utilities, insurance, transportation, property maintenance, child support, etc.) to maintain a certain standard of living.[7] It is the amount of an individual's income available for spending after the essentials have been taken care of:
 Despite the definitions above, disposable income is often incorrectly used to denote discretionary income. For example, people commonly refer to disposable income as the amount of ""play money"" left to spend or save. The Consumer Leverage Ratio is the expression of the ratio of total household debt to disposable income.
"
Dissaving,Economics,6,"Dissaving is negative saving. If spending is greater than disposable income, dissaving is taking place. This spending is financed by already accumulated savings, such as money in a savings account, or it can be borrowed.
 Dissaving was reported as a typical response to deficits, for households with normal income and expenditure patterns during the depression of the 1930s.[1] The life-cycle hypothesis of saving, of Ando and Modigliani, proposes that people work and save when they are young and retire and dissave when they become elderly.[2] Hayashi, Ando, and Ferris investigated whether the elderly save or dissave and found for the United States that families after retirement dissave on average about a third of their peak wealth by the time of death, leaving the rest (mostly their homes) as bequests.[3] In contrast they found that for Japan the elderly forming independent households and those living with children continue to save, for all but the most elderly. From age 80 or more and, also the single elderly of all ages, the dissaving patterns were evident.
 Later evidence presented by Horioka reinforces the life cycle hypothesis in Japan.[4]"
Distribution_(economics),Economics,6,"In economics, distribution is the way total output, income, or wealth is distributed among individuals or among the factors of production (such as labour, land, and capital).[1] In general theory and the national income and product accounts, each unit of output corresponds to a unit of income.  One use of national accounts is for classifying factor incomes[2] and measuring their respective shares, as in national Income. But, where focus is on income of persons or households, adjustments to the national accounts or other data sources are frequently used. Here, interest is often on the fraction of income going to the top (or bottom) x percent of households, the next x percent, and so forth (defined by equally spaced cut points, say quintiles), and on the factors that might affect them (globalization, tax policy, technology, etc.).
"
Domestic_final_demand,Economics,6,"In macroeconomics, aggregate demand (AD) or domestic final demand (DFD) is the total demand for final goods and services in an economy at a given time.[1] It is often called effective demand, though at other times this term is distinguished. This is the demand for the gross domestic product of a country. It specifies the amount of goods and services that will be purchased at all possible price levels.[2] The aggregate demand curve is plotted with real output on the horizontal axis and the price level on the vertical axis. While it is theorized to be downward sloping, the Sonnenschein–Mantel–Debreu results show that the slope of the curve cannot be mathematically derived from assumptions about individual rational behavior.[3][4] Instead, the downward sloping aggregate demand curve is derived with the help of three macroeconomic assumptions about the functioning of markets: Pigou's wealth effect, Keynes' interest rate effect and the Mundell–Fleming exchange-rate effect. The Pigou effect states that a higher price level implies lower real wealth and therefore lower consumption spending, giving a lower quantity of goods demanded in the aggregate. The Keynes effect states that a higher price level implies a lower real money supply and therefore higher interest rates resulting from financial market equilibrium, in turn resulting in lower investment spending on new physical capital and hence a lower quantity of goods being demanded in the aggregate.
 The Mundell–Fleming exchange-rate effect is an extension of the IS–LM model. Whereas the traditional IS-LM Model deals with a closed economy, Mundell–Fleming describes a small open economy. The Mundell–Fleming model portrays the short-run relationship between an economy's nominal exchange rate, interest rate, and output (in contrast to the closed-economy IS–LM model, which focuses only on the relationship between the interest rate and output).
 The aggregate demand curve illustrates the relationship between two factors: the quantity of output that is demanded and the aggregate price level. Aggregate demand is expressed contingent upon a fixed level of the nominal money supply. There are many factors that can shift the AD curve. Rightward shifts result from increases in the money supply, in government expenditure, or in autonomous components of investment or consumption spending, or from decreases in taxes.
 According to the aggregate demand-aggregate supply model, when aggregate demand increases, there is movement up along the aggregate supply curve, giving a higher level of prices.[5]"
Duopoly,Economics,6,"A duopoly (from Greek δύο, duo (two) + πωλεῖν, polein (to sell)) is a type of oligopoly where two firms have dominant or exclusive control over a market. It is the most commonly studied form of oligopoly due to its simplicity. Duopolies sell to consumers in a competitive market where the choice of an individual consumer can not affect the firm. The defining characteristic of both duopolies and oligopolies is that decisions made by sellers are dependent on each other.
"
Dynamic_stochastic_general_equilibrium,Economics,6,"Dynamic stochastic general equilibrium modeling (abbreviated as DSGE, or DGE, or sometimes SDGE) is a method in macroeconomics that attempts to explain economic phenomena, such as economic growth and business cycles, and the effects of economic policy, through econometric models based on applied general equilibrium theory and microeconomic principles.
"
Econometrics,Economics,6,"
 Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships.[1] More precisely, it is ""the quantitative analysis of actual economic phenomena based on the concurrent development of theory and observation, related by appropriate methods of inference"".[2] An introductory economics textbook describes econometrics as allowing economists ""to sift through mountains of data to extract simple relationships"".[3] The first known use of the term ""econometrics"" (in cognate form) was by Polish economist Paweł Ciompa in 1910.[4] Jan Tinbergen is considered by many to be one of the founding fathers of econometrics.[5][6][7] Ragnar Frisch is credited with coining the term in the sense in which it is used today.[8] A basic tool for econometrics is the multiple linear regression model.[9] Econometric theory uses statistical theory and mathematical statistics to evaluate and develop econometric methods.[10][11] Econometricians try to find estimators that have desirable statistical properties including unbiasedness, efficiency, and consistency. Applied econometrics uses theoretical econometrics and real-world data for assessing economic theories, developing econometric models, analysing economic history, and forecasting.
"
Economic_development,Economics,6,"In the economic study of the public sector, economic and social development is the process by which the economic well-being and quality of life of a nation, region, local community, or an individual are improved according to targeted goals and objectives.
 The term has been used frequently in the 20th and 21st centuries, but the concept has existed in the West for far longer. ""Modernization"", ""Westernization"", and especially ""industrialization"" are other terms often used while discussing economic development.
 Whereas economic development is a policy intervention aiming to improve the well-being of people, economic growth is a phenomenon of market productivity and increases in GDP; economist Amartya Sen describes economic growth as but ""one aspect of the process of economic development"". Economists primarily focus on the growth aspect and the economy at large, whereas researchers of community economic development concern themselves with socioeconomic development as well.
 Many institutions of higher education offer economic development as an area of study and research such as McGill University, London School of Economics, International Institute of Social Studies, Balsillie School of International Affairs, and the Norman Paterson School of International Affairs.
"
Economic_efficiency,Economics,6,"In microeconomics, economic efficiency is, roughly speaking, a situation in which nothing can be improved without something else being hurt. Depending on the context, it is usually one of the following two related concepts:
 These definitions are not equivalent: a market or other economic system may be allocatively but not productively efficient, or productively but not allocatively efficient. There are also other definitions and measures. All characterizations of economic efficiency are encompassed by the more general engineering concept that a system is efficient or optimal when it maximizes desired outputs (such as utility) given available inputs.
"
Economic_equilibrium,Economics,6,"In economics, economic equilibrium is a situation in which economic forces such as supply and demand are balanced and in the absence of external influences the (equilibrium) values of economic variables will not change. For example, in the standard text   perfect competition, equilibrium occurs at the point at which quantity demanded and quantity supplied are equal.[1] Market equilibrium in this case is a condition where a market price is established through competition such that the amount of goods or services sought by buyers is equal to the amount of goods or services produced by sellers. This price is often called the competitive price or market clearing price and will tend not to change unless demand or supply changes, and quantity is called the ""competitive quantity"" or market clearing quantity. But the concept of equilibrium in economics also applies to imperfectly competitive markets, where it takes the form of a Nash equilibrium.
"
Economic_growth,Economics,6,"One can define economic growth as the increase in the inflation-adjusted market value of the goods and services produced by an economy over time. Statisticians conventionally measure such growth as the percent rate of increase in real gross domestic product, or real GDP.[1] Growth is usually calculated in real terms – i.e., inflation-adjusted terms – to eliminate the distorting effect of inflation on the prices of goods produced. Measurement of economic growth uses national income accounting.[2] Since economic growth is measured as the annual percent change of gross domestic product (GDP), it has all the advantages and drawbacks of that measure. The economic growth-rates of countries are commonly compared[by whom?] using the ratio of the GDP to population (per-capita income).[3] The ""rate of economic growth"" refers to the geometric annual rate of growth in GDP between the first and the last year over a period of time. This growth rate represents the trend in the average level of GDP over the period, and ignores any fluctuations in the GDP around this trend.
 Economists refer to an increase in economic growth caused by more efficient use of inputs (increased productivity of labor, of physical capital, of energy or of materials) as intensive growth. In contrast, GDP growth caused only by increases in the amount of inputs available for use (increased population, for example, or new territory) counts as extensive growth.[4] Development of new goods and services also generates economic growth.[citation needed] As it so happens, in the U.S. about 60% of consumer spending in 2013 went on goods and services that did not exist in 1869.[5]"
Economic_indicator,Economics,6,"An economic indicator is a statistic about an economic activity. Economic indicators allow analysis of economic performance and predictions of future performance. One application of economic indicators is the study of business cycles. Economic indicators include various indices, earnings reports, and economic summaries: for example, the unemployment rate, quits rate (quit rate in American English), housing starts, consumer price index (a measure for inflation), consumer leverage ratio, industrial production, bankruptcies, gross domestic product, broadband internet penetration, retail sales, stock market prices, and money supply changes.
 The leading business cycle dating committee in the United States of America is the private National Bureau of Economic Research. The Bureau of Labor Statistics is the principal fact-finding agency for the U.S. government in the field of labor economics and statistics. Other producers of economic indicators includes the United States Census Bureau and United States Bureau of Economic Analysis.
"
Economic_interdependence,Economics,6,"Economic interdependence is a consequence of specialization or the division of labour. The participants in any economic system must belong to a trading network to obtain the products they cannot produce efficiently for themselves. A change in the economic behavior of any participant on such a network usually affects many others, so that the demands and incomes of the participants are interdependent. 
 A. A. Cournot  wrote in Mathematical Researches into the theory of Wealth ""...the economic system is a whole in which all of the parts are connected and react on one another. An increase in the income of the producers of commodity A will affect the demand for commodities B,  C, etc. and the incomes of their producers, and by their reaction will affect the demand for commodity A.""  [1] Such complex reactions are evident in general equilibrium theory.
 The economic interdependence of nations has been studied extensively by professors all around the world. Such an international economic interaction is commonly thought of as a dollar value of the transaction of goods and services between nations (Cooper);[2] several academics have challenged this fundamental paradigm over time. Baldwin suggests that economic interdependence may be conceived as the opportunity costs incurred from potential exit costs that incur as a result of breaking existing economic ties between nations. Whitman, cited by Baldwin, further expands on Cooper's definition and proposes that economic interdependence should also involve the degree of sensitivity of a country’s economic behaviour to policies and development of countries outside its border. However, empirical evidence to support the latter definition is a lot harder to find, given its ambiguity (Baldwin).[3] Global economic interdependence has grown exponentially in the span of a generation, as a result of great technological progress and associated policies that were aimed at opening national economies internally and externally to global competition.[4] Paehlke notes that investment and international trade have drastically increased over the last 100 of years except during the World War I and World War II.[5] Over time, economic interdependence has incorporated other aspects that were brought about by contemporary globalisation - as a result of the onset of the age of computerisation, telecommunications and low-cost travel and shipping. As international trade have been increasing at a rate beyond 8% during the 1950s to 1970s, and has also been driven by improvements in information technology in the 1990s, economic interdependence between countries has increased even more rapidly.[6] Given such rapid increase in international trade and capital flows that are traditionally associated with globalisation, there has been increasing interest in the issues of financial and economic interdependence, partly driven by the contagion that resulted from the financial crisis.[7]"
Economic_model,Economics,6,"In economics, a model is a theoretical construct representing economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified, often mathematical, framework designed to illustrate complex processes. Frequently, economic models posit structural parameters.[1] A model may have various exogenous variables, and those variables may change to create various responses by economic variables. Methodological uses of models include investigation, theorizing, and fitting theories to the world.[2]"
Economic_rent,Economics,6,"In economics, economic rent is any payment to an owner or factor of production in excess of the costs needed to bring that factor into production. In classical economics, economic rent is any payment made (including imputed value) or benefit received for non-produced inputs such as location (land) and for assets formed by creating official privilege over natural opportunities (e.g., patents). In the moral economy of neoclassical economics, economic rent includes income gained by labor or state beneficiaries of other ""contrived"" (assuming the market is natural, and does not come about by state and social contrivance) exclusivity, such as labor guilds and unofficial corruption.
 In the moral economy of the economics tradition broadly, economic rent is opposed to producer surplus, or normal profit, both of which are theorized to involve productive human action. Economic rent is also independent of opportunity cost, unlike economic profit, where opportunity cost is an essential component. Economic rent is viewed as unearned revenue [1] while economic profit is a narrower term describing surplus income earned by choosing between risk-adjusted alternatives.  Unlike economic profit, economic rent cannot be theoretically eliminated by competition because any actions the recipient of the income may take such as improving the object to be rented will then change the total income to contract rent.  Still, the total income is made up of economic profit (earned) plus economic rent (unearned).
 For a produced commodity, economic rent may be due to the legal ownership of a patent (a politically enforced right to the use of a process or ingredient). For education and occupational licensing, it is the knowledge, performance, and ethical standards, as well as the cost of permits and licenses that are collectively controlled as to their number, regardless of the competence and willingness of those who wish to compete on price alone in the area being licensed. In regard to labor, economic rent can be created by the existence of mass education, labor laws, state social reproduction supports, democracy, guilds, and labor unions (e.g., higher pay for some workers, where collective action creates a scarcity of such workers, as opposed to an ideal condition where labor competes with other factors of production on price alone). For most other production, including agriculture and extraction, economic rent is due to a scarcity (uneven distribution) of natural resources (e.g., land, oil, or minerals).
 When economic rent is privatized, the recipient of economic rent is referred to as a rentier.
 By contrast, in production theory, if there is no exclusivity and there is perfect competition, there are no economic rents, as competition drives prices down to their floor.[2][3] Economic rent is different from other unearned and passive income, including contract rent. This distinction has important implications for public revenue and tax policy.[4][5][6] As long as there is sufficient accounting profit, governments can collect a portion of economic rent for the purpose of public finance. For example, economic rent can be collected by a government as royalties or extraction fees in the case of resources such as minerals and oil and gas.
 Historically, theories of rent have typically applied to rent received by different factor owners within a single economy. Hossein Mahdavy was the first to introduce the concept of ""external rent"", whereby one economy received rent from other economies.[7]"
Economic_rent,Economics,6,"In economics, economic rent is any payment to an owner or factor of production in excess of the costs needed to bring that factor into production. In classical economics, economic rent is any payment made (including imputed value) or benefit received for non-produced inputs such as location (land) and for assets formed by creating official privilege over natural opportunities (e.g., patents). In the moral economy of neoclassical economics, economic rent includes income gained by labor or state beneficiaries of other ""contrived"" (assuming the market is natural, and does not come about by state and social contrivance) exclusivity, such as labor guilds and unofficial corruption.
 In the moral economy of the economics tradition broadly, economic rent is opposed to producer surplus, or normal profit, both of which are theorized to involve productive human action. Economic rent is also independent of opportunity cost, unlike economic profit, where opportunity cost is an essential component. Economic rent is viewed as unearned revenue [1] while economic profit is a narrower term describing surplus income earned by choosing between risk-adjusted alternatives.  Unlike economic profit, economic rent cannot be theoretically eliminated by competition because any actions the recipient of the income may take such as improving the object to be rented will then change the total income to contract rent.  Still, the total income is made up of economic profit (earned) plus economic rent (unearned).
 For a produced commodity, economic rent may be due to the legal ownership of a patent (a politically enforced right to the use of a process or ingredient). For education and occupational licensing, it is the knowledge, performance, and ethical standards, as well as the cost of permits and licenses that are collectively controlled as to their number, regardless of the competence and willingness of those who wish to compete on price alone in the area being licensed. In regard to labor, economic rent can be created by the existence of mass education, labor laws, state social reproduction supports, democracy, guilds, and labor unions (e.g., higher pay for some workers, where collective action creates a scarcity of such workers, as opposed to an ideal condition where labor competes with other factors of production on price alone). For most other production, including agriculture and extraction, economic rent is due to a scarcity (uneven distribution) of natural resources (e.g., land, oil, or minerals).
 When economic rent is privatized, the recipient of economic rent is referred to as a rentier.
 By contrast, in production theory, if there is no exclusivity and there is perfect competition, there are no economic rents, as competition drives prices down to their floor.[2][3] Economic rent is different from other unearned and passive income, including contract rent. This distinction has important implications for public revenue and tax policy.[4][5][6] As long as there is sufficient accounting profit, governments can collect a portion of economic rent for the purpose of public finance. For example, economic rent can be collected by a government as royalties or extraction fees in the case of resources such as minerals and oil and gas.
 Historically, theories of rent have typically applied to rent received by different factor owners within a single economy. Hossein Mahdavy was the first to introduce the concept of ""external rent"", whereby one economy received rent from other economies.[7]"
Economic_shortage,Economics,6,"
 In economics, a shortage or excess demand is a situation in which the demand for a product or service exceeds its supply in a market. It is the opposite of an excess supply (surplus).
"
Economic_surplus,Economics,6,"In mainstream economics, economic surplus, also known as total welfare or Marshallian surplus (after Alfred Marshall), refers to two related quantities: 
"
Economic_system,Economics,6,"An economic system, or economic order,[1] is a system of production, resource allocation and distribution of goods and services within a society or a given geographic area. It includes the combination of the various institutions, agencies, entities, decision-making processes and patterns of consumption that comprise the economic structure of a given community. 
 An economic system is a type of social system. The mode of production is a related concept.[2] All economic systems have three basic questions to ask: what to produce, how to produce and in what quantities, and who receives the output of production.
 The study of economic systems includes how these various agencies and institutions are linked to one another, how information flows between them and the social relations within the system (including property rights and the structure of management). The analysis of economic systems traditionally focused on the dichotomies and comparisons between market economies and planned economies and on the distinctions between capitalism and socialism.[3] Subsequently, the categorization of economic systems expanded to include other topics and models that do not conform to the traditional dichotomy. 
 Today the dominant form of economic organization at the world level is based on market-oriented mixed economies.[4] An economic system can be considered a part of the social system and hierarchically equal to the law system, political system, cultural and so on. There is often a strong correlation between certain ideologies, political systems and certain economic systems (for example, consider the meanings of the term ""communism""). Many economic systems overlap each other in various areas (for example, the term ""mixed economy"" can be argued to include elements from various systems). There are also various mutually exclusive hierarchical categorizations.
"
Economics,Economics,6,"
 
 Economics (/ɛkəˈnɒmɪks, iːkə-/)[1][2][3] is the social science that studies how people interact with things of value; in particular, the production, distribution, and consumption of goods and services.[4] Economics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyzes basic elements in the economy, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyzes the economy as a system where production, consumption, saving, and investment interact,  and factors affecting it: employment of the resources of labour, capital, and land, currency inflation, economic growth, and public policies that have impact on these elements.
 Other broad distinctions within economics include those between positive economics, describing ""what is"", and normative economics, advocating ""what ought to be""; between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics.[5] Economic analysis can be applied throughout society, in real estate,[6] business,[7] finance, health care,[8] and government.[9] Economic analysis is sometimes also applied to such diverse subjects as crime, education,[10] the family, law, politics, religion,[11] social institutions, war,[12] science,[13] and the environment.[14]"
Economies_of_agglomeration,Economics,6,"Economies of agglomeration or agglomeration effects are cost savings arising from urban agglomeration, a major topic of urban economics. One aspect of agglomeration is that firms are often located near to each other.[1]:1  This concept relates to the idea of economies of scale and network effects.
 As more firms in related fields of business cluster together, their costs of production may decline significantly (firms have competing multiple suppliers; greater specialization and division of labor result).  Even when competing firms in the same sector cluster, there may be advantages because the cluster attracts more suppliers and customers than a single firm could achieve alone. Cities form and grow to exploit economies of agglomeration.
 Diseconomies of agglomeration are the opposite. For example, spatially concentrated growth in automobile-oriented fields may create problems of crowding and traffic congestion. It is the tension between economies and diseconomies that allows cities to grow but keeps them from becoming too large.
 The basic concept of agglomeration economies is that production is facilitated when there is a clustering of economic activity. The existence of agglomeration economies is central to the explanation of how cities increase in size and population, which places the phenomenon on a larger scale. The concentration of economic activity in cities is one reason for their development and growth.
"
Economies_of_scale,Economics,6,"In microeconomics, economies of scale are the cost advantages that enterprises obtain due to their scale of operation (typically measured by the amount of output produced), with cost per unit of output decreasing with increasing scale. At the basis of economies of scale there may be technical, statistical, organizational or related factors to the degree of market control.
 Economies of scale apply to a variety of organizational and business situations and at various levels, such as a production, plant or an entire enterprise. When average costs start falling as output increases, then economies of scale occur.
Some economies of scale, such as capital cost of manufacturing facilities and friction loss of transportation and industrial equipment, have a physical or engineering basis.
 Another source of scale economies[1] is the possibility of purchasing inputs at a lower per-unit cost when they are purchased in large quantities.
 The economic concept dates back to Adam Smith and the idea of obtaining larger production returns through the use of division of labor.[2] Diseconomies of scale are the opposite.
 Economies of scale often have limits, such as passing the optimum design point where costs per additional unit begin to increase.  Common limits include exceeding the nearby raw material supply, such as wood in the lumber, pulp and paper industry. A common limit for a low cost per unit weight commodities is saturating the regional market, thus having to ship product uneconomic distances. Other limits include using energy less efficiently or having a higher defect rate.
 Large producers are usually efficient at long runs of a product grade (a commodity) and find it costly to switch grades frequently.  They will, therefore, avoid specialty grades even though they have higher margins.  Often smaller (usually older) manufacturing facilities remain viable by changing from commodity-grade production to specialty products.[3][4] Economies of scale must be distinguished from economies stemming from an increase in the production of a given plant. When a plant is used below its optimal production capacity, increases in its degree of utilization bring about decreases in the total average cost of production. As noticed, among the others, by Nicholas Georgescu-Roegen (1966) and Nicholas Kaldor (1972) these economies are not economies of scale.
"
Economies_of_scope,Economics,6,"
 
 Economies of scope are ""efficiencies formed by variety, not volume"" (the latter concept is ""economies of scale"").[1] In economics, ""economies"" is synonymous with cost savings and ""scope"" is synonymous with broadening production/services through diversified products. For example, a gas station that sells gasoline can sell soda, milk, baked goods, etc. through their customer service representatives and thus gasoline companies achieve economies of scope.[2]"
Economist,Economics,6,"An economist is a practitioner in the social science discipline of economics.
 The individual may also study, develop, and apply theories and concepts from economics and write about economic policy. Within this field there are many sub-fields, ranging from the broad philosophical theories to the focused study of minutiae within specific markets, macroeconomic analysis, microeconomic analysis or financial statement analysis, involving analytical methods and tools such as econometrics, statistics, economics computational models, financial economics, mathematical finance and mathematical economics.
"
Economy,Economics,6,"
 An economy (from Greek οίκος – ""household"" and νέμoμαι – ""manage"") is an area of the production, distribution and trade, as well as consumption of goods and services by different agents. Understood in its broadest sense, 'The economy is defined as a social domain that emphasize the practices, discourses, and material expressions associated with the production, use, and management of resources'.[1] A given economy is the result of a set of processes that involves its culture, values, education, technological evolution, history, social organization, political structure and legal systems, as well as its geography, natural resource endowment, and ecology, as main factors. These factors give context, content, and set the conditions and parameters in which an economy functions. In other words, the economic domain is a social domain of human practices and transactions. It does not stand alone.
 Economic agents can be individuals, businesses, organizations, or governments. Economic transactions occur when two groups or parties agree to the value or price of the transacted good or service, commonly expressed in a certain currency. However, monetary transactions only account for a small part of the economic domain. 
 Economic activity is spurred by production which uses natural resources, labor and capital. It has changed over time due to technology (automation, accelerator of process, reduction of cost functions), innovation (new products, services, processes, expanding markets, diversification of markets, niche markets, increases revenue functions) such as, that which produces intellectual property and changes in industrial relations (most notably child labor being replaced in some parts of the world with universal access to education). 
 A market-based economy is one where goods and services are produced and exchanged according to demand and supply between participants (economic agents) by barter or a medium of exchange with a credit or debit value accepted within the network, such as a unit of currency. A command-based economy is one where political agents directly control what is produced and how it is sold and distributed. A green economy is low-carbon, resource efficient and socially inclusive. In a green economy, growth in income and employment is driven by public and private investments that reduce carbon emissions and pollution, enhance energy and resource efficiency, and prevent the loss of biodiversity and ecosystem services.[2] A gig economy is one in which short-term jobs are assigned or chosen via online platforms.[3] New economy is a term that referred to the whole emerging ecosystem where new standards and practices were introduced, usually as a result of technological innovations. The global economy refers to humanity's economic system or systems overall.
"
Effective_demand,Economics,6,"In economics, effective demand (ED) in a market is the demand for a product or service which occurs when purchasers are constrained in a different market.  It contrasts with notional demand, which is the demand that occurs when purchasers are not constrained in any other market.  In the aggregated market for goods in general, demand, notional or effective, is referred to as aggregate demand. The concept of effective supply parallels the concept of effective demand.  The concept of effective demand or supply becomes relevant when markets do not continuously maintain equilibrium prices.[1][2][3]"
Price_elasticity_of_demand,Economics,6,"Price elasticity of demand (or elasticity), is the degree to which the effective desire for something changes as its price changes. In general, people desire things less as those things become more expensive. However, for some products, the customer's desire could drop sharply even with a little price increase, and for other products, it could stay almost the same even with a big price increase. Economists use the term elasticity to denote this sensitivity to price increases. More precisely, price elasticity gives the percentage change in quantity demanded when there is a one percent increase in price, holding everything else constant.
 Price elasticities are almost always negative, although analysts tend to ignore the sign even though this can lead to ambiguity. Only goods which do not conform to the law of demand, such as Veblen and Giffen goods, have a positive elasticity. In general, the demand for a good is said to be inelastic (or relatively inelastic) when the elasticity is less than one (in absolute value): that is, changes in price have a relatively small effect on the quantity of the good demanded. The demand for a good is said to be elastic (or relatively elastic) when its elasticity is greater than one.
 Revenue is maximised when price is set so that the elasticity is exactly one. The  good's elasticity can also be used to predict the incidence (or ""burden"") of a tax on that good. Various research methods are used to determine price elasticity, including test markets, analysis of historical sales data and conjoint analysis.   
Price elasticity of demand further divided into:
Perfectly Elastic Demand		(∞),
Perfectly Inelastic Demand		( 0 ),
Relatively Elastic Demand		(> 1),
Relatively Inelastic Demand		(< 1),
Unitary Elasticity Demand 		(= 1).
"
Elasticity_(economics),Economics,6,"In economics, elasticity  is the measurement of the percentage change of one economic variable in response to a change in another.  
 An elastic variable (with an absolute elasticity value greater than 1) is one which responds more than proportionally to changes in other variables.  In contrast, an inelastic variable (with an absolute elasticity value less than 1) is one which changes less than proportionally in response to changes in other variables. A variable can have different values of its elasticity at different starting points: for example, the quantity of a good supplied by producers might be elastic at low prices but inelastic at higher prices, so that a rise from an initially low price might bring on a more-than-proportionate increase in quantity supplied while a rise from an initially high price might bring on a less-than-proportionate rise in quantity supplied.
 Elasticity can be quantified as the ratio of the percentage change in one variable to the percentage change in another variable, when the latter variable has a causal influence on the former. A more precise definition is given in terms of differential calculus. It is a tool for measuring the responsiveness of one variable to changes in another, causative variable. Elasticity has the advantage of being a unitless ratio, independent of the type of quantities being varied. Frequently used elasticities include price elasticity of demand, price elasticity of supply, income elasticity of demand, elasticity of substitution between factors of production and elasticity of intertemporal substitution.
 Elasticity is one of the most important concepts in neoclassical economic theory. It is useful in understanding the incidence of indirect taxation, marginal concepts as they relate to the theory of the firm, and distribution of wealth and different types of goods as they relate to the theory of consumer choice. Elasticity is also crucially important in any discussion of welfare distribution, in particular consumer surplus, producer surplus, or government surplus.
 In empirical work an elasticity is the estimated coefficient in a linear regression equation where both the dependent variable and the independent variable are in natural logs. Elasticity is a popular tool among empiricists because it is independent of units and thus simplifies data analysis.
 A major study of the price elasticity of supply and the price elasticity of demand for US products was undertaken by Joshua Levy and Trevor Pollock in the late 1960s.[1]"
Engineering_economics,Economics,6,"
 For the application of engineering economics in the practice of civil engineering see  Engineering economics (Civil Engineering).
 Engineering economics, previously known as engineering economy, is a subset of economics concerned with the use and ""...application of economic principles""[1] in the analysis of engineering decisions.[2] As a discipline, it is focused on the branch of economics known as microeconomics in that it studies the behavior of individuals and firms in making decisions regarding the allocation of limited resources. Thus, it focuses on the decision making process, its context and environment.[1] It is pragmatic by nature, integrating economic theory with engineering practice.[1] But, it is also a simplified application of microeconomic theory in that it avoids a number of microeconomic concepts such as price determination, competition and demand/supply.[1]  As a discipline though, it is closely related to others such as statistics, mathematics and cost accounting.[1] It draws upon the logical framework of economics but adds to that the analytical power of mathematics and statistics.[1] Engineers seek solutions to problems, and the economic viability of each potential solution is normally considered along with the technical aspects.
Fundamentally, engineering economics involves formulating, estimating, and evaluating the economic outcomes when alternatives to accomplish a defined purpose are available.[3] In some U.S. undergraduate civil engineering curricula, engineering economics is a required course.[4] It is a topic on the Fundamentals of Engineering examination, and questions might also be asked on the Principles and Practice of Engineering examination; both are part of the Professional Engineering registration process.
 Considering the time value of money is central to most engineering economic analyses.  Cash flows are discounted using an interest rate, except in the most basic economic studies.
 For each problem, there are usually many possible alternatives. One option that must be considered in each analysis, and is often the choice, is the do nothing alternative.  The opportunity cost of making one choice over another must also be considered.  There are also non-economic factors to be considered, like color, style, public image, etc.; such factors are termed attributes.[5] Costs as well as revenues are considered, for each alternative, for an analysis period that is either a fixed number of years or the estimated life of the project.  The salvage value is often forgotten, but is important, and is either the net cost or revenue for decommissioning the project.
 Some other topics that may be addressed in engineering economics are inflation, uncertainty, replacements, depreciation, resource depletion, taxes, tax credits, accounting, cost estimations, or capital financing. All these topics are primary skills and knowledge areas in the field of cost engineering.
 Since engineering is an important part of the manufacturing sector of the economy, engineering industrial economics is an important part of industrial or business economics. Major topics in engineering industrial economics are:
"
Entrepreneurship,Economics,6,"
 
 Entrepreneurship is the creation or extraction of value.[1][2][3] With this definition, entrepreneurship is viewed as change, which may include other values than simply economic ones. 
 More narrow definitions have described entrepreneurship as the process of designing, launching and running a new business, which is often initially a small business, or as the ""capacity and willingness to develop, organize and manage a business venture along with any of its risks to make a profit.""[4] The people who create these businesses are often referred to as entrepreneurs.[5][6]  While definitions of entrepreneurship typically focus on the launching and running of businesses, due to the high risks involved in launching a start-up, a significant proportion of start-up businesses have to close due to ""lack of funding, bad business decisions, government policies, an economic crisis, lack of market demand, or a combination of all of these.""[7] A somewhat broader definition of the term is sometimes used, especially in the field of economics. In this usage, an entrepreneur is an entity which has the ability to find and act upon opportunities to translate inventions or technologies into products and services: ""The entrepreneur is able to recognize the commercial potential of the invention and organize the capital, talent, and other resources that turn an invention into a commercially viable innovation.""[8] In this sense, the term ""entrepreneurship"" also captures innovative activities on the part of established firms, in addition to similar activities on the part of new businesses. Yet, the definition is still narrow in the sense that it still focuses on the creation of economic (commercial) value.
"
Environmental_economics,Economics,6,"Environmental economics is a sub-field of economics concerned with environmental issues. It has become a widely studied subject due to growing environmental concerns in the twenty-first century. Environmental economics ""undertakes theoretical or empirical studies of the economic effects of national or local environmental policies around the world.... Particular issues include the costs and benefits of alternative environmental policies to deal with air pollution, water quality, toxic substances, solid waste, and global warming.""[1] Environmental economics is distinguished from ecological economics in that ecological economics emphasizes the economy as a subsystem of the ecosystem with its focus upon preserving natural capital.[2] One survey of German economists found that ecological and environmental economics are different schools of economic thought, with ecological economists emphasizing ""strong"" sustainability and rejecting the proposition that human-made (""physical"") capital can substitute for natural capital.[3]"
Equal_opportunity,Economics,6,"
 Equal opportunity is a state of fairness in which individuals are treated similarly, unhampered by artificial barriers or prejudices or preferences, except when particular distinctions can be explicitly justified.[1] The intent is that the important jobs in an organization should go to the people who are most qualified – persons most likely to perform ably in a given task – and not go to persons for reasons deemed arbitrary or irrelevant, such as circumstances of birth, upbringing, having well-connected relatives or friends,[2] religion, sex,[3] ethnicity,[3] race, caste,[4] or involuntary personal attributes such as disability, age, gender identity, or sexual orientation.[4][5] Chances for advancement should be open to everybody interested,[6] such that they have ""an equal chance to compete within the framework of goals and the structure of rules established"".[7][8] The idea is to remove arbitrariness from the selection process and base it on some ""pre-agreed basis of fairness, with the assessment process being related to the type of position""[2] and emphasizing procedural and legal means.[4][9] Individuals should succeed or fail based on their own efforts and not extraneous circumstances such as having well-connected parents.[10] It is opposed to nepotism[2] and plays a role in whether a social structure is seen as legitimate.[2][4][11] The concept is applicable in areas of public life in which benefits are earned and received such as employment and education, although it can apply to many other areas as well. Equal opportunity is central to the concept of meritocracy.[12]"
Equilibrium_price,Economics,6,"In economics, economic equilibrium is a situation in which economic forces such as supply and demand are balanced and in the absence of external influences the (equilibrium) values of economic variables will not change. For example, in the standard text   perfect competition, equilibrium occurs at the point at which quantity demanded and quantity supplied are equal.[1] Market equilibrium in this case is a condition where a market price is established through competition such that the amount of goods or services sought by buyers is equal to the amount of goods or services produced by sellers. This price is often called the competitive price or market clearing price and will tend not to change unless demand or supply changes, and quantity is called the ""competitive quantity"" or market clearing quantity. But the concept of equilibrium in economics also applies to imperfectly competitive markets, where it takes the form of a Nash equilibrium.
"
Equity_(economics),Economics,6,"Equity or Economic equality is the concept or idea of fairness in economics, particularly in regard to taxation or welfare economics. More specifically, it may refer to equal life chances regardless of identity, to provide all citizens with a basic and equal minimum of income, goods, and services or to increase funds and commitment for redistribution.[1]"
Excess_supply,Economics,6,"In economics, an excess supply or economic surplus[1] is a situation in which the quantity of a good or service supplied is more than the quantity demanded,[2] and the price is above the equilibrium level determined by supply and demand. That is, the quantity of the product that producers wish to sell exceeds the quantity that potential buyers are willing to buy at the prevailing price.  It is the opposite of an economic shortage (excess demand).  
 In cultural evolution, agricultural surplus in the Neolithic period is theorized to have produced a greater division of labor, resulting in social stratification and class.[not verified in body]"
Exchange_rate,Economics,6,"In  finance, an exchange rate is the rate at which one currency will be exchanged for another. It is also regarded as the value of one country's currency in relation to another currency.[1] For example, an interbank exchange rate of 114 Japanese yen  to the United States dollar means that ¥114 will be exchanged for US$1 or that US$1 will be exchanged for ¥114. In this case it is said that the price of a dollar in relation to yen is ¥114, or equivalently that the price of a yen in relation to dollars is $1/114. 
 Each country determines the exchange rate regime that will apply to its currency. For example, a currency may be floating, pegged (fixed), or a hybrid. Governments can impose certain limits and controls on exchange rates.
 In floating exchange rate regimes, exchange rates are  determined in the foreign exchange market,[2] which is open to a wide range of different types of buyers and sellers, and where currency trading is continuous: 24 hours a day except weekends (i.e. trading from 20:15 GMT on Sunday until 22:00 GMT Friday). The spot exchange rate is the current exchange rate, while the forward exchange rate is an exchange rate that is quoted and traded today but for delivery and payment on a specific future date.
 In the retail currency exchange market, different buying and selling rates will be quoted by money dealers. Most trades are to or from the local currency. The buying rate is the rate at which money dealers will buy foreign currency, and the selling rate is the rate at which they will sell that currency. The quoted rates will incorporate an allowance for a dealer's margin (or profit) in trading, or else the margin may be recovered in the form of a commission or in some other way. Different rates may also be quoted for cash, a documentary transaction or for electronic transfers. The higher rate on documentary transactions has been justified as compensating for the additional time and cost of clearing the document. On the other hand, cash is available for resale immediately, but incurs security, storage, and transportation costs, and the cost of tying up capital in a stock of banknotes (bills).
"
Excludability,Economics,6,"In economics, a good, service or resource are broadly assigned two fundamental characteristics; a degree of excludability and a degree of rivalry. Excludability is defined as the degree to which a good, service or resource can be limited to only paying customers, or conversely, the degree to which a supplier, producer or other managing body (e.g. a government) can prevent ""free"" consumption of a good.
 Excludability was originally proposed in 1954 by American economist Paul Samuelson where he formalised the concept now known as public goods, i.e. goods that are both non-rivalrous and non-excludeable.[1] Samuelson additionally highlighted the market failure of the free-rider problem that can occur with non-excludable goods. Samuelson's theory of good classification was then further expanded upon by Richard Musgrave in 1959, Garret Hardin in 1968 who expanded upon another key market inefficiency of non-excludeable goods; the tragedy of the commons.[2] Excludability was further expanded upon by Elinor Ostrom in 1990 to be a continuous characteristic, as opposed to the discreet characteristic proposed by Samuelson (who presented excludability as either being present or absent).[1] Ostrom's theory proposed that excludability can be placed on a scale that would range from fully excludable (i.e. a good that could theoretically fully exclude non-paying consumers) to fully non-excludeable (a good that cannot exclude non-paying customers at all).[3] This scale allows producers and providers more in-depth information that can then be used to generate more efficient price equations (for public goods in particular), that would then maximize benefits and positive externalities for all consumers of the good[4]"
Expected_utility_hypothesis,Economics,6,"The expected utility hypothesis  is a popular concept in economics, game theory and decision theory that serves as a reference guide for judging decisions involving uncertainty.[1] The theory recommends which option a rational individual should choose in a complex situation, based on his tolerance for risk and personal preferences.  
 The expected utility of an agent's risky decision is the mathematical expectation of his utility from different outcomes given their probabilities. If an agent   derives 0  utils from 0 apples, 2 utils from one apple, and 3 utils from two apples, his expected utility for  a 50-50 gamble between zero apples and two is .5u(0) + .5u(3) = .5(0) + .5(3) = 1.5 utils. Under the expected utility hypothesis, the consumer would prefer 1 apple with certainty (giving him 2 utils) to the gamble between zero and two.  
 Standard utility functions represent ordinal preferences.  The expected utility hypothesis imposes limitations on the utility function and makes utility cardinal (though still not comparable across individuals). In the example above, any function such that u(0) < (1) < u(2) would represent the same preferences; we could specify u(0)= 0, u(1) = 2, and u(2) = 40, for example. Under the expected utility hypothesis,  setting  u(2) = 3 requires if the agent is indifferent between one apple with certainty and a gamble with a 1/3 probability of no apple and a 2/3 probability of two apples, the utility of two apples must be set to u(2) = 2. This is because it requires that  (1/3)u(0) + (2/3)u(2) = u(1), and  2 = (1/3)(0) + (2/3)(3). 
 The idea has antecedents in Daniel Bernoulli's 1738 St. Petersburg Paradox,[2]
and was developed by Frank Ramsey and Leonard Jimmie Savage.   The von Neumann–Morgenstern utility theorem provides necessary and sufficient conditions under which the expected utility hypothesis holds. From relatively early on, it was accepted that some of these conditions would be violated by real decision-makers in practice but that the conditions could be interpreted nonetheless as 'axioms' of rational choice.  Until the mid-twentieth century, the standard term for the expected utility was the moral expectation, contrasted with ""mathematical expectation"" for the expected value.[3] Although the expected utility hypothesis is standard in economic modelling, largely because of its simplicity and convenience, it has been found to be violated in psychology experiments. For many years, psychologists and economic theorists have been developing new theories to explain these deficiencies.[4] These include prospect theory, rank-dependent expected utility and cumulative prospect theory.
"
Expeditionary_economics,Economics,6,"Expeditionary economics is an emerging field of economic enquiry that focuses on the rebuilding and reconstructing of economies in post-conflict nations and providing support to disaster-struck nations.
 The term was first introduced in 2010 in an essay by Carl Schramm, the former president and CEO of the Ewing Marion Kauffman Foundation.[1] It focuses on the need for good economic planning on the part of developed nations to help prevent the creation of failed states. It also emphasizes the need for the structuring on new firms to rebuild national economies.[2] Since then, the theory has been used by the U.S. Government and the U.S. Army to restructure the economies of countries such as Iraq and Afghanistan and helping Haiti after its severe earthquake. Its aim is to provide economic stabilization and support the counterinsurgency tactics in such nations.[3]"
Experimental_economics,Economics,6,"Experimental economics is the application of experimental methods[1] to study economic questions. Data collected in experiments are used to estimate effect size, test the validity of economic theories, and illuminate market mechanisms. Economic experiments usually use cash to motivate subjects, in order to mimic real-world incentives. Experiments are used to help understand how and why markets and other exchange systems function as they do. Experimental economics have also expanded to understand institutions and the law (experimental law and economics).[2] A fundamental aspect of the subject is design of experiments.  Experiments may be conducted in the field or in laboratory settings, whether of individual or group behavior.[3] Variants of the subject outside such formal confines include natural and quasi-natural experiments.[4]"
Externality,Economics,6,"In economics, an externality is the cost or benefit that is imposed by one or several parties on a third party who did not agree to incur that cost or benefit.  The concept of externality was first developed by economist Arthur Pigou in the 1920s.[1] The prototypical example of a negative externality is environmental pollution.  Pigou argued that a tax (later called a ""Pigouvian tax"") on negative externalities could be used to reduce their incidence to an efficient level.[1]  Subsequent thinkers have debated whether it is preferable to tax or to regulate negative externalities,[2]  the optimally efficient level of the Pigouvian taxation,[3] and what factors cause or exacerbate negative externalities, such as providing investors in corporations with limited liability for harms committed by the corporation.[4][5][6] Externalities often occur when the production or consumption of a product or service's private price equilibrium cannot reflect the true costs or benefits of that product or service for society as a whole.[7][8] This causes the externality competitive equilibrium to not adhere to the condition of Pareto optimality. Thus, since resources can be better allocated, externalities are an example of market failure.[9] Externalities can be either positive or negative. Governments and institutions often take actions to internalize externalities, thus market-priced transactions can incorporate all the benefits and costs associated with transactions between economic agents.[10][11] The most common way this is done is by imposing taxes on the producers of this externality. This is usually done similar to a quote where there is no tax imposed and then once the externality reaches a certain point there is a very high tax imposed. However, since regulators do not always have all the information on the externality it can be difficult to impose the right tax. Once the externality is internalized through imposing a tax the competitive equilibrium is now Pareto optimal.
 For example, manufacturing activities that cause air pollution impose health and clean-up costs on the whole society, whereas the neighbors of individuals who choose to fire-proof their homes may benefit from a reduced risk of a fire spreading to their own houses. If external costs exist, such as pollution, the producer may choose to produce more of the product than would be produced if the producer were required to pay all associated environmental costs. Because responsibility or consequence for self-directed action lies partly outside the self, an element of externalization is involved. If there are external benefits, such as in public safety, less of the good may be produced than would be the case if the producer were to receive payment for the external benefits to others.  For the purpose of these statements, overall cost and benefit to society is defined as the sum of the imputed monetary value of benefits and costs to all parties involved.[12][13]"
Factors_of_production,Economics,6,"In economics, factors of production, resources, or inputs are what is used in the production process to produce output—that is, finished goods and services. The utilized amounts of the various inputs determine the quantity of output according to the relationship called the production function. There are three basic resources or factors of production: land, labour and capital. The factors are also frequently labeled ""producer goods or services"" to distinguish them from the goods or services purchased by consumers, which are frequently labeled ""consumer goods"".
 There are two types of factors: primary and secondary. The previously mentioned primary factors are land, labour and capital. Materials and energy are considered secondary factors in classical economics because they are obtained from land, labour, and capital. The primary factors facilitate production but neither becomes part of the product (as with raw materials) nor becomes significantly transformed by the production process (as with fuel used to power machinery). Land includes not only the site of production but also natural resources above or below the soil. Recent usage has distinguished human capital (the stock of knowledge in the labor force) from labor.[1] Entrepreneurship is also sometimes considered a factor of production.[2] Sometimes the overall state of technology is described as a factor of production.[3] The number and definition of factors vary, depending on theoretical purpose, empirical emphasis, or school of economics.[4]"
Federal_Open_Market_Committee,Economics,6,"
  The Federal Open Market Committee (FOMC), a committee within the Federal Reserve System (the Fed), is charged under United States law with overseeing the nation's open market operations (e.g., the Fed's buying and selling of United States Treasury securities).[1] This Federal Reserve committee makes key decisions about interest rates and the growth of the United States money supply.[2]  Under the terms of the original Federal Reserve Act, each of the Federal Reserve banks was authorized to buy and sell in the open market bonds and short term obligations of the United States Government, bank acceptances, cable transfers, and bills of exchange.  Hence, the reserve banks were at times bidding against each other in the open market.  In 1922, an informal committee was established to execute purchases and sales.  The Banking Act of 1933 formed an official FOMC.[3] The FOMC is the principal organ of United States national monetary policy. The Committee sets monetary policy by specifying the short-term objective for the Fed's open market operations, which is usually a target level for the federal funds rate (the rate that commercial banks charge between themselves for overnight loans).
 The FOMC also directs operations undertaken by the Federal Reserve System in foreign exchange markets, although any intervention in foreign exchange markets is coordinated with the U.S. Treasury, which has responsibility for formulating U.S. policies regarding the exchange value of the dollar.
"
Federal_Reserve_System,Economics,6,"
 The Federal Reserve System (also known as the Federal Reserve or simply the Fed) is the central banking system of the United States of America. It was created on December 23, 1913, with the enactment of the Federal Reserve Act, after a series of financial panics (particularly the panic of 1907) led to the desire for central control of the monetary system in order to alleviate financial crises.[list 1] Over the years, events such as the Great Depression in the 1930s and the Great Recession during the 2000s have led to the expansion of the roles and responsibilities of the Federal Reserve System.[8][13][14] The U.S. Congress established three key objectives for monetary policy in the Federal Reserve Act: maximizing employment, stabilizing prices, and moderating long-term interest rates.[15] The first two objectives are sometimes referred to as the Federal Reserve's dual mandate.[16] Its duties have expanded over the years, and currently also include supervising and regulating banks, maintaining the stability of the financial system, and providing financial services to depository institutions, the U.S. government, and foreign official institutions.[17] The Fed also conducts research into the economy and provides numerous publications, such as the Beige Book and the FRED database.
 The Federal Reserve System is composed of several layers. It is governed by the presidentially appointed board of governors or Federal Reserve Board (FRB). Twelve regional Federal Reserve Banks, located in cities throughout the nation, regulate and oversee privately owned commercial banks.[18][19][20] Nationally chartered commercial banks are required to hold stock in, and can elect some of the board members of, the Federal Reserve Bank of their region. The Federal Open Market Committee (FOMC) sets monetary policy. It consists of all seven members of the board of governors and the twelve regional Federal Reserve Bank presidents, though only five bank presidents vote at a time (the president of the New York Fed and four others who rotate through one-year voting terms). There are also various advisory councils. Thus, the Federal Reserve System has both public and private components.[list 2] It has a structure unique among central banks, and is also unusual in that the United States Department of the Treasury, an entity outside of the central bank, prints the currency used.[25] The federal government sets the salaries of the board's seven governors, and it receives all the system's annual profits, after dividends on member banks' capital investments are paid, and an account surplus is maintained. In 2015, the Federal Reserve earned a net income of $100.2 billion and transferred $97.7 billion to the U.S. Treasury.[26] Although an instrument of the US Government, the Federal Reserve System considers itself ""an independent central bank because its monetary policy decisions do not have to be approved by the President or anyone else in the executive or legislative branches of government, it does not receive funding appropriated by Congress, and the terms of the members of the board of governors span multiple presidential and congressional terms.""[27]"
Finance,Economics,6,"Finance is a term for matters regarding the management, creation, and study of money and investments.[1] Specifically, it deals with the questions of how and why an individual, company or government acquires the money needed –  called capital in the company context –  and how they spend or invest that money. [2] Finance is then often split into the following major categories:  corporate finance, personal finance and public finance.[1] At the same time, and correspondingly, finance is about the overall ""system""
[1]
 –  i.e., the financial markets that allow the flow of money, via investments and other financial instruments,  between and within these areas; 
this ""flow"" is facilitated by the financial services sector.
A major focus within finance is thus investment management –  called money management for individuals, and asset management for institutions –  and finance then includes the associated activities of securities trading and stock broking, investment banking, financial engineering, and risk management.  
 Given its wide scope, finance is studied in several academic disciplines, and, correspondingly, there are several related professional qualifications that can lead to the field.
"
Financial_economics,Economics,6,"Financial economics is the branch of economics characterized by a ""concentration on monetary activities"", in which ""money of one type or another is likely to appear on both sides of a trade"". [1] 
Its concern is thus the interrelation of financial variables, such as prices, interest rates and shares, as opposed to those concerning the real economy. 
It has two main areas of focus:[2] asset pricing and corporate finance; the first being the perspective of providers of capital, i.e. investors, and the second of users of capital.
It thus provides the theoretical underpinning for much of finance.
 The subject is concerned with ""the allocation and deployment of economic resources, both spatially and across time, in an uncertain environment"".[3] It therefore centers on decision making under uncertainty in the context of the financial markets, and the resultant economic and financial models and principles, and is concerned with deriving testable or policy implications from acceptable assumptions. It is built on the foundations of microeconomics and decision theory.
 Financial econometrics is the branch of financial economics that uses econometric techniques to parameterise these relationships. Mathematical finance is related in that it will derive and extend the mathematical or numerical models suggested by financial economics. The emphasis there is mathematical consistency, as opposed to compatibility with economic theory. Financial economics has a primarily microeconomic focus, whereas monetary economics is primarily macroeconomic in nature.
"
Financial_institution,Economics,6,"Financial institutions, otherwise known as banking institutions, are corporations that provide services as intermediaries of financial markets. Broadly speaking, there are three major types of financial institutions:[1][2] Financial institutions can be distinguished broadly into two categories according to ownership structure:
 Some experts see a trend toward homogenisation of financial institutions, meaning a tendency to invest in similar areas and have similar business strategies. A consequence of this might be fewer banks serving specific target groups, and small-scale producers may be under-served.[3] This is why a target of the United Nations Sustainable Development Goal 10 is to improve the regulation and monitoring of global financial institutions and strengthen such regulations.[4]"
Financial_planning,Economics,6,"In general usage, a financial plan is a comprehensive evaluation of an individual's current pay and future financial state by using current known variables to predict future income, asset values and withdrawal plans.[1]  This often includes a budget which organizes an individual's finances and sometimes includes a series of steps or specific goals for spending and saving in the future. This plan allocates future income to various types of expenses, such as rent or utilities, and also reserves some income for short-term and long-term savings. A financial plan is sometimes referred to as an investment plan, but in personal finance a financial plan can focus on other specific areas such as risk management, estates, college, or retirement.
"
Financial_risk,Economics,6,"Financial risk is any of various types of risk associated with financing, including financial transactions that include company loans in risk of default.[1][2] Often it is understood to include only downside risk, meaning the potential for financial loss and uncertainty about its extent.[3][4] A science has evolved around managing market and financial risk under the general title of modern portfolio theory initiated by Dr. Harry Markowitz in 1952 with his article, ""Portfolio Selection"".[5]  In modern portfolio theory, the variance (or standard deviation) of a portfolio is used as the definition of risk.
"
Financial_transaction,Economics,6,"A financial transaction is an agreement, or communication, carried out between a buyer and a seller to exchange an asset for payment.
 It involves a change in the status of the finances of two or more businesses or individuals. The buyer and seller are separate entities or objects, often involving the exchange of items of value, such as information, goods, services, and money. It is still a transaction if the goods are exchanged at one time, and the money at another. This is known as a two-part transaction: part one is giving the money, part two is receiving the goods.
"
Fiscal_policy,Economics,6,"In economics and political science, fiscal policy is the use of government revenue collection (taxes or tax cuts) and expenditure (spending) to influence a country's economy. The use of government revenues and expenditures to influence macroeconomic variables developed as a result of the Great Depression, when the previous laissez-faire approach to economic management became unpopular. Fiscal policy is based on the theories of the British economist John Maynard Keynes, whose Keynesian economics theorized that government changes in the levels of taxation and government spending influences aggregate demand and the level of economic activity. Fiscal and monetary policy are the key strategies used by a country's government and central bank to advance its economic objectives. The combination of these policies enables these authorities to target inflation (which is considered ""healthy"" at the level in the range 2%–3%) and to increase employment. Additionally, it is designed to try to keep GDP growth at 2%–3% and the unemployment rate near the natural unemployment rate of 4%–5%.[1] This implies that fiscal policy is used to stabilize the economy over the course of the business cycle.[2] Changes in the level and composition of taxation and government spending can affect macroeconomic variables, including:
 Fiscal policy can be distinguished from monetary policy, in that fiscal policy deals with taxation and government spending and is often administered by a government department; while monetary policy deals with the money supply, interest rates and is often administered by a country's central bank. Both fiscal and monetary policies influence a country's economic performance.
"
Fixed_cost,Economics,6,"In accounting and economics, fixed costs, also known as indirect costs or overhead costs, are business expenses that are not dependent on the level of goods or services produced by the business. They tend to be recurring, such as interest or rents being paid per month. This is in contrast to variable costs, which are volume-related (and are paid per quantity produced) and unknown at the beginning of the accounting year.
 For example, a retailer must pay rent and utility bills irrespective of sales. As another example, for a bakery the monthly rent and phone line are fixed costs, irrespective of how much bread is produced and sold; on the other hand, the wages are variable costs, as more workers would need to be hired for the production to increase.
 Fixed cost are considered an entry barrier for new entrepreneurs. In marketing, it is necessary to know how costs divide between variable and fixed costs. This distinction is crucial in forecasting the earnings generated by various changes in unit sales and thus the financial impact of proposed marketing campaigns. In a survey of nearly 200 senior marketing managers, 60 percent responded that they found the ""variable and fixed costs"" metric very useful.[1]"
Foreign_exchange_market,Economics,6,"
 The foreign exchange market (Forex, FX, or currency market) is a global decentralized or over-the-counter (OTC) market for the trading of currencies. This market determines foreign exchange rates for every currency. It includes all aspects of buying, selling and exchanging currencies at current or determined prices. In terms of trading volume, it is by far the largest market in the world, followed by the credit market.[1] The main participants in this market are the larger international banks. Financial centers around the world function as anchors of trading between a wide range of multiple types of buyers and sellers around the clock, with the exception of weekends. Since currencies are always traded in pairs, the foreign exchange market does not set a currency's absolute value but rather determines its relative value by setting the market price of one currency if paid for with another. Ex: US$1 is worth X CAD, or CHF, or JPY, etc.
 The foreign exchange market works through financial institutions and operates on several levels. Behind the scenes, banks turn to a smaller number of financial firms known as ""dealers"", who are involved in large quantities of foreign exchange trading. Most foreign exchange dealers are banks, so this behind-the-scenes market is sometimes called the ""interbank market"" (although a few insurance companies and other kinds of financial firms are involved). Trades between foreign exchange dealers can be very large, involving hundreds of millions of dollars. Because of the sovereignty issue when involving two currencies, Forex has little (if any) supervisory entity regulating its actions.
 The foreign exchange market assists international trade and investments by enabling currency conversion. For example, it permits a business in the United States to import goods from  European Union member states, especially Eurozone members, and pay Euros, even though its income is in United States dollars. It also supports direct speculation and evaluation relative to the value of currencies and the carry trade speculation, based on the differential interest rate between two currencies.[2] In a typical foreign exchange transaction, a party purchases some quantity of one currency by paying with some quantity of another currency.
 The modern foreign exchange market began forming during the 1970s. This followed three decades of government restrictions on foreign exchange transactions under the Bretton Woods system of monetary management, which set out the rules for commercial and financial relations among the world's major industrial states after World War II. Countries gradually switched to floating exchange rates from the previous exchange rate regime, which remained fixed per the Bretton Woods system.	
 The foreign exchange market is unique because of the following characteristics:
 As such, it has been referred to as the market closest to the ideal of perfect competition, notwithstanding currency intervention by central banks.
 According to the Bank for International Settlements, the preliminary global results from the 2019 Triennial Central Bank Survey of Foreign Exchange and OTC Derivatives Markets Activity show that trading in foreign exchange markets averaged $6.6 trillion per day in April 2019. This is up from $5.1 trillion in April 2016. Measured by value, foreign exchange swaps were traded more than any other instrument in April 2019, at $3.2 trillion per day, followed by spot trading at $2 trillion.[3] The $6.6 trillion break-down is as follows:
"
Free_market,Economics,6,"In economics, a free market is a system in which the prices for goods and services are self-regulated by the open market and by consumers. In a free market, the laws and forces of supply and demand are free from any intervention by a government or other authority, and from all forms of economic privilege, monopolies and artificial scarcities.[1] Proponents of the concept of free market contrast it with a regulated market in which a government intervenes in supply and demand through various methods such as tariffs used to restrict trade and to protect the local economy. In an idealized free-market economy, prices for goods and services are set freely by the forces of supply and demand and are allowed to reach their point of equilibrium without intervention by government policy.
 Scholars contrast the concept of a free market with the concept of a coordinated market in fields of study such as political economy, new institutional economics, economic sociology and political science. All of these fields emphasize the importance in currently existing market systems of rule-making institutions external to the simple forces of supply and demand which create space for those forces to operate to control productive output and distribution. Although free markets are commonly associated with capitalism within a market economy in contemporary usage and popular culture, free markets have also been advocated by anarchists, socialists and some proponents of cooperatives and advocates of profit sharing.[2] Criticism of the theoretical concept may regard systems with significant market power, inequality of bargaining power, or information asymmetry as less than free, with regulation being necessary to control those imbalances in order to allow markets to function more efficiently as well as produce more desirable social outcomes.
"
Free_trade,Economics,6,"Free trade is a trade policy that does not restrict imports or exports. It can also be understood as the free market idea applied to international trade. In government, free trade is predominantly advocated by political parties that hold liberal economic positions while economically left-wing and nationalist political parties generally support protectionism,[1][2][3][4] the opposite of free trade.
 Most nations are today members of the World Trade Organization multilateral trade agreements. Free trade was best exemplified by the unilateral stance of Great Britain who reduced regulations and duties on imports and exports from the mid-nineteenth century to the 1920s.[5] An alternative approach, of creating free trade areas between groups of countries by agreement, such as that of the European Economic Area and the Mercosur open markets, creates a protectionist barrier between that free trade area and the rest of the world. Most governments still impose some protectionist policies that are intended to support local employment, such as applying tariffs to imports or subsidies to exports. Governments may also restrict free trade to limit exports of natural resources. Other barriers that may hinder trade include import quotas, taxes and non-tariff barriers, such as regulatory legislation.
 Historically, openness to free trade substantially increased from 1815 to the outbreak of World War I. Trade openness increased again during the 1920s, but collapsed (in particular in Europe and North America) during the Great Depression. Trade openness increased substantially again from the 1950s onwards (albeit with a slowdown during the oil crisis of the 1970s). Economists and economic historians contend that current levels of trade openness are the highest they have ever been.[6][7][8] There is a broad consensus among economists that protectionism has a negative effect on economic growth and economic welfare while free trade and the reduction of trade barriers has a positive effect on economic growth[9][10][11][12][13][14] and economic stability.[15] However, liberalization of trade can cause significant and unequally distributed losses, and the economic dislocation of workers in import-competing sectors.[10]"
Frictional_unemployment,Economics,6,"Frictional unemployment is a type of unemployment. It is sometimes called search unemployment and can be based on the circumstances of the individual. It is time spent between jobs when a worker is searching for a job or transferring from one job to another.[1] Frictional unemployment is one of the three broad categories of unemployment, the others being structural unemployment and cyclical unemployment. A person may be looking for a job change for better opportunities, services, salary and wages, or because of dissatisfaction with the previous job. Strikes by trade unions also give rise to frictional unemployment.
"
Full_employment,Economics,6,"Full employment is a situation in which there is no cyclical or deficient-demand unemployment.[1] Full employment does not entail the disappearance of all unemployment, as other kinds of unemployment, namely structural and frictional, may remain. For instance, workers who are ""between jobs"" for short periods of time as they search for better employment are not counted against full employment, as such unemployment is frictional rather than cyclical. An economy with full employment might also have unemployment or underemployment where part-time workers cannot find jobs appropriate to their skill level,[2] as such unemployment is considered structural rather than cyclical. Full employment marks the point past which expansionary fiscal and/or monetary policy cannot reduce unemployment any further without causing inflation.
 Some economists define full employment somewhat differently, as the unemployment rate at which inflation does not continuously increase. Advocacy of avoiding accelerating inflation is based on a theory centered on the concept of the Non-Accelerating Inflation Rate of Unemployment (NAIRU), and those who hold it usually mean NAIRU when speaking of full employment.[3][4] The NAIRU has also been described by Milton Friedman, among others, as the ""natural"" rate of unemployment. Such views tend to emphasize sustainability, noting that a government cannot sustain unemployment rates below the NAIRU forever: inflation will continue to grow so long as unemployment lies below the NAIRU.
 For the United States, economist William T. Dickens found that full-employment unemployment rate varied a lot over time but equaled about 5.5 percent of the civilian labor force during the 2000s.[5] Recently, economists have emphasized the idea that full employment represents a ""range"" of possible unemployment rates. For example, in 1999, in the United States, the Organisation for Economic Co-operation and Development (OECD) gives an estimate of the ""full-employment unemployment rate"" of 4 to 6.4%.  This is the estimated unemployment rate at full employment, plus and minus the standard error of the estimate.[6] The concept of full employment of labor corresponds to the concept of potential output or potential real GDP and the long run aggregate supply (LRAS) curve. In neoclassical macroeconomics, the highest sustainable level of aggregate real GDP or ""potential"" is seen as corresponding to a vertical LRAS curve: any increase in the demand for real GDP can only lead to rising prices in the long run, while any increase in output is temporary.
"
Functions_of_money,Economics,6,"
 Money is any item or verifiable record that is generally accepted as payment for goods and services and repayment of debts, such as taxes, in a particular country or socio-economic context.[1][2][3] The main functions of money are distinguished as: a medium of exchange, a unit of account, a store of value and sometimes, a standard of deferred payment.[4][5] Any item or verifiable record that fulfils these functions can be considered as money.
 Money is historically an emergent market phenomenon establishing a commodity money, but nearly all contemporary money systems are based on fiat money.[4] Fiat money, like any check or note of debt, is without use value as a physical commodity.[citation needed] It derives its value by being declared by a government to be legal tender; that is, it must be accepted as a form of payment within the boundaries of the country, for ""all debts, public and private"".[6][better source needed] Counterfeit money can cause good money to lose its value.
 The money supply of a country consists of currency (banknotes and coins) and, depending on the particular definition used, one or more types of bank money (the balances held in checking accounts, savings accounts, and other types of bank accounts). Bank money, which consists only of records (mostly computerized in modern banking), forms by far the largest part of broad money in developed countries.[7][8][9]"
Future_value,Economics,6,"Future value is the value of an asset at a specific date.[1] It measures the nominal future sum of money that a given sum of money is ""worth"" at a specified time in the future assuming a certain interest rate, or more generally, rate of return; it is the present value multiplied by the accumulation function.[2]
The value does not include corrections for inflation or other factors that affect the true value of money in the future.  This is used in time value of money calculations.
"
GDP_deflator,Economics,6,"In economics, the GDP deflator (implicit price deflator) is a measure of the level of prices of all new, domestically produced, final goods and services in an economy in a year.  GDP stands for gross domestic product, the total monetary value of all final goods  and services produced within the territory of a country over a particular period of time (quarterly or annually).
 Like the consumer price index (CPI), the GDP deflator is a measure of price inflation/deflation with respect to a specific base year; the GDP deflator of the base year itself is equal to 100. Unlike the CPI, the GDP deflator is not based on a fixed basket of goods and services; the ""basket"" for the GDP deflator is allowed to change from year to year with people's consumption and investment patterns.
"
General_equilibrium_theory,Economics,6,"In economics, general equilibrium theory attempts to explain the behavior of supply, demand, and prices in a whole economy with several or many interacting markets, by seeking to prove that the interaction of demand and supply will result in an overall general equilibrium. General equilibrium theory contrasts to the theory of partial equilibrium, which only analyzes single markets.
 General equilibrium theory both studies economies using the model of equilibrium pricing and seeks to determine in which circumstances the assumptions of general equilibrium will hold. The theory dates to the 1870s, particularly the work of French economist Léon Walras in his pioneering 1874 work Elements of Pure Economics.[1]"
Gift_economy,Economics,6,"A gift economy or gift culture is a mode of exchange where valuables are not traded or sold, but rather given without an explicit agreement for immediate or future rewards.[1] Social norms and customs govern gifting in a gift culture, gifts are not given in an explicit exchange of goods or services for money, or some other commodity or service.[2] This contrasts with a barter economy or a market economy, where goods and services are primarily explicitly exchanged for value received.
 The nature of gift economies is the subject of a foundational debate in anthropology. Anthropological research into gift economies began with Bronisław Malinowski's description of the Kula ring[3] in the Trobriand Islands during World War I.[4] The Kula trade appeared to be gift-like since Trobrianders would travel great distances over dangerous seas to give what were considered valuable objects without any guarantee of a return. Malinowski's debate with the French anthropologist Marcel Mauss quickly established the complexity of ""gift exchange"" and introduced a series of technical terms such as reciprocity, inalienable possessions, and presentation to distinguish between the different forms of exchange.[5][6] According to anthropologists Maurice Bloch and Jonathan Parry, it is the unsettled relationship between market and non-market exchange that attracts the most attention. Some authors argue that gift economies build community,[7] while markets harm community relationships.[8] Gift exchange is distinguished from other forms of exchange by a number of principles, such as the form of property rights governing the articles exchanged; whether gifting forms a distinct ""sphere of exchange"" that can be characterized as an ""economic system""; and the character of the social relationship that the gift exchange establishes. Gift ideology in highly commercialized societies differs from the ""prestations"" typical of non-market societies. Gift economies also differ from related phenomena, such as common property regimes and the exchange of non-commodified labour.
"
Good_(economics),Economics,6,"In economics, goods are items that satisfy human wants[1][dead link] and provide utility, for example, to a consumer making a purchase of a satisfying product. A common distinction is made between goods which are transferable, and services, which are not transferable.[2] A good may be a consumable item that is useful to people but scarce in relation to its demand, so that human effort is required to obtain it. In contrast, free goods, such as air, are naturally in abundant supply and need no conscious effort to obtain them. Private goods are things owned by people, such as televisions, living room furniture, wallets, cellular telephones, almost anything owned or used on a daily basis that is not food related. 
 A consumer good or ""final good"" is any item that is ultimately consumed, rather than used in the production of another good. For example, a microwave oven or a bicycle that is sold to a consumer is a final good or consumer good, but the components that are sold to be used in those goods are intermediate goods. For example, textiles or transistors can be used to make some further goods.
 Commercial goods are construed as tangible products that are manufactured and then made available for supply to be used in an industry of commerce. Commercial goods could be tractors, commercial vehicles, mobile structures, airplanes and even roofing materials. Commercial and personal goods as categories are very broad and cover almost everything a person sees from the time they wake up in their home, on their commute to work to their arrival at the workplace.
 Commodities may be used as a synonym for economic goods but often refer to marketable raw materials and primary products.[3] Although common goods are tangible, certain classes of goods, such as information, only take intangible forms. For example, among other goods an apple is a tangible object, while news belongs to an intangible class of goods and can be perceived only by means of an instrument such as print or television.
"
Government_revenue,Economics,6,"Government revenue or National revenue is money received by a government from taxes and non-tax sources to enable it to undertake government expenditures. 
 Government revenue as well as government spending are components of the government budget and important tools of the government's fiscal policy. 
 Seignorage is one of the ways a government can increase revenue, by deflating the value of its currency in exchange for surplus revenue, by saving money this way governments can increase the price of goods too far.
"
Government_spending,Economics,6,"
 Government spending or expenditure includes all government consumption, investment, and transfer payments.[1][2] In national income accounting, the acquisition by governments of goods and services for current use, to directly satisfy the individual or collective needs of the community, is classed as government final consumption expenditure. Government acquisition of goods and services intended to create future benefits, such as infrastructure investment or research spending, is classed as government investment (government gross capital formation). These two types of government spending, on final consumption and on gross capital formation, together constitute one of the major components of gross domestic product.
 Government spending can be financed by government borrowing, or taxes. When Governments choose to borrow money, they have to pay interest on the money borrowed which can lead to government debt.[3] Changes in government spending is a major component of fiscal policy used to stabilize the macroeconomic business cycle.
"
Gross_domestic_product,Economics,6,"Gross domestic product (GDP) is a monetary measure of the market value of all the final goods and services produced in a specific time period.[2][3] GDP (nominal) per capita does not, however, reflect differences in the cost of living and the inflation rates of the countries; therefore, using a basis of GDP per capita at purchasing power parity (PPP) is arguably more useful when comparing living standards between nations, while nominal GDP is more useful comparing national economies on the international market.[4] The OECD defines GDP as ""an aggregate measure of production equal to the sum of the gross values added of all resident and institutional units engaged in production and services (plus any taxes, and minus any subsidies, on products not included in the value of their outputs)"".[5]  An IMF publication states that, ""GDP measures the monetary value of final goods and services—that are bought by the final user—produced in a country in a given period of time (say a quarter or a year).""[6] Total GDP can also be broken down into the contribution of each industry or sector of the economy.[7] The ratio of GDP to the total population of the region is the per capita GDP and the same is called Mean Standard of Living.
 GDP is often used as a metric for international comparisons as well as a broad measure of economic progress. It is often considered to be the ""world's most powerful statistical indicator of national development and progress"".[8]"
Growth_recession,Economics,6,"Growth Recession is a term in economics that refers to a situation where economic growth is slow, but not low enough to be a technical recession, yet unemployment still increases.[1]"
Health_economics,Economics,6,"
 Health economics is a branch of economics concerned with issues related to efficiency, effectiveness, value and behavior in the production and consumption of health and healthcare.  Health economics is important in determining how to improve health outcomes and lifestyle patterns through interactions between individuals, healthcare providers and clinical settings.[2] In broad terms, health economists study the functioning of healthcare systems and health-affecting behaviors such as smoking, diabetes, and obesity.
 A seminal 1963 article by Kenneth Arrow is often credited with giving rise to health economics as a discipline. His theory drew conceptual distinctions between health and other goods.[3] Factors that distinguish health economics from other areas include extensive government intervention, intractable uncertainty in several dimensions, asymmetric information, barriers to entry,  externality and the presence of a third-party agent.[4]  In healthcare, the third-party agent is the patient's health insurer, who is financially responsible for the healthcare goods and services consumed by the insured patient. 
 Health economists evaluate multiple types of financial information: costs, charges and expenditures. 
 Uncertainty is intrinsic to health, both in patient outcomes and financial concerns. The knowledge gap that exists between a physician and a patient creates a situation of distinct advantage for the physician, which is called asymmetric information.
 Externalities arise frequently when considering health and health care, notably in the context of the health impacts as with infectious disease or opioid abuse . For example, making an effort to avoid catching the common cold affects people other than the decision maker [5][6][7][8] or finding sustainable, humane and effective solutions to the opioid epidemic.
"
Heterodox_economics,Economics,6,"Heterodox economics is any economic thought or theory that contrasts with orthodox schools of economic thought, or that may be beyond neoclassical economics.[1][2] These include institutional, evolutionary, feminist,[3] social, post-Keynesian (not to be confused with New Keynesian),[2] ecological, Georgist, Austrian, Marxian, socialist and anarchist economics, among others.[4] Economics may be called orthodox or conventional economics by its critics.[5] Alternatively, mainstream economics deals with the ""rationality–individualism–equilibrium nexus"" and heterodox economics is more ""radical"" in dealing with the ""institutions–history–social structure nexus"".[6] Many economists dismiss heterodox economics as ""fringe"" and ""irrelevant"",[7] with little or no influence on the vast majority of academic mainstream economists in the English-speaking world.
 A recent review documented several prominent groups of heterodox economists since at least the 1990s as working together with a resulting increase in coherence across different constituents.[2] Along these lines, the International Confederation of Associations for Pluralism in Economics (ICAPE) does not define ""heterodox economics"" and has avoided defining its scope. ICAPE defines its mission as ""promoting pluralism in economics.""
 In defining a common ground in the ""critical commentary,""  one writer described fellow heterodox economists as trying  to do three things: (1) identify shared ideas that generate a pattern of heterodox critique across topics and chapters of introductory macro texts; (2) give special attention to ideas that link methodological differences to policy differences; and (3) characterize the common ground in ways that permit distinct paradigms to develop common differences with textbook economics in different ways.[8] One study suggests four key factors as important to the study of economics by self-identified heterodox economists: history, natural systems, uncertainty, and power.[9]"
Household,Economics,6,"A household consists of one or more people who live in the same dwelling and share meals. It may also consist of a single family or another group of people.[1] A dwelling is considered[by whom?] to contain multiple households if meals or living spaces are not shared. The household is the basic unit of analysis in many social, microeconomic and government models, and is important to economics and inheritance.[2] Household models include families, blended families, shared housing, group homes, boarding houses, houses of multiple occupancy (UK), and single room occupancy (US). In feudal societies, the Royal Household and medieval households of the wealthy included  servants and other retainers.
"
Housing_starts,Economics,6,"Housing starts is an economic indicator that reflects the number of privately owned new houses (technically housing units) on which construction has been started in a given period. These data are divided into three types: single-family houses, townhouses or small condos, and apartment buildings with five or more units.
 Each apartment unit is considered a single start. The construction of a 30-unit apartment building is counted as 30 housing starts.
"
Hyperinflation,Economics,6,"
 In economics, hyperinflation is very high and typically accelerating inflation. It quickly erodes the real value of the local currency, as the prices of all goods increase. This causes people to minimize their holdings in that currency as they usually switch to more stable foreign currencies, in recent history often the US dollar.[1] Prices typically remain stable in terms of other relatively stable currencies. 
 Unlike low inflation, where the process of rising prices is protracted and not generally noticeable except by studying past market prices, hyperinflation sees a rapid and continuing increase in nominal prices, the nominal cost of goods, and in the supply of currency.[2] Typically, however, the general price level rises even more rapidly than the money supply as people try ridding themselves of the devaluing currency as quickly as possible. As this happens, the real stock of money (i.e., the amount of circulating money divided by the price level) decreases considerably.[3] Almost all hyperinflations have been caused by government budget deficits financed by currency creation. Hyperinflation is often associated with some stress to the government budget, such as wars or their aftermath, sociopolitical upheavals, a collapse in aggregate supply or one in export prices, or other crises that make it difficult for the government to collect tax revenue. A sharp decrease in real tax revenue coupled with a strong need to maintain government spending, together with an inability or unwillingness to borrow, can lead a country into hyperinflation.[3]"
Implicit_cost,Economics,6,"In economics, an implicit cost, also called an imputed cost and payment is not made other, implied cost, or notional cost, is the opportunity cost equal to what a firm must give up in order to use a factor of production for which it already owns and thus does not pay rent. It is the opposite of an explicit cost, which is borne directly.[1] In other words, an implicit cost is any cost that results from using an asset instead of renting it out or selling it. The term also applies to foregone income from choosing not to work.
 Implicit costs also represent the divergence between economic profit (total revenues minus total costs, where total costs are the sum of implicit and explicit costs) and accounting profit (total revenues minus only explicit costs). Since economic profit includes these extra opportunity costs, it will always be less than or equal to accounting profit.[2] Lipsey (1975) uses the example of a firm sitting on an expensive plot worth $10,000 a month in rent which it bought for a mere $50 a hundred years before. If the firm cannot obtain a profit after deducting $10,000 a month for this implicit cost, it ought to move premises (or close down completely) and take the rent instead.[1] In calculating this figure, the firm ought to ignore the figure of $50, and remember instead to look at the land's current value.[1]"
Import_quota,Economics,6,"An import quota is a type of trade restriction that sets a physical limit on the quantity of a good that can be imported into a country in a given period of time.[1] Quotas, like other trade restrictions, are typically used to benefit the producers of a good in that economy.
"
Import,Economics,6,"An import in the receiving country is an export from the sending country. Importation and exportation are the defining financial transactions of international trade.[3] In international trade, the importation and exportation of goods are limited by import quotas and mandates from the customs authority. The importing and exporting jurisdictions may impose a tariff (tax) on the goods. In addition, the importation and exportation of goods are subject to trade agreements between the importing and exporting jurisdictions.
"
Incentive,Economics,6,"An incentive is something that motivates or drives one to do something or behave in a certain way.[1] There are two type of incentives that affect human decision making. These are: intrinsic and extrinsic incentives. Intrinsic incentives are those that motivate a person to do something out of their own self interest or desires, without any outside pressure or promised reward.[2] However, extrinsic incentives are motivated by rewards such as an increase in pay for achieving a certain result; or avoiding punishments such as disciplinary action or criticism as a result of not doing something.[1][2]   
 Some examples of extrinsic incentives are letter grades in the formal school system, monetary bonuses for increased productivity or withholding of pay for underperforming in the workplace.[2] Examples of intrinsic incentives include wanting to learn a new language to be able to speak to locals in a foreign country or learning how to paint for self enjoyment. 
 In the context of economics, incentives are most studied in the area of personnel economics where human resources management practices focus on how firms manage employee incentives such as pay and career concerns, compensation and performance evaluation.[3]"
Income,Economics,6,"Income is the consumption and saving opportunity gained by an entity within a specified timeframe, which is generally expressed in monetary terms.[1][2][3] For households and individuals, ""income is the sum of all the wages, salaries, profits, interest payments, rents, and other forms of earnings received in a given period of time.""[4] (also known as gross income). Net income is defined as the gross income minus taxes and other deductions (e.g., mandatory pension contributions), and is usually the basis to calculate how much income tax is owed. 
 In the field of public economics, the concept may comprise the accumulation of both monetary and non-monetary consumption ability, with the former (monetary) being used as a proxy for total income.
 For a firm, gross income can be defined as sum of all revenue minus the cost of goods sold. Net income nets out expenses: net income equals revenue minus cost of goods sold, expenses, depreciation, interest, and taxes.[3]"
Income_distribution,Economics,6,"In economics, income distribution covers how a country's total GDP is distributed amongst its population.[1] Economic theory and economic policy have long seen income and its distribution as a central concern. Classical economists such as Adam Smith (1723-1790), Thomas Malthus (1766-1834), and David Ricardo (1772-1823) concentrated their attention on factor income-distribution, that is, the distribution of income between the primary factors of production ( land, labour and capital). Modern economists have also addressed issues of income distribution, but have focussed more on the distribution of income across individuals and households. Important theoretical and policy concerns include the balance between income inequality and economic growth, and their often inverse relationship.[2] The Lorenz curve can represent the distribution of income within a society. The Lorenz curve is closely associated with measures of income inequality, such as the Gini coefficient.
"
Income_effect,Economics,6,"The theory of consumer choice is the branch of microeconomics that relates preferences to consumption expenditures and to consumer demand curves. It analyzes how consumers maximize the 
desirability of their consumption as measured by their preferences subject to limitations on their expenditures, by maximizing utility subject to a consumer budget constraint.[1] Consumption is separated from production, logically, because two different economic agents are involved. In the first case consumption is by the primary individual; in the second case, a producer might make something that he would not consume himself. Therefore, different motivations and abilities are involved. The models that make up consumer theory are used to represent prospectively observable demand patterns for an individual buyer on the hypothesis of constrained optimization.  Prominent variables used to explain the rate at which the good is purchased (demanded) are the price per unit of that good, prices of related goods, and wealth of the consumer.
 The law of demand states that the rate of consumption falls as the price of the good rises, even when the consumer is monetarily compensated for the effect of the higher price; this is called the substitution effect. As the price of a good rises, consumers will substitute away from that good, choosing more of other alternatives. If no compensation for the price rise occurs, as is usual, then the decline in overall purchasing power due to the price rise leads, for most goods, to a further decline in the quantity demanded; this is called the income effect.
 In addition, as the wealth of the individual rises, demand for most products increases, shifting the demand curve higher at all possible prices.
 The basic problem of consumer theory takes the following inputs:
"
Indifference_curve,Economics,6,"In economics, an indifference curve connects points on a graph representing different quantities of two goods, points between which a consumer is indifferent. That is, any combinations of two products indicated by the curve will provide the consumer with equal levels of utility, and the consumer has no preference for one combination or bundle of goods over a different combination on the same curve. One can also refer to each point on the indifference curve as rendering the same level of utility (satisfaction) for the consumer. In other words, an indifference curve is the locus of various points showing different combinations of two goods providing equal utility to the consumer. Utility is then a device to represent preferences rather than something from which preferences come.[1] The main use of indifference curves is in the representation of potentially observable demand patterns for individual consumers over commodity bundles.[2] There are infinitely many indifference curves: one passes through each combination. A collection of (selected) indifference curves, illustrated graphically, is referred to as an indifference map. ""Slope of IC curve is MRS (Marginal rate of substitution)"" MRS fall which lead to 
convex shaped IC curve""
"
Individual_Retirement_Account,Economics,6,"An individual retirement account[1] (IRA) in the United States is a form of ""individual retirement plan"",[2] provided by many financial institutions, that provides tax advantages for retirement savings. An individual retirement account is a type of ""individual retirement arrangement""[3] as described in IRS Publication 590, individual retirement arrangements (IRAs).[4] The term IRA, used to describe both individual retirement accounts and the broader category of individual retirement arrangements, encompasses an individual retirement account; a trust or custodial account set up for the exclusive benefit of taxpayers or their beneficiaries; and an individual retirement annuity,[5] by which the taxpayers purchase an annuity contract or an endowment contract from a life insurance company.[6]"
Industrial_organization,Economics,6,"In economics, industrial organization is a field that builds on the theory of the firm by examining the structure of (and, therefore, the boundaries between) firms and markets. Industrial organization adds real-world complications to the perfectly competitive model, complications such as transaction costs,[1] limited information, and barriers to entry of new firms that may be associated with imperfect competition. It analyzes determinants of firm and market organization and behavior on a continuum between competition[2] and monopoly,[3] including from government actions.
 There are different approaches to the subject. One approach is descriptive in providing an overview of industrial organization, such as measures of competition and the size-concentration of firms in an industry. A second approach uses microeconomic models to explain internal firm organization and market strategy, which includes internal research and development along with issues of internal reorganization and renewal.[4] A third aspect is oriented to public policy related to economic regulation,[5] antitrust law,[6] and, more generally, the economic governance of law in defining property rights, enforcing contracts, and providing organizational infrastructure.[7][8] The extensive use of game theory in industrial economics has led to the export of this tool to other branches of microeconomics, such as behavioral economics and corporate finance. Industrial organization has also had significant practical impacts on antitrust law and competition policy.[9] The development of industrial organization as a separate field owes much to Edward Chamberlin,[10] Joan Robinson, Edward S. Mason,[11] J. M. Clark,[12] Joe S. Bain[13] and Paolo Sylos Labini, among others.[14][15]"
Industry_(economics),Economics,6,"In macroeconomics, an industry is a branch of an economy that produces a closely related set of raw materials, goods, or services.[1] For example, one might refer to the wood industry or the insurance industry.
 For a single group or company, its dominant source of revenue is typically used to classify it within a specific industry.[2] However, a single business need not belong to one industry, such as when a large business is diversified across separate industries (often referred to as a conglomerate).
 Because industries are tied to specific products, processes, and consumer markets, they can evolve over time. One distinct industry (for example, barrelmaking) may become limited to a tiny niche market and mostly absorbed into another industry using new techniques. At the same time, entirely new industries may branch off from older ones once a significant market becomes apparent (as the semiconductor industry developed from the wider electronics industry).
 Industry classification is valuable for economic analysis because it leads to largely distinct categories with simple relationships. However, more complex cases, such as otherwise different processes yielding similar products, require an element of standardization and prevent any one schema from fitting all possible uses.
 Economic theories group industries further into economic sectors.
"
Inelastic_demand,Economics,6,"Price elasticity of demand (or elasticity), is the degree to which the effective desire for something changes as its price changes. In general, people desire things less as those things become more expensive. However, for some products, the customer's desire could drop sharply even with a little price increase, and for other products, it could stay almost the same even with a big price increase. Economists use the term elasticity to denote this sensitivity to price increases. More precisely, price elasticity gives the percentage change in quantity demanded when there is a one percent increase in price, holding everything else constant.
 Price elasticities are almost always negative, although analysts tend to ignore the sign even though this can lead to ambiguity. Only goods which do not conform to the law of demand, such as Veblen and Giffen goods, have a positive elasticity. In general, the demand for a good is said to be inelastic (or relatively inelastic) when the elasticity is less than one (in absolute value): that is, changes in price have a relatively small effect on the quantity of the good demanded. The demand for a good is said to be elastic (or relatively elastic) when its elasticity is greater than one.
 Revenue is maximised when price is set so that the elasticity is exactly one. The  good's elasticity can also be used to predict the incidence (or ""burden"") of a tax on that good. Various research methods are used to determine price elasticity, including test markets, analysis of historical sales data and conjoint analysis.   
Price elasticity of demand further divided into:
Perfectly Elastic Demand		(∞),
Perfectly Inelastic Demand		( 0 ),
Relatively Elastic Demand		(> 1),
Relatively Inelastic Demand		(< 1),
Unitary Elasticity Demand 		(= 1).
"
Inflation,Economics,6,"
 In economics, inflation (or less frequently, price inflation) is a general rise in the price level in an economy over a period of time, resulting in a sustained drop in the purchasing power of money.[1][2][3][4]
When the general price level rises, each unit of currency buys fewer goods and services; consequently, inflation reflects a reduction in the purchasing power per unit of money –  a loss of real value in the medium of exchange and unit of account within the economy.[5][6] The opposite of inflation is deflation, a sustained decrease in the general price level of goods and services. The common measure of inflation is the inflation rate, the annualized percentage change in a general price index, usually the consumer price index, over time.[7] Economists generally believe that very high rates of inflation and hyperinflation are harmful, and are caused by an excessive growth of the money supply.[8] Views on which factors determine low to moderate rates of inflation are more varied. Low or moderate inflation may be attributed to fluctuations in real demand for goods and services, or changes in available supplies such as during scarcities.[9] However, the consensus view is that a long sustained period of inflation is caused by money supply growing faster than the rate of economic growth.[10][11] Inflation affects economies in various positive and negative ways. The negative effects of inflation include an increase in the opportunity cost of holding money, uncertainty over future inflation which may discourage investment and savings, and if inflation were rapid enough, shortages of goods as consumers begin hoarding out of concern that prices will increase in the future. Positive effects include reducing unemployment due to nominal wage rigidity,[12] allowing the central bank more leeway in carrying out monetary policy, encouraging loans and investment instead of money hoarding, and avoiding the inefficiencies associated with deflation.
 Today, most economists favor a low and steady rate of inflation.[13] Low (as opposed to zero or negative) inflation reduces the severity of economic recessions by enabling the labor market to adjust more quickly in a downturn, and reduces the risk that a liquidity trap prevents monetary policy from stabilizing the economy.[14] The task of keeping the rate of inflation low and stable is usually given to monetary authorities. Generally, these monetary authorities are the central banks that control monetary policy through the setting of interest rates, through open market operations, and through the setting of banking reserve requirements.[15]"
Information_economics,Economics,6,"Information economics or the economics of information
is a branch of microeconomic theory that studies how information and information systems affect an economy and economic decisions.  Information has special characteristics: It is easy to create but hard to trust. It is easy to spread but hard to control.  It influences many decisions.   These special characteristics (as compared with other types of goods) complicate many standard economic theories.[1] The subject of ""information economics"" is treated under Journal of Economic Literature classification code JEL D8 – Information, Knowledge, and Uncertainty. The present article reflects topics included in that code. There are several subfields of information economics. Information as signal has been described as a kind of negative measure of uncertainty.[2] It includes complete and scientific knowledge as special cases. The first insights in information economics related to the economics of information goods.
 In recent decades, there have been influential advances in the study of information asymmetries[3] and their implications for contract theory, including market failure as a possibility.[4] Information economics is formally related to game theory as two different types of games that may apply, including games with perfect information,[5] complete information,[6] and incomplete information.[7] Experimental and game-theory methods have been developed to model and test theories of information economics,[8] including potential public-policy applications such as mechanism design to elicit information-sharing and otherwise welfare-enhancing behavior.[9]"
Interest,Economics,6,"
 Interest, in finance and economics, is payment from a borrower or deposit-taking financial institution to a lender or depositor of an amount above repayment of the principal sum (that is, the amount borrowed), at a particular rate.[1] It is distinct from a fee which the borrower may pay the lender or some third party. It is also distinct from dividend which is paid by a company to its shareholders (owners) from its profit or reserve, but not at a particular rate decided beforehand, rather on a pro rata basis as a share in the reward gained by risk taking entrepreneurs when the revenue earned exceeds the total costs.[2][3] For example, a customer would usually pay interest to borrow from a bank, so they pay the bank an amount which is more than the amount they borrowed; or a customer may earn interest on their savings, and so they may withdraw more than they originally deposited. In the case of savings, the customer is the lender, and the bank plays the role of the borrower.
 Interest differs from profit, in that interest is received by a lender, whereas profit is received by the owner of an asset, investment or enterprise. (Interest may be part or the whole of the profit on an investment, but the two concepts are distinct from each other from an accounting perspective.)
 The rate of interest is equal to the interest amount paid or received over a particular period divided by the principal sum borrowed or lent (usually expressed as a percentage).
 Compound interest means that interest is earned on prior interest in addition to the principal. Due to compounding, the total amount of debt grows exponentially, and its mathematical study led to the discovery of the number e.[4] In practice, interest is most often calculated on a daily, monthly, or yearly basis, and its impact is influenced greatly by its compounding rate.
"
Interest_rate,Economics,6,"An interest rate is the amount of interest due per period, as a proportion of the amount lent, deposited or borrowed (called the principal sum). The total interest on an amount lent or borrowed depends on the principal sum, the interest rate, the compounding frequency, and the length of time over which it is lent, deposited or borrowed.
 It is defined as the proportion of an amount loaned which a lender charges as interest to the borrower, normally expressed as an annual percentage.[1] It is the rate a bank or other lender charges to borrow its money, or the rate a bank pays its savers for keeping money in an account.[2] The annual interest rate is the rate over a period of one year. Other interest rates apply over different periods, such as a month or a day, but they are usually annualised.
"
International_economics,Economics,6,"International economics is concerned with the effects upon economic activity from international differences in productive resources and  consumer preferences and the international institutions that affect them. It seeks to explain the patterns  and consequences  of transactions and interactions  between  the inhabitants of different countries, including trade, investment and transaction.[1]"
Intertemporal_choice,Economics,6,"Intertemporal choice is the process by which people make decisions about what and how much to do at various points in time, when choices at one time influence the possibilities available at other points in time. These choices are influenced by the relative value people assign to two or more payoffs at different points in time. Most choices require decision-makers to trade off costs and benefits at different points in time. These decisions may be about saving, work effort, education, nutrition, exercise, health care and so forth. Greater preference for immediate smaller rewards has been associated with many negative outcomes ranging from lower salary[1] to drug addiction.[2] Since early in the twentieth century, economists have analyzed intertemporal decisions using the discounted utility  model, which assumes that people evaluate the pleasures and pains resulting from a decision in much the same way that financial markets evaluate losses and gains, exponentially 'discounting' the value of outcomes according to how delayed they are in time. Discounted utility has been used to describe how people actually make intertemporal choices and it has been used as a tool for public policy. Policy decisions about how much to spend on research and development, health and education all depend on the discount rate used to analyze the decision.[3]"
Inventory_bounce,Economics,6,"Inventory bounce is a term used in economics to describe an economy's bounce back to normal GDP levels after a recession. It is also sometimes called Inventory bounce-back.[1] Firms usually keep a certain amount of inventory. When an economy faces a recession, sales might be unexpectedly low, which results in unexpectedly high inventory. In the next period, firms cut production so that inventory will drop to their desired levels, which results in even lower GDP. Subsequently, firms might increase the production back up to maintain the usual level of inventory, which causes the GDP to bounce back. This bounce back is called an inventory bounce. We care about it because if GDP recovers is only an inventory bounce, the recovery of GDP might not be sustained, which means that economy might not have truly recovered from the recession.[2]"
Investment,Economics,6,"To invest is to allocate money in the expectation of some benefit in the future. 
 In finance, the benefit from an investment is called a return. The return may consist of a gain or a loss realized from the sale of a property or an investment, unrealized capital appreciation (or depreciation), or investment income such as dividends, interest, rental income etc., or a combination of capital gain and income. The return may also include currency gains or losses due to changes in the foreign currency exchange rates.
 Investors generally expect higher returns from riskier investments. When a low-risk investment is made, the return is also generally low. Similarly, high risk comes with high returns. 
 Investors, particularly novices, are often advised to adopt a particular investment strategy and diversify their portfolio. Diversification has the statistical effect of reducing overall risk.
"
Investment_fund,Economics,6,"An investment fund is a way of investing money alongside other investors in order to benefit from the inherent advantages of working as part of a group such as reducing the risks of the investment by a significant percentage. These advantages include an ability to:
 It remains unclear whether professional active investment managers can reliably enhance risk adjusted returns by an amount that exceeds fees and expenses of investment management. Terminology varies with country but investment funds are often referred to as investment pools, collective investment vehicles, collective investment schemes, managed funds, or simply funds.  The regulatory term is undertaking for collective investment in transferable securities, or short collective investment undertaking (cf. Law). An investment fund may be held by the public, such as a mutual fund, exchange-traded fund, special-purpose acquisition company or closed-end fund,[1] or it may be sold only in a private placement, such as a hedge fund or private equity fund.[2]  The term also includes specialized vehicles such as collective and common trust funds, which are unique bank-managed funds structured primarily to commingle assets from qualifying pension plans or trusts.[3] Investment funds are promoted with a wide range of investment aims either targeting specific geographic regions (e.g., emerging markets or Europe) or specified industry sectors (e.g., technology). Depending on the country there is normally a bias towards the domestic market due to familiarity, and the lack of currency risk. Funds are often selected on the basis of these specified investment aims, their past investment performance, and other factors such as fees.
"
Invisible_hand,Economics,6,"The invisible hand describes the unintended social benefits of an individual's self-interested actions, a concept that was first introduced by Adam Smith in The Theory of Moral Sentiments, written in 1759, invoking it in reference to income distribution.[1] By the time he wrote The Wealth of Nations in 1776, Smith had studied the economic models of the French Physiocrats for many years, and in this work, the invisible hand is more directly linked to production, to the employment of capital in support of domestic industry. The only use of ""invisible hand"" found in The Wealth of Nations is in Book IV, Chapter II, ""Of Restraints upon the Importation from foreign Countries of such Goods as can be produced at Home."" The exact phrase is used just three times in Smith's writings.
 Smith may have come up with the two meanings of the phrase from Richard Cantillon who developed both economic applications in his model of the isolated estate.[2] The idea of trade and market exchange automatically channeling self-interest toward socially desirable ends is a central justification for the laissez-faire economic philosophy, which lies behind neoclassical economics.[3] In this sense, the central disagreement between economic ideologies can be viewed as a disagreement about how powerful the ""invisible hand"" is. In alternative models, forces that were nascent during Smith's lifetime, such as large-scale industry, finance, and advertising, reduce its effectiveness.[4] Interpretations of the term have been generalized beyond the usage by Smith.
"
IS%E2%80%93LM_model,Economics,6,"The IS–LM model, or Hicks–Hansen model, is a  two-dimensional macroeconomic tool that shows the relationship between interest rates and assets market (also known as real output in goods and services market plus money market).[citation needed] The intersection of the ""investment–saving"" (IS) and ""liquidity preference–money supply"" (LM) curves models ""general equilibrium"" where supposed simultaneous equilibria occur in both the goods and the asset markets.[1] Yet two equivalent interpretations are possible: first, the IS–LM model explains changes in national income when price level is fixed short-run; second, the IS–LM model shows why an aggregate demand curve can shift.[2]
Hence, this tool is sometimes used not only to analyse economic fluctuations but also to suggest potential levels for appropriate stabilisation policies.[3] The model was developed by John Hicks in 1937,[4] and later extended by Alvin Hansen,[5] as a mathematical representation of Keynesian macroeconomic theory. Between the 1940s and mid-1970s, it was the leading framework of macroeconomic analysis.[6] While it has been largely absent from macroeconomic research ever since, it is still a backbone conceptual introductory tool in many macroeconomics textbooks.[7] By itself, the IS–LM model is used to study the short run when prices are fixed or sticky and no inflation is taken into consideration. But in practice the main role of the model is as a path to explain the AD–AS model.[2]"
JEL_classification_codes,Economics,6,"Articles in economics journals are usually given classification codes according to a system originated by the Journal of Economic Literature.  The JEL is published quarterly by the American Economic Association (AEA) and contains survey articles and information on recently published books and dissertations. The AEA maintains EconLit, a searchable data base of citations for articles, books, reviews, dissertations, and working papers classified by JEL codes for the years from 1969. A recent addition to EconLit is indexing of economics-journal articles from 1886 to 1968[1] parallel to the print series Index of Economic Articles.[2]"
Job_hunting,Economics,6,"
Job hunting, job seeking, or job searching is the act of looking for employment, due to unemployment, underemployment, discontent with a current position, or a desire for a better position.
 As of 2010, less than 10% of U.S. jobs are filled through online ads.[1]"
Joint_product_pricing,Economics,6,"In microeconomics, joint product pricing is the firm's problem of choosing prices for joint products, which are two or more products produced from the same process or operation, each considered to be of value. Pricing for joint products is more complex than pricing for a single product. To begin with, there are two demand curves. The characteristics of each could be different. Demand for one product could be greater than for the other. Consumers of one product could be more price elastic than consumers of the other (and therefore more sensitive to changes in the product's price).
 To complicate things further, both products, because they are produced jointly, share a common marginal cost curve. There are also complexities in the production function. Their production could be linked in the sense that they are bi-products (referred to as complements in production) or in the sense that they can be produced by the same inputs (referred to as substitutes in production). Further, production of the joint product could be in fixed proportions or in variable proportions.
"
Just_price,Economics,6,"The just price is a theory of ethics in economics that attempts to set standards of fairness in transactions. With intellectual roots in ancient Greek philosophy, it was advanced by Thomas Aquinas based on an argument against usury, which in his time referred to the making of any rate of interest on loans. It gave rise to the contractual principle of laesio enormis.
"
Keynesian_economics,Economics,6,"
 Keynesian economics (/ˈkeɪnziən/ KAYN-zee-ən; sometimes Keynesianism, named for the economist John Maynard Keynes) are various macroeconomic theories about how economic output is strongly influenced by aggregate demand (total spending in the economy). In the Keynesian view, aggregate demand does not necessarily equal the productive capacity of the economy. Instead, it is influenced by a host of factors. According to Keynes, the productive capacity of the economy sometimes behaves erratically, affecting production, employment, and inflation.[1] Keynesian economics developed during and after the Great Depression from the ideas presented by Keynes in his 1936 book, The General Theory of Employment, Interest and Money.[2] Keynes' approach was a stark contrast to the aggregate supply-focused classical economics that preceded his book. Interpreting Keynes's work is a contentious topic, and several schools of economic thought claim his legacy.
 Keynesian economics, as part of the neoclassical synthesis, served as the standard macroeconomic model in the developed nations during the later part of the Great Depression, World War II, and the post-war economic expansion (1945–1973). It lost some influence following the Nixon shock, oil shock and resulting stagflation of the 1970s.[3] Keynesian economics was later redeveloped as New Keynesian economics, becoming part of the contemporary new neoclassical synthesis.[4] The advent of the financial crisis of 2007–2008 caused a resurgence of popular interest in Keynesian thought.[5] Keynesian economists generally argue that aggregate demand is volatile and unstable. They propose that a market economy often experiences inefficient macroeconomic outcomes in the form of economic recessions (when demand is low) and inflation (when demand is high), and that these can be mitigated by economic policy responses. In particular, monetary policy actions by the central bank and fiscal policy actions by the government can help stabilize output over the business cycle.[6] Keynesian economists generally advocate a market economy – predominantly private sector, but with an active role for government intervention during recessions and depressions.[7]"
Wage_labour,Economics,6,"Wage labour (also wage labor in American English), usually referred to as paid work, paid employment, or paid labour, refers to the socioeconomic relationship between a worker and an employer in which the worker sells their labour power under a formal or informal employment contract.[1] These transactions usually occur in a labour market where wages or salaries are market-determined.
[2] In exchange for the money paid as wages (usual for short-term work-contracts) or salaries (in permanent employment contracts), the work product generally becomes the undifferentiated property of the employer, except  for special cases such as the vesting of intellectual property patents in the United States where patent rights are usually vested in the employee personally responsible for the invention. A wage labourer is a person whose primary means of income is from the selling of their labour in this way.
"
Labor_economics,Economics,6,"Labour economics seeks to understand the functioning and dynamics of the markets for wage labour. Labour is a commodity that is supplied by labourers in exchange for a wage paid by demanding firms.[1] Labour markets or job markets function through the interaction of workers and employers. Labour economics looks at the suppliers of labour services (workers) and the demanders of labour services (employers), and attempts to understand the resulting pattern of wages, employment, and income. Labour markets are normally geographically bounded, but the rise of the internet has brought about a 'planetary labour market' in some sectors.[2] Labour is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. Some theories focus on human capital (referring to the skills that workers possess, not necessarily their actual work). Labour is unique to study because it is a special type of good that cannot be separated from the owner (i.e. the work cannot be separated from the person who does it). A labour market is also different from other markets in that workers are the suppliers and firms are the demanders.[1]"
Laissez-faire,Economics,6,"Laissez-faire (/ˌlɛseɪˈfɛər/; French: [lɛsefɛʁ] (listen); from French: laissez faire, lit. 'let do') is an economic system in which transactions between private groups of people are absent of any form of economic interventionism such as regulation and subsidies. As a system of thought, laissez-faire rests on the axioms[1] that the individual is the basic unit in society and has a natural right to freedom; that the physical order of nature is a harmonious and self-regulating system; and that corporations are creatures of the state and therefore the citizens must watch them closely due to their propensity to disrupt the Smithian spontaneous order.[2] These axioms constitute the basic elements of laissez-faire thought. Another basic principle holds that markets should be competitive, a rule that the early advocates of laissez-faire always emphasized.[1] With the aims of maximizing freedom and of allowing markets to self-regulate, early advocates of laissez-faire proposed a impôt unique, a tax on land rent (similar to Georgism) to replace all taxes that they saw as damaging welfare by penalizing production.[3] Proponents of laissez-faire argue for a complete separation of government from the economic sector.[4][verification needed] The phrase laissez-faire is part of a larger French phrase and literally translates to ""let [it/them] do"", but in this context the phrase usually means to ""let it be"" and in expression ""laid back.""[5] Laissez-faire capitalism started being practiced in the mid-18th century and was further popularized by Adam Smith's book The Wealth of Nations.[6][7] It has been most prominent in Britain and the United States in the 19th century. While associated with capitalism in common usage, there are also non-capitalist forms of laissez-faire, including some forms of market socialism.
"
Law_of_Demand,Economics,6,"In microeconomics, the law of demand is a fundamental principle which states that, ""conditional on all else being equal, as the price of a good increases (↑), quantity demanded will decrease (↓); conversely, as the price of a good decreases (↓), quantity demanded will increase (↑)"".[1] The only factor which influences the quantity demanded is the price. The law of demand is the inverse relationship between demand and price. [2] It also “works with the law of supply to explain how market economies allocate resources and determine the prices of goods and services that we observe in everyday transactions” [3] The law of demand describes an inverse relationship between price and quantity demanded of a good. Alternatively, other things being constant, quantity demanded of a commodity is inversely related to the price of the commodity. For example, a consumer may demand 2 kgs of apples at $70 per kg; he may, however, demand 1 kg if the price rises to $80 per kg. This has been the general human behaviour on relationship between the price of the commodity and the quantity demanded. The factors held constant refer to other determinants of demand, such as the prices of other goods and the consumer's income.[4] There are, however, some possible exceptions to the law of demand, such as Giffen goods and Veblen goods.
 "
Law_of_increasing_costs,Economics,6,"In economics, the law of increasing costs is a principle that states that once all factors of production (land, labor, capital) are at maximum output and efficiency, producing more will cost more than average. As production increases, the opportunity cost does as well. The best way to look at this is to review an example of an economy that only produces two things - cars and oranges.  If all the resources of the economy are put into producing only oranges, there will not be any factors of production available to produce cars.  So the result is an output of X number of oranges but 0 cars.  The reverse is also true - if all the factors of production are used for the production of cars, 0 oranges will be produced.  In between these two extremes are situations where some oranges and some cars are produced.  There are three assumptions that are made in this possibility.  The economy is experiencing full employment (everyone who wants to work has a job), the best technology is being used and production efficiency is being maximized.  So the question becomes, what is the cost of producing more oranges or cars?  If the economy is at the maximum for all inputs, then the cost of each unit will be more expensive.  The economy will have to incur more variable costs, such as overtime, to produce the unit.
 The law also applies to switching production in a maxed out economy. Essentially, the economy is still producing more, so the law still applies. The only difference is that resources are being taken from one area and applied to another, instead of simply producing more of the same (as assumed in the first paragraph)[1]"
Law_of_supply,Economics,6,"The law of supply is a fundamental principle of economic theory which states that, keeping other factors constant, an increase in price results in an increase in quantity supplied.[1] In other words, there is a direct relationship between price and quantity: quantities respond in the same direction as price changes. This means that producers are willing to offer more of a product for sale on the market at higher prices by increasing production as a way of increasing profits.[2] In short, the law of supply is a positive relationship between quantity supplied and price and is the reason for the upward slope of the supply curve.
"
Lease,Economics,6,"
 A lease is a contractual arrangement calling for the lessee (user) to pay the lessor (owner) for use of an asset.[1] Property, buildings and vehicles are common assets that are leased. Industrial or business equipment is also leased.
 Broadly put, a lease agreement is a contract between two parties, the lessor and the lessee. The lessor is the legal owner of the asset, while the lessee obtains the right to use the asset in return for regular rental payments.[2] The lessee also agrees to abide by various conditions regarding their use of the property or equipment. For example, a person leasing a car may agree to the condition that the car will only be used for personal use.
 The term rental agreement can refer to two kinds of leases. First is a lease in which the asset is tangible property.[3] Here, the user rents the asset (e.g. land or goods) let out or rented out by the owner. (The verb to lease is less precise because it can refer to either of these actions.)[4] Examples of a lease for intangible property include use of a computer program (similar to a license, but with different provisions), or use of a radio frequency (such as a contract with a cell-phone provider).
 Rental agreement can also refer to a periodic lease agreement (most often a month-to-month lease) internationally and in some regions of the United States.[5]"
Leprechaun_economics,Economics,6,"
 Leprechaun economics was the 26.3 per cent rise in Irish 2015 GDP, later revised to 34.4 per cent,[a] in a 12 July 2016 publication by the Irish Central Statistics Office (CSO), restating 2015 Irish national accounts.[5][6][7][8] The term was coined by Nobel Prize–winning economist Paul Krugman to mark the fact that the long-term distortion of Irish economic data by tax-driven accounting flows had reached a climax;[1][2][9] by 2020, Krugman said the term was a feature of all tax havens.[10] While the event that caused the artificial Irish GDP growth occurred in Q1 2015, the Irish CSO had to delay its GDP revision, and redact the release of its regular economic data in 2016–2017 to protect the source's identity, as required by Irish law.[11]  Only in Q1 2018 could economists confirm Apple as the source [12][13][14] and that leprechaun economics was the largest ever base erosion and profit shifting (BEPS) action,[15] as well as the largest hybrid–tax inversion of a U.S. corporation.[3][16] Leprechaun economics marked the replacement of Ireland's prohibited BEPS tool, the Double Irish, with the more powerful Capital Allowances for Intangible Assets (CAIA) tool, or  ""Green Jersey"".[13][14]  Apple used the CAIA BEPS tool to restructure out of its hybrid–Double Irish BEPS tool, on which the EU Commission would levy a €13 billion fine in August 2016 for illegal avoidance of Irish taxes.[b][13][14]  As a result of Leprechaun economics, Ireland, already qualified as a ""major tax haven"", was estimated by academics as the world's largest tax haven.[18][19][20][21] Leprechaun economics had follow-on consequences. In September 2016, Ireland became the first of the major tax havens to be ""blacklisted"" by a G20 economy, Brazil.[22] In February 2017, Ireland replaced GDP with ""Modified GNI (or GNI*)"" (2017 Irish GDP was 162% of 2017 Irish GNI*, where as EU–28 2017 GDP was 100% of GNI).[23][24]  In December 2017, the U.S and the EU introduced countermeasures to Irish BEPS tools.[25][26][27] In October 2018, Ireland introduced a ""reverse Leprechaun tax"", to discourage IP from leaving Ireland.[28]  In 2018, the OECD showed Ireland's public ""debt metrics"" differ dramatically depending on whether Debt-to-GDP, Debt-to-GNI* or Debt-per-Capita is used;[29][30][31] and in 2019, the IMF estimated 60 per cent of Irish foreign direct investment was ""phantom"".[32][33][34]"
Liability_(financial_accounting),Economics,6,"In financial accounting, a liability is defined as the future sacrifices of economic benefits that the entity is
obliged to make to other entities as a result of past transactions or other past events,[1] the settlement of which may result in the transfer or use of assets, provision of services or other yielding of economic benefits in the future.
 A liability is defined by the following characteristics:
 Liabilities in financial accounting need not be legally enforceable; but can be based on equitable obligations or constructive obligations. An equitable obligation is a duty based on ethical or moral considerations. A constructive obligation is an obligation that is implied by a set of circumstances in a particular situation, as opposed to a contractually based obligation.
 The accounting equation relates assets, liabilities, and owner's equity:
 The accounting equation is the mathematical structure of the balance sheet.
 Probably the most accepted accounting definition of liability is the one used by the International Accounting Standards Board (IASB). The following is a quotation from IFRS Framework:
 A liability is a present obligation of the enterprise arising from past events, the settlement of which is expected to result in an outflow from the enterprise of resources embodying economic benefits Regulations as to the recognition of liabilities are different all over the world, but are roughly similar to those of the IASB.
 Examples of types of liabilities include: money owing on a loan, money owing on a mortgage, or an IOU.
 Liabilities are debts and obligations of the business they represent as creditor's claim on business assets.
"
Loan,Economics,6,"In finance, a loan is the lending of money by one or more individuals, organizations, or other entities to other individuals, organizations etc. The recipient (i.e., the borrower) incurs a debt and is usually liable to pay interest on that debt until it is repaid as well as to repay the principal amount borrowed.
 The document evidencing the debt (e.g., a promissory note) will normally specify, among other things, the principal amount of money borrowed, the interest rate the lender is charging, and the date of repayment. A loan entails the reallocation of the subject asset(s) for a period of time, between the lender and the borrower.
 The interest provides an incentive for the lender to engage in the loan. In a legal loan, each of these obligations and restrictions is enforced by contract, which can also place the borrower under additional restrictions known as loan covenants. Although this article focuses on monetary loans, in practice, any material object might be lent.
 Acting as a provider of loans is one of the main activities of financial institutions such as banks and credit card companies. For other institutions, issuing of debt contracts such as bonds is a typical source of funding.
"
Local_tax,Economics,6,"A comparison of tax rates by countries is difficult and somewhat subjective, as tax laws in most countries are extremely complex and the tax burden falls differently on different groups in each country and sub-national unit. The list focuses on the main indicative types of taxes: corporate tax, individual income tax, and sales tax, including VAT and GST, but does not list capital gains tax.
 Some other taxes (for instance property tax, substantial in many countries, such as the United States) and payroll tax are not shown here. The table is not exhaustive in representing the true tax burden to either the corporation or the individual in the listed country. The tax rates displayed are marginal and do not account for deductions, exemptions or rebates. The effective rate is usually lower than the marginal rate. The tax rates given for federations (such as the United States and Canada) are averages and vary depending on the state or province. Territories that have different rates to their respective nation are in italics.
"
Long_run,Economics,6,"In economics the long run is a theoretical concept in which all markets are in equilibrium, and all prices and quantities have fully adjusted and are in equilibrium. The long run contrasts with the short run, in which there are some constraints and markets are not fully in equilibrium. 
 More specifically, in microeconomics there are no fixed factors of production in the long run, and there is enough time for adjustment so that there are no constraints preventing changing the output level by changing the capital stock or by entering or leaving an industry. This contrasts with the short run, where some factors are variable (dependent on the quantity produced) and others are fixed (paid once), constraining entry or exit from an industry.  In macroeconomics, the long run is the period when the general price level,  contractual wage rates, and expectations adjust fully to the state of the economy, in contrast to the short run when these variables may not fully adjust.[1][2]"
Macroeconomics,Economics,6,"Macroeconomics (from the Greek prefix makro- meaning ""large"" + economics) is a branch of economics dealing with the performance, structure, behavior, and decision-making of an economy as a whole. This includes regional, national, and global economies.[1][2] Macroeconomists study topics such as GDP, unemployment rates, national income, price indices, output, consumption, unemployment, inflation, saving, investment, energy, international trade, and international finance.
 Macroeconomics and microeconomics are the two most general fields in economics.[3] The United Nations Sustainable Development Goal 17 has a target to enhance global macroeconomic stability through policy coordination and coherence as part of the 2030 Agenda.[4] In an opinion piece expressing despondency in the field shortly before his retirement, economist Robert J. Samuelson expressed that Macroeconomics ""means using interest rates, taxes and government spending to regulate an economy’s growth and stability.""[opinion][5]"
Managerial_economics,Economics,6,"Managerial economics is a branch of economics which deals with the application of the economic concepts, theories, tools, and methodologies to solve practical problems in a business these business decisions not only affect daily decisions, also affects the economic power of long-term planning decisions, its theory is mainly around the demand, production, cost, market and so on several factors.
In other words, managerial economics is a combination of economics theory and managerial theory. It helps the manager in decision-making and acts as a link between practice and theory.[1] It is sometimes referred to as business economics and is a branch of economics that applies microeconomic analysis to decision methods of businesses or other management units.
 As such, it bridges economic theory and economics in practice.[2] It draws heavily from quantitative techniques such as regression analysis, correlation and calculus.[3] If there is a unifying theme that runs through most of managerial economics, it is the attempt to optimize business decisions given the firm's objectives and given constraints imposed by scarcity, for example through the use of operations research, mathematical programming, game theory for strategic decisions,[4] and other computational methods.[5]"
Marginal_cost,Economics,6,"In economics, marginal cost is the change in the total cost that arises when the quantity produced is incremented by one unit; that is, it is the cost of producing one more unit of a good.[1] Intuitively, marginal cost at each level of production includes the cost of any additional inputs required to produce the next unit. At each level of production and time period being considered, marginal costs include all costs that vary with the level of production, whereas other costs that do not vary with production are fixed and thus have no marginal cost. For example, the marginal cost of producing an automobile will generally include the costs of labor and parts needed for the additional automobile but not the fixed costs of the factory that have already been incurred. In practice, marginal analysis is segregated into short and long-run cases, so that, over the long run, all costs (including fixed costs) become marginal. Where there are economies of scale, prices set at marginal cost will fail to cover total costs, thus requiring a subsidy. Marginal cost pricing is not a matter of merely lowering the general level of prices with the aid of a subsidy; with or without subsidy it calls for a drastic restructuring of pricing practices, with opportunities for very substantial improvements in efficiency at critical points.[2] If the cost function 



C


{  C}
 is continuous and differentiable, the marginal cost 



M
C


{  MC}
 is the first derivative of the cost function with respect to the output quantity 



Q


{  Q}
:[3] The marginal cost can be a function of quantity if the cost function is non-linear. If the cost function is not differentiable, the marginal cost can be expressed as follows:
 where 



Δ


{  \Delta }
 denotes an incremental change of one unit.
"
Marginal_product_of_labor,Economics,6,"In economics, the marginal product of labor (MPL) is the change in output that results from employing an added unit of labor.[1] It is a feature of the production function, and depends on the amounts of physical capital and labor already in use.
"
Marginal_propensity_to_consume,Economics,6,"In economics, the marginal propensity to consume (MPC) is a metric that quantifies induced consumption, the concept that the increase in personal consumer spending (consumption) occurs with an increase in disposable income (income after taxes and transfers). The proportion of disposable income which individuals spend on consumption is known as propensity to consume. MPC is the proportion of additional income that an individual consumes. For example, if a household earns one extra dollar of disposable income, and the marginal propensity to consume is 0.65, then of that dollar, the household will spend 65 cents and save 35 cents. Obviously, the household cannot spend more than the extra dollar (without borrowing).
 According to John Maynard Keynes, marginal propensity to consume is less than one.[1]"
Marginal_revenue,Economics,6,"Marginal revenue (or marginal benefit) is a central concept in microeconomics that describes the additional total revenue generated by increasing product sales by 1 unit.[1][2][3][4][5] To derive the value of marginal revenue, it is required to examine the difference between the aggregate benefits a firm received from the quantity of a good and service produced last period and the current period with one extra unit increase in the rate of production.[6] Marginal revenue is a fundamental tool for economic decision making within a firm's setting, together with marginal cost to be considered.[7] In a perfectly competitive market, the incremental revenue generated by selling an additional unit of a good is equal to the price the firm is able to charge the buyer of the good.[3][8]  This is because a firm in a competitive market will always get the same price for every unit it sells regardless of the number of units the firm sells since the firm's sales can never impact the industry's price.[1][3]Therefore, in a perfectly competitive market, firms set the price level equal to their marginal revenue 



(
M
R
=
P
)


{  (MR=P)}
.[6] In imperfect competition, a monopoly firm is a large producer in the market and changes in its output levels impact market prices, determining the whole industry's sales. Therefore, a monopoly firm lowers its price on all units sold in order to increase output (quantity) by 1 unit.[1][3][6] Since a reduction in price leads to a decline in revenue on each good sold by the firm, the marginal revenue generated is always lower than the price level charged 



(
M
R
<
P
)


{  (MR<P)}
.[1][3][6] The marginal revenue (the increase in total revenue) is the price the firm gets on the additional unit sold, less the revenue lost by reducing the price on all other units that were sold prior to the decrease in price. Marginal revenue is the concept of a firm sacrificing the opportunity to sell the current output at a certain price, in order to sell a higher quantity at a reduced price.[6] Profit maximization occurs at the point where marginal revenue (MR) equals marginal cost (MC). If 



M
R
>
M
C


{  MR>MC}
 then a profit-maximizing firm will increase output to generate more profit, while if 



M
R
<
M
C


{  MR<MC}
 then the firm will decrease output to gain additional profit. Thus the firm will choose the profit-maximizing level of output for which 



M
R
=
M
C


{  MR=MC}
.[9]"
Marginal_utility,Economics,6,"In economics, utility is the satisfaction or benefit derived by consuming a product; thus the marginal utility of a good or service is the change in the utility from an increase in the consumption of that good or service.
 In the context of cardinal utility, economists sometimes speak of a law of diminishing marginal utility, meaning that the first unit of consumption of a good or service yields more utility than the second and subsequent units, with a continuing reduction for greater amounts. Therefore, the fall in marginal utility as consumption increases is known as diminishing marginal utility. This concept is used by economists to determine how much of a good a consumer is willing to purchase.
"
Marginal_value,Economics,6,"A marginal value is
 (This third case is actually a special case of the second).
 In the case of differentiability, at the limit, a marginal change is a mathematical differential, or the corresponding mathematical derivative.
 These uses of the term “marginal” are especially common in economics, and result from conceptualizing constraints as borders or as margins.[1]  The sorts of marginal values most common to economic analysis are those associated with unit changes of resources and, in mainstream economics, those associated with infinitesimal changes.  Marginal values associated with units are considered because many decisions are made by unit, and marginalism explains unit price in terms of such marginal values.  Mainstream economics uses infinitesimal values in much of its analysis for reasons of mathematical tractability.
"
Marginalism,Economics,6,"
 Marginalism is a theory of economics that attempts to explain the discrepancy in the value of goods and services by reference to their secondary, or marginal, utility. The reason why the price of diamonds is higher than that of water, for example, owes to the greater additional satisfaction of the diamonds over the water. Thus, while the water has greater total utility, the diamond has greater marginal utility.
 Although the central concept of marginalism is that of marginal utility, marginalists, following the lead of Alfred Marshall, drew upon the idea of marginal physical productivity in explanation of cost. The neoclassical tradition that emerged from British marginalism abandoned the concept of utility and gave marginal rates of substitution a more fundamental role in analysis.[citation needed] Marginalism is an integral part of mainstream economic theory.
"
Market_(economics),Economics,6,"A market is one of a composition of systems, institutions, procedures, social relations or infrastructures whereby parties engage in exchange. While parties may exchange goods and services by barter, most markets rely on sellers offering their goods or services (including labour power) in exchange for money from buyers. It can be said that a market is the process by which the prices of goods and services are established. Markets facilitate trade and enable the distribution and resource allocation in a society. Markets allow any trade-able item to be evaluated and priced. A market emerges more or less spontaneously or may be constructed deliberately by human interaction in order to enable the exchange of rights (cf. ownership) of services and goods. Markets generally supplant gift economies and are often held in place through rules and customs, such as a booth fee, competitive pricing, and source of goods for sale (local produce or stock registration).
 Markets can differ by products (goods, services) or factors (labour and capital) sold, product differentiation, place in which exchanges are carried, buyers targeted, duration, selling process, government regulation, taxes, subsidies, minimum wages, price ceilings, legality of exchange, liquidity, intensity of speculation, size, concentration, exchange asymmetry, relative prices, volatility and geographic extension. The geographic boundaries of a market may vary considerably, for example the food market in a single building, the real estate market in a local city, the consumer market in an entire country, or the economy of an international trade bloc where the same rules apply throughout. Markets can also be worldwide, see for example the global diamond trade. National economies can also be classified as developed markets or developing markets.
 In mainstream economics, the concept of a market is any structure that allows buyers and sellers to exchange any type of goods, services and information. The exchange of goods or services, with or without money, is a transaction.[1] Market participants consist of all the buyers and sellers of a good who influence its price, which is a major topic of study of economics and has given rise to several theories and models concerning the basic market forces of supply and demand. A major topic of debate is how much a given market can be considered to be a ""free market"", that is free from government intervention. Microeconomics traditionally focuses on the study of market structure and the efficiency of market equilibrium; when the latter (if it exists) is not efficient, then economists say that a market failure has occurred. However, it is not always clear how the allocation of resources can be improved since there is always the possibility of government failure.
"
Market_basket,Economics,6,"
 
 A market basket or commodity bundle is a fixed list of items, in given proportions.  Its most common use is to track the progress of inflation in an economy or specific market. That is, to measure the changes in the value of money over time.  A market basket is also used with the theory of purchasing price parity to measure the value of money in different places.
"
Market_economy,Economics,6,"A market economy is an economic system in which the decisions regarding investment, production and distribution are guided by the price signals created by the forces of supply and demand. The major characteristic of a market economy is the existence of factor markets that play a dominant role in the allocation of capital and the factors of production.[1][2] Market economies range from minimally regulated free-market and laissez-faire systems where state activity is restricted to providing public goods and services and safeguarding private ownership,[3] to interventionist forms where the government plays an active role in correcting market failures and promoting social welfare to market socialism involving worker ownership of enterprise but in a market framework. State-directed or dirigist economies are those where the state plays a directive role in guiding the overall development of the market through industrial policies or indicative planning—which guides yet does not substitute the market for economic planning—a form sometimes referred to as a mixed economy.[4][5] Market economies are contrasted with planned economies where investment and production decisions are embodied in an integrated economy-wide economic plan. In a centrally planned economy, economic planning is the principal allocation mechanism between firms rather than markets, with the economy's means of production being owned and operated by a single organizational body.[6][better source needed]"
Market_failure,Economics,6,"In neoclassical economics, market failure is a situation in which the allocation of goods and services by a free market is not Pareto efficient, often leading to a net loss of economic value.[1] Market failures can be viewed as scenarios where individuals' pursuit of pure self-interest leads to results that are not efficient– that can be improved upon from the societal point of view.[2][3] The first known use of the term by economists was in 1958,[4] but the concept has been traced back to the Victorian philosopher Henry Sidgwick.[5]
Market failures are often associated with public goods,[6] time-inconsistent preferences,[7] information asymmetries,[8] non-competitive markets, principal–agent problems, or externalities.[9] The existence of a market failure is often the reason that self-regulatory organizations, governments or supra-national institutions intervene in a particular market.[10][11] Economists, especially microeconomists, are often concerned with the causes of market failure and possible means of correction.[12] Such analysis plays an important role in many types of public policy decisions and studies. 
 However, government policy interventions, such as taxes, subsidies, wage and price controls, and regulations, may also lead to an inefficient allocation of resources, sometimes called government failure.[13] Given the tension between the economic costs caused by market failure and costs caused by ""government failure"", policymakers attempting to maximize economic value are sometimes (but not always) faced with a choice between two inefficient outcomes, i.e. inefficient market outcomes with or without government interventions. 
 Most mainstream economists believe that there are  circumstances (like building codes or endangered species) in which it is possible for government or other organizations to improve the inefficient market outcome.  Several heterodox schools of thought disagree with this as a matter of ideology.[14] An ecological market failure exists when human activity in a market economy is exhausting critical non-renewable resources, disrupting fragile ecosystems, or overloading biospheric waste absorption capacities. In none of these cases does the criterion of Pareto efficiency obtain.[15]"
Market_structure,Economics,6,"Market Structure in economics, depicts how firms are differentiated and categorised based on types of goods they sell (homogenous/hetergenous) and how their operations are affected by external factors and elements. Market structure makes it easier to understand the characteristics of diverse markets.
"
Market_system,Economics,6,"A market system (or market ecosystem[1]) is any systematic  process enabling many market players to bid and ask: helping bidders and sellers interact and make deals.  It is not just the price mechanism but the entire system of regulation, qualification, credentials, reputations and clearing that surrounds that mechanism and makes it operate in a social context.[2] Some authors use the term ""market system"" to refer to specifically to the free market system.[3] This article focuses on the more general sense of the term according to which there are a variety of different market systems.
 Market systems are different from voting systems. A market system relies on buyers and sellers being constantly involved and unequally enabled; in a voting system, candidates seek the support of voters on a less regular basis. In addition (a) buyers make decisions on their own behalves, whereas voters make decisions for collectives, (b) voters are usually fully aware of their participation in social decision-making, whereas buyers are often unaware of the secondary repercussions of their acts, (c) responsibility for making purchasing decisions is concentrated on the individual buyer, whereas responsibility for making collective decisions is divided, (d) different buying decisions at the same time are made under conditions of scarcity --- the selection of one thing precludes the selection of another, whereas different voting decisions are not --- one can vote for a president and a judge in the same election without one vote precluding the other, and (e) under ordinary conditions, a buyer is choosing to buy an actual good and is therefore never overruled in his choice, whereas it is the nature of voting that the voter is choosing among potential alternatives and may be overruled by other voters.[4] However, the interactions between market and voting systems are an important aspect of political economy,[5] and some argue they are hard to differentiate; for example, systems like cumulative voting and runoff voting involve a degree of market-like bargaining and trade-off, rather than simple statements of choice.
"
Mercantilism,Economics,6,"Mercantilism is an economic policy that is designed to maximize the exports and minimize the imports for an economy.  It promotes imperialism, tariffs and subsidies on traded goods to achieve that goal. The policy aims to reduce a possible current account deficit or reach a current account surplus, and it includes measures aimed at accumulating monetary reserves by a positive balance of trade, especially of finished goods. Historically, such policies frequently led to war and motivated colonial expansion.[1] Mercantilist theory varies in sophistication from one writer to another and has evolved over time.
 Mercantilism was dominant in modernized parts of Europe, and some areas in Africa from the 16th to the 18th centuries, a period of proto-industrialization,[2] before it fell into decline, but some commentators argue that it is still practiced in the economies of industrializing countries,[3] in the form of economic interventionism.[4][5][6][7][8] It promotes government regulation of a nation's economy for the purpose of augmenting state power at the expense of rival national powers. High tariffs, especially on manufactured goods, were almost universally a feature of mercantilist policy.[9] With the efforts of supranational organizations such as the World Trade Organization to reduce tariffs globally, non-tariff barriers to trade have assumed a greater importance in neomercantilism.
"
Microeconomics,Economics,6,"
 Microeconomics (from Greek prefix mikro- meaning ""small"" + economics) is a branch of economics that studies the behavior of individuals and firms in making decisions regarding the allocation of scarce resources and the interactions among these individuals and firms.[1][2][3] One goal of microeconomics is to analyze the market mechanisms that establish relative prices among goods and services and allocate limited resources among alternative uses. Microeconomics shows conditions under which free markets lead to desirable allocations. It also analyzes market failure, where markets fail to produce efficient results.
 While microeconomics focuses on firms and individuals, macroeconomics focuses on the sum total of economic activity, dealing with the issues of growth, inflation, and unemployment and with national policies relating to these issues.[2] Microeconomics also deals with the effects of economic policies (such as changing taxation levels) on microeconomic behavior and thus on the aforementioned aspects of the economy.[4] Particularly in the wake of the Lucas critique, much of modern macroeconomic theories has been built upon microfoundations—i.e. based upon basic assumptions about micro-level behavior.
"
Monetarism,Economics,6,"Monetarism is a school of thought in monetary economics that emphasizes the role of governments in controlling the amount of money in circulation. Monetarist theory asserts that variations in the money supply have major influences on national output in the short run and on price levels over longer periods. Monetarists assert that the objectives of monetary policy are best met by targeting the growth rate of the money supply rather than by engaging in discretionary monetary policy.[1] Monetarism today is mainly associated with the work of Milton Friedman, who was among the generation of economists to accept Keynesian economics and then criticise Keynes's theory of fighting economic downturns using fiscal policy (government spending). Friedman and Anna Schwartz wrote an influential book, A Monetary History of the United States, 1867–1960, and argued ""inflation is always and everywhere a monetary phenomenon"".[2] Though he opposed the existence of the Federal Reserve,[3] Friedman advocated, given its existence, a central bank policy aimed at keeping the growth of the money supply at a rate commensurate with the growth in productivity and demand for goods.
"
Monetary_economics,Economics,6,"Monetary economics is the branch of economics that studies the different competing theories of money: it provides a framework for analyzing money and considers its functions (such as medium of exchange, store of value and unit of account), and it considers how money, for example fiat currency, can gain acceptance purely because of its convenience as a public good.[1] The discipline has historically prefigured, and remains integrally linked to, macroeconomics.[2] This branch also examines the effects of monetary systems, including regulation of money and associated financial institutions[3] and international aspects.[4] Modern analysis has attempted to provide microfoundations for the demand for money[5]  and to distinguish valid nominal and real monetary relationships for micro or macro uses, including their influence on the  aggregate demand for output.[6] Its methods include deriving and testing the implications of money as a substitute for other assets[7] and as based on explicit frictions.[8]"
Monetary_policy,Economics,6,"Monetary policy is policy adopted by the monetary authority of a nation to control either the interest rate payable for very short-term borrowing (borrowing by banks from each other to meet their short-term needs) or the money supply, often as an attempt to reduce inflation or the interest rate to ensure price stability and general trust of the value and stability of the nation's currency.[1][2][3] Unlike fiscal policy, which relies on taxation, government spending, and government borrowing,[4] as methods for a government to manage business cycle phenomena such as recessions, monetary policy is a modification of the supply of money, i.e. 'printing' more money or decreasing the money supply by changing interest rates or removing excess reserves.
 Further purposes of a monetary policy are usually to contribute to the stability of gross domestic product, to achieve and maintain low unemployment, and to maintain predictable exchange rates with other currencies.
 Monetary economics can provide insight into crafting optimal monetary policy. In developed countries, monetary policy is generally formed separately from fiscal policy.
 Monetary policy is referred to as being either expansionary or contractionary.
 Expansionary policy occurs when a monetary authority uses its procedures to stimulate the economy. An expansionary policy maintains short-term interest rates at a lower than usual rate or increases the total supply of money in the economy more rapidly than usual. It is traditionally used to try to reduce unemployment during a recession by decreasing interest rates in the hope that less expensive credit will entice businesses into borrowing more money and thereby expanding. This would increase aggregate demand (the overall demand for all goods and services in an economy), which would increase short-term growth as measured by increase of gross domestic product (GDP). Expansionary monetary policy, by increasing the amount of currency in circulation, usually diminishes the value of the currency relative to other currencies (the exchange rate), in which case foreign purchasers will be able to purchase more with their currency in the country with the devalued currency.[5] Contractionary monetary policy maintains short-term interest rates greater than usual, slows the rate of growth of the money supply, or even decreases it to slow short-term economic growth and lessen inflation. Contractionary monetary policy can result in increased unemployment and depressed borrowing and spending by consumers and businesses, which can eventually result in an economic recession if implemented too vigorously.[6]"
Monetary_system,Economics,6,"A monetary system is a system by which a government provides money in a country's economy. Modern monetary systems usually consist of the national treasury, the mint, the central banks and commercial banks.[1]"
Money,Economics,6,"
 Money is any item or verifiable record that is generally accepted as payment for goods and services and repayment of debts, such as taxes, in a particular country or socio-economic context.[1][2][3] The main functions of money are distinguished as: a medium of exchange, a unit of account, a store of value and sometimes, a standard of deferred payment.[4][5] Any item or verifiable record that fulfils these functions can be considered as money.
 Money is historically an emergent market phenomenon establishing a commodity money, but nearly all contemporary money systems are based on fiat money.[4] Fiat money, like any check or note of debt, is without use value as a physical commodity.[citation needed] It derives its value by being declared by a government to be legal tender; that is, it must be accepted as a form of payment within the boundaries of the country, for ""all debts, public and private"".[6][better source needed] Counterfeit money can cause good money to lose its value.
 The money supply of a country consists of currency (banknotes and coins) and, depending on the particular definition used, one or more types of bank money (the balances held in checking accounts, savings accounts, and other types of bank accounts). Bank money, which consists only of records (mostly computerized in modern banking), forms by far the largest part of broad money in developed countries.[7][8][9]"
Money_supply,Economics,6,"
 In macroeconomics, the money supply (or money stock) is the total value of money available in an economy at a point of time. There are several ways to define ""money"", but standard measures usually include currency in circulation and demand deposits (depositors' easily accessed assets on the books of financial institutions).[1][2] The central bank of each country may use a definition of what constitutes money for its purposes.
 Money supply data is recorded and published, usually by the government or the central bank of the country. Public and private sector analysts monitor changes in the money supply because of the belief that such changes affect the price levels of securities, inflation, the exchange rates, and the business cycle.[3] The relationship between money and prices has historically been associated with the quantity theory of money. There is strong empirical evidence of a direct relationship between the growth of the money supply and long-term price inflation, at least for rapid increases in the amount of money in the economy. For example, a country such as Zimbabwe which saw extremely rapid increases in its money supply also saw extremely rapid increases in prices (hyperinflation). This is one reason for the reliance on monetary policy as a means of controlling inflation.[4][5] The nature of this causal chain is the subject of some debate. Some heterodox economists argue that the money supply is endogenous (determined by the workings of the economy, not by the central bank) and that the sources of inflation must be found in the distributional structure of the economy.[6] In addition, those economists seeing the central bank's control over the money supply as feeble say that there are two weak links between the growth of the money supply and the inflation rate. First, in the aftermath of a recession, when many resources are underutilized, an increase in the money supply can cause a sustained increase in real production instead of inflation. Second, if the velocity of money (i.e., the ratio between nominal GDP and money supply) changes, an increase in the money supply could have either no effect, an exaggerated effect, or an unpredictable effect on the growth of nominal GDP.
"
Monopolistic_competition,Economics,6,"
 Monopolistic competition is a type of imperfect competition such that there are many producers competing against each other, but selling products that are differentiated from one another (e.g. by branding or quality) and hence are not perfect substitutes. In monopolistic competition, a firm takes the prices charged by its rivals as given and ignores the impact of its own prices on the prices of other firms.[1][2] In the presence of coercive government, monopolistic competition will fall into government-granted monopoly. Unlike perfect competition, the firm maintains spare capacity. Models of monopolistic competition are often used to model industries. Textbook examples of industries with market structures similar to monopolistic competition include restaurants, cereal, clothing, shoes, and service industries in large cities. The ""founding father"" of the theory of monopolistic competition is Edward Hastings Chamberlin, who wrote a pioneering book on the subject, Theory of Monopolistic Competition (1933).[3] Joan Robinson published a book The Economics of Imperfect Competition with a comparable theme of distinguishing perfect from imperfect competition. Further work on monopolistic competition was undertaken by Dixit and Stiglitz who created the Dixit-Stiglitz model which has proved applicable used in the sub fields of international trade theory, macroeconomics and economic geography. 
 Monopolistically competitive markets have the following characteristics:
 The long-run characteristics of a monopolistically competitive market are almost the same as a perfectly competitive market.  Two differences between the two are that monopolistic competition produces heterogeneous products and that monopolistic competition involves a great deal of non-price competition, which is based on subtle product differentiation.  A firm making profits in the short run will nonetheless only break even in the long run because demand will decrease and average total cost will increase.  This means in the long run, a monopolistically competitive firm will make zero economic profit. This illustrates the amount of influence the firm has over the market; because of brand loyalty, it can raise its prices without losing all of its customers. This means that an individual firm's demand curve is downward sloping, in contrast to perfect competition, which has a perfectly elastic demand schedule.
"
Monopoly,Economics,6,"A monopoly (from Greek μόνος, mónos, 'single, alone' and πωλεῖν, pōleîn, 'to sell') exists when a specific person or enterprise is the only supplier of a particular commodity. This contrasts with a monopsony which relates to a single entity's control of a market to purchase a good or service, and with oligopoly and duopoly which consists of a few sellers dominating a market.[1] Monopolies are thus characterized by a lack of economic competition to produce the good or service, a lack of viable substitute goods, and the possibility of a high monopoly price well above the seller's marginal cost that leads to a high monopoly profit.[2] The verb monopolise or monopolize refers to the process by which a company gains the ability to raise prices or exclude competitors. In economics, a monopoly is a single seller. In law, a monopoly is a business entity that has significant market power, that is, the power to charge overly high prices.[3] Although monopolies may be big businesses, size is not a characteristic of a monopoly. A small business may still have the power to raise prices in a small industry (or market).[3] A monopoly may also have monopsony control of a sector of a market. Likewise, a monopoly should be distinguished from a cartel (a form of oligopoly), in which several providers act together to coordinate services, prices or sale of goods. Monopolies, monopsonies and oligopolies are all situations in which one or a few entities have market power and therefore interact with their customers (monopoly or oligopoly), or suppliers (monopsony) in ways that distort the market.[citation needed] Monopolies can be established by a government, form naturally, or form by integration. In many jurisdictions, competition laws restrict monopolies due to government concerns over potential adverse effects. Holding a dominant position or a monopoly in a market is often not illegal in itself, however certain categories of behavior can be considered abusive and therefore incur legal sanctions when business is dominant. A government-granted monopoly or legal monopoly, by contrast, is sanctioned by the state, often to provide an incentive to invest in a risky venture or enrich a domestic interest group. Patents, copyrights, and trademarks are sometimes used as examples of government-granted monopolies. The government may also reserve the venture for itself, thus forming a government monopoly, for example with a state-owned company.[citation needed] Monopolies may be naturally occurring due to limited competition because the industry is resource intensive and requires substantial costs to operate (e.g., certain railroad systems).
"
Monopsony,Economics,6,"In economics, a monopsony is a market structure in which a single buyer substantially controls the market as the major purchaser of goods and services offered by many would-be sellers. The microeconomic theory of monopsony assumes a single entity to have market power over all sellers as the only purchaser of a good or service. This is a similar power to that of a monopolist, which can influence the price for its buyers in a monopoly, where multiple buyers have only one seller of a good or service available to purchase from.
"
Mortgage,Economics,6,"
 A mortgage loan or simply mortgage (/ˈmɔːrɡɪdʒ/) is a loan used either by purchasers of real property to raise funds to buy real estate, or alternatively by existing property owners to raise funds for any purpose while putting a lien on the property being mortgaged. The loan is ""secured"" on the borrower's property through a process known as mortgage origination. This means that a legal mechanism is put into place which allows the lender to take possession and sell the secured property (""foreclosure"" or ""repossession"") to pay off the loan in the event the borrower defaults on the loan or otherwise fails to abide by its terms. The word mortgage is derived from a Law French term used in Britain in the Middle Ages meaning ""death pledge"" and refers to the pledge ending (dying) when either the obligation is fulfilled or the property is taken through foreclosure.[1] A mortgage can also be described as ""a borrower giving consideration in the form of a collateral for a benefit (loan)"".
 Mortgage borrowers can be individuals mortgaging their home or they can be businesses mortgaging commercial property (for example, their own business premises, residential property let to tenants, or an investment portfolio). The lender will typically be a financial institution, such as a bank, credit union or building society, depending on the country concerned, and the loan arrangements can be made either directly or indirectly through intermediaries. Features of mortgage loans such as the size of the loan, maturity of the loan, interest rate, method of paying off the loan, and other characteristics can vary considerably. The lender's rights over the secured property take priority over the borrower's other creditors, which means that if the borrower becomes bankrupt or insolvent, the other creditors will only be repaid the debts owed to them from a sale of the secured property if the mortgage lender is repaid in full first.
 In many jurisdictions, it is normal for home purchases to be funded by a mortgage loan. Few individuals have enough savings or liquid funds to enable them to purchase property outright. In countries where the demand for home ownership is highest, strong domestic markets for mortgages have developed. Mortgages can either be funded through the banking sector (that is, through short-term deposits) or through the capital markets through a process called ""securitization"", which converts pools of mortgages into fungible bonds that can be sold to investors in small denominations.
"
Motivation,Economics,6,"Motivation is a reason for actions, willingness, and goals. Motivation is derived from the word motive, or a need that requires satisfaction. These needs, wants or desires may be acquired through influence of culture, society, lifestyle, or may be generally innate. An individual's motivation may be inspired by outside forces (extrinsic motivation)[1] or by themselves (intrinsic motivation).[2] Motivation has been considered one of the most important reasons to move forward.[3] Motivation results from the interaction of both conscious and unconscious factors. Mastering motivation to allow sustained and deliberate practice is central to high levels of achievement, e.g. in elite sport, medicine, or music.[4]
Motivation governs choices among alternative forms of voluntary activity.[5]"
Multiplier_(economics),Economics,6,"In macroeconomics, a multiplier is a factor of proportionality that measures how much an endogenous variable changes in response to a change in some exogenous variable.
 For example, suppose variable x 
changes by 1 unit, which causes another variable y to change by M units. Then the multiplier is M.
"
Mutual_fund,Economics,6,"A mutual fund is an open-end professionally managed investment fund[1] that pools money from many investors to purchase securities. Mutual funds are ""the largest proportion of equity of U.S. corporations.""[2]:2 Mutual fund investors may be retail or institutional in nature. The term is typically used in the United States, Canada, and India, while similar structures across the globe include the SICAV in Europe ('investment company with variable capital') and open-ended investment company (OEIC) in the UK.
 Mutual funds have advantages and disadvantages compared to direct investing in individual securities. The advantages of mutual funds include economies of scale, diversification, liquidity, and professional management. However, these come with mutual fund fees and expenses.
 Primary structures of mutual funds are open-end funds, unit investment trusts, closed-end funds and exchange-traded funds (ETFs). 
 Mutual funds are often classified by their principal investments as money market funds, bond or fixed income funds, stock or equity funds, hybrid funds, or other. Funds may also be categorized as index funds, which are passively managed funds that match the performance of an index, or actively managed funds. Hedge funds are not mutual funds as hedge funds cannot be sold to the general public.
"
Nash_equilibrium,Economics,6,"In game theory, the Nash equilibrium, named after the mathematician John Forbes Nash Jr., is a proposed solution of a non-cooperative game involving two or more players in which each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy.[1]The utilization of Nash Equilibriums, and its principals date data back to the time of Cournot, a prominent Philosopher and mathematician who pioneered the understanding of economic equilibria.  [2] If each player has chosen a strategy—an action plan choosing its own action based on what it has seen happen so far in the game—and no player can increase its own expected payoff by changing its strategy while the other players keep theirs unchanged, then the current set of strategy choices constitutes a Nash equilibrium.
 If two players Alice and Bob choose strategies A and B, (A, B) is a Nash equilibrium if Alice has no other strategy available that does better than A at maximizing her payoff in response to Bob choosing B, and Bob has no other strategy available that does better than B at maximizing his payoff in response to Alice choosing A. In a game in which Carol and Dan are also players, (A, B, C, D) is a Nash equilibrium if A is Alice's best response to (B, C, D), B is Bob's best response to (A, C, D), and so forth.
 Nash showed that there is a Nash equilibrium for every finite game: see further the article on strategy.
"
National_tax,Economics,6,"A comparison of tax rates by countries is difficult and somewhat subjective, as tax laws in most countries are extremely complex and the tax burden falls differently on different groups in each country and sub-national unit. The list focuses on the main indicative types of taxes: corporate tax, individual income tax, and sales tax, including VAT and GST, but does not list capital gains tax.
 Some other taxes (for instance property tax, substantial in many countries, such as the United States) and payroll tax are not shown here. The table is not exhaustive in representing the true tax burden to either the corporation or the individual in the listed country. The tax rates displayed are marginal and do not account for deductions, exemptions or rebates. The effective rate is usually lower than the marginal rate. The tax rates given for federations (such as the United States and Canada) are averages and vary depending on the state or province. Territories that have different rates to their respective nation are in italics.
"
National_wealth,Economics,6,"
 National net wealth, also known as national net worth, is the total sum of the value of a nation's assets minus its liabilities. It refers to the total value of net wealth possessed by the citizens of a nation at a set point in time.[1] This figure is an important indicator of a nation's ability to take on debt and sustain spending and is influenced not only by real estate prices, equity market prices, exchange rates, liabilities and incidence in a country of the adult population, but also human resources, natural resources and capital and technological advancements, which may create new assets or render others worthless in the future. The most significant component by far among most developed nations is commonly reported as household net wealth or worth and reflects infrastructure investment. National wealth can fluctuate, as evidenced in the United States data following the 2008 financial crisis and subsequent economic recovery. During periods when equity markets experienced strong growth, the relative national and per capita wealth of the countries where people are more exposed on those markets, such as the United States and United Kingdom, tend to rise. On the other hand, when equity markets are depressed, the relative wealth the countries where people invest more in real estate or bonds, such as France and Italy, tend to rise instead.[2]"
Natural_monopoly,Economics,6,"A natural monopoly is a monopoly in an industry in which high infrastructural costs and other barriers to entry relative to the size of the market give the largest supplier in an industry, often the first supplier in a market, an overwhelming advantage over potential competitors. This frequently occurs in industries where capital costs predominate, creating economies of scale that are large in relation to the size of the market; examples include public utilities such as water services and electricity.[1] Natural monopolies were recognized as potential sources of market failure as early as the 19th century; John Stuart Mill advocated government regulation to make them serve the public good.
"
Natural_resource_economics,Economics,6,"Natural resource economics deals with the supply, demand, and allocation of the Earth's natural resources. One main objective of natural resource economics is to better understand the role of natural resources in the economy in order to develop more sustainable methods of managing those resources to ensure their availability for future generations. Resource economists study interactions between economic and natural systems, with the goal of developing a sustainable and efficient economy.[2]"
Need,Economics,6,"A need is something that is necessary for an organism to live a healthy life. Needs are distinguished from wants. In the case of a need, a deficiency causes a clear adverse outcome:  a dysfunction or death. In other words, a need is something required for a safe, stable and healthy life (e.g. air, water, food, land, shelter) while a want is a desire, wish or aspiration. When needs or wants are backed by purchasing power, they have the potential to become economic demands. 
 Basic needs such as air, water, food and protection from environmental dangers are necessary for an organism to live. In addition to basic needs, humans also have needs of a social or societal nature such as the human need to socialise or belong to a family unit or group. Needs can be objective and physical, such as the need for food, or psychical and subjective, such as the need for self-esteem.
 Needs and wants are a matter of interest in, and form a common substrate for, the fields of philosophy, biology, psychology, social science, economics, marketing and politics.
"
Non-convexity_(economics),Economics,6,"In economics, non-convexity refers to violations of the convexity assumptions of elementary economics. Basic economics textbooks concentrate on consumers with convex preferences (that do not prefer extremes to in-between values) and convex budget sets and on producers with convex production sets; for convex models, the predicted economic behavior is well understood.[1][2]  When convexity assumptions are violated, then many of the good properties of competitive markets need not hold: Thus, non-convexity is associated with market failures,[3][4] where supply and demand differ or where market equilibria can be inefficient.[1][4][5][6][7][8] Non-convex economies are studied with nonsmooth analysis, which is a generalization of convex analysis.[8][9][10][11]"
Rivalry_(economics),Economics,6,"In economics, a good is said to be rivalrous or a rival if its consumption by one consumer prevents simultaneous consumption by other consumers,[1] or if consumption by one party reduces the ability of another party to consume it. A good is considered non-rivalrous or non-rival if, for any level of production, the cost of providing it to a marginal (additional) individual is zero.[2] A good can be placed along a continuum ranging from rivalrous to non-rivalrous. The same characteristic is sometimes referred to as jointness of supply or subtractable or non-subtractable.[3]Economist Paul Samuelson made the distinction between private and public goods in 1954 by introducing the concept of nonrival consumption. Economist Richard Musgrave followed on and added rivalry and excludability as criteria for defining consumption goods in 1959 and 1969.[4]   
 Most tangible goods, both durable and nondurable, are rival goods. A hammer is a durable rival good. One person's use of the hammer presents a significant barrier to others who desire to use that hammer at the same time. However, the first user does not ""use up"" the hammer, meaning that some rival goods can still be shared through time. An apple is a nondurable rival good: once an apple is eaten, it is ""used up"" and can no longer be eaten by others. Non-tangible goods can also be rivalrous. Examples include the ownership of radio spectra and domain names. In more general terms, almost all private goods are rivalrous.
 In contrast, non-rival goods may be consumed by one consumer without preventing simultaneous consumption by others. Most examples of non-rival goods are intangible. Broadcast television is an example of a non-rival good;  when a consumer turns on a TV set, this does not prevent the TV in another consumer's house from working.  The television itself is a rival good, but television broadcasts are non-rival goods. Other examples of non-rival goods include a beautiful scenic view, national defense, clean air, street lights, and public safety. More generally, most intellectual property is non-rival. In fact, certain types of intellectual property become more valuable as more people consume them (anti-rival). For example, the more people use a particular language, the more valuable that language becomes.
 Non-rivalry does not imply that the total production costs are low, but that the marginal production costs are zero. In reality, few goods are completely non-rival as rivalry can emerge at certain levels. For instance, use of public roads, the Internet, or police/law courts is non-rival up to a certain capacity, after which congestion means that each additional user decreases speed for others. For that, recent economic theory views rivalry as a continuum, not as a binary category,[5] where many goods are somewhere between the two extremes of completely rival and completely non-rival. A perfectly non-rival good can be consumed simultaneously by an unlimited number of consumers.
 There are four types of goods based on the characteristics of rival in consumption and excludability: Public Goods, Private Goods, Common Resources, and Club Goods.[6] Goods that are both non-rival and non-excludable are called public goods. Examples include clean air, national defense, and free-to-air broadcast TV. It is generally accepted by mainstream economists that the market mechanism will under-provide public goods, so these goods have to be produced by other means, including government provision.
 On the other hand, private goods are rival and excludable. An example of this is could be a Big Mac burger provided by McDonalds. An individual who consumes a Big Mac denies another individual from consuming the same one. It is excludable because consumption is only offered to those willing to pay the price.[7] Common resources are rival in consumption and non-excludable. An example is that of fisheries, which harvest fish from a shared common resource pool of fish stock. Fish caught by one group fishers are no longer accessible to another group, thus being rivalrous. However, oftentimes, due to an absence of well-defined property rights, it is difficult to restrict access to fishers who may overfish.[8] Goods that are both non-rival and excludable are called club goods. Cable television is an example of this. A large television service provider would already have infrastructure in place which would allow for the addition of new customers without infringing on existing customers viewing abilities. This would also mean that marginal cost would be close to zero, which satisfies the criteria for a good to be considered non-rival. However, access to cable TV services are only available to consumers willing to pay the price, demonstrating the excludability aspect.[9]    
"
Oligopoly,Economics,6,"
 An oligopoly (ολιγοπώλιο) (Greek: ὀλίγοι πωλητές ""few authorities"") is a market form wherein a market or industry is dominated by a small group of large sellers (oligopolists).  Oligopolies can result from various forms of collusion that reduce market competition which then typically leads to higher prices for consumers. Oligopolies have their own market structure.[1] With few sellers, each oligopolist is likely to be aware of the actions of the others. According to game theory, the decisions of one firm therefore influence and are influenced by decisions of other firms. Strategic planning by oligopolists needs to take into account the likely responses of the other market participants.  Entry barriers include high investment requirements, strong consumer loyalty for existing brands and economies of scale. In developed economies oligopolies dominate the economy as the perfectly competitive model is of negligible importance for consumers. Oligopolies differ from price takers in that they do not have a supply curve. Instead, they search for the best price-output combination.[2]"
Oligopsony,Economics,6,"An oligopsony (from Ancient Greek ὀλίγοι (oligoi) ""few"" + ὀψωνία (opsōnia) ""purchase"") is a market form in which the number of buyers is small while the number of sellers in theory could be large. This typically happens in a market for inputs where numerous suppliers are competing to sell their product to a small number of (often large and powerful) buyers. It contrasts with an oligopoly, where there are many buyers but few sellers. An oligopsony is a form of imperfect competition.
 The terms monopoly (one seller), monopsony (one buyer), and bilateral monopoly have a similar relationship.
"
Opportunity_cost,Economics,6,"In microeconomic theory, opportunity cost, or alternative cost, is the loss of potential gain from other alternatives when one particular alternative is chosen over the others.[1] In simple terms, opportunity cost is the loss of the benefit that could have been enjoyed had a given choice not been made.
 
As a representation of the relationship between scarcity and choice,[2] the objective of opportunity cost is to ensure efficient use of scarce resources.[3] It incorporates all associated costs of a decision, both explicit and implicit.[4] Opportunity cost also includes the utility or economic benefit an individual lost, it is indeed more than the monetary payment or actions taken. As an example, to go for a walk may not have any financial costs imbedded to it. Yet, the opportunity forgone is the time spent walking which could have been used instead for other purposes such as earning an income.[3] Regardless of the time of occurrence of an activity, if scarcity was non-existent then all demands of a person are satiated. It’s only through scarcity that choice becomes essential which results in ultimately making a selection and/or decision.[2] Sacrifice is a given measurement in opportunity cost of which the decision maker forgoes the opportunity of the next best alternative.[5] In other words, to disregard the equivalent utility of the best alternative choice to gain the utility of the best perceived option.[6] If there were decisions to be made that require no sacrifice then these would be cost free decisions with zero opportunity cost.[7]"
Organizational_economics,Economics,6,"Organizational economics (also referred to as economics of organization) involves the use of economic logic and methods to understand the existence, nature, design, and performance of organizations, especially managed ones.
 Organizational economics is primarily concerned with the obstacles to coordination of activities inside and between organizations (firms, alliances, institutions, and market as a whole).
 Organizational economics is known for its contribution to and its use of:
 Notable theorists and contributors in the field of organizational economics:[1][2][3]"
Pareto_efficiency,Economics,6,"
 Pareto efficiency or Pareto optimality is a situation where no individual or preference criterion can be better off without making at least one individual or preference criterion worse off or without any loss thereof. The concept is named after Vilfredo Pareto (1848–1923), Italian engineer and economist, who used the concept in his studies of economic efficiency and income distribution. The following three concepts are closely related:
 The Pareto frontier is the set of all Pareto efficient allocations, conventionally shown graphically. It also is variously known as the Pareto front or Pareto set.[1] ""Pareto efficiency"" is considered as a minimal notion of efficiency that does not necessarily result in a socially desirable distribution of resources: it makes no statement about equality, or the overall well-being of a society.[2][3]:46–49 In addition to the context of efficiency in allocation, the concept of Pareto efficiency also arises in the context of efficiency in production vs. x-inefficiency: a set of outputs of goods is Pareto efficient if there is no feasible re-allocation of productive inputs such that output of one product increases while the outputs of all other goods either increase or remain the same.[4]:459 Besides economics, the notion of Pareto efficiency has been applied to the selection of alternatives in engineering and biology. Each option is first assessed, under multiple criteria, and then a subset of options is ostensibly identified with the property that no other option can categorically outperform the specified option.  It is a statement of impossibility of improving one variable without harming other variables in the subject of multi-objective optimization (also termed Pareto optimization).
"
Participation_(ownership),Economics,6,"In finance, ""participation""  is an ownership interest in a mortgage or other loan. In particular, loan participation is a cooperation of multiple lenders to issue a loan (known as participation loan) to one borrower. This is usually done in order to reduce individual risks of the lenders.
 The term is also used as a synonym to profit sharing, an incentive whereby employees of a company receive a share of the profits of the company.
"
Partnership,Economics,6,"A partnership is an arrangement where parties, known as business partners, agree to cooperate to advance their mutual interests. The partners in a partnership may be individuals, businesses, interest-based organizations, schools, governments or combinations. Organizations may partner to increase the likelihood of each achieving their mission and to amplify their reach. A partnership may result in issuing and holding equity or may be only governed by a contract.
"
Per_capita,Economics,6,"Per capita is a Latin prepositional phrase: per (preposition, taking the accusative case, meaning ""by means of"") and capita (accusative plural of the noun caput, ""head""). The phrase thus means ""by heads"" or ""for each head"", i.e., per individual/person.  The term is used in a wide variety of social sciences and statistical research contexts, including government statistics, economic indicators, and built environment studies.
 It is commonly and usually used in the field of statistics in place of saying ""per person""[1] (although per caput is the Latin for ""per head""[2]).
It is also used in wills to indicate that each of the named beneficiaries should receive, by devise or bequest, equal shares of the estate.[2]  This is in contrast to a per stirpes division, in which each branch (Latin stirps, plural stirpes) of the inheriting family inherits an equal share of the estate. This is often used with the ‘2-0 rule’, a statistical principle that determines which group is larger per capita. Under the 2-0 rule, a group is the largest per capita if it has both the biggest total size and size of the group of the objects in question, therefore resulting in a 2-0 score.
"
Perfect_competition,Economics,6,"In economics, specifically general equilibrium theory, a perfect market, also known as an atomistic market, is defined by several idealizing conditions, collectively called perfect competition, or atomistic competition. In theoretical models where conditions of perfect competition hold, it has been demonstrated that a market will reach an equilibrium in which the quantity supplied for every product or service, including labor, equals the quantity demanded at the current price. This equilibrium would be a Pareto optimum.[1] Perfect competition provides both allocative efficiency and productive efficiency:
 The theory of perfect competition has its roots in late-19th century economic thought. Léon Walras[2] gave the first rigorous definition of perfect competition and derived some of its main results. In the 1950s, the theory was further formalized by Kenneth Arrow and Gérard Debreu.[3] Real markets are never perfect. Those economists who believe in perfect competition as a useful approximation to real markets may classify those as ranging from close-to-perfect to very imperfect. Share and foreign exchange markets are commonly said to be the most similar to the perfect market. The real estate market is an example of a very imperfect market. In such markets, the theory of the second best proves that if one optimality condition in an economic model cannot be satisfied, it is possible that the next-best solution involves changing other variables away from the values that would otherwise be optimal.[4]"
Personal_property,Economics,6,"Personal property is property that is movable.[1] In common law systems, personal property may also be called chattels or personalty. In civil law systems, personal property is often called movable property or movables – any property that can be moved from one location to another.
 Personal property can be understood in comparison to  real estate, immovable property or real property (such as land and buildings). 
 Movable property on land (larger livestock, for example) was not automatically sold with the land, it was ""personal"" to the owner and moved with the owner. 
 The word cattle is the Old Norman variant of Old French chatel, chattel (derived from Latin capitalis, “of the head”), which was once synonymous with general movable personal property. [2]"
Physical_capital,Economics,6,"Physical capital represents in economics one of the three primary factors of production. Physical capital is the apparatus used to produce a good and services. Physical capital represents the tangible man-made goods that help and support the production inventory, cash, equipment or real estate are all examples of physical capital
 Definition
N.G. Mankiw definition from the book Economics: 
Capital is the equipment and structures used to produce goods and services. Physical capital consists of man-made goods (or input into the process of production) that assist in the production process. Cash, real estate, equipment, and inventory are examples of physical capital.[1] Capital goods represents one of the key factors of corporation function. Generally, capital allows a company to preserve liquidity while growing operations, it refers to physical assets in business and the way a company have reached their physical capital. While referring how companies have obtained their capital it is important to consider both - physical capital and human capital.[2]  Biased on economic theory, physical capital represents one of the three primary factors of production, that is also recognized as inputs production function. The others are natural resources (including land), and labour. The word ""Physical"" is used to distinguish physical capital from human capital and financial capital. ""Physical capital"" denote to fixed capital, all other sorts of real physical asset that are not included in the production of a product is distinguished from circulating capital. [3] Physical capital in accounting
Biased on the order of solvency of a physical capital, it is listed on the balance sheet. The impact of investments of human capital and physical capital can be measured and analysed with the same ratios to measure and analyse the investment performance of physical assets. Both of these investments lead to fundamental improvements in the business model and better overall decision-making. The balance sheet provides an overview, which consist of both physical and human capital, of the value of all physical and some non-physical assets, but it also provides an overview of the capital raised to pay for those assets.
Physical capital is noted on the balance sheet as an asset at historical cost, not market value. As a result, the book value of assets is generally higher than market value. Accountants refer to physical capital as a tangible asset. Compering the physical capital and human capital is easy to find on the balance, but the human capital is often only assumed. In addition to goodwill, analysts can value the impact of human capital on operations with efficiency ratios, such as return on assets (ROA) and return on equity (ROE). The value of human capital can be also determined by the investors in the mark-up on products sold or the industry premium on salary, for instance a company is willing to pay more for an experienced programmer who can produce a higher-margin product. In this case the value of the programmer's experience is the amount the company is willing to pay over and above the market price. [2] Production function
Production function definition by N.G. Mankiw:
Production function is the relationship between the quantity of inputs used to make a good and the quantity of output of that good. [4] Co-operation of four factors of production capital, land, labour and organization crates the result in production of goods, biased on this fact no commodity is possible to produce without the help of these four factors, actually all four are usually used in some technical proportion, with the aim to maximize profit with a minimal cost by the best combination of factors of production. Best combination for producer is enabled by applying the principles of equip-marginal returns and substitution. The principle of equip-marginal returns states that, any producer can have maximum production only when the marginal returns of all the factors of production are equal. For instance, when the marginal product of the land is equal to that of labour, capital and organisation, the production becomes maximum. Production function shoes how much output producer can expect in exact proportion of labour and capital as well as of labour etc. Differently, production function is an indicator of the physical relationship between the inputs and output of a firm.
Like the demand function a production function is for a definite period. It shows the flow of inputs resulting into a flow of output during some time. The production function of a firm depends on the state of technology. With every development in technology the production function of the firm undergoes a change.
The new production function brought about by developing technology displays same inputs and more output or the same output with lesser inputs. Sometimes a new production function of the firm may be adverse as it takes more inputs to produce the same output.
Mathematical description of basic relationship between inputs and outputs:
 Q = f (L, C, N)
Q = Quantity of output
L = Labour
C = Capital
N = Land
 The level of output (Q) depends on the quantities of different inputs (L, C, N) available to the firm. In the simplest case, where there are only two inputs, labour (L) and capital (C) and one output (Q), the production function becomes.
Q =f (L, C)
The production function is a technical or engineering relation between input and output. If the natural laws of technology remain unchanged, the production function remains unchanged.
 Features of Production Function:
The production function consists of 3 main features – Substitutability, Complementarity and Specificity. 1. Substitutability: By changing the number and amount of some inputs, while the others stay unchanged, we achieve the possibility, to modify the total output. It is the substitutability of the factors of production that gives rise to the laws of variable proportions. 2. Complementarity: is that two or more inputs are to be used together as nothing will be produced if the quantity of either of the inputs used in the production process is zero. Another example of complementarity is the principles of returns to scale of inputs as it reveals that the quantity of all inputs has to increased simultaneously in order to attain a higher scale of total output. 3. Specificity: Every product has its own specific number and type of inputs. Machines and equipment's, specialized workers and raw materials or commodities are a few examples of the specificity of factors of production. This reveals that in the production process none of the factors can be ignored and in some cases ignorance to even slightest extent is not possible if the factors are perfectly specific. Production consists of time; hence, the way the inputs are combined is determined to a large extent by the time period under consideration. The greater the time period, the greater the freedom the producer must vary the quantities of various inputs used in the production process. In the production function, variation in total output by varying the quantities of all inputs is possible only in the long run whereas the variation in total output by varying the quantity of single input may be possible even in the short run. [5]"
Physiocracy,Economics,6,"Physiocracy (French: physiocratie; from the Greek for ""government of nature"") is an economic theory developed by a group of 18th-century Age of Enlightenment French economists who believed that the wealth of nations derived solely from the value of ""land agriculture"" or ""land development"" and that agricultural products should be highly priced.[1] Their theories originated in France and were most popular during the second half of the 18th century. Physiocracy became one of the first well-developed theories of economics.
 François Quesnay (1694–1774), the marquis de Mirabeau (1715-1789) and Anne-Robert-Jacques Turgot (1727–1781) dominated the movement,[2] which immediately preceded the first modern school, classical economics, which began with the publication of Adam Smith's The Wealth of Nations in 1776.
 The physiocrats made a significant contribution in their emphasis on productive work as the source of national wealth. This contrasted with earlier schools, in particular mercantilism, which often focused on the ruler's wealth, accumulation of gold, or the balance of trade. Whereas the mercantilist school of economics held that value in the products of society was created at the point of sale,[3] by the seller exchanging his products for more money than the products had ""previously"" been worth, the physiocratic school of economics was the first to see labor as the sole source of value.  However, for the physiocrats, only agricultural labor created this value in the products of society.[3]  All ""industrial"" and non-agricultural labors were ""unproductive appendages"" to agricultural labor.[3] Quesnay was likely influenced by his medical training. The earlier work of William Harvey had explained how blood flow and the circulatory system is vital to the human body; Quesay held that the circulation of wealth was vital to the economy. Societies at the time were also overwhelmingly agrarian. This may be why they viewed agriculture as the primary source of a nation's wealth. This is an idea which Quesay purported to demonstrate with data, comparing a workshop to a farm. He analyzed ""how money flowed between the three classes of farmers, proprietors, and artisans, in the same mechanical way that blood flows between different organs"" and claimed only the farm produced a surplus that added to the nation's wealth.
Physiocrats viewed the production of goods and services as equivalent to the consumption of the agricultural surplus, since human or animal muscle provided the main source of power and all energy derived from the surplus from agricultural production.  Profit in capitalist production was really only the ""rent"" obtained by the owner of the land on which the agricultural production took place.[3] ""The physiocrats damned cities for their artificiality and praised more natural styles of living. They celebrated farmers.""[4] They called themselves les Économistes, but are generally referred to as ""physiocrats"" to distinguish them from the many schools of economic thought that followed them.[5]"
Population_economics,Economics,6,"Demographic economics or population economics is the application of economic analysis to demography, the study of human populations, including size, growth, density, distribution, and vital statistics.[1][2]"
Preference_(economics),Economics,6," In economics and other social sciences, preference is the order that a person (an agent) gives to alternatives based on their relative utility, a process which results in an optimal ""choice"" (whether real or theoretical). Instead of the prices of goods, personal income, or availability of goods, the character of the preferences is determined purely by a person's tastes. However, persons are still expected to act in their best (that is, rational) interest.[1] Using the scientific method, social scientists try to model how people make practical decisions in order to test predictions about human behavior. Although economists are usually not interested in what causes a person to have certain preferences, they are interested in the theory of choice because it gives a background to empirical demand analysis.[2]"
Price,Economics,6,"A price is the (usually not negative) quantity of payment or compensation given by one party to another in return for one unit of goods or services.[1] A price is influenced by production costs, supply of the desired item, and demand for the product. A price may be determined by a monopolist or may be imposed on the firm by market conditions.
 In modern economies, prices are generally expressed in units of some form of currency. (For commodities, they are expressed as currency per unit weight of the commodity, e.g. euros per kilogram or Rands per KG.) Although prices could be quoted as quantities of other goods or services, this sort of barter exchange is rarely seen. Prices are sometimes quoted in terms of vouchers such as trading stamps and air miles. In some circumstances, cigarettes have been used as currency, for example in prisons, in times of hyperinflation, and in some places during World War II. In a black market economy, barter is also relatively common.
 In many financial transactions, it is customary to quote prices in other ways. The most obvious example is in pricing a loan, when the cost will be expressed as the percentage rate of interest. The total amount of interest payable depends upon credit risk, the loan amount and the period of the loan. Other examples can be found in pricing financial derivatives and other financial assets. For instance the price of inflation-linked government securities in several countries is quoted as the actual price divided by a factor representing inflation since the security was issued.
 ""Price"" sometimes refers to the quantity of payment requested by a seller of goods or services, rather than the eventual payment amount. This requested amount is often called the asking price or selling price, while the actual payment may be called the transaction price or traded price. Likewise, the bid price or buying price is the quantity of payment offered by a buyer of goods or services, although this meaning is more common in asset or financial markets than in consumer markets.
 Economic price theory asserts that in a free market economy the market price reflects interaction between supply and demand: the price is set so as to equate the quantity being supplied and that being demanded. In turn, these quantities are determined by the marginal utility of the asset to different buyers and to different sellers. Supply and demand, and hence price, may be influenced by other factors, such as government subsidy or manipulation through industry collusion.
 When a commodity is for sale at multiple locations, the law of one price is generally believed to hold. This essentially states that the cost difference between the locations cannot be greater than that representing shipping, taxes, other distribution costs and more.
"
Price_ceiling,Economics,6,"A price ceiling is a government- or group-imposed price control, or limit, on how high a price is charged for a product, commodity, or service. Governments use price ceilings to protect consumers from conditions that could make commodities prohibitively expensive. Such conditions can occur during periods of high inflation, in the event of an investment bubble, or in the event of  monopoly ownership of a product,  all of which can cause problems if imposed for a long period without controlled rationing, leading to shortages.[1] Further problems can occur if a government sets unrealistic price ceilings, causing business failures, stock crashes, or even economic crises. In unregulated market economies, price ceilings do not exist. 
 While price ceilings are often imposed by governments, there are also price ceilings that are implemented by non-governmental organizations such as companies, such as the practice of resale price maintenance. With resale price maintenance, a manufacturer and its distributors agree that the distributors will sell the manufacturer's product at certain prices (resale price maintenance), at or below a price ceiling (maximum resale price maintenance) or at or above a price floor. 
"
Price_controls,Economics,6,"Price controls are restrictions set in place and enforced by governments, on the prices that can be charged for goods and services in a market. The intent behind implementing such controls can stem from the desire to maintain affordability of goods even during shortages, and to slow inflation, or, alternatively, to ensure a minimum income for providers of certain goods or to try to achieve a living wage. There are two primary forms of price control: a price ceiling, the maximum price that can be charged; and a price floor, the minimum price that can be charged. A well-known example of a price ceiling is rent control, which limits the increases in rent. A widely used price floor is minimum wage (wages are the price of labor). Historically, price controls have often been imposed as part of a larger incomes policy package also employing wage controls and other regulatory elements.
 Although price controls are widely used by governments, economists usually agree that price controls do not accomplish what they are intended to do and are generally to be avoided.[1] For example, nearly three-quarters of economists surveyed disagreed with the statement, ""Wage-price controls are a useful policy option in the control of inflation.""[2]"
Price_elasticity_of_demand,Economics,6,"Price elasticity of demand (or elasticity), is the degree to which the effective desire for something changes as its price changes. In general, people desire things less as those things become more expensive. However, for some products, the customer's desire could drop sharply even with a little price increase, and for other products, it could stay almost the same even with a big price increase. Economists use the term elasticity to denote this sensitivity to price increases. More precisely, price elasticity gives the percentage change in quantity demanded when there is a one percent increase in price, holding everything else constant.
 Price elasticities are almost always negative, although analysts tend to ignore the sign even though this can lead to ambiguity. Only goods which do not conform to the law of demand, such as Veblen and Giffen goods, have a positive elasticity. In general, the demand for a good is said to be inelastic (or relatively inelastic) when the elasticity is less than one (in absolute value): that is, changes in price have a relatively small effect on the quantity of the good demanded. The demand for a good is said to be elastic (or relatively elastic) when its elasticity is greater than one.
 Revenue is maximised when price is set so that the elasticity is exactly one. The  good's elasticity can also be used to predict the incidence (or ""burden"") of a tax on that good. Various research methods are used to determine price elasticity, including test markets, analysis of historical sales data and conjoint analysis.   
Price elasticity of demand further divided into:
Perfectly Elastic Demand		(∞),
Perfectly Inelastic Demand		( 0 ),
Relatively Elastic Demand		(> 1),
Relatively Inelastic Demand		(< 1),
Unitary Elasticity Demand 		(= 1).
"
Price_elasticity_of_supply,Economics,6,"The price elasticity of supply (PES or Es) is a measure used in economics to show the responsiveness, or elasticity, of the quantity supplied of a good or service to a change in its price.
 The elasticity is represented in numerical form, and is defined as the percentage change in the quantity supplied divided by the percentage change in price.
 When the elasticity is less than one, the supply of the good can be described as inelastic; when it is greater than one, the supply can be described as elastic.[1] An elasticity of zero indicates that quantity supplied does not respond to a price change: the good is ""fixed"" in supply. Such goods often have no labor component or are not produced, limiting the short run prospects of expansion. If the elasticity is exactly one, the good is said to be unit-elastic.
 The quantity of goods supplied can, in the short term, be different from the amount produced, as manufacturers will have stocks which they can build up or run down.
"
Price_floor,Economics,6,"A price floor is a government- or group-imposed price control or limit on how low a price can be charged for a product,[1] good, commodity, or service. A price floor must be higher than the equilibrium price in order to be effective. The equilibrium price, commonly called the ""market price"", is the price where economic forces such as supply and demand are balanced and in the absence of external influences the (equilibrium) values of economic variables will not change, often described as the point at which quantity demanded and quantity supplied are equal (in a perfectly competitive market). Governments use price floors to keep certain prices from going too low.
 Two common price floors are minimum wage laws and supply management in Canadian agriculture. Other price floors include regulated US airfares prior to 1978 and minimum price per-drink laws for alcohol. While price floors are often imposed by governments, there are also price floors which are implemented by non-governmental organizations such as companies, such as the practice of resale price maintenance. With resale price maintenance, a manufacturer and its distributors agree that the distributors will sell the manufacturer's product at certain prices (resale price maintenance), at or above a price floor (minimum resale price maintenance) or at or below a price ceiling (maximum resale price maintenance). A related government- or group-imposed intervention, which is also a price control, is the price ceiling; it sets the maximum price that can legally be charged for a good or service, with a common government-imposed example being rent control.
"
Price_index,Economics,6,"A price index (plural: ""price indices"" or ""price indexes"") is a normalized average (typically a weighted average) of price relatives for a given class of goods or  services in a given region, during a given interval of time.  It is a statistic designed to help to compare how these price relatives, taken as a whole, differ between time periods or geographical locations.
 Price indices have several potential uses.  For particularly broad indices, the index can be said to measure the economy's general price level or a cost of living.  More narrow price indices can help producers with business plans and pricing.  Sometimes, they can be useful in helping to guide investment.
 Some notable price indices include:
"
Price_level,Economics,6,"The general price level is a hypothetical measure of overall prices for some set of goods and services (the consumer basket), in an economy or monetary union during a given interval (generally one day), normalized relative to some base set.  Typically, the general price level is approximated with a daily price index, normally the Daily CPI. The general price level can change more than once per day during hyperinflation.
"
Pricing,Economics,6,"Pricing is the process whereby a business sets the price at which it will sell its products and services, and may be part of the business's marketing plan. In setting prices, the business will take into account the price at which it could acquire the goods, the manufacturing cost, the marketplace, competition, market condition, brand, and quality of product.
 Pricing is a fundamental aspect of financial modeling and is one of the four Ps of the marketing mix, the other three aspects being product, promotion, and place. Price is the only revenue generating element amongst the four Ps, the rest being cost centers. However, the other Ps of marketing will contribute to decreasing price elasticity and so enable price increases to drive greater revenue and profits.
 Pricing can be a manual or automatic process of applying prices to purchase and sales orders, based on factors such as: a fixed amount, quantity break, promotion or sales campaign, specific vendor quote, price prevailing on entry, shipment or invoice date, combination of multiple orders or lines, and many others. Automated pricing systems require more setup and maintenance but may prevent pricing errors. The needs of the consumer can be converted into demand only if the consumer has the willingness and capacity to buy the product. Thus, pricing is the most important concept in the field of marketing, it is used as a tactical decision in response to changing competitive, market and organisational situations.
"
Prime_rate,Economics,6,"A prime rate or prime lending rate is an interest rate used by banks, usually the interest rate at which banks lend to customers with good credit. Some variable interest rates may be expressed as a percentage above or below prime rate.[1]:8"
Producer_price_index,Economics,6,"A producer price index (PPI) is a price index that measures the average changes in prices received by domestic producers for their output.
 Its importance[clarification needed] is being undermined by the steady decline in manufactured goods as a share of spending.[1]"
Economic_surplus,Economics,6,"In mainstream economics, economic surplus, also known as total welfare or Marshallian surplus (after Alfred Marshall), refers to two related quantities: 
"
Product_differentiation,Economics,6,"In economics and marketing, product differentiation (or simply differentiation) is  
the process of distinguishing a product or service from others, to make it more attractive to a particular target market. This involves differentiating it from competitors' products as well as a firm's own products. The concept was proposed by Edward Chamberlin in his  1933 The Theory of Monopolistic Competition.[1]"
Production_(economics),Economics,6,"Production is a process of combining various material inputs and immaterial inputs (plans, know-how) in order to make something for consumption (output). It is the act of creating an output, a good or service which has value and contributes to the utility of individuals.[1] The area of economics that focuses on production is referred to as production theory, which in many respects is similar to the consumption (or consumer) theory in economics.[2] Economic well-being is created in a production process, meaning all economic activities that aim directly or indirectly to satisfy human wants and needs. The degree to which the needs are satisfied is often accepted as a measure of economic well-being. In production there are two features which explain increasing economic well-being. They are improving quality-price-ratio of goods and services and increasing incomes from growing and more efficient market production or total production which help in increasing GDP.
The most important forms of production are:
 In order to understand the origin of economic well-being, we must understand these three production processes. All of them produce commodities which have value and contribute to well-being of individuals.
 The satisfaction of needs originates from the use of the commodities which are produced. The need satisfaction increases when the quality-price-ratio of the commodities improves and more satisfaction is achieved at less cost. Improving the quality-price-ratio of commodities is to a producer an essential way to improve the competitiveness of products but this kind of gains distributed to customers cannot be measured with production data. Improving the competitiveness of products means often to the producer lower product prices and therefore losses in incomes which are to be compensated with the growth of sales volume.
 Economic well-being also increases due to the growth of incomes that are gained from the growing and more efficient market production. Market production is the only production form that creates and distributes incomes to stakeholders. Public production and household production are financed by the incomes generated in market production. Thus market production has a double role in creating well-being, i.e. the role of producing goods and services and the role of creating income. Because of this double role market production is the “primus motor” of economic well-being and therefore here under review.[citation needed]"
Production_possibilities_curve,Economics,6,"A production–possibility frontier (PPF), production possibility curve (PPC), or production possibility boundary (PPB), or Transformation curve/boundary/frontier is a curve which shows various combinations of the amounts of two goods which can be produced within the given resources and technology/a graphical representation showing all the possible options of output for two products that can be produced using all factors of production, where the given resources are fully and efficiently utilized per unit time. A PPF illustrates several economic concepts, such as allocative efficiency, economies of scale, opportunity cost (or marginal rate of transformation), productive efficiency,  and scarcity of resources (the fundamental economic problem that all societies face).[1] This tradeoff is usually considered for an economy, but also applies to each individual, household, and economic organization. One good can only be produced by diverting resources from other goods, and so by producing less of them.
 Graphically bounding the production set for fixed input quantities, the PPF curve shows the maximum possible production level of one commodity for any given production level of the other, given the existing state of technology. By doing so, it defines productive efficiency in the context of that production set: a point on the frontier indicates efficient use of the available inputs (such as points B, D and C in the graph), a point beneath the curve (such as A) indicates inefficiency, and a point beyond the curve (such as X) indicates impossibility.
 PPFs are normally drawn as bulging upwards or outwards from the origin (""concave"" when viewed from the origin), but they can be represented as bulging downward (inwards) or linear (straight), depending on a number of assumptions. 
 An outward shift of the PPC results from growth of the availability of inputs, such as physical capital or labour, or from technological progress in knowledge of how to transform inputs into outputs. Such a shift reflects, for instance, economic growth of an economy already operating at its full productivity (on the PPF), which means that more of both outputs can now be produced during the specified period of time without sacrificing the output of either good. Conversely, the PPF will shift inward if the labour force shrinks, the supply of raw materials is depleted, or a natural disaster decreases the stock of physical capital.
 However, most economic contractions reflect not that less can be produced but that the economy has started operating below the frontier, as typically, both labour and physical capital are underemployed, remaining therefore idle.
 In microeconomics, the PPF shows the options open to an individual, household, or firm in a two good world. By definition, each point on the curve is productively efficient, but, given the nature of market demand, some points will be more profitable than others. Equilibrium for a firm will be the combination of outputs on the PPF that is most profitable.[2] From a macroeconomic perspective, the PPF illustrates the production possibilities available to a nation or economy during a given period of time for broad categories of output. It is traditionally used to show the movement between committing all funds to consumption on the y-axis versus investment on the x-axis.  However, an economy may achieve productive efficiency without necessarily being allocatively efficient. Market failure (such as imperfect competition or externalities) and some institutions of social decision-making (such as government and tradition) may lead to the wrong combination of goods being produced (hence the wrong mix of resources being allocated between producing the two goods) compared to what consumers would prefer, given what is feasible on the PPF.[3]"
Production_set,Economics,6,"In economics the production set is a construct representing the possible inputs and outputs to a production process.
 A production vector represents a process as a vector containing an entry for every commodity in the economy. Outputs are represented by positive entries giving the quantities produced and inputs by negative entries giving the quantities consumed.
 If the commodities in the economy are (labour,corn,flour,bread ) and a mill uses one unit of labour to produce 8 units of flour from 10 units of corn, then its production vector is (–1,–10,8,0). If it needs the same amount of labour to run at half capacity then the production vector (–1,–5,4,0) would also be operationally possible. The set of all operationally possible production vectors is the mill’s production set.
 If y is a production vector and p is the economy’s price vector, then p ·y is the value of net output. The mill’s owner will normally choose y from the production set to maximise this quantity. p ·y is defined as the ‘profit’ of the vector y, and the mill-owner’s behaviour is described as ‘profit-maximising’.[1]"
Profit_(economics),Economics,6,"
 An economic profit is the difference between the revenue a business has received from its outputs and the opportunity costs of its inputs.[1] An economic profit differs from an accounting profit as it considers both the firms implicit and explicit costs, where as an accounting profit only considers the explicit costs which appear on a firms financial statements. Due to the additional implicit costs being considered, the economic profit usually differs from the accounting profit.[2] From an economic standpoint, we often view economic profits in conjunction with normal profits, as both consider a firm's implicit costs. A normal profit is the profit that is necessary to cover both the implicit and explicit costs of a firm and the owner-manager or investors who fund it. In the absence of this profit, these parties would withdraw their time and funds from the firm and use them to better advantage elsewhere, as to not forgo a better opportunity. In contrast, an economic profit sometimes called an excess profit, is the profit remaining after both the implicit and explicit costs are covered. [2] The enterprise component of normal profit is the profit that a business owner considers necessary to make running the business worth his or her time, i.e., it is comparable to the next-best amount the entrepreneur could earn doing another job.[3] In particular, if enterprise is not included as a factor of production, it can also be viewed as a return to capital for investors including the entrepreneur, equivalent to the return the capital owner could have expected (in a safe investment), plus compensation for risk.[4] Normal profit varies both within and across industries; it is commensurate with the riskiness associated with each type of investment, as per the risk-return spectrum.
 Economic profits arise in markets which are non-competitive and have significant barriers to entry, such as Monopolies and Oligopolies. The inefficiencies and lack of competition in these markets creates an environment where firms can set prices or quantities, instead of being a price-taker, which occurs in a perfectly competitive market. [5]. In a perfectly competitive market when long-run economic equilibrium is reached, an economic profit becomes non-existent. This is because there is no incentive for firms to either enter or leave the industry.[6]"
Profit_motive,Economics,6,"In economics, the profit motive is the motivation of firms that operate so as to maximize their profits. Mainstream microeconomic theory posits that the ultimate goal of a business is ""to make money"" - not in the sense of increasing the firm's stock of means of payment (which is usually kept to a necessary minimum because means of payment incur costs, i.e. interest or foregone yields), but in the sense of ""increasing net worth"". Stated differently, the reason for a business's existence is to turn a profit.[1]
The profit motive is a key tenet of rational choice theory, or the theory that economic agents tend to pursue what is in their own best interests. In accordance with this doctrine, businesses seek to benefit themselves and/or their shareholders by maximizing profits.
 As it extends beyond economics into ideology, the profit motive has been a major matter of contention.
"
Progressive_tax,Economics,6,"A progressive tax is a tax in which the tax rate increases as the taxable amount increases.[1][2][3][4][5] The term progressive refers to the way the tax rate progresses from low to high, with the result that a taxpayer's average tax rate is less than the person's marginal tax rate.[6][7] The term can be applied to individual taxes or to a tax system as a whole. Progressive taxes are imposed in an attempt to reduce the tax incidence of people with a lower ability to pay, as such taxes shift the incidence increasingly to those with a higher ability-to-pay. The opposite of a progressive tax is a regressive tax, such as a sales tax, where the poor pay a larger proportion of their income compared to the rich.[5] The term is frequently applied in reference to personal income taxes, in which people with lower income pay a lower percentage of that income in tax than do those with higher income. It can also apply to adjustments of the tax base by using tax exemptions, tax credits, or selective taxation that creates progressive distribution effects. For example, a wealth or property tax,[8] a sales tax on luxury goods, or the exemption of sales taxes on basic necessities, may be described as having progressive effects as it increases the tax burden of higher income families and reduces it on lower income families.[9][10][11] Progressive taxation is often suggested as a way to mitigate the societal ills associated with higher income inequality,[12] as the tax structure reduces inequality,[13] but economists disagree on the tax policy's economic and long-term effects.[14][15][16] One study suggests progressive taxation can be positively associated with happiness, the subjective well-being of nations and citizen satisfaction with public goods, such as education and transportation.[17]"
Proportional_tax,Economics,6,"A proportional tax is a tax imposed so that the tax rate is fixed, with no change as the taxable base amount increases or decreases. The amount of the tax is in proportion to the amount subject to taxation.[1] ""Proportional"" describes a distribution effect on income or expenditure, referring to the way the rate remains consistent (does not progress from ""low to high"" or ""high to low"" as income or consumption changes), where the marginal tax rate is equal to the average tax rate.[2][3] It can be applied to individual taxes or to a tax system as a whole; a year, multi-year, or lifetime. Proportional taxes maintain equal tax incidence regardless of the ability-to-pay and do not shift the incidence disproportionately to those with a higher or lower economic well-being.
 Flat taxes are defined as levying a fixed (“flat”) fraction of taxable income. They usually exempt from taxation household income below a statutorily determined level that is a function of the type and size of the household. As a result, such a flat marginal rate is consistent with a progressive average tax rate. A progressive tax is a tax imposed so that the tax rate increases as the amount subject to taxation increases.[4][5][6] The opposite of a progressive tax is a regressive tax, where the tax rate decreases as the amount subject to taxation increases. 
 
The French Declaration of the Rights of Man and of the Citizen of 1789 proclaims: .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0} A common contribution is essential for the maintenance of the public forces and for the cost of administration. This should be equitably distributed among all the citizens in proportion to their means.[7]"
Proxemics,Economics,6,"Proxemics is the study of human use of space and the effects that population density has on behaviour, communication, and social interaction.[1] Proxemics is one among several subcategories in the study of nonverbal communication, including haptics (touch), kinesics (body movement), vocalics (paralanguage), and chronemics (structure of time).[2] Edward T. Hall, the cultural anthropologist who coined the term in 1963, defined proxemics as ""the interrelated observations and theories of humans use of space as a specialized elaboration of culture"".[3] In his foundational work on proxemics, The Hidden Dimension, Hall emphasized the impact of proxemic behavior (the use of space) on interpersonal communication. According to Hall, the study of proxemics is valuable in evaluating not only the way people interact with others in daily life, but also ""the organization of space in [their] houses and buildings, and ultimately the layout of [their] towns"".[4] Proxemics remains a hidden component of interpersonal communication that is uncovered through observation and strongly influenced by culture.
"
Public_economics,Economics,6,"Public economics (or economics of the public sector) is the study of government policy through the lens of economic efficiency and equity. Public economics builds on the theory of welfare economics and is ultimately used as a tool to improve social welfare.
 Public economics provides a framework for thinking about whether or not the government should participate in economic markets and to what extent it should do so.  Microeconomic theory is utilized to assess whether the private market is likely to provide efficient outcomes in the absence of governmental interference; this study involves the analysis of government taxation and expenditures.
 This subject encompasses a host of topics notably market failures such as, public goods, externalities and Imperfect Competition, and the creation and implementation of government policy.[1] Broad methods and topics include:
 Emphasis is on analytical and scientific methods and normative-ethical analysis, as distinguished from ideology.  Examples of topics covered are tax incidence,[7] optimal taxation,[8] and the theory of public goods.[9]"
Public_good_(economics),Economics,6,"
 In economics, a public good (also referred to as a social good or collective good) is a good that is both non-excludable and non-rivalrous. For such utilities, users cannot be barred from accessing and/or using them for failing to pay for them. Also, use by one person neither prevents access of other people nor does it reduce availability to others. Therefore, the good can be used simultaneously by more than one person.[1] This is in contrast to a common good such as wild fish stocks in the ocean, which is non-excludable but rivalrous to a certain degree. If too many fish were harvested, the stocks would deplete, limiting the access of fish for others.
 Public goods include knowledge, official statistics, national security, and common languages. Additionally, flood control systems, lighthouses, and street lighting are also common social goods. Collective goods that are spread all over the face of the earth may be referred to as global public goods.[2]  For instance, knowledge is well shared globally. Information about men, women and youth health awareness, environmental issues, and maintaining biodiversity is common knowledge that every individual in the society can get without necessarily preventing others access. Also, sharing and interpreting contemporary history with a cultural lexicon, particularly about protected cultural heritage sites and monuments are other sources of knowledge that the people can freely access. Popular and entertaining tourist attractions, libraries and universities are other examples of public goods.   
 Many public goods may at times be subject to excessive use resulting in negative externalities affecting all users; for example air pollution and traffic congestion. The closeness of the people while interacting with other people in the public utilities also has appeared to cause negative impact to people. The result of this is a faster and increased spread of infectious diseases such as SARS and COVID-19.[3] Public goods problems are often closely related to the ""free-rider"" problem, in which people not paying for the good may continue to access it. Thus, the good may be under-produced, overused or degraded.[4] Public goods may also become subject to restrictions on access and may then be considered to be club goods; exclusion mechanisms include toll roads, congestion pricing, and pay television with an encoded signal that can be decrypted only by paid subscribers.
 There is a good deal of debate and literature on how to measure the significance of public goods problems in an economy, and to identify the best remedies.
"
Pure_competition,Economics,6,"In economics, specifically general equilibrium theory, a perfect market, also known as an atomistic market, is defined by several idealizing conditions, collectively called perfect competition, or atomistic competition. In theoretical models where conditions of perfect competition hold, it has been demonstrated that a market will reach an equilibrium in which the quantity supplied for every product or service, including labor, equals the quantity demanded at the current price. This equilibrium would be a Pareto optimum.[1] Perfect competition provides both allocative efficiency and productive efficiency:
 The theory of perfect competition has its roots in late-19th century economic thought. Léon Walras[2] gave the first rigorous definition of perfect competition and derived some of its main results. In the 1950s, the theory was further formalized by Kenneth Arrow and Gérard Debreu.[3] Real markets are never perfect. Those economists who believe in perfect competition as a useful approximation to real markets may classify those as ranging from close-to-perfect to very imperfect. Share and foreign exchange markets are commonly said to be the most similar to the perfect market. The real estate market is an example of a very imperfect market. In such markets, the theory of the second best proves that if one optimality condition in an economic model cannot be satisfied, it is possible that the next-best solution involves changing other variables away from the values that would otherwise be optimal.[4]"
Purchasing_power_parity,Economics,6,"Purchasing power parity (PPP)[1] is a measurement of prices in different countries that uses the prices of specific goods to compare the absolute purchasing power of the countries' currencies. In many cases, PPP produces an inflation rate that is equal to the price of the basket of goods at one location divided by the price of the basket of goods at a different location. The PPP inflation and exchange rate may differ from the  market exchange rate because of poverty, tariffs, and other transaction costs.
"
Quantitative_easing,Economics,6,"
 Quantitative easing (QE) is a monetary policy whereby a central bank purchases at scale government bonds or other financial assets in order to inject money into the economy to expand economic activity.[1] Quantitative easing is considered to be an ""unconventional"" form of monetary policy,[2] which is usually used when inflation is very low or negative, and when standard monetary policy instruments have become ineffective. The term ""quantitative easing"" has been coined by german economist Richard Werner in 1995[3] in the context of the Japanese crisis. 
 A central bank implements quantitative easing by buying financial assets from commercial banks and other financial institutions, thus raising the prices of those financial assets and lowering their yield, while simultaneously increasing the money supply. In contrast to conventional open-market operations, quantitative easing involve the purchase of more risky assets (than short-term government bonds) and at a large scale, over a pre-committed period of time.
 Central banks usually resort to quantitative easing policies when their key interest rates approach or reach zero (a situation described as the ""zero lower bound"") which induces a ""liquidity trap"" where people prefer to hold cash or very liquid assets, given the perceived low profitability on other assets. In such circumstances, monetary authorities may then use quantitative easing to further stimulate the economy. 
 Quantitative easing has been largely undertaken by all major central banks globally following the global financial crisis of 2007–08 and in response to the COVID-19 pandemic. Quantitative easing can help bring the economy out of recession[4] and help ensure that inflation does not fall below the central bank's inflation target.[5] However QE programmes are also being criticized for its side-effects and risks, which include the policy being more effective than intended in acting against deflation (leading to higher inflation in the longer term), or not being effective enough if banks remain reluctant to lend and potential borrowers are unwilling to borrow. 
"
Quantity_theory_of_money,Economics,6,"In monetary economics, the quantity theory of money (QTM) states that the general price level of goods and services is directly proportional to the amount of money in circulation, or money supply. For example, if the amount of money in an economy doubles, QTM predicts that price levels will also double. The theory was originally formulated by Polish mathematician Nicolaus Copernicus in 1517,[1] and was influentially restated by philosophers John Locke, David Hume, Jean Bodin, and by economists Milton Friedman and Anna Schwartz in A Monetary History of the United States published in 1963.[2][3] The theory was challenged by Keynesian economics,[4] but updated and reinvigorated by the monetarist school of economics. Critics of the theory argue that money velocity is not stable and, in the short-run, prices are sticky, so the direct relationship between money supply and price level does not hold. In mainstream macroeconomic theory, changes in the money supply play no role in determining the inflation rate as it is measured by the CPI, although some outspoken critics such as Peter Schiff believe that an expansion of the money supply necessarily begets an increase in prices in a non-zero number of asset classes. In models where the expansion of the money supply does not impact inflation, inflation is determined by the monetary policy reaction function.
 Alternative theories include the real bills doctrine and the more recent fiscal theory of the price level.
"
Rate_of_profit,Economics,6,"In economics and finance, the profit rate is the relative profitability of an investment project, a capitalist enterprise or a whole capitalist economy. It is similar to the concept of rate of return on investment. 
"
Rational_choice,Economics,6,"Rational choice theory, also known as theory of rational choice, choice theory or rational action theory, is a framework for understanding and often formally modeling social and economic behavior.[1] The basic premise of rational choice theory is that aggregate social behavior results from the behavior of individual actors, each of whom is making their individual decisions. The theory also focuses on the determinants of the individual choices (methodological individualism). Rational choice theory then assumes that an individual has preferences among the available choice alternatives that allow them to state which option they prefer. These preferences are assumed to be complete (the person can always say which of two alternatives they consider preferable or that neither is preferred to the other) and transitive (if option A is preferred over option B and option B is preferred over option C, then A is preferred over C). The rational agent is assumed to take account of available information, probabilities of events, and potential costs and benefits in determining preferences, and to act consistently in choosing the self-determined best choice of action. In simpler terms, this theory dictates that every person, even when carrying out the most mundane of tasks, perform their own personal cost and benefit analysis in order to determine whether the action is worth pursuing for the best possible outcome.[2] And following this, a person will choose the optimum venture in every case. This could culminate in a student deciding on whether to attend a lecture or stay in bed, a shopper deciding to provide their own bag to avoid the five pence charge or even a voter deciding which candidate or party based on who will fulfill their needs the best on issues that have an impact on themselves especially.
 Rationality is widely used as an assumption of the behavior of individuals in microeconomic models and analyses and appears in almost all economics textbook treatments of human decision-making. It is also used in political science,[3] sociology,[4] and philosophy. Gary Becker was an early proponent of applying rational actor models more widely.[5] Becker won the 1992 Nobel Memorial Prize in Economic Sciences for his studies of discrimination, crime, and human capital.[6] A particular version of rationality is instrumental rationality, which involves seeking the most cost-effective means to achieve a specific goal without reflecting on the worthiness of that goal.
 Rational choice theorists do not claim that the theory describes the choice process, but rather that it predicts the outcome and pattern of choices.  
An assumption often added to the rational choice paradigm is that individual preferences are self-interested, in which case the individual can be referred to as a homo economicus. Such an individual acts as if balancing costs against benefits to arrive at action that maximizes personal advantage.[7] Proponents of such models, particularly those associated with the Chicago school of economics, do not claim that a model's assumptions are an accurate description of reality, only that they help formulate clear and falsifiable hypotheses.[citation needed] In this view, the only way to judge the success of a hypothesis is empirical tests.[7] To use an example from Milton Friedman, if a theory that says that the behavior of the leaves of a tree is explained by their rationality passes the empirical test, it is seen as successful.
 Without specifying the individual's goal or preferences it may not be possible to empirically test, or falsify, the rationality assumption. However, the predictions made by a specific version of the theory are testable. In recent years, the most prevalent version of rational choice theory, expected utility theory, has been challenged by the experimental results of behavioral economics. Economists are learning from other fields, such as psychology, and are enriching their theories of choice in order to get a more accurate view of human decision-making. For example, the behavioral economist and experimental psychologist Daniel Kahneman won the Nobel Memorial Prize in Economic Sciences in 2002 for his work in this field.
 Rational choice theory has become increasingly employed in social sciences other than economics, such as sociology, evolutionary theory and political science in recent decades.[8][9] It has had far-reaching impacts on the study of political science, especially in fields like the study of interest groups, elections, behaviour in legislatures, coalitions, and bureaucracy.[10] In these fields, the use of the rational choice paradigm to explain broad social phenomena is the subject of controversy.[11][12] Human action that is in rational choice theory has been described as outcome of two choices. First, those feasible region will be chosen within all the possible and related action. Second, after the preferred option has been chosen, the feasible region that has been selected was picked based on restriction of financial, legal, social, physical or emotional restrictions that the agent is facing. After that, a choice will be made based on the preference order. [13] The concept of rationality used in rational choice theory is different from the colloquial and most philosophical use of the word. Colloquially, ""rational"" behaviour typically means ""sensible"", ""predictable"", or ""in a thoughtful, clear-headed manner."" Rational choice theory uses a narrower definition of rationality. At its most basic level, behavior is rational if it is goal-oriented, reflective (evaluative), and consistent (across time and different choice situations). This contrasts with behavior that is random, impulsive, conditioned, or adopted by (unevaluative) imitation.[citation needed] Early neoclassical economists writing about rational choice, including William Stanley Jevons, assumed that agents make consumption choices so as to maximize their happiness, or utility. Contemporary theory bases rational choice on a set of choice axioms that need to be satisfied, and typically does not specify where the goal (preferences, desires) comes from. It mandates just a consistent ranking of the alternatives.[14]:501 Individuals choose the best action according to their personal preferences and the constraints facing them. E.g., there is nothing irrational in preferring fish to meat the first time, but there is something irrational in preferring fish to meat in one instant and preferring meat to fish in another, without anything else having changed.
"
Rational_expectations,Economics,6,"In economics, ""rational expectations"" are model-consistent expectations, in that agents inside the model are assumed to ""know the model"" and on average take the model's predictions as valid.[1] Rational expectations ensure internal consistency in models involving uncertainty. To obtain consistency within a model, the predictions of future values of economically relevant variables from the model are assumed to be the same as that of the decision-makers in the model, given their information set, the nature of the random processes involved, and model structure. The rational expectations assumption is used especially in many contemporary macroeconomic models.
 Since most macroeconomic models today study decisions under uncertainty and over many periods, the expectations of individuals, firms, and government institutions about future economic conditions are an essential part of the model. To assume rational expectations is to assume that agents' expectations may be wrong, but are correct on average over time. In other words, although the future is not fully predictable, agents' expectations are assumed not to be systematically biased and collectively use all relevant information in forming expectations of economic variables. This way of modeling expectations was originally proposed by John F. Muth (1961)[2] and later became influential when it was used by Robert Lucas, Jr. in macroeconomics.
 Deirdre McCloskey emphasizes that ""rational expectations"" is an expression of intellectual modesty:[3].mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0} Muth's notion was that the professors [of economics], even if correct in their model of man, could do no better in predicting than could the hog farmer or steelmaker or insurance company. The notion is one of intellectual modesty. The common sense is ""rationality"": therefore Muth called the argument ""rational expectations"". Hence, it is important to distinguish the rational-expectations assumption from assumptions of individual rationality and to note that the first does not imply the latter. Rational expectations is an assumption of aggregate consistency in dynamic models. In contrast, rational choice theory studies individual decision making and is used extensively in, among others, game theory and contract theory.[4]. In fact, Muth cited survey data exhibiting ""considerable cross-sectional differences of opinion"" and was quite explicit in stating that his rational-expexctations hypothesis does not assert... that predictions of entrepreneurs are perfect or that their expectations are all the same. In Muth's version of rational expectations, each individual holds beliefs that are model inconsistent, although the distribution of these diverse beliefs is unbiased relative to the data generated by the actions resulting from these expectations.
"
Rationing,Economics,6,"
 Rationing is the controlled distribution of scarce resources, goods, services,[1] or an artificial restriction of demand. Rationing controls the size of the ration, which is one's allowed portion of the resources being distributed on a particular day or at a particular time. There are many forms of rationing,
and in western civilization people experience some of them in daily life without realizing it.[2] Rationing is often done to keep price below the market-clearing price determined by the process of supply and demand in an unfettered market. Thus, rationing can be complementary to price controls. An example of rationing in the face of rising prices took place in the various countries where there was rationing of gasoline during the 1973 energy crisis.
 A reason for setting the price lower than would clear the market may be that there is a shortage, which would drive the market price very high. High prices, especially in the case of necessities, are undesirable with regard to those who cannot afford them. Traditionalist economists argue, however, that high prices act to reduce waste of the scarce resource while also providing incentive to produce more.
 Rationing using ration stamps is only one kind of non-price rationing. For example, scarce products can be rationed using queues. This is seen, for example, at amusement parks, where one pays a price to get in and then need not pay any price to go on the rides. Similarly, in the absence of road pricing, access to roads is rationed in a first come, first served queueing process, leading to congestion.
 Authorities which introduce rationing often have to deal with the rationed goods being sold illegally on the black market. Despite the fact that rationing systems are sometimes necessary as the only viable option for societies facing severe consumer goods shortages, they are usually extremely unpopular with the general public, as they enforce limits on individual consumption.[3][4][5]"
Real_GDP,Economics,6,"Real gross domestic product (real GDP for short) is a macroeconomic measure of the value of economic output adjusted for price changes (i.e. inflation or deflation).[1] This adjustment transforms the money-value measure, nominal GDP, into an index for quantity of total output. Although GDP is total output, it is primarily useful because it closely approximates the total spending: the sum of consumer spending, investment made by industry, excess of exports over imports, and government spending.  Due to inflation, GDP increases and does not actually reflect the true growth in an economy.  That is why the GDP must be divided by the inflation rate (raised to the power of units of time in which the rate is measured) to get the growth of the real GDP. Different organizations use different types of 'Real GDP' measures, for example, the UNCTAD uses 2005 Constant prices and exchange rates while the FRED uses 2009 constant prices and exchange rates, and recently the World Bank switched from 2005 to 2010 constant prices and exchange rates.[2][3][4]"
Recession,Economics,6,"In economics, a recession is a business cycle contraction when there is a general decline in economic activity.[1][2] Recessions generally occur when there is a widespread drop in spending (an adverse demand shock). This may be triggered by various events, such as a financial crisis, an external trade shock, an adverse supply shock, the bursting of an economic bubble, or a large-scale anthropogenic or natural disaster (e.g. a pandemic). In the United States, it is defined as ""a significant decline in economic activity spread across the market, lasting more than a few months, normally visible in real GDP, real income, employment, industrial production, and wholesale-retail sales"".[3] In the United Kingdom, it is defined as a negative economic growth for two consecutive quarters.[4][5] Governments usually respond to recessions by adopting expansionary macroeconomic policies, such as increasing money supply or increasing government spending and decreasing taxation.
 Put simply, a recession is the decline of economic activity, which means that the public has stopped buying products for a while which can cause the downfall of GDP after a period of economic expansion (a time where products become popular and the income profit of a business becomes large). This causes inflation (the rise of product prices). In a recession, the rate of inflation slows down, stops, or becomes negative.
"
Reflation,Economics,6,"Reflation is the act of stimulating the economy by increasing the money supply or by reducing taxes, seeking to bring the economy (specifically price level) back up to the long-term trend, following a dip in the business cycle. It is the opposite of disinflation, which seeks to return the economy back down to the long-term trend.
 Reflation, which can be considered a form of inflation (increase in the price level), is contrasted with inflation (narrowly speaking) in that ""bad"" inflation is inflation above the long-term trend line, while reflation is a recovery of the price level when it has fallen below the trend line. For example, if inflation had been running at a 3% rate, but for one year it falls to 0%, the following year would need 6% inflation (actually 6.09% due to compounding) to catch back up to the long-term trend. This higher than normal inflation is considered reflation, since it is a return to trend, not exceeding the long-term trend.
 This distinction is predicated on a theory of economic growth where there is long-term growth in the economy and price level, which is widely accepted in economics. Just as disinflation is considered an acceptable antidote to high inflation, reflation is considered to be an antidote to deflation (which, unlike inflation, is considered bad regardless of its magnitude).
"
Regional_science,Economics,6,"Regional science is a field of the social sciences concerned with analytical approaches to problems that are specifically urban, rural, or regional. Topics in regional science include, but are not limited to location theory or spatial economics, location modeling, transportation, migration analysis, land use and urban development, interindustry analysis, environmental and ecological analysis, resource management, urban and regional policy analysis, geographical information systems, and spatial data analysis. In the broadest sense, any social science analysis that has a spatial dimension is embraced by regional scientists.
"
Regressive_tax,Economics,6,"A regressive tax is a tax imposed in such a manner that the tax rate decreases as the amount subject to taxation increases.[1][2][3][4][5] ""Regressive"" describes a distribution effect on income or expenditure, referring to the way the rate progresses from high to low, so that the average tax rate exceeds the marginal tax rate.[6][7] In terms of individual income and wealth, a regressive tax imposes a greater burden (relative to resources) on the poor than on the rich: there is an inverse relationship between the tax rate and the taxpayer's ability to pay, as measured by assets, consumption, or income.  These taxes tend to reduce the tax burden of the people with a higher ability to pay, as they shift the relative burden increasingly to those with a lower ability to pay.
 The regressivity of a particular tax can also factor the propensity of the taxpayers to engage in the taxed activity relative to their resources (the demographics of the tax base). In other words, if the activity being taxed is more likely to be carried out by the poor and less likely to be carried out by the rich, the tax may be considered regressive.[8] To measure the effect, the income elasticity of the good being taxed as well as the income effect on consumption must be considered. The measure can be applied to individual taxes or to a tax system as a whole; a year, multi-year, or lifetime.
 The opposite of a regressive tax is a progressive tax, in which the average tax rate increases as the amount subject to taxation rises[9][10][11][12]  In between is a flat or proportional tax, where the tax rate is fixed as the amount subject to taxation increases.
"
Regulation,Economics,6,"Regulation is the management of complex systems according to a set of rules and trends. In systems theory, these types of rules exist in various fields of biology and society, but the term has slightly different meanings according to context. For example:
"
Retail_sales,Economics,6,"
 Retail is the process of selling consumer goods or services to customers through multiple channels of distribution to earn a profit. Retailers satisfy demand identified through a supply chain. The term ""retailer"" is typically applied where a service provider fills the small orders of many individuals, who are end-users, rather than large orders of a small number of wholesale, corporate or government clientele. Shopping generally refers to the act of buying products. Sometimes this is done to obtain final goods, including necessities such as food and clothing; sometimes it takes place as a recreational activity. Recreational shopping often involves window shopping and browsing: it does not always result in a purchase.
 Retail markets and shops have a very ancient history, dating back to antiquity. Some of the earliest retailers were itinerant peddlers. Over the centuries, retail shops were transformed from little more than ""rude booths"" to the sophisticated shopping malls of the modern era.
 Most modern retailers typically make a variety of strategic level decisions including the type of store, the market to be served, the optimal product assortment, customer service, supporting services and the store's overall market positioning. Once the strategic retail plan is in place, retailers devise the retail mix which includes product, price, place, promotion, personnel, and presentation. In the digital age, an increasing number of retailers are seeking to reach broader markets by selling through multiple channels, including both bricks and mortar and online retailing. Digital technologies are also changing the way that consumers pay for goods and services. Retailing support services may also include the provision of credit, delivery services, advisory services, stylist services and a range of other supporting services.
 Retail shops occur in a diverse range of types and in many different contexts – from strip shopping centres in residential streets through to large, indoor shopping malls. Shopping streets may restrict traffic to pedestrians only. Sometimes a shopping street has a partial or full roof to create a more comfortable shopping environment – protecting customers from various types of weather conditions such as extreme temperatures, winds or precipitation.[relevant?  – discuss] Forms of non-shop retailing include online retailing (a type of electronic-commerce used for business-to-consumer (B2C) transactions) and mail order.
"
Returns_to_scale,Economics,6,"In economics, returns to scale describe what happens to long run returns as the scale of production increases, when all input levels including physical capital usage are variable (able to be set by the firm). The concept of returns to scale arises in the context of a firm's production function.  It explains the long run linkage of the rate of increase in output (production) relative to associated increases in the inputs (factors of production). In the long run, all factors of production are variable and subject to change in response to a given increase in production scale. While economies of scale show the effect of an increased output level on unit costs, returns to scale focus only on the relation between input and output quantities. 
 There are three possible types of returns to scale: increasing returns to scale, constant returns to scale, and diminishing (or decreasing) returns to scale.
If output increases by the same proportional change as all inputs change then there are constant returns to scale (CRS). If output increases by less than the proportional change in all inputs, there are decreasing returns to scale (DRS). If output increases by more than the proportional change in all inputs, there are increasing returns to scale (IRS). A firm's production function could exhibit different types of returns to scale in different ranges of output.  Typically, there could be increasing returns at relatively low output levels, decreasing returns at relatively high output levels, and constant returns at some range of output levels between those extremes.[citation needed] In mainstream microeconomics, the returns to scale faced by a firm are purely technologically imposed and are not influenced by economic decisions or by market conditions (i.e., conclusions about returns to scale are derived from the specific mathematical structure of the production function in isolation).
"
Revenue,Economics,6,"In accounting, revenue is the income or increase in net assets[1]
that an entity has from its normal activities (in the case of a business, usually from the sale of goods and services to customers). Commercial revenue may also be referred to as sales or as turnover. Some  companies receive revenue from interest, royalties, or other fees.[2] ""Revenue"" may refer to income in general, or it may refer to the amount, in a monetary unit, earned during a period of time, as in ""Last year, Company X had revenue of $42 million"". Profits or net income generally imply total revenue minus total expenses in a given period. In accounting, in the balance statement, revenue is a subsection of the Equity section and revenue increases equity, it is often referred to as the ""top line"" due to its position on the income statement at the very top. This is to be contrasted with the ""bottom line"" which denotes net income (gross revenues minus total expenses).[3] In general usage, revenue is income received by an organization in the form of cash or cash equivalents. Sales revenue is income received from selling goods or services over a period of time. Tax revenue is income that a government receives from taxpayers. Fundraising revenue is income received by a  charity from donors etc. to further its social purposes.
 In more formal usage, revenue is a calculation or estimation of periodic income based on a particular standard accounting practice or the rules established by a government or government agency. Two common accounting methods, cash basis accounting and accrual basis accounting, do not use the same process for measuring revenue. Corporations that offer shares for sale to the public are usually required by law to report revenue based on generally accepted accounting principles or on International Financial Reporting Standards.
 In a double-entry bookkeeping system, revenue accounts are general ledger accounts that are summarized periodically under the heading ""Revenue"" or ""Revenues"" on an income statement. Revenue account-names describe the type of revenue, such as ""Repair service revenue"", ""Rent revenue earned"" or ""Sales"".[4]"
Rights,Economics,6,"Rights are legal, social, or ethical principles of freedom or entitlement; that is, rights are the fundamental normative rules about what is allowed of people or owed to people according to some legal system, social convention, or ethical theory.[1] Rights are of essential importance in such disciplines as law and ethics, especially theories of justice and deontology.
 Rights are often considered fundamental to civilization, for they are regarded as established pillars of society and culture,[2] and the history of social conflicts can be found in the history of each right and its development. According to the Stanford Encyclopedia of Philosophy, ""rights structure the form of governments, the content of laws, and the shape of morality as it is currently perceived"".[1]"
Right_to_work_law,Economics,6,"
 In the context of US labor politics, ""right-to-work laws"" refers to state laws that prohibit union security agreements between employers and labor unions. Under these laws, employees in unionized workplaces are banned from negotiating contracts which require all members who benefit from the union contract to contribute to the costs of union representation.[1] According to the National Right to Work Legal Defense Foundation, right-to-work laws prohibit union security agreements, or agreements between employers and labor unions, that govern the extent to which an established union can require employees' membership, payment of union dues, or fees as a condition of employment, either before or after hiring. Right-to-work laws do not aim to provide general guarantee of employment to people seeking work, but rather are a government ban on contractual agreements between employers and union employees requiring workers to pay for the costs of union representation.[2] Right-to-work laws (either by statutes or by constitutional provision) exist in 27 US states, in the Southern, Midwestern, and interior Western states.[3][4] Such laws are allowed under the 1947 federal Taft–Hartley Act. A further distinction is often made within the law between people employed by state and municipal governments and those employed by the private sector, with states that are otherwise union shop (i.e., workers must pay for union representation in order to obtain or retain a job) having right to work laws in effect for government employees; provided, however, that the law also permits an ""agency shop"" where employees pay their share for representation (less than union dues), while not joining the union as members.
"
Risk_aversion,Economics,6,"In economics and finance, risk aversion is the behavior of humans (especially consumers and investors), who, when exposed to uncertainty, attempt to lower that uncertainty. It is the hesitation of a person to agree to a situation with an unknown payoff rather than another situation with a more predictable payoff but possibly lower expected payoff. For example, a risk-averse investor might choose to put their money into a bank account with a low but guaranteed interest rate, rather than into a stock that may have high expected returns, but also involves a chance of losing value.
"
Rivalry_(economics),Economics,6,"In economics, a good is said to be rivalrous or a rival if its consumption by one consumer prevents simultaneous consumption by other consumers,[1] or if consumption by one party reduces the ability of another party to consume it. A good is considered non-rivalrous or non-rival if, for any level of production, the cost of providing it to a marginal (additional) individual is zero.[2] A good can be placed along a continuum ranging from rivalrous to non-rivalrous. The same characteristic is sometimes referred to as jointness of supply or subtractable or non-subtractable.[3]Economist Paul Samuelson made the distinction between private and public goods in 1954 by introducing the concept of nonrival consumption. Economist Richard Musgrave followed on and added rivalry and excludability as criteria for defining consumption goods in 1959 and 1969.[4]   
 Most tangible goods, both durable and nondurable, are rival goods. A hammer is a durable rival good. One person's use of the hammer presents a significant barrier to others who desire to use that hammer at the same time. However, the first user does not ""use up"" the hammer, meaning that some rival goods can still be shared through time. An apple is a nondurable rival good: once an apple is eaten, it is ""used up"" and can no longer be eaten by others. Non-tangible goods can also be rivalrous. Examples include the ownership of radio spectra and domain names. In more general terms, almost all private goods are rivalrous.
 In contrast, non-rival goods may be consumed by one consumer without preventing simultaneous consumption by others. Most examples of non-rival goods are intangible. Broadcast television is an example of a non-rival good;  when a consumer turns on a TV set, this does not prevent the TV in another consumer's house from working.  The television itself is a rival good, but television broadcasts are non-rival goods. Other examples of non-rival goods include a beautiful scenic view, national defense, clean air, street lights, and public safety. More generally, most intellectual property is non-rival. In fact, certain types of intellectual property become more valuable as more people consume them (anti-rival). For example, the more people use a particular language, the more valuable that language becomes.
 Non-rivalry does not imply that the total production costs are low, but that the marginal production costs are zero. In reality, few goods are completely non-rival as rivalry can emerge at certain levels. For instance, use of public roads, the Internet, or police/law courts is non-rival up to a certain capacity, after which congestion means that each additional user decreases speed for others. For that, recent economic theory views rivalry as a continuum, not as a binary category,[5] where many goods are somewhere between the two extremes of completely rival and completely non-rival. A perfectly non-rival good can be consumed simultaneously by an unlimited number of consumers.
 There are four types of goods based on the characteristics of rival in consumption and excludability: Public Goods, Private Goods, Common Resources, and Club Goods.[6] Goods that are both non-rival and non-excludable are called public goods. Examples include clean air, national defense, and free-to-air broadcast TV. It is generally accepted by mainstream economists that the market mechanism will under-provide public goods, so these goods have to be produced by other means, including government provision.
 On the other hand, private goods are rival and excludable. An example of this is could be a Big Mac burger provided by McDonalds. An individual who consumes a Big Mac denies another individual from consuming the same one. It is excludable because consumption is only offered to those willing to pay the price.[7] Common resources are rival in consumption and non-excludable. An example is that of fisheries, which harvest fish from a shared common resource pool of fish stock. Fish caught by one group fishers are no longer accessible to another group, thus being rivalrous. However, oftentimes, due to an absence of well-defined property rights, it is difficult to restrict access to fishers who may overfish.[8] Goods that are both non-rival and excludable are called club goods. Cable television is an example of this. A large television service provider would already have infrastructure in place which would allow for the addition of new customers without infringing on existing customers viewing abilities. This would also mean that marginal cost would be close to zero, which satisfies the criteria for a good to be considered non-rival. However, access to cable TV services are only available to consumers willing to pay the price, demonstrating the excludability aspect.[9]    
"
Saving,Economics,6,"Saving is income not spent, or deferred consumption. Methods of saving include putting money aside in, for example, a deposit account, a pension account, an investment fund, or as cash.[1] Saving also involves reducing expenditures, such as recurring costs. In terms of personal finance, saving generally specifies low-risk preservation of money, as in a deposit account, versus investment, wherein risk is a lot higher; in economics more broadly, it refers to any income not used for immediate consumption. Saving does not automatically include interest. 
 Saving differs from savings. The former refers to the act of not consuming one's assets, whereas the latter refers to either multiple opportunities to reduce costs; or one's assets in the form of cash. Saving refers to an activity occurring over time, a flow variable, whereas savings refers to something that exists at any one time, a stock variable. This distinction is often misunderstood, and even professional economists and investment professionals will often refer to ""saving"" as ""savings"".[2] In different contexts there can be subtle differences in what counts as saving. For example, the part of a person's income that is spent on mortgage loan principal repayments is not spent on present consumption and is therefore saving by the above definition, even though people do not always think of repaying a loan as saving. However, in the U.S. measurement of the numbers behind its gross national product (i.e., the National Income and Product Accounts), personal interest payments are not treated as ""saving"" unless the institutions and people who receive them save them.
 Saving is closely related to physical investment, in that the former provides a source of funds for the latter. By not using income to buy consumer goods and services, it is possible for resources to instead be invested by being used to produce fixed capital, such as factories and machinery. Saving can therefore be vital to increase the amount of fixed capital available, which contributes to economic growth.
 However, increased saving does not always correspond to increased investment. If savings are not deposited into a financial intermediary such as a bank, there is no chance for those savings to be recycled as investment by business. This means that saving may increase without increasing investment, possibly causing a short-fall of demand (a pile-up of inventories, a cut-back of production, employment, and income, and thus a recession) rather than to economic growth. In the short term, if saving falls below investment, it can lead to a growth of aggregate demand and an economic boom. In the long term if saving falls below investment it eventually reduces investment and detracts from future growth. Future growth is made possible by foregoing present consumption to increase investment. However, savings not deposited into a financial intermediary amount to an (interest-free) loan to the government or central bank, who can recycle this loan.
 In a primitive agricultural economy, savings might take the form of holding back the best of the corn harvest as seed corn for the next planting season. If the whole crop were consumed the economy would convert to hunting and gathering the next season.
"
Scarcity,Economics,6,"Scarcity is the limited availability of a commodity, which may be in demand in the market or by the commons. Scarcity also includes an individual's lack of resources to buy commodities.[1] The opposite of scarcity is abundance.
"
Economic_sector,Economics,6,"One classical breakdown of economic activity distinguishes three sectors:[1] In the 20th century, economists began to suggest that traditional tertiary services could be further distinguished from ""quaternary"" and quinary service sectors. Economic activity in the hypothetical quaternary sector comprises information- and knowledge-based services, while quinary services include industry related to human services and hospitality.[2]"
Service_(economics),Economics,6,"In economics, a service is a transaction in which no physical goods are transferred from the seller to the buyer. The benefits of such a service are held to be demonstrated by the buyer's willingness to make the exchange. Public services are those that society (nation state, fiscal union or region) as a whole pays for. Using resources, skill, ingenuity, and experience, service providers benefit service consumers. Service is intangible in nature.
 In a narrower sense, service refers to quality of customer service: the measured appropriateness of assistance and support provided to a customer. This particular usage occurs frequently in retailing.
"
Service_economy,Economics,6,"Service economy can refer to one or both of two recent economic developments:
 The old dichotomy between product and service has been replaced by a service–product continuum. Many products are being transformed into services.
 For example, IBM treats its business as a service business. Although it still manufactures computers, it sees the physical goods as a small part of the ""business solutions"" industry. They have found that the price elasticity of demand for ""business solutions"" is much less than for hardware. There has been a corresponding shift to a subscription pricing model. Rather than receiving a single payment for a piece of manufactured equipment, many manufacturers are now receiving a steady stream of revenue for ongoing contracts.
 Full cost accounting and most accounting reform and monetary reform measures are usually thought to be impossible to achieve without a good model of the service economy.
 Since the 1950s, the global economy has undergone a structural transformation. For this change, the American economist Victor R. Fuchs called it “the service economy” in 1968. He believes that the United States has taken the lead in entering the service economy and society in the Western countries. The declaration heralded the arrival of a service economy that began in the United States on a global scale. With the rapid development of information revolution and technology, the service economy has also shown new development trends.[1]"
Shift_work,Economics,6,"Shift work is an employment practice designed to make use of, or provide service across, all 24 hours of the clock each day of the week (often abbreviated as 24/7). The practice typically sees the day divided into shifts, set periods of time during which different groups of workers perform their duties. The term ""shift work"" includes both long-term night shifts and work schedules in which employees change or rotate shifts.[1][2][3] In medicine and epidemiology, shift work is considered a risk factor for some health problems in some individuals, as disruption to circadian rhythms may increase the probability of developing cardiovascular disease, cognitive impairment, diabetes, and obesity, among other conditions.[4][5] Shift work can also contribute to strain in marital, family, and personal relationships.[6] A marriage where one partner works an irregular shift is six times more likely to end in divorce than a marriage where both partners work days.[7]"
Short_run,Economics,6,"In economics the long run is a theoretical concept in which all markets are in equilibrium, and all prices and quantities have fully adjusted and are in equilibrium. The long run contrasts with the short run, in which there are some constraints and markets are not fully in equilibrium. 
 More specifically, in microeconomics there are no fixed factors of production in the long run, and there is enough time for adjustment so that there are no constraints preventing changing the output level by changing the capital stock or by entering or leaving an industry. This contrasts with the short run, where some factors are variable (dependent on the quantity produced) and others are fixed (paid once), constraining entry or exit from an industry.  In macroeconomics, the long run is the period when the general price level,  contractual wage rates, and expectations adjust fully to the state of the economy, in contrast to the short run when these variables may not fully adjust.[1][2]"
Shortage,Economics,6,"
 In economics, a shortage or excess demand is a situation in which the demand for a product or service exceeds its supply in a market. It is the opposite of an excess supply (surplus).
"
Shrinkflation,Economics,6,"In economics, shrinkflation is the process of items shrinking in size or quantity, or even sometimes reformulating or reducing quality[1] while their prices remain the same or increase.[2][3]  The word is a portmanteau of the words shrink and inflation. First usage of the term has been attributed to both Pippa Malmgren and Brian Domitrovic.[4] Shrinkflation allows companies to increase their operating margin and profitability by reducing costs whilst maintaining sales volume.[5] Consumer protection groups are critical of the practice.
"
Social_behavior,Economics,6,"
 Social behavior is behavior among two or more organisms within the same species, and encompasses any behavior in which one member affects the other. This is due to an interaction among those members.[1][2] Social behavior can be seen as similar to an exchange of goods, with the expectation that when you give, you will receive the same.[3] This behavior can be effected by both the qualities of the individual and the environmental (situational) factors. Therefore, social behavior arises as a result of an interaction between the two—the organism and its environment. This means that, in regards to humans, social behavior can be determined by both the individual characteristics of the person, and the situation they are in.[4] A major aspect of social behavior is communication, which is the basis for survival and reproduction.[5] Social behavior is said to be determined by two different processes, that can either work together or oppose one another. The dual-systems model of reflective and impulsive determinants of social behavior came out of the realization that behavior cannot just be determined by one single factor. Instead, behavior can arise by those consciously behaving (where there is an awareness and intent), or by pure impulse. These factors that determine behavior can work in different situations and moments, and can even oppose one another. While at times one can behave with a specific goal in mind, other times they can behave without rational control, and driven by impulse instead.[6] There are also distinctions between different types of social behavior, such as mundane versus defensive social behavior. Mundane social behavior is a result of interactions in day-to-day life, and are behaviors learned as one is exposed to those different situations. On the other hand, defensive behavior arises out of impulse, when one is faced with conflicting desires.[7]"
Social_choice_theory,Economics,6,"Social choice theory or social choice  is a theoretical framework for analysis of combining individual opinions, preferences, interests, or welfares to reach a collective decision or social welfare in some sense.[1]  A non-theoretical example of a collective decision is enacting a law or set of laws under a constitution. Social choice theory dates from Condorcet's formulation of the voting paradox. Kenneth Arrow's Social Choice and Individual Values (1951) and Arrow's impossibility theorem in it are generally acknowledged as the basis of the modern social choice theory.[1] In addition to Arrow's theorem and the voting paradox, the Gibbard–Satterthwaite theorem, the Condorcet jury theorem, the median voter theorem, and May's theorem are among the more well known results from social choice theory.
 Social choice blends elements of welfare economics and voting theory. It is methodologically individualistic, in that it aggregates preferences and behaviors of individual members of society.  Using elements of formal logic for generality, analysis  proceeds from a set of seemingly reasonable axioms of social choice to form a social welfare function (or constitution).[2] Results uncovered the logical incompatibility of various axioms, as in Arrow's theorem, revealing an aggregation problem and suggesting reformulation or theoretical triage in dropping some axiom(s).[1] Later work also considers approaches to compensations and fairness, liberty and rights, axiomatic domain restrictions on preferences of agents, variable populations, strategy-proofing of social-choice mechanisms, natural resources,[1][3] capabilities and functionings,[4] and welfare,[5] justice,[6] and poverty.[7] Social choice and public choice theory may overlap but are disjoint if narrowly construed.  The Journal of Economic Literature classification codes place Social Choice  under Microeconomics at JEL D71   (with Clubs, Committees, and Associations) whereas most Public Choice subcategories are in JEL D72 (Economic Models of Political Processes: Rent-Seeking, Elections, Legislatures, and Voting Behavior).
"
Social_mobility,Economics,6,"
 
Social mobility is the movement of individuals, families, households, or other categories of people within or between social strata in a society.[1]  It is a change in social status relative to one's current social location within a given society. This movement occurs between layers or tiers in an open system of social stratification. Open stratification systems are those in which at least some value is given to achieved status characteristics in a society. The movement can be in a downward or upward direction.[2] Markers for social mobility, such as education and class,  are used to predict, discuss, and learn more about an individual or a group's mobility in society.  ""Class mobility to capture a person’s (or group’s) abilities (1) to accumulate wealth, and thereby ascend class hierarchies, and (2) to maintain a higher class status once attained.""[3]:50
"
Socialist_economics,Economics,6,"Socialist economics comprises the economic theories, practices and norms of hypothetical and existing socialist economic systems.[1] A socialist economic system is characterized by social ownership and operation of the means of production[2][3][4][5][6][7] that may take the form of autonomous cooperatives or direct public ownership wherein production is carried out directly for use rather than for profit.[8][9][10][11] Socialist systems that utilize markets for allocating capital goods and factors of production among economic units are designated market socialism. When planning is utilized, the economic system is designated as a socialist planned economy. Non-market forms of socialism usually include a system of accounting based on calculation-in-kind to value resources and goods.[12][13] Socialist economics has been associated with different schools of economic thought. Marxian economics provided a foundation for socialism based on analysis of capitalism[14] while neoclassical economics and evolutionary economics provided comprehensive models of socialism.[15] During the 20th century, proposals and models for both socialist planned and market economies were based heavily on neoclassical economics or a synthesis of neoclassical economics with Marxian or institutional economics.[16][17][18][19][20][21] As a term, socialist economics may also be applied to the analysis of former and existing economic systems that were implemented in socialist states such as in the works of Hungarian economist János Kornai.[22] 19th-century American individualist anarchist Benjamin Tucker, who connected the classical economics of Adam Smith and the Ricardian socialists as well as that of Pierre-Joseph Proudhon, Karl Marx and Josiah Warren to socialism, held that there were two schools of socialist thought, namely anarchist socialism and state socialism, maintaining that what they had in common was the labor theory of value.[23] Socialists disagree about the degree to which social control or regulation of the economy is necessary; how far society should intervene and whether government, particularly existing government, is the correct vehicle for change are issues of disagreement.[24]"
Sociality,Economics,6,"Sociality is the degree to which individuals in an animal population tend to associate in social groups (gregariousness) and form cooperative societies.
 Sociality is a survival response to evolutionary pressures.[1] For example, when a mother wasp stays near her larvae in the nest, parasites are less likely to eat the larvae.[2] Biologists suspect that pressures from parasites and other predators selected this behavior in wasps of the family Vespidae.
 This wasp behaviour evidences the most fundamental characteristic of animal sociality: parental investment. Parental investment is any expenditure of resources (time, energy, social capital) to benefit one's offspring. Parental investment detracts from a parent's capacity to invest in future reproduction and aid to kin (including other offspring). An animal that cares for its young but shows no other sociality traits is said to be subsocial.
 An animal that exhibits a high degree of sociality is called a social animal. The highest degree of sociality recognized by sociobiologists is eusociality. A eusocial taxon is one that exhibits overlapping adult generations, reproductive division of labor, cooperative care of young, and—in the most refined cases—a biological caste system.
"
Socioeconomics,Economics,6,"Socioeconomics (also known as social economics) is the social science that studies how economic activity affects and is shaped by social processes. In general it analyzes how modern societies progress, stagnate, or regress because of their local or regional economy, or the global economy. Societies are divided into three groups: social, cultural and economic. It also refers to the ways that social and economic factors influence the environment.
"
Sole_proprietorship,Economics,6,"A sole proprietorship, also known as the sole trader, individual entrepreneurship or proprietorship, is a type of enterprise that is owned and run by one person and in which there is no legal distinction between the owner and the business entity.  A sole trader does not necessarily work 'alone'—it is possible for the sole trader to employ other people.[1] The sole trader receives all profits (subject to taxation specific to the business) and has unlimited responsibility for all losses and debts. Every asset of the business is owned by the proprietor and all debts of the business are the proprietor's. It is a ""sole"" proprietorship in contrast with partnerships (which have at least two owners).
 A sole proprietor may use a trade name or business name other than their or its legal name. They may have to legally trademark their business name if it differs from their own legal name, the process varying depending upon country of residence.[2]"
Stagflation,Economics,6,"
 In economics, stagflation or recession-inflation is a situation in which the inflation rate is high, the economic growth rate slows, and unemployment remains steadily high. It presents a dilemma for economic policy, since actions intended to lower inflation may exacerbate unemployment.
 
The term, a portmanteau of stagnation and inflation, is generally attributed to Iain Macleod, a British Conservative Party politician who became Chancellor of the Exchequer in 1970. Macleod used the word in a 1965 speech to Parliament during a period of simultaneously high inflation and unemployment in the United Kingdom.[1][2][3][4] Warning the House of Commons of the gravity of the situation, he said: .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0} We now have the worst of both worlds—not just inflation on the one side or stagnation on the other, but both of them together. We have a sort of ""stagflation"" situation. And history, in modern terms, is indeed being made.[3][5] Macleod used the term again on 7 July 1970, and the media began also to use it, for example in The Economist on 15 August 1970, and Newsweek on 19 March 1973.
 John Maynard Keynes did not use the term, but some of his work refers to the conditions that most would recognise as stagflation. In the version of Keynesian macroeconomic theory that was dominant between the end of World War II and the late 1970s, inflation and recession were regarded as mutually exclusive, the relationship between the two being described by the Phillips curve. Stagflation is very costly and difficult to eradicate once it starts, both in social terms and in budget deficits.
"
Standard_of_living,Economics,6,"Standard of living is the level of comfort, material goods, and other whims available[clarification needed] to a person or group.  For the purposes of economics, politics and policy, it is usually compared across time or between groups defined by social, economic or geographical parameters.  Standard of living includes factors as a whole quality and availability of employment, class disparity, poverty rate, quality and housing affordability, hours of work are required to purchase necessities, gross domestic product, inflation rate, amount of leisure time, access to and quality of  healthcare, quality and availability of education, literacy rates, life expectancy, occurrence of diseases, cost of goods and services, infrastructure, access to, quality and affordability of public transportation, national economic growth, economic and political stability, freedom, environmental quality, climate and safety.  Standard of living is closely related to quality of life.[1]"
Stockholm_School,Economics,6,"
 The Stockholm School (Swedish: Stockholmsskolan) is a school of economic thought. It refers to a loosely organized group of Swedish economists that worked together, in Stockholm, Sweden primarily in the 1930s.
 The Stockholm School had—like John Maynard Keynes—come to the same conclusions in macroeconomics and the theories of demand and supply. Like Keynes, they were inspired by the works of Knut Wicksell, a Swedish economist active in the early years of the twentieth century.
 William Barber's comment upon Gunnar Myrdal's work on monetary theory goes like this:
 ""If his contribution had been available to readers of English before 1936, it is interesting to speculate whether the 'revolution' in macroeconomic theory of the depression decade would be referred to as 'Myrdalian' as much as 'Keynesian'”[1]"
Structural_unemployment,Economics,6,"Structural unemployment is a form of involuntary unemployment caused by a mismatch between the skills that workers in the economy can offer, and the skills demanded of workers by employers (also known as the skills gap).  Structural unemployment is often brought about by technological changes that make the job skills of many workers obsolete. 
 Structural unemployment is one of three categories of unemployment distinguished by economists, the others being frictional unemployment and cyclical unemployment.
 Because it requires either migration or re-training, structural unemployment can be long-term and slow to fix.[1]"
Substitution_effect,Economics,6,"
 
 In economics and particularly in consumer choice theory, the substitution effect is one component of the effect of a change in the price of a good upon the amount of that good demanded by a consumer, the other being the income effect. 
 When a good's price decreases, if hypothetically the same consumption bundle were to be retained, income would be freed up which could be spent on a combination of more of each of the goods. Thus the new total consumption bundle chosen, compared to the old one, reflects both the effect of the changed relative prices of the two goods (one unit of one good can now be traded for a different quantity of the other good than before as the ratio of their prices has changed) and the effect of the freed-up income. The effect of the relative price change is called the substitution effect, while the effect due to income having been freed up is called the income effect.     
 If income is altered in response to the price change such that a new budget line is drawn passing through the old consumption bundle but with the slope determined by the new prices and the consumer's optimal choice is on this budget line, the resulting change in consumption is called the Slutsky substitution effect. The idea is that the consumer is given enough money to purchase her old bundle at the new prices, and her choice changes are seen.
 If instead, a new budget line is found with the slope determined by the new prices but tangent to the indifference curve going through the old bundle, the difference between the new point of tangency and the old bundle is the Hicks substitution effect. The idea now is that the consumer is given just enough income to achieve her old utility at the new prices, and how her choice changes is seen. The Hicks substitution effect is illustrated in the next section.
 Some authors refer to one of these two concepts as simply the substitution effect. The popular textbook by Varian [1] describes the Slutsky variant as the primary one, but also gives a good explanation of the distinction.
 The same concepts also apply if the price of one good goes up instead of down, with the substitution effect reflecting the change in relative prices and the income effect reflecting the fact the income has been soaked up into additional spending on the retained units of the now-pricier good.
 For example, consider coffee and tea. If the price of coffee increased, consumers of hot drinks may decide to start drinking tea instead. This will cause the demand for tea to increase. Likewise, if the price of coffee was to decrease, tea-drinkers may decide to shift their drinking habits and substitute coffee for their daily drinking habits, causing the demand for tea to decrease.
 Economists had long understood that changes in price could lead to two main responses by consumers, initial work on this subject had been done by Vilfredo Pareto in the 1890s, but it wasn’t until Slutsky’s 1915 article that rigor was brought to the subject. Slutsky’s original paper was published during World War I in Italian, because of this economists in the Anglo-American world did not become aware of Slutsky’s contributions until the 1930s.[2] The English world was fully introduced to Slutsky's ideas in 1934 when ""A Reconsideration of the Theory of Value"" was published by John Hicks and RGD Allen, this paper built upon work by Pareto and came to conclusions Slutsky had realized two decades prior.[3] "
Substitute_good,Economics,6,"In microeconomics, two goods are substitutes if the products could be used for the same purpose by the consumers.[1] That is, a consumer perceives both goods as similar or comparable, so that having more of one good causes the consumer to desire less of the other good. Contrary to complementary goods and independent goods, substitute goods may replace each other in use due to changing economic conditions.[2]"
Sunk_costs,Economics,6,"In economics and business decision-making, a sunk cost (also known as retrospective cost) is a cost that has already been incurred and cannot be recovered.[1][2][3] Sunk costs are contrasted with prospective costs, which are future costs that may be avoided if action is taken.[4] In other words, a sunk cost is a sum paid in the past that is no longer relevant to decisions about the future.[3] Even though economists argue that sunk costs are no longer relevant to future rational decision-making, in everyday life, people often take previous expenditures in situations, such as repairing a car or house, into their future decisions regarding those properties.
"
Supply_(economics),Economics,6,"In economics, supply is the amount of a resource that firms, producers, labourers, providers of financial assets, or other economic agents are willing and able to provide to the marketplace or directly to another agent in the marketplace. Supply can be in currency, time, raw materials, or any other scarce or valuable object that can be provided to another agent. This is often fairly abstract. For example in the case of time, supply is not transferred to one agent from another, but one agent may offer some other resource in exchange for the first spending time doing something. Supply is often plotted graphically as a supply curve, with the quantity provided (the dependent variable) plotted horizontally and the price (the independent variable) plotted vertically.
 In the goods market, supply is the amount of a product per unit of time that producers are willing to sell at various given prices when all other factors are held constant. In the labor market, the supply of labor is the amount of time per week, month, or year that individuals are willing to spend working, as a function of the wage rate.
 In financial markets, the money supply is the amount of highly liquid assets available in the money market, which is either determined or influenced by a country's monetary authority. This can vary based on which type of money supply one is discussing. M1 for example is commonly used to refer to narrow money, coins, cash, and other money equivalents that can be converted to currency nearly instantly. M2 by contrast includes all of M1 but also includes short-term deposits and certain types of market funds.
"
Supply_and_demand,Economics,6,"In microeconomics, supply and demand is an economic model of price determination in a market. It postulates that, holding all else equal, in a competitive market, the unit price for a particular good, or other traded item such as labor or liquid financial assets, will vary until it settles at a point where the quantity demanded (at the current price) will equal the quantity supplied (at the current price), resulting in an economic equilibrium for price and quantity transacted.
"
Supply_chain,Economics,6,"In commerce, a supply chain is a system of organizations, people, activities, information, and resources involved in
supplying a product or service to a consumer. Supply chain activities involve the transformation of natural resources, raw materials, and components into a finished product that is delivered to the end customer.[3] In sophisticated supply chain systems, used products may re-enter the supply chain at any point where residual value is recyclable. Supply chains link value chains.[4]"
Supply_curve,Economics,6,"In economics, supply is the amount of a resource that firms, producers, labourers, providers of financial assets, or other economic agents are willing and able to provide to the marketplace or directly to another agent in the marketplace. Supply can be in currency, time, raw materials, or any other scarce or valuable object that can be provided to another agent. This is often fairly abstract. For example in the case of time, supply is not transferred to one agent from another, but one agent may offer some other resource in exchange for the first spending time doing something. Supply is often plotted graphically as a supply curve, with the quantity provided (the dependent variable) plotted horizontally and the price (the independent variable) plotted vertically.
 In the goods market, supply is the amount of a product per unit of time that producers are willing to sell at various given prices when all other factors are held constant. In the labor market, the supply of labor is the amount of time per week, month, or year that individuals are willing to spend working, as a function of the wage rate.
 In financial markets, the money supply is the amount of highly liquid assets available in the money market, which is either determined or influenced by a country's monetary authority. This can vary based on which type of money supply one is discussing. M1 for example is commonly used to refer to narrow money, coins, cash, and other money equivalents that can be converted to currency nearly instantly. M2 by contrast includes all of M1 but also includes short-term deposits and certain types of market funds.
"
Supply_(economics),Economics,6,"In economics, supply is the amount of a resource that firms, producers, labourers, providers of financial assets, or other economic agents are willing and able to provide to the marketplace or directly to another agent in the marketplace. Supply can be in currency, time, raw materials, or any other scarce or valuable object that can be provided to another agent. This is often fairly abstract. For example in the case of time, supply is not transferred to one agent from another, but one agent may offer some other resource in exchange for the first spending time doing something. Supply is often plotted graphically as a supply curve, with the quantity provided (the dependent variable) plotted horizontally and the price (the independent variable) plotted vertically.
 In the goods market, supply is the amount of a product per unit of time that producers are willing to sell at various given prices when all other factors are held constant. In the labor market, the supply of labor is the amount of time per week, month, or year that individuals are willing to spend working, as a function of the wage rate.
 In financial markets, the money supply is the amount of highly liquid assets available in the money market, which is either determined or influenced by a country's monetary authority. This can vary based on which type of money supply one is discussing. M1 for example is commonly used to refer to narrow money, coins, cash, and other money equivalents that can be converted to currency nearly instantly. M2 by contrast includes all of M1 but also includes short-term deposits and certain types of market funds.
"
Supply_shock,Economics,6,"A supply shock is an event that suddenly increases or decreases the supply of a commodity or service, or of commodities and services in general. This sudden change affects the equilibrium price of the good or service or the economy's general price level.
 In the short run, an economy-wide negative supply shock will shift the aggregate supply curve leftward, decreasing the output and increasing the price level.[1] For example, the imposition of an embargo on trade in oil would cause an adverse supply shock, since oil is a key factor of production for a wide variety of goods.  A supply shock can cause stagflation due to a combination of rising prices and falling output.
 In the short run, an economy-wide positive supply shock will shift the aggregate supply curve rightward, increasing output and decreasing the price level.[1] A positive supply shock could be an advance in technology (a technology shock) which makes production more efficient, thus increasing output.
"
Supply-side_economics,Economics,6,"Supply-side economics is a macroeconomic theory arguing that economic growth can be most effectively created by lowering taxes and decreasing regulation,[1][2] by which it is directly opposed to demand-side economics. According to supply-side economics, consumers will then benefit from a greater supply of goods and services at lower prices and employment will increase.[3] The Laffer curve, a theoretical relationship between rates of taxation and government revenue which suggests that lower tax rates when the tax level is too high will actually boost government revenue because of higher economic growth, is one of the main[citation needed] theoretical constructs of supply-side economics.[4][5][6] The term ""supply-side economics"" was thought for some time to have been coined by journalist Jude Wanniski in 1975, but according to Robert D. Atkinson the term ""supply side"" was first used in 1976 by Herbert Stein (a former economic adviser to President Richard Nixon) and only later that year was this term repeated by Jude Wanniski.[7] Its use connotes the ideas of economists Robert Mundell and Arthur Laffer.
"
Economic_surplus,Economics,6,"In mainstream economics, economic surplus, also known as total welfare or Marshallian surplus (after Alfred Marshall), refers to two related quantities: 
"
Tariff,Economics,6,"A tariff is a tax imposed by a government on imports or exports of goods. Besides being a source of revenue for the government, import duties can also be a form of regulation of foreign trade and policy that taxes foreign products to encourage or safeguard domestic industry. Tariffs are among the most widely used instruments of protectionism, along with import and export quotas.
 Tariffs can be fixed (a constant sum per unit of imported goods or a percentage of the price) or variable (the amount varies according to the price). Taxing imports means people are less likely to buy them as they become more expensive. The intention is that they buy local products instead – boosting the country's economy. Tariffs therefore provide an incentive to develop production and replace imports with domestic products. Tariffs are meant to reduce pressure from foreign competition and reduce the trade deficit. They have historically been justified as a means to protect infant industries and to allow import substitution industrialization. Tariffs may also be used to rectify artificially low prices for certain imported goods, due to 'dumping', export subsidies or currency manipulation.
 There is near unanimous consensus among economists that tariffs have a negative effect on economic growth and economic welfare while free trade and the reduction of trade barriers has a positive effect on economic growth.[1][2][3][4][5][6] However, liberalization of trade can cause significant and unequally distributed losses, and the economic dislocation of workers in import-competing sectors.[2]"
Tax,Economics,6,"
 A tax is a compulsory financial charge or some other type of levy imposed on a taxpayer (an individual or legal entity) by a governmental organization in order to fund government spending and various public expenditures.[2] A failure to pay, along with evasion of or resistance to taxation, is punishable by law. Taxes consist of direct or indirect taxes and may be paid in money or as its labour equivalent. The first known taxation took place in Ancient Egypt around 3000–2800 BC.
 Most countries have a tax system in place to pay for public, common, or agreed national needs and government functions. Some levy a flat percentage rate of taxation on personal annual income, but most scale taxes based on annual income amounts. Most countries charge a tax on an individual's income as well as on corporate income. Countries or subunits often also impose wealth taxes, inheritance taxes, estate taxes, gift taxes, property taxes, sales taxes, payroll taxes or tariffs.
 In economic terms, taxation transfers wealth from households or businesses to the government. This has effects which can both increase and reduce economic growth and economic welfare. Consequently, taxation is a highly debated topic.
"
Tax_rate,Economics,6,"In a tax system, the tax rate is the ratio (usually expressed as a percentage) at which a business or person is taxed.  There are several methods used to present a tax rate: statutory, average, marginal, and effective.  These rates can also be presented using different definitions applied to a tax base: inclusive and exclusive.
"
Terms_of_trade,Economics,6,"The terms of trade (TOT) is the relative price of exports in terms of imports[1] and is defined as the ratio of export prices to import prices.[2] It can be interpreted as the amount of import goods an economy can purchase per unit of export goods.
 An improvement of a nation's terms of trade benefits that country in the sense that it can buy more imports for any given level of exports.  The terms of trade may be influenced by the exchange rate because a rise in the value of a country's currency lowers the domestic prices of its imports but may not directly affect the prices of the commodities it exports.
"
Theory_of_the_firm,Economics,6,"The theory of the firm consists of a number of economic theories that explain and predict the nature of the firm, company, or corporation, including its existence, behaviour, structure, and relationship to the market.[1]"
Thermoeconomics,Economics,6,"Thermoeconomics, also referred to as biophysical economics, is a school of heterodox economics that applies the laws of statistical mechanics to economic theory.[1]  Thermoeconomics can be thought of as the statistical physics of economic value[2] and is a subfield of econophysics.
"
Time_value_of_money,Economics,6,"The time value of money is the widely accepted conjecture that there is greater benefit to receiving a sum of money now rather than an identical sum later. It may be seen as an implication of the later-developed concept of time preference.
 The time value of money is the reason why interest is paid or earned: interest, whether it is on a bank deposit or debt, compensates the depositor or lender for the time value of money. Hence, It also underlies investment. Investors are willing to forgo spending their money now only if they expect a favorable return on their investment in the future, such that the increased value to be available later is sufficiently high to offset the preference to spending money now; see required rate of return.
"
Total_cost,Economics,6,"In economics, total cost (TC) is the total economic cost of production and is made up of variable cost, which varies according to the quantity of a good produced and includes inputs such as labor and raw materials, plus fixed cost, which is independent of the quantity of a good produced and includes inputs that cannot be varied in the short term: fixed costs such as buildings and machinery, including sunk costs if any.
 Total cost in economics, unlike in cost accounting, includes the total opportunity cost (implicit cost) of each factor of production as part of its fixed or variable costs.
 The rate at which total cost changes as the amount produced changes is called marginal cost. This is also known as the marginal unit variable cost.
 The total cost of producing a specific level of output is the cost of all the factors of input used. Often, economists use models with two inputs: physical capital, with quantity X and labor, with quantity L. Capital is assumed to be the fixed input, meaning that the amount of capital used does not vary with the level of production in the short run. The rental price per unit of capital is denoted r. Thus, the total fixed cost equals Kr. Labor is the variable input, meaning that the amount of labor used varies with the level of output. In fact, in the short run, the only way to vary output is by varying the amount of the variable input. Labor usage is denoted L and the per unit cost, or wage rate, is denoted w, so the variable cost is Lw. Consequently, total cost is fixed cost (FC) plus variable cost (VC), or TC = FC + VC = Kr+Lw. In the long run, however, both capital usage and labor usage are variable.
 Other economic models have the total variable cost curve (and therefore total cost curve) illustrate the concepts of increasing, and later diminishing, marginal return.
 In marketing, it is necessary to know how total costs divide between variable and fixed. ""This distinction is crucial in forecasting the earnings generated by various changes in unit sales and thus the financial impact of proposed marketing campaigns.""[citation needed] In a survey of nearly 200 senior marketing managers, 60% responded that they found the ""variable and fixed costs"" metric very useful.[1]"
Trade,Economics,6,"Trade involves the transfer of goods or services from one person or entity to another, often in exchange for money. Economists refer to a system or network that allows trade as a market.
 An early form of trade, barter, saw the direct exchange of goods and services for other goods and services.[1][need quotation to verify] Barter involves trading things without the use of money.[1]  When either bartering party started to involve precious metals, these gained symbolic as well as practical importance.[citation needed] Modern traders generally negotiate through a medium of exchange, such as money. As a result, buying can be separated from selling, or earning. The  invention of money (and later of credit, paper money and  non-physical money) greatly simplified and promoted trade.  Trade between two traders is called bilateral trade, while trade involving more than two traders is called multilateral trade.
 In one modern view, trade exists due to specialization and the division of labor, a predominant form of economic activity in which individuals and groups concentrate on a small aspect of production, but use their output in trades for other products and needs.[2] Trade exists between regions because different regions may have a comparative advantage (perceived or real) in the production of some trade-able commodity—including production of natural resources scarce or limited elsewhere. For example: different regions' sizes may encourage mass production. In such circumstances, trade at market prices between locations can benefit both locations.
 Retail trade consists of the sale of goods or merchandise from a very fixed location[3] (such as a department store, boutique or kiosk), online or by mail, in small or individual lots for direct consumption or use by the purchaser.[4] Wholesale trade is defined[by whom?] as traffic in goods that are sold as merchandise to retailers, or to industrial, commercial, institutional, or other professional business users, or to other wholesalers and related subordinated services.
 Historically, openness to free trade substantially increased in some areas from 1815 to the outbreak of World War I[citation needed] in 1914. Trade openness increased again during the 1920s, but collapsed (in particular in Europe and North America) during the Great Depression of the 1930s. Trade openness increased substantially again from the 1950s onwards (albeit with a slowdown during the oil crisis of the 1970s). Economists and economic historians[which?] contend that current levels of trade openness are the highest they have ever been.[5][6][7]"
Transaction_cost,Economics,6,"In economics and related disciplines, a transaction cost is a cost in making any economic trade when participating in a market.[1] Oliver E. Williamson defines transaction costs as the costs of running an economic system of companies, and unlike production costs, decision-makers determine strategies of companies by measuring transaction costs and production costs. Transaction costs are the total costs of making a transaction, including the cost of planning, deciding, changing plans, resolving disputes, and after-sales. Therefore, the transaction cost is one of the most significant factors in business operation and management[2] Oliver E. Williamson's Transaction Cost Economics popularized the concept of transaction costs.[3] Douglass C. North argues that institutions, understood as the set of rules in a society, are key in the determination of transaction costs. In this sense, institutions that facilitate low transaction costs, boost economic growth.[4] Douglass North states that there are four factors that comprise transaction costs – ""measurement,"" ""enforcement,"" ""ideological attitudes and perceptions,"" and ""the size of the market.""[4] Measurement refers to the calculation of the value of all aspects of the good or service involved in the transaction.[4] Enforcement can be defined as the need for an unbiased third party to ensure that neither party involved in the transaction reneges on their part of the deal.[4] These first two factors appear in the concept of ideological attitudes and perceptions, North's third aspect of transaction costs.[4] Ideological attitudes and perceptions encapsulate each individual's set of values, which influences their interpretation of the world.[4] The final aspect of transaction costs, according to North, is market size, which affects the partiality or impartiality of transactions.[4] Transaction costs can be divided into three broad categories:[5] For example, the buyer of a used car faces a variety of different transaction costs. The search costs are the costs of finding a car and determining the car's condition. The bargaining costs are the costs of negotiating a price with the seller. The policing and enforcement costs are the costs of ensuring that the seller delivers the car in the promised condition.
"
Transport_economics,Economics,6,"
 Transport economics is a branch of economics founded in 1959 by American economist John R. Meyer that deals with the allocation of resources within the transport sector. It has strong links to civil engineering. Transport economics differs from some other branches of economics in that the assumption of a spaceless, instantaneous economy does not hold. People and goods flow over networks at certain speeds.  Demands peak.  Advance ticket purchase is often induced by lower fares.  The networks themselves may or may not be competitive. A single trip (the final good, in the consumer's eyes) may require the bundling of services provided by several firms, agencies and modes.[1] Although transport systems follow the same supply and demand theory as other industries, the complications of network effects and choices between dissimilar goods (e.g. car and bus travel) make estimating the demand for transportation facilities difficult. The development of models to estimate the likely choices between the goods involved in transport decisions (discrete choice models) led to the development of an important branch of econometrics, as well as a Nobel Prize for Daniel McFadden.
 In transport, demand can be measured in number of journeys made or in total distance traveled across all journeys (e.g. passenger-kilometers for public transport or vehicle-kilometers of travel (VKT) for private transport). Supply is considered to be a measure of capacity. The price of the good (travel) is measured using the generalised cost of travel, which includes both money and time expenditure.
 The effect of increases in supply (i.e. capacity) are of particular interest in transport economics (see induced demand), as the potential environmental consequences are significant (see externalities below).
"
Trough_(economics),Economics,6,"In economics, a trough is a low turning point or a local minimum of a business cycle.  The time evolution of many variables of economics exhibit a wave like behavior with local maxima (peaks) followed by local minima (troughs).  A business cycle may be defined as the period between two consecutive peaks.[1][2] The period of the business cycle in which real GDP is increasing is called the expansion. In which the real GDP moves from the trough towards the peak.
[3]"
Underemployment,Economics,6,"Underemployment is the underuse of a worker because a job does not use the worker's skills, is part-time, or leaves the worker idle.[2] Examples include holding a part-time job despite desiring full-time work, and overqualification, in which the employee has education, experience, or skills beyond the requirements of the job.[3][4] Underemployment has been studied from a variety of perspectives, including economics, management, psychology, and sociology. In economics, for example, the term underemployment has three different distinct meanings and applications. All of the meanings involve a situation in which a person is working, unlike unemployment, where a person who is searching for work cannot find a job. All meanings involve under-utilization of labor which is missed by most official (governmental agency) definitions and measurements of unemployment.
 In economics, underemployment can refer to:
 Underemployment is a significant cause of poverty because although the worker may be able to find part-time work, the part-time pay may not be sufficient for basic needs. Underemployment is a problem particularly in developing countries, where the unemployment rate is often quite low, as most workers are doing subsistence work or occasional part-time jobs. The global average of full-time workers per adult population is only 26%, compared to 30–52% in developed countries and 5–20% in most of Africa.[7]"
Unemployment,Economics,6,"Unemployment, according to the OECD (Organisation for Economic Co-operation and Development), is persons above a specified age (usually 15)[2] not being in paid employment or self-employment but currently available for work during the reference period.[3] Unemployment is measured by the unemployment rate, which is the number of people who are unemployed as a percentage of the labour force (the total number of people employed added to those unemployed).[4] Unemployment can have many sources, such as the following:
 Unemployment and the status of the economy can be influenced by a country through, for example, fiscal policy. Furthermore, the monetary authority of a country, such as the central bank, can influence the availability and cost for money through its monetary policy.
 In addition to theories of unemployment, a few categorisations of unemployment are used for more precisely modelling the effects of unemployment within the economic system. Some of the main types of unemployment include structural unemployment, frictional unemployment, cyclical unemployment, involuntary unemployment and classical unemployment. Structural unemployment focuses on foundational problems in the economy and inefficiencies inherent in labor markets, including a mismatch between the supply and demand of laborers with necessary skill sets. Structural arguments emphasize causes and solutions related to disruptive technologies and globalization. Discussions of frictional unemployment focus on voluntary decisions to work based on individuals' valuation of their own work and how that compares to current wage rates added to the time and effort required to find a job. Causes and solutions for frictional unemployment often address job entry threshold and wage rates.
 According to the UN's International Labour Organization (ILO), there were 172 million people worldwide (or 5% of the reported global workforce) without work in 2018.[5] Because of the difficulty in measuring the unemployment rate by, for example, using surveys (as in the United States) or through registered unemployed citizens (as in some European countries), statistical figures such as the employment-to-population ratio might be more suitable for evaluating the status of the workforce and the economy if they were based on people who are registered, for example, as taxpayers.[6]"
Unit_of_account,Economics,6,"In economics, unit of account is one of the functions of money. The value of something is measured in a specific currency. This allows different things to be compared against each other; for example, goods, services, assets, liabilities, labor, income, expenses.[1] It lends meaning to profits, losses, liability and assets.
 A unit of account in financial accounting refers to the numbers that are used to describe the specific assets and liabilities that are reported in financial statements rather than the units used to measure them.[2]  Unit of account and unit of measure are sometimes treated as synonyms in financial accounting and economics.[2] Historically, many transaction prices were agreed using a quantity of a unit of account.  The actual payment might include a number of  other types of coin, and other goods, agreed to be equivalent in total to the sum agreed.
"
Unskilled_labor,Economics,6,"Skill is a measure of the amount of worker's expertise, specialization, wages, and supervisory capacity. Skilled workers are generally more trained, higher paid, and have more responsibilities than unskilled workers.[1] Skilled workers have long had historical import (see Division of labor) as masons, carpenters, blacksmiths, bakers, brewers, coopers, printers and other occupations that are economically productive. Skilled workers were often politically active through their craft guilds.

"
Urban_economics,Economics,6,"Urban economics is broadly the economic study of urban areas; as such, it involves using the tools of economics to analyze urban issues such as crime, education, public transit, housing, and local government finance. More specifically, it is a branch of microeconomics that studies urban spatial structure and the location of households and firms (Quigley 2008).
 Much urban economic analysis relies on a particular model of urban spatial structure, the monocentric city model pioneered in the 1960s by William Alonso, Richard Muth, and Edwin Mills. While most other forms of neoclassical economics do not account for spatial relationships between individuals and organizations, urban economics focuses on these spatial relationships to understand the economic motivations underlying the formation, functioning, and development of cities.
 Since its formulation in 1964, Alonso's monocentric city model of a disc-shaped Central Business District (CBD) and the surrounding residential region has served as a starting point for urban economic analysis. Monocentricity has weakened over time because of changes in technology, particularly, faster and cheaper transportation (which makes it possible for commuters to live farther from their jobs in the CBD) and communications (which allow back-office operations to move out of the CBD).
 Additionally, recent research has sought to explain the polycentricity described in Joel Garreau's Edge City. Several explanations for phallocentric expansion have been proposed and summarized in models that account for factors such as utility gains from lower average land rents and increasing (or constant returns) due to economies of agglomeration (Strange 2008).
"
Utilitarianism,Economics,6,"
 Utilitarianism is a family of normative ethical theories that prescribe actions that maximize happiness and well-being for all affected individuals.[1][2] Although different varieties of utilitarianism admit different characterizations, the basic idea behind all of them is to in some sense maximize utility, which is often defined in terms of well-being or related concepts. For instance, Jeremy Bentham, the founder of utilitarianism, described utility as ""that property in any object, whereby it tends to produce benefit, advantage, pleasure, good, or happiness...[or] to prevent the happening of mischief, pain, evil, or unhappiness to the party whose interest is considered."" 
 Utilitarianism is a version of consequentialism, which states that the consequences of any action are the only standard of right and wrong. Unlike other forms of consequentialism, such as egoism and altruism, utilitarianism considers the interests of all humans equally. Proponents of utilitarianism have disagreed on a number of points, such as whether actions should be chosen based on their likely results (act utilitarianism), or whether agents should conform to rules that maximize utility (rule utilitarianism). There is also disagreement as to whether total (total utilitarianism), average (average utilitarianism) or minimum utility[3] should be maximized.
 Though the seeds of the theory can be found in the hedonists Aristippus and Epicurus, who viewed happiness as the only good, and in the work of the medieval Indian philosopher Śāntideva, the tradition of utilitarianism properly began with Bentham, and has included John Stuart Mill, Henry Sidgwick, R. M. Hare, and Peter Singer. The concept has been applied towards social welfare economics, the crisis of global poverty, the ethics of raising animals for food, and the importance of avoiding existential risks to humanity.
"
Value_(economics),Economics,6,"In economics, economic value is a measure of the benefit provided by a good or service to an economic agent. It is generally measured relative to units of currency, and the interpretation is therefore ""what is the maximum amount of money a specific actor is willing and able to pay for the good or service""?
 Among the competing schools of economic theory there are differing theories of value.
 Economic value is not the same as market price, nor is economic value the same thing as market value. If a consumer is willing to buy a good, it implies that the customer places a higher value on the good than the market price. The difference between the value to the consumer and the market price is called ""consumer surplus"".[1] It is easy to see situations where the actual value is considerably larger than the market price: purchase of drinking water is one example.
"
Value-added_tax,Economics,6,"
 A value-added tax (VAT), known in some countries as a goods and services tax (GST), is a type of tax that is assessed incrementally. It is levied on the price of a product or service at each stage of production, distribution, or sale to the end consumer. If the ultimate consumer is a business that collects and pays to the government VAT on its products or services, it can reclaim the tax paid. It is similar to, and is often compared with, a sales tax.
 VAT essentially compensates for the shared service and infrastructure provided in a certain locality by a state and funded by its taxpayers that were used in the provision of that product or service.[citation needed] Not all localities require VAT to be charged, and exports are often exempt. VAT is usually implemented as a destination-based tax, where the tax rate is based on the location of the consumer and applied to the sales price. The terms VAT, GST, and the more general consumption tax are sometimes used interchangeably. VAT raises about a fifth of total tax revenues both worldwide and among the members of the Organisation for Economic Co-operation and Development (OECD).[1]:14 As of 2018, 166 of the 193 countries with full UN membership employ a VAT, including all OECD members except the United States,[1]:14 where many states use a sales tax system instead.
 There are two main methods of calculating VAT: the credit-invoice or invoice-based method, and the subtraction or accounts-based method. Using the credit-invoice method, sales transactions are taxed, with the customer informed of the VAT on the transaction, and businesses may receive a credit for VAT paid on input materials and services. The credit-invoice method is the most widely employed method, used by all national VATs except for Japan. Using the subtraction method, at the end of a reporting period, a business calculates the value of all taxable sales then subtracts the sum of all taxable purchases and the VAT rate is applied to the difference. The subtraction method VAT is currently only used by Japan, although subtraction method VATs, often using the name ""flat tax,"" have been part of many recent tax reform proposals by US politicians.[2][3][4] With both methods, there are exceptions in the calculation method for certain goods and transactions, created for either pragmatic collection reasons or to counter tax fraud and evasion.
"
Variable_cost,Economics,6,"Variable costs are costs that change as the quantity of the good or service that a business produces changes.[1] Variable costs are the sum of marginal costs over all units produced. They can also be considered normal costs. Fixed costs and variable costs make up the two components of total cost. Direct costs are costs that can easily be associated with a particular cost object.[2] However, not all variable costs are direct costs. For example, variable manufacturing overhead costs are variable costs that are indirect costs, not direct costs.  Variable costs are sometimes called unit-level costs as they vary with the number of units produced.
 Direct labor and overhead are often called conversion cost,[3] while direct material and direct labor are often referred to as prime cost.[3] In marketing, it is necessary to know how costs divide between variable and fixed. This distinction is crucial in forecasting the earnings generated by various changes in unit sales and thus the financial impact of proposed marketing campaigns. In a survey of nearly 200 senior marketing managers, 60 percent responded that they found the ""variable and fixed costs"" metric very useful.[4] The level of variable cost is influenced by many factors, such as fixed cost, duration of project, uncertainty and discount rate. An analytical formula of variable cost as a function of these factors has been derived. It can be used to assess how different factors impact variable cost and total return in an investment.[5]"
Velocity_of_money,Economics,6,"The velocity of money (or the velocity of circulation of money) is a measure of the number of times that the average unit of currency is used to purchase goods and services within a given time period.[3] The concept relates the size of economic activity to a given money supply and the speed of money exchange is one of the variables that determine inflation. The measure of the velocity of money is usually the ratio of gross national product (GNP) to a country's money supply.
 If the velocity of money is increasing, then transactions are occurring between individuals more frequently.[3] The velocity of money changes over time and is influenced by a variety of factors.[4]"
Wage,Economics,6,"A wage is the distribution from an employer of a security (expected return or profits derived solely from others) paid to an employee. Like interest is paid out to an investor on his investments, a wage is paid as earnings to the employee on his invested assets (time, money, labor, resources, and thought). Some examples of wage distributions include compensatory payments such as minimum wage, prevailing wage, and yearly bonuses, and remunerative payments such as prizes and tip payouts.
 Wages are part of the expenses that are involved in running a business, and add value to the employee in honor of his principal protected note or net investment.
 Payment by wage contrasts with salaried work, in which the employer pays an arranged amount at steady intervals (such as a week or month) regardless of hours worked, with commission which conditions pay on individual performance, and with compensation based on the performance of the company as a whole. Waged employees may also receive tips or gratuity paid directly by clients and employee benefits which are non-monetary forms of compensation. Since wage labour is the predominant form of work, the term ""wage"" sometimes refers to all forms (or all monetary forms) of employee compensation.
"
Want,Economics,6,"The idea of want can be examined from many perspectives. In secular societies want might be considered similar to the emotion desire, which can be studied scientifically through the disciplines of psychology or sociology. Want might also be examined in economics as a necessary ingredient in sustaining and perpetuating capitalist societies that are organised around principles like consumerism. Alternatively want can be studied in a non-secular, spiritual, moralistic or religious way, particularly by Buddhism but also Christianity, Islam and Judaism.
 In economics, a want is something that is desired. It is said that every person has unlimited wants, but limited resources (economics is based on the assumption that only limited resources are available to us). Thus, people cannot have everything they want and must look for the most affordable alternatives.
 Wants are often distinguished from needs. A need is something that is necessary for survival (such as food and shelter), whereas a want is simply something that a person would like to have.[1] Some economists have rejected this distinction and maintain that all of these are simply wants, with varying levels of importance. By this viewpoint, wants and needs can be understood as examples of the overall concept of demand.
"
Wealth,Economics,6,"
 Wealth is the abundance of valuable financial assets or physical possessions which can be converted into a form that can be used for transactions. This includes the core meaning as held in the originating old English word weal, which is from an Indo-European word stem.[2] The modern concept of wealth is of significance in all areas of economics, and clearly so for growth economics and development economics, yet the meaning of wealth is context-dependent. An individual possessing a substantial net worth is known as wealthy. Net worth is defined as the current value of one's assets less liabilities (excluding the principal in trust accounts).[3] At the most general level, economists may define wealth as ""anything of value"" that captures both the subjective nature of the idea and the idea that it is not a fixed or static concept. Various definitions and concepts of wealth have been asserted by various individuals and in different contexts.[4] Defining wealth can be a normative process with various ethical implications, since often wealth maximization is seen as a goal or is thought to be a normative principle of its own.[5][6] A community, region or country that possesses an abundance of such possessions or resources to the benefit of the common good is known as wealthy.
 The United Nations definition of inclusive wealth is a monetary measure which includes the sum of natural, human, and physical assets.[7][8]  Natural capital includes land, forests, energy resources, and minerals.  Human capital is the population's education and skills.  Physical (or ""manufactured"") capital includes such things as machinery, buildings, and infrastructure.
"
Wealth_effect,Economics,6,"The wealth effect is the change in spending that accompanies a change in perceived wealth.[1]
Usually the wealth effect is positive: spending changes in the same direction as perceived wealth.
"
Welfare,Economics,6,"
 Welfare is a type of government support intended to ensure that members of a society can meet basic human needs such as food and shelter.[1] Social security may either be synonymous with welfare,[a] or refer specifically to social insurance programs, which provide support only to those who have previously contributed (e.g. most pension systems), as opposed to social assistance programs, which provide support on the basis of need alone (e.g. most disability benefits).[6][7] The International Labour Organization defines social security as covering support for those in old age, support for the maintenance of children, medical treatment, parental and sick leave, unemployment and disability benefits, and support for sufferers of occupational injury.[8][9] More broadly, welfare may also encompass efforts to provide a basic level of well-being through free or subsidized social services such as healthcare, education, vocational training and public housing.[10][11] In a welfare state, the State assumes responsibility for the health, education, and welfare of society, providing a range of social services such as those described.[11] The first welfare state was Imperial Germany (1871–1918), where the Bismarck government introduced social security in 1889.[12] In the early 20th century, the United Kingdom introduced social security around 1913, and adopted the welfare state with the National Insurance Act 1946, during the Attlee government (1945–51).[11] In the countries of western Europe, Scandinavia, and Australasia, social welfare is mainly provided by the government out of the national tax revenues, and to a lesser extent by non-government organizations (NGOs), and charities (social and religious).[11] A right to social security and an adequate standard of living is asserted in Articles 22 and 25 of the Universal Declaration of Human Rights.[6][b]"
Welfare_economics,Economics,6,"
 Welfare economics is a branch of economics that uses microeconomic techniques to evaluate well-being (welfare) at the aggregate (economy-wide) level.[1] Attempting to apply the principles of welfare economics gives rise to the field of public economics, the study of how government might intervene to improve social welfare. Welfare economics also provides the theoretical foundations for particular instruments of public economics, including cost–benefit analysis, while the combination of welfare economics and insights from behavioral economics has led to the creation of a new subfield, behavioral welfare economics.[2] The field of welfare economics is associated with two fundamental theorems. The first states that given certain assumptions, competitive markets produce (Pareto) efficient outcomes;[3] it captures the logic of Adam Smith's invisible hand.[4] The second states that given further restrictions, any Pareto efficient outcome can be supported as a competitive market equilibrium.[3] Thus a social planner could use a social welfare function to pick the most equitable efficient outcome, then use lump sum transfers followed by competitive trade to bring it about.[3][5] Because of welfare economics' close ties to social choice theory, Arrow's impossibility theorem is sometimes listed as a third fundamental theorem.[6] A typical methodology begins with the derivation (or assumption) of a social welfare function, which can then be used to rank economically feasible allocations of resources in terms of the social welfare they entail. Such functions typically include measures of economic efficiency and equity, though more recent attempts to quantify social welfare have included a broader range of measures including economic freedom (as in the capability approach).
"
Willingness_to_accept,Economics,6,"In economics, willingness to accept (WTA) is the minimum monetary amount that а person is willing to accept to sell a good or service, or to bear a negative externality, such as pollution[1]. This is in contrast to willingness to pay (WTP), which is the maximum amount of money a consumer (a buyer) is willing to sacrifice to purchase a good/service or avoid something undesirable[1]. The price of any transaction will thus be any point between a buyer's willingness to pay and a seller's willingness to accept, the net difference of which is known as economic surplus.
 Several methods have been developed to measure consumer willingness to pay or accept payment. These methods can be differentiated by whether they measure consumers' hypothetical or actual willingness to pay/accept, and whether they measure consumer willingness to pay/accept directly or indirectly.
 Choice modelling techniques may be used to estimate the value of the WTP or WTA through a choice experiment.
 Contingent Value techniques are also a common tool used to measure WTA or WTP, through directly asking respondents what they would be willing to pay and accept for different hypothetical scenarios[2].
"
Willingness_to_pay,Economics,6,"Willingness to pay (WTP) is the maximum price at or below which a consumer will definitely buy one unit of a product.[1] This corresponds to the standard economic view of a consumer reservation price. Some researchers, however, conceptualize WTP as a range.
 According to the constructive preference view, consumer willingness to pay is a context-sensitive construct; that is, a consumer's WTP for a product depends on the concrete decision context. For example, consumers tend to be willing to pay more for a soft drink in a luxury hotel resort in comparison to a beach bar or a local retail store.
"
Yield_(finance),Economics,6,"In finance, the yield on a security is a measure of the ex-ante return to a holder of the security. It is measure applied to common, preferred stock, convertible stocks and bonds, fixed income instruments, including bonds, including government bonds and corporate bonds, notes and annuities. 
 There are various types of yield, and the method of calculation depends on the particular type of yield and the type of security.  
 Dividend yield measures the past year's dividends on a stock (a share), expressed as a percentage of the share's market value.
 Yield to maturity is applied to a fixed income security. It is the holder's ex-ante total return, calculated assuming it is held until maturity, and assuming there is no default, using the internal rate of return (IRR) method. 
 Because of these differences, the yields comparisons between different yields measured on different types of financial products should be treated with caution.  This page is mainly a series of links to other pages with increased details.
"
Zero-sum_game,Economics,6,"In game theory and economic theory, a zero-sum game is a mathematical representation of a situation in which each participant's gain or loss of utility is exactly balanced by the losses or gains of the utility of the other participants. If the total gains of the participants are added up and the total losses are subtracted, they will sum to zero. Thus, cutting a cake, where taking a larger piece reduces the amount of cake available for others as much as it increases the amount available for that taker, is a zero-sum game if all participants value each unit of cake equally (see marginal utility).
 In contrast, non-zero-sum describes a situation in which the interacting parties' aggregate gains and losses can be less than or more than zero. A zero-sum game is also called a strictly competitive game while non-zero-sum games can be either competitive or non-competitive. Zero-sum games are most often solved with the minimax theorem which is closely related to linear programming duality,[1] or with Nash equilibrium.
 Many people have a cognitive bias towards seeing situations as zero-sum, known as zero-sum bias.
"
Tensor,Mathematics,3,"In mathematics, a tensor is an algebraic object that describes a (multilinear) relationship between sets of algebraic objects related to a vector space. Objects that tensors may map between include vectors and scalars, and even other tensors. Tensors can take several different forms – for example: scalars and vectors (which are the simplest tensors), dual vectors, multilinear maps between vector spaces, and even some operations such as the dot product. Tensors are defined independent of any basis, although they are often referred to by their components in a basis related to a particular coordinate system.
 Tensors are important in physics because they provide a concise mathematical framework for formulating and solving physics problems in areas such as mechanics (stress, elasticity, fluid mechanics, moment of inertia, ...), electrodynamics (electromagnetic tensor, Maxwell tensor, permittivity, magnetic susceptibility, ...), or general relativity (stress–energy tensor, curvature tensor, ... ) and others. In applications, it is common to study situations in which a different tensor can occur at each point of an object; for example the stress within an object may vary from one location to another. This leads to the concept of a tensor field. In some areas, tensor fields are so ubiquitous that they are often simply called ""tensors"".
 Tensors were conceived in 1900 by Tullio Levi-Civita and Gregorio Ricci-Curbastro, who continued the earlier work of Bernhard Riemann and Elwin Bruno Christoffel and others, as part of the absolute differential calculus. The concept enabled an alternative formulation of the intrinsic differential geometry of a manifold in the form of the Riemann curvature tensor.[1]"
Absolute_geometry,Mathematics,3,"Absolute geometry is a geometry based on an axiom system for Euclidean geometry without the parallel postulate or any of its alternatives. Traditionally, this has meant using only the first four of Euclid's postulates, but since these are not sufficient as a basis of Euclidean geometry, other systems, such as  Hilbert's axioms without the parallel axiom, are used.[1] The term was introduced by János Bolyai in 1832.[2] It is sometimes referred to as neutral geometry,[3] as it is neutral with respect to the parallel postulate.
"
Abstract_algebra,Mathematics,3,"In algebra, which is a broad division of mathematics, abstract algebra (occasionally called modern algebra) is the study of algebraic structures. Algebraic structures include groups, rings, fields, modules, vector spaces, lattices, and algebras. The term abstract algebra was coined in the early 20th century to distinguish this area of study from the other parts of algebra.
 Algebraic structures, with their associated homomorphisms, form mathematical categories. Category theory is a formalism that allows a unified  way for expressing properties and constructions that are similar for various structures.
 Universal algebra is a related subject that studies types of algebraic structures as single objects. For example, the structure of groups is a single object in universal algebra, which is called variety of groups.
"
Abstract_analytic_number_theory,Mathematics,3,"Abstract analytic number theory is a branch of mathematics which takes the ideas and techniques of classical analytic number theory and applies them to a variety of different mathematical fields. The classical prime number theorem serves as a prototypical example, and the emphasis is on abstract asymptotic distribution results. The theory was invented and developed by mathematicians such as John Knopfmacher and Arne Beurling in the twentieth century.
"
Abstract_differential_geometry,Mathematics,3,"The adjective abstract has often been applied to differential geometry before, but the abstract differential geometry (ADG) of this article is a form of differential geometry without the calculus notion of smoothness, developed by Anastasios Mallios and Ioannis Raptis from 1998 onwards.[1] Instead of calculus, an axiomatic treatment of differential geometry is built via sheaf theory and sheaf cohomology using vector sheaves in place of bundles based on arbitrary topological spaces.[2] Mallios says noncommutative geometry can be considered a special case of ADG, and that ADG is similar to synthetic differential geometry.
"
Abstract_harmonic_analysis,Mathematics,3,"
 Harmonic analysis is a branch of mathematics concerned with the representation of functions or signals as the superposition of basic waves, and the study of and generalization of the notions of Fourier series and Fourier transforms (i.e. an extended form of Fourier analysis). In the past two centuries, it has become a vast subject with applications in areas as diverse as number theory, representation theory, signal processing, quantum mechanics, tidal analysis and neuroscience.
 The term ""harmonics"" originated as the Ancient Greek word harmonikos, meaning ""skilled in music"".[1] In physical eigenvalue problems, it began to mean waves whose frequencies are integer multiples of one another, as are the frequencies of the harmonics of music notes, but the term has been generalized beyond its original meaning.
 The classical Fourier transform on Rn is still an area of ongoing research, particularly concerning Fourier transformation on more general objects such as tempered distributions. For instance, if we impose some requirements on a distribution f, we can attempt to translate these requirements in terms of the Fourier transform of f. The Paley–Wiener theorem is an example of this. The Paley–Wiener theorem immediately implies that if f is a nonzero distribution of compact support (these include functions of compact support), then its Fourier transform is never compactly supported. This is a very elementary form of an uncertainty principle in a harmonic-analysis setting.
 Fourier series can be conveniently studied in the context of Hilbert spaces, which provides a connection between harmonic analysis and functional analysis.
"
Homotopy_theory,Mathematics,3,"In mathematics, homotopy theory is a systematic study of situations in which maps come with homotopies between them. It originated as a topic in algebraic topology but nowadays it is studied as an independent discipline. Besides algebraic topology, the theory has also been in used in other areas of mathematics such as algebraic geometry (e.g., A¹ homotopy theory) and category theory (specifically the study of higher categories).
"
Actuarial_science,Mathematics,3,"Actuarial science is the discipline that applies mathematical and statistical methods to assess risk in insurance, finance, and other industries and professions. More generally, actuaries apply rigorous mathematics to model matters of uncertainty.
 Actuaries are professionals trained in this discipline. In many countries, actuaries must demonstrate their competence by passing a series of rigorous professional examinations.
 Actuarial science includes a number of interrelated subjects, including mathematics, probability theory, statistics, finance, economics, and computer science. Historically, actuarial science used deterministic models in the construction of tables and premiums. The science has gone through revolutionary changes since the 1980s due to the proliferation of high speed computers and the union of stochastic actuarial models with modern financial theory (Frees 1990).
 Many universities have undergraduate and graduate degree programs in actuarial science. In 2010, a study published by job search website CareerCast ranked actuary as the #1 job in the United States (Needleman 2010). The study used five key criteria to rank jobs: environment, income, employment outlook, physical demands, and stress. A similar study by U.S. News & World Report in 2006 included actuaries among the 25 Best Professions that it expects will be in great demand in the future (Nemko 2006).
"
Additive_combinatorics,Mathematics,3,"Additive combinatorics is an area of combinatorics in mathematics. One major area of study in additive combinatorics are inverse problems: given the size of the sumset A + B is small, what can we say about the structures of 



A


{  A}
 and 



B


{  B}
? In the case of the integers, the classical Freiman's theorem provides a partial answer to this question in terms of multi-dimensional arithmetic progressions.
 Another typical problem is to find a lower bound for 




|

A
+
B

|



{  |A+B|}
 in terms of  




|

A

|



{  |A|}
 and 




|

B

|



{  |B|}
. This can be viewed as an inverse problem with the given information that 




|

A
+
B

|



{  |A+B|}
 is sufficiently small and the structural conclusion is then of the form that either 



A


{  A}
 or 



B


{  B}
 is the empty set; however, in literature, such problems are sometimes considered to be direct problems as well. Examples of this type include the Erdős–Heilbronn Conjecture (for a restricted sumset) and the Cauchy–Davenport Theorem. The methods used for tackling such questions often come from many different fields of mathematics, including combinatorics, ergodic theory, analysis, graph theory, group theory, and linear algebraic and polynomial methods.
"
Additive_number_theory,Mathematics,3,"Additive number theory is the subfield of number theory concerning the study of subsets of integers and their behavior under addition. More abstractly, the field of additive number theory includes the study of abelian groups and commutative semigroups with an operation of addition. Additive number theory has close ties to combinatorial number theory and the geometry of numbers. Two principal objects of study are the sumset of two subsets A and B of elements from an abelian group G,
 and the h-fold sumset of A, 
"
Affine_geometry,Mathematics,3,"In mathematics, affine geometry is what remains of Euclidean geometry when not using (mathematicians often say ""when forgetting""[1][2]) the metric notions of distance and angle.
 As the notion of parallel lines is one of the main properties that is independent of any metric, affine geometry is often considered as the study of parallel lines. Therefore, Playfair's axiom (given a line L and a point P not on L, there is exactly one line parallel to L that passes through P) is fundamental in affine geometry. Comparisons of figures in affine geometry are made with affine transformations, which are mappings that preserve alignment of points and parallelism of lines.
 Affine geometry can be developed in two ways that are essentially equivalent.[3] In synthetic geometry, an affine space is a set of points to which is associated a set of lines, which satisfy some axioms (such as Playfair's axiom).
 Affine geometry can also be developed on the basis of linear algebra. In this context an affine space is a set of points equipped with a set of transformations (that is bijective mappings), the translations, which forms a vector space (over a given field, commonly the real numbers), and such that for any given ordered pair of points there is a unique translation sending the first point to the second; the composition of two translations is their sum in the vector space of the translations.
 In more concrete terms, this amounts to having an operation that associates to any ordered pair of points a vector and another operation that allows translation of a point by a vector to give another point; these operations are required to satisfy a number of axioms (notably that two successive translations have the effect of translation by the sum vector). By choosing any point as ""origin"", the points are in one-to-one correspondence with the vectors, but there is no preferred choice for the origin; thus an affine space may be viewed as obtained from its associated vector space by ""forgetting"" the origin (zero vector).
 Although this article only discusses affine spaces, the notion of ""forgetting the metric"" is much more general, and can be applied to arbitrary manifolds, in general.  This extension of the notion of affine spaces to manifolds in general is developed in the article on the affine connection.
"
Affine_geometry_of_curves,Mathematics,3,"In the mathematical field of differential geometry, the affine geometry of curves is the study of curves in an affine space, and specifically the properties of such curves which are invariant under the special affine group 





SL


(
n
,

R

)
⋉


R


n


.


{  {\mbox{SL}}(n,\mathbb {R} )\ltimes \mathbb {R} ^{n}.}

 In the classical Euclidean geometry of curves, the fundamental tool is the Frenet–Serret frame.  In affine geometry, the Frenet–Serret frame is no longer well-defined, but it is possible to define another canonical moving frame along a curve which plays a similar decisive role.  The theory was developed in the early 20th century, largely from the efforts of Wilhelm Blaschke and Jean Favard.
"
Affine_differential_geometry,Mathematics,3,"Affine differential geometry is a type of differential geometry in which the differential invariants are invariant under volume-preserving affine transformations. The name affine differential geometry follows from Klein's Erlangen program. The basic difference between affine and Riemannian differential geometry is that in the affine case we introduce volume forms over a manifold instead of metrics.
"
Ahlfors_theory,Mathematics,3,"Ahlfors theory is a mathematical theory invented by Lars Ahlfors as a geometric counterpart of the Nevanlinna theory.  Ahlfors was awarded one of the two very first Fields Medals for this theory in 1936.
 It can be considered as a generalization of the basic properties of covering maps to the
maps which are ""almost coverings"" in some well defined sense. It applies to bordered Riemann surfaces equipped with conformal Riemannian metrics.
"
Algebra,Mathematics,3,"
 Algebra (from Arabic: الجبر‎ al-jabr, meaning ""reunion of broken parts""[1] and ""bonesetting""[2]) is one of the broad parts of mathematics, together with number theory, geometry and analysis. In its most general form, algebra is the study of mathematical symbols and the rules for manipulating these symbols;[3] it is a unifying thread of almost all of mathematics.[4] It includes everything from elementary equation solving to the study of abstractions such as groups, rings, and fields. The more basic parts of algebra are called elementary algebra; the more abstract parts are called abstract algebra or modern algebra. Elementary algebra is generally considered to be essential for any study of mathematics, science, or engineering, as well as such applications as medicine and economics. Abstract algebra is a major area in advanced mathematics, studied primarily by professional mathematicians.
 Elementary algebra differs from arithmetic in the use of abstractions, such as using letters to stand for numbers that are either unknown or allowed to take on many values.[5] For example, in 



x
+
2
=
5


{  x+2=5}
 the letter 



x


{  x}
 is unknown, but applying additive inverses can reveal its value: 



x
=
3


{  x=3}
. In E = mc2, the letters 



E


{  E}
 and 



m


{  m}
 are variables, and the letter 



c


{  c}
 is a constant, the speed of light in a vacuum. Algebra gives methods for writing formulas and solving equations that are much clearer and easier than the older method of writing everything out in words.
 The word algebra is also used in certain specialized ways. A special kind of mathematical object in abstract algebra is called an ""algebra"", and the word is used, for example, in the phrases linear algebra and algebraic topology.
 A mathematician who does research in algebra is called an algebraist.
"
Algebraic_analysis,Mathematics,3,"Algebraic analysis is an area of mathematics that deals with systems of linear partial differential equations by using sheaf theory and complex analysis to study properties and generalizations of functions such as hyperfunctions and microfunctions.  As a research programme, it was started by Mikio Sato in 1959.[1]"
Algebraic_combinatorics,Mathematics,3,"Algebraic combinatorics is an area of mathematics that employs methods of abstract algebra, notably group theory and representation theory, in various combinatorial contexts and, conversely, applies combinatorial techniques to problems in algebra.
"
Algebraic_computation,Mathematics,3,"In mathematics and computer science,[1] computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols.
 Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.
 Computer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, as in public key cryptography, or for some non-linear problems.
"
Algebraic_geometry,Mathematics,3,"Algebraic geometry is a branch of mathematics, classically studying zeros of multivariate polynomials. Modern algebraic geometry is based on the use of abstract algebraic techniques, mainly from commutative algebra, for solving  geometrical problems about these sets of zeros.
 The fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are: plane algebraic curves, which include lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves, and quartic curves like lemniscates and Cassini ovals. A point of the plane belongs to an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of the points of special interest like the singular points, the inflection points and the points at infinity. More advanced questions involve the topology of the curve and relations between the curves given by different equations.
 Algebraic geometry occupies a central place in modern mathematics and has multiple conceptual connections with such diverse fields as complex analysis, topology and number theory. Initially a study of systems of polynomial equations in several variables, the subject of algebraic geometry starts where equation solving leaves off, and it becomes even more important to understand the intrinsic properties of the totality of solutions of a system of equations, than to find a specific solution; this leads into some of the deepest areas in all of mathematics, both conceptually and in terms of technique.
 In the 20th century, algebraic geometry split into several subareas.
 Much of the development of the mainstream of algebraic geometry in the 20th century occurred within an abstract algebraic framework, with increasing emphasis being placed on ""intrinsic"" properties of algebraic varieties not dependent on any particular way of embedding the variety in an ambient coordinate space; this parallels developments in topology, differential and complex geometry. One key achievement of this abstract algebraic geometry is Grothendieck's scheme theory which allows one to use sheaf theory to study algebraic varieties in a way which is very similar to its use in the study of differential and analytic manifolds. This is obtained by extending the notion of point: In classical algebraic geometry, a point of an affine variety may be identified, through Hilbert's Nullstellensatz, with a maximal ideal of the coordinate ring, while the points of the corresponding affine scheme are all prime ideals of this ring. This means that a point of such a scheme may be either a usual point or a subvariety. This approach also enables a unification of the language and the tools of classical algebraic geometry, mainly concerned with complex points, and of algebraic number theory. Wiles' proof of the longstanding conjecture called Fermat's last theorem is an example of the power of this approach.
"
Algebraic_graph_theory,Mathematics,3,"Algebraic graph theory is a branch of mathematics in which algebraic methods are applied to problems about graphs. This is in contrast to geometric, combinatoric, or algorithmic approaches.  There are three main branches of algebraic graph theory, involving the use of linear algebra, the use of group theory, and the study of graph invariants.
"
Algebraic_K-theory,Mathematics,3,"Algebraic K-theory is a subject area in mathematics with connections to geometry, topology, ring theory, and number theory.  Geometric, algebraic, and arithmetic objects are assigned objects called K-groups.  These are groups in the sense of abstract algebra.  They contain detailed information about the original object but are notoriously difficult to compute; for example, an important outstanding problem is to compute the K-groups of the integers.
 K-theory was invented in the late 1950s by Alexander Grothendieck in his study of intersection theory on algebraic varieties. In the modern language, Grothendieck defined only K0, the zeroth K-group, but even this single group has plenty of applications, such as the Grothendieck–Riemann–Roch theorem. Intersection theory is still a motivating force in the development of (higher) algebraic K-theory through its links with motivic cohomology and specifically Chow groups.  The subject also includes classical number-theoretic topics like quadratic reciprocity and embeddings of number fields into the real numbers and complex numbers, as well as more modern concerns like the construction of higher regulators and special values of L-functions.
 The lower K-groups were discovered first, in the sense that adequate descriptions of these groups in terms of other algebraic structures were found.  For example, if F is a field, then K0(F) is isomorphic to the integers Z and is closely related to the notion of vector space dimension.  For a commutative ring R, the group K0(R) is related to the Picard group of R, and when R is the ring of integers in a number field, this generalizes the classical construction of the class group.  The group K1(R) is closely related to the group of units R×, and if R is a field, it is exactly the group of units.  For a number field F, the group K2(F) is related to class field theory, the Hilbert symbol, and the solvability of quadratic equations over completions.  In contrast, finding the correct definition of the higher K-groups of rings was a difficult achievement of Daniel Quillen, and many of the basic facts about the higher K-groups of algebraic varieties were not known until the work of Robert Thomason.
"
Algebraic_number_theory,Mathematics,3,"Algebraic number theory is a branch of number theory that uses the techniques of abstract algebra to study the integers, rational numbers, and their generalizations. Number-theoretic questions are expressed in terms of properties of algebraic objects such as algebraic number fields and their rings of integers, finite fields, and function fields. These properties, such as whether a ring admits unique factorization, the behavior of ideals, and the Galois groups of fields, can resolve questions of primary importance in number theory, like the existence of solutions to Diophantine equations.
"
Algebraic_statistics,Mathematics,3,"Algebraic statistics is the use of algebra to advance statistics. Algebra has been useful for experimental design, parameter estimation, and hypothesis testing.
 Traditionally, algebraic statistics has been associated with the design of experiments and multivariate analysis (especially time series). In recent years, the term ""algebraic statistics"" has been sometimes restricted, sometimes being used to label the use of algebraic geometry and commutative algebra in statistics.
"
Algebraic_topology,Mathematics,3,"Algebraic topology is a branch of mathematics that uses tools from abstract algebra to study topological spaces. The basic goal is to find algebraic invariants that classify topological spaces up to homeomorphism, though usually most classify up to homotopy equivalence.
 Although algebraic topology primarily uses algebra to study topological problems, using topology to solve algebraic problems is sometimes also possible. Algebraic topology, for example, allows for a convenient proof that any subgroup of a free group is again a free group.
"
Algorithmic_number_theory,Mathematics,3,"In mathematics and computer science, computational number theory, also known as algorithmic number theory, is the study of 
computational methods for investigating and solving problems in number theory and arithmetic geometry, including algorithms for primality testing and integer factorization, finding solutions to diophantine equations, and explicit methods in arithmetic geometry.[1]
Computational number theory has applications to cryptography, including RSA, elliptic curve cryptography and post-quantum cryptography, and is used to investigate conjectures and open problems in number theory, including the Riemann hypothesis, the Birch and Swinnerton-Dyer conjecture, the ABC conjecture, the modularity conjecture, the Sato-Tate conjecture, and explicit aspects of the Langlands program.[1][2][3]"
Anabelian_geometry,Mathematics,3,"Anabelian geometry is a theory in number theory, which describes the way in which the algebraic fundamental group G of a certain arithmetic variety V, or some related geometric object, can help to restore V. The first traditional conjectures, originating from Alexander Grothendieck and introduced in Esquisse d'un Programme were about how topological homomorphisms between two groups of two hyperbolic curves over number fields correspond to maps between the curves. These Grothendieck conjectures  were partially solved by Hiroaki Nakamura and Akio Tamagawa, while complete proofs were given by Shinichi Mochizuki. Before anabelian geometry proper began with the famous letter to Gerd Faltings and Esquisse d'un Programme, the Neukirch–Uchida theorem hinted at the program from the perspective of Galois groups, which themselves can be shown to be étale fundamental groups.
 More recently, Mochizuki introduced and developed a so called mono-anabelian geometry which restores, for a certain class of hyperbolic curves over number fields, the curve from its algebraic fundamental group. Key results of mono-anabelian geometry were published in Mochizuki's ""Topics in Absolute Anabelian Geometry.""
"
Mathematical_analysis,Mathematics,3,"Mathematical analysis is the branch of mathematics dealing with limits
and related theories, such as differentiation, integration, measure, infinite series, and analytic functions.[1][2] These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis.
Analysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects  (a metric space).
"
Analytic_combinatorics,Mathematics,3,"In combinatorics, especially in analytic combinatorics, the symbolic method is a technique for counting combinatorial objects.  It uses the internal structure of the objects to derive formulas for their generating functions.  The method is mostly associated with Philippe Flajolet and is detailed in Part A of his book with Robert Sedgewick, Analytic Combinatorics.
Similar languages for specifying combinatorial classes and their generating functions are found in work by
Bender and Goldman,[1] Foata and Schützenberger,[2] and Joyal.[3]
The presentation in this article borrows somewhat from Joyal's  combinatorial species.
"
Analytic_geometry,Mathematics,3,"In classical mathematics, analytic geometry, also known as coordinate geometry or Cartesian geometry, is the study of geometry using a coordinate system. This contrasts with synthetic geometry.
 Analytic geometry is used in physics and engineering, and also in aviation, rocketry, space science, and spaceflight. It is the foundation of most modern fields of geometry, including algebraic, differential, discrete and computational geometry.
 Usually the Cartesian coordinate system is applied to manipulate equations for planes, straight lines, and squares, often in two and sometimes three dimensions. Geometrically, one studies the Euclidean plane (two dimensions) and Euclidean space (three dimensions). As taught in school books, analytic geometry can be explained more simply: it is concerned with defining and representing geometrical shapes in a numerical way and extracting numerical information from shapes' numerical definitions and representations. That the algebra of the real numbers can be employed to yield results about the linear continuum of geometry relies on the Cantor–Dedekind axiom.
"
Analytic_number_theory,Mathematics,3,"In mathematics, analytic number theory is a branch of number theory that uses methods from mathematical analysis to solve problems about the integers.[1] It is often said to have begun with Peter Gustav Lejeune Dirichlet's 1837 introduction of Dirichlet L-functions to give the first proof of Dirichlet's theorem on arithmetic progressions.[1][2] It is well known for its results on prime numbers (involving the Prime Number Theorem and Riemann zeta function) and additive number theory (such as the Goldbach conjecture and Waring's problem).
"
Applied_mathematics,Mathematics,3,"Applied mathematics is the application of mathematical methods by different fields such as physics, engineering, medicine, biology, business, computer science, and industry. Thus, applied mathematics is a combination of mathematical science and specialized knowledge. The term ""applied mathematics"" also describes the professional specialty  in which mathematicians work on practical problems by formulating and studying mathematical models. 
 In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics where abstract concepts are studied for their own sake. The activity of applied mathematics is thus intimately connected with research in pure mathematics.
"
Approximation_theory,Mathematics,3,"
 In mathematics, approximation theory is concerned with how functions can best be approximated with simpler functions, and with quantitatively characterizing the errors introduced thereby. Note that what is meant by best and simpler will depend on the application.
 A closely related topic is the approximation of functions by generalized Fourier series, that is, approximations based upon summation of a series of terms based upon orthogonal polynomials.
 One problem of particular interest is that of approximating a function in a computer mathematical library, using operations that can be performed on the computer or calculator (e.g. addition and multiplication), such that the result is as close to the actual function as possible.  This is typically done with polynomial or rational (ratio of polynomials) approximations.
 The objective is to make the approximation as close as possible to the actual function, typically with an accuracy close to that of the underlying computer's floating point arithmetic.  This is accomplished by using a polynomial of high degree, and/or narrowing the domain over which the polynomial has to approximate the function.
Narrowing the domain can often be done through the use of various addition or scaling formulas for the function being approximated.  Modern mathematical libraries often reduce the domain into many tiny segments and use a low-degree polynomial for each segment.
"
Arakelov_geometry,Mathematics,3,"In mathematics, Arakelov theory (or Arakelov geometry) is an approach to Diophantine geometry, named for Suren Arakelov. It is used to study Diophantine equations in higher dimensions.
"
Arakelov_theory,Mathematics,3,"In mathematics, Arakelov theory (or Arakelov geometry) is an approach to Diophantine geometry, named for Suren Arakelov. It is used to study Diophantine equations in higher dimensions.
"
Arithmetic,Mathematics,3,"Arithmetic (from the Greek ἀριθμός arithmos, 'number' and τική [τέχνη], tiké [téchne], 'art') is a branch of mathematics that consists of the study of numbers, especially the properties of the traditional operations on them—addition, subtraction, multiplication, division, exponentiation and extraction of roots.[1][2][3] Arithmetic is an elementary part of number theory, and number theory is considered to be one of the top-level divisions of modern mathematics, along with algebra, geometry, and analysis. The terms arithmetic and higher arithmetic were used until the beginning of the 20th century as synonyms for number theory, and are sometimes still used to refer to a wider part of number theory.[4]"
Arithmetic_algebraic_geometry,Mathematics,3,"In mathematics, arithmetic geometry is roughly the application of techniques from algebraic geometry to problems in number theory.[1] Arithmetic geometry is centered around Diophantine geometry, the study of rational points of algebraic varieties.[2][3] In more abstract terms, arithmetic geometry can be defined as the study of schemes of finite type over the spectrum of the ring of integers.[4]"
Arithmetic_combinatorics,Mathematics,3,"In mathematics, arithmetic combinatorics is a field in the intersection of number theory, combinatorics, ergodic theory and harmonic analysis. 
"
Arithmetic_dynamics,Mathematics,3,"Arithmetic dynamics[1] is a field that amalgamates two areas of mathematics, dynamical systems and number theory. Classically, discrete dynamics refers to the study of the iteration of self-maps of the complex plane or real line. Arithmetic dynamics is the study of the number-theoretic properties of integer, rational, p-adic, and/or algebraic points under repeated application of a polynomial or rational function. A fundamental goal is to describe arithmetic properties in terms of underlying geometric structures.
 Global arithmetic dynamics is the study of analogues of classical diophantine geometry  in the setting of discrete dynamical systems, while local arithmetic dynamics, also called p-adic or nonarchimedean dynamics, is an analogue of classical dynamics in which one replaces the complex numbers C by a p-adic field such as Qp or Cp and studies chaotic behavior and the Fatou and Julia sets.
 The following table describes a rough correspondence between Diophantine equations, especially abelian varieties, and dynamical systems:
"
Arithmetic_geometry,Mathematics,3,"In mathematics, arithmetic geometry is roughly the application of techniques from algebraic geometry to problems in number theory.[1] Arithmetic geometry is centered around Diophantine geometry, the study of rational points of algebraic varieties.[2][3] In more abstract terms, arithmetic geometry can be defined as the study of schemes of finite type over the spectrum of the ring of integers.[4]"
Arithmetic_topology,Mathematics,3,"Arithmetic topology is an area of mathematics that is a combination of algebraic number theory and topology. It establishes an analogy between number fields and closed, orientable 3-manifolds.
"
Arithmetical_algebraic_geometry,Mathematics,3,"In mathematics, arithmetic geometry is roughly the application of techniques from algebraic geometry to problems in number theory.[1] Arithmetic geometry is centered around Diophantine geometry, the study of rational points of algebraic varieties.[2][3] In more abstract terms, arithmetic geometry can be defined as the study of schemes of finite type over the spectrum of the ring of integers.[4]"
Assignment_problem,Mathematics,3,"The assignment problem is a fundamental combinatorial optimization problem. In its most general form, the problem is as follows:
 Alternatively, describing the problem using graph theory:
 If the numbers of agents and tasks are equal, then the problem is called balanced assignment. Otherwise, it is called unbalanced assignment.[1] If the total cost of the assignment for all tasks is equal to the sum of the costs for each agent (or the sum of the costs for each task, which is the same thing in this case), then the problem is called linear assignment. Commonly, when speaking of the assignment problem without any additional qualification, then the linear balanced assignment problem is meant.
"
Asymptotic_combinatorics,Mathematics,3,"In combinatorics, especially in analytic combinatorics, the symbolic method is a technique for counting combinatorial objects.  It uses the internal structure of the objects to derive formulas for their generating functions.  The method is mostly associated with Philippe Flajolet and is detailed in Part A of his book with Robert Sedgewick, Analytic Combinatorics.
Similar languages for specifying combinatorial classes and their generating functions are found in work by
Bender and Goldman,[1] Foata and Schützenberger,[2] and Joyal.[3]
The presentation in this article borrows somewhat from Joyal's  combinatorial species.
"
Asymptotic_theory,Mathematics,3,"In mathematical analysis, asymptotic analysis, also known as asymptotics, is a method of describing limiting behavior.
 As an illustration, suppose that we are interested in the properties of a function f(n) as n becomes very large.  If f(n) = n2 + 3n, then as n becomes very large, the term 3n  becomes insignificant compared to n2.  The function f(n) is said to be ""asymptotically equivalent to n2, as n → ∞"". This is often written symbolically as f(n) ~ n2, which is read as ""f(n) is asymptotic to n2"".
 An example of an important asymptotic result is the prime number theorem. Let π(x) denote the prime-counting function (which is not directly related to the constant pi), i.e. π(x) is the number of prime numbers that are less than or equal to x. Then the theorem states that
"
Auslander%E2%80%93Reiten_theory,Mathematics,3,"In algebra, Auslander–Reiten theory studies the representation theory of Artinian rings using techniques such as Auslander–Reiten sequences (also called almost split sequences) and Auslander–Reiten quivers. Auslander–Reiten theory was introduced by Maurice Auslander and Idun Reiten (1975) and developed by them in several subsequent papers.
 For survey articles on Auslander–Reiten theory see Auslander (1982), Gabriel (1980), Reiten (1982), and the book Auslander, Reiten & Smalø (1997). Many of the original papers on Auslander–Reiten theory are reprinted in  Auslander (1999a, 1999b).
"
Axiomatic_set_theory,Mathematics,3,"Set theory is a branch of mathematical logic that studies sets, which informally are collections of objects. Although any type of object can be collected into a set, set theory is applied most often to objects that are relevant to mathematics. The language of set theory can be used to define nearly all mathematical objects.
 The modern study of set theory was initiated by Georg Cantor and Richard Dedekind in the 1870s. After the discovery of paradoxes in naive set theory, such as Russell's paradox, numerous axiom systems were proposed in the early twentieth century, of which the Zermelo–Fraenkel axioms, with or without the axiom of choice, are the best-known.
 Set theory is commonly employed as a foundational system for mathematics, particularly in the form of Zermelo–Fraenkel set theory with the axiom of choice.[1] Beyond its foundational role, set theory is a branch of mathematics in its own right, with an active research community. Contemporary research into set theory includes a diverse collection of topics, ranging from the structure of the real number line to the study of the consistency of large cardinals.
"
Bifurcation_theory,Mathematics,3,"Bifurcation theory is the mathematical study of changes in the qualitative or topological structure of a given family, such as the integral curves of a family of vector fields, and the solutions of a family of differential equations. Most commonly applied to the mathematical study of dynamical systems, a bifurcation occurs when a small smooth change made to the parameter values (the bifurcation parameters) of a system causes a sudden 'qualitative' or topological change in its behavior.[1] Bifurcations occur in both continuous systems (described by ODEs, DDEs or PDEs) and discrete systems (described by maps). The name ""bifurcation"" was first introduced by Henri Poincaré in 1885 in the first paper in mathematics showing such a behavior.[2] Henri Poincaré also later named various types of stationary points and classified them with motif
"
Biostatistics,Mathematics,3,"Biostatistics are the development and application of statistical methods to a wide range of topics in biology. It encompasses the design of biological experiments, the collection and analysis of data from those experiments and the interpretation of the results.
"
Birational_geometry,Mathematics,3,"In mathematics, birational geometry is a field of algebraic geometry in which the goal is to determine when two algebraic varieties are isomorphic outside lower-dimensional subsets. This amounts to studying mappings that are given by rational functions rather than polynomials; the map may fail to be defined where the rational functions have poles.
"
C*-algebra,Mathematics,3,"In mathematics, specifically in functional analysis, a C∗-algebra (pronounced ""C-star"") is a Banach algebra together with an involution satisfying the properties of the adjoint. A particular case is that of a complex algebra A of continuous linear operators on a complex Hilbert space with two additional properties:
 Another important class of non-Hilbert C*-algebra include the algebra of continuous functions 




C

0


(
X
)


{  C_{0}(X)}
.
 C*-algebras were first considered primarily for their use in quantum mechanics to model algebras of physical observables.  This line of research began with Werner Heisenberg's matrix mechanics and in a more mathematically developed form with Pascual Jordan around 1933.  Subsequently, John von Neumann attempted to establish a general framework for these algebras which culminated in a series of papers on rings of operators.  These papers considered a special class of C*-algebras which are now known as von Neumann algebras.
 Around 1943, the work of Israel Gelfand and Mark Naimark yielded an abstract characterisation of C*-algebras making no reference to operators on a Hilbert space.
 C*-algebras are now an important tool in the theory of unitary representations of locally compact groups, and are also used in algebraic formulations of quantum mechanics. Another active area of research is the program to obtain classification, or to determine the extent of which classification is possible, for separable simple nuclear C*-algebras.
"
Cartesian_geometry,Mathematics,3,"In classical mathematics, analytic geometry, also known as coordinate geometry or Cartesian geometry, is the study of geometry using a coordinate system. This contrasts with synthetic geometry.
 Analytic geometry is used in physics and engineering, and also in aviation, rocketry, space science, and spaceflight. It is the foundation of most modern fields of geometry, including algebraic, differential, discrete and computational geometry.
 Usually the Cartesian coordinate system is applied to manipulate equations for planes, straight lines, and squares, often in two and sometimes three dimensions. Geometrically, one studies the Euclidean plane (two dimensions) and Euclidean space (three dimensions). As taught in school books, analytic geometry can be explained more simply: it is concerned with defining and representing geometrical shapes in a numerical way and extracting numerical information from shapes' numerical definitions and representations. That the algebra of the real numbers can be employed to yield results about the linear continuum of geometry relies on the Cantor–Dedekind axiom.
"
Calculus,Mathematics,3,"
 Calculus, originally called infinitesimal calculus or ""the calculus of infinitesimals"", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations.
 It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while  integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Today, calculus has widespread uses in science, engineering, and economics.[4] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly  devoted to the study of functions and limits. The word calculus (plural calculi) is a  Latin word, meaning originally ""small pebble"" (this meaning is kept in medicine – see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.
"
Infinitesimal_calculus,Mathematics,3,"
 Calculus, originally called infinitesimal calculus or ""the calculus of infinitesimals"", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations.
 It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while  integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Today, calculus has widespread uses in science, engineering, and economics.[4] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly  devoted to the study of functions and limits. The word calculus (plural calculi) is a  Latin word, meaning originally ""small pebble"" (this meaning is kept in medicine – see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.
"
Calculus_of_moving_surfaces,Mathematics,3,"The calculus of moving surfaces (CMS) [1] is an extension of the classical tensor calculus to deforming manifolds. Central to the CMS is the Tensorial Time Derivative 






∇
˙





{  {\dot {\nabla }}}
 whose original definition [2] was put forth by Jacques Hadamard. It plays the role analogous to that of the covariant derivative 




∇

α




{  \nabla _{\alpha }}
 on differential manifolds. in that it produces a tensor when applied to a tensor.
 Suppose that 




Σ

t




{  \Sigma _{t}}
 is the evolution of the surface 



Σ


{  \Sigma }
 indexed by a time-like parameter 



t


{  t}
. The definitions of the surface velocity 



C


{  C}
 and the operator 






∇
˙





{  {\dot {\nabla }}}
 are the geometric foundations of the CMS. The velocity C is the rate of deformation of the surface 



Σ


{  \Sigma }
 in the instantaneous normal direction. The value of 



C


{  C}
 at a point 



P


{  P}
 is defined as the limit
 where 




P

∗




{  P^{*}}
 is the point on 




Σ

t
+
h




{  \Sigma _{t+h}}
 that lies on the straight line perpendicular to 




Σ

t




{  \Sigma _{t}}
 at point P. This definition is illustrated in the first geometric figure below. The velocity 



C


{  C}
 is a signed quantity: it is positive when 






P

P

∗



¯




{  {\overline {PP^{*}}}}
 points in the direction of the chosen normal, and negative otherwise. The relationship between 




Σ

t




{  \Sigma _{t}}
 and 



C


{  C}
 is analogous to the relationship between location and velocity in elementary calculus: knowing either quantity allows one to construct the other by differentiation or integration.
 The Tensorial Time Derivative 






∇
˙





{  {\dot {\nabla }}}
 for a scalar field F defined on 




Σ

t




{  \Sigma _{t}}
 is the rate of change in 



F


{  F}
 in the instantaneously normal direction:
 This definition is also illustrated in second geometric figure.
 The above definitions are geometric. In analytical settings, direct application of these definitions may not be possible. The CMS gives analytical definitions of C and 






∇
˙





{  {\dot {\nabla }}}
 in terms of elementary operations from calculus and differential geometry.
"
Calculus_of_variations,Mathematics,3,"The calculus of variations is a field of mathematical analysis that uses variations, which are small changes in functions
and functionals, to find maxima and minima of functionals: mappings from a set of functions to the real numbers.[a] Functionals are often expressed as definite integrals involving functions and their derivatives. Functions that maximize or minimize functionals may be found using the Euler–Lagrange equation of the calculus of variations.
 A simple example of such a problem is to find the curve of shortest length connecting two points. If there are no constraints, the solution is a straight line between the points. However, if the curve is constrained to lie on a surface in space, then the solution is less obvious, and possibly many solutions may exist. Such solutions are known as geodesics. A related problem is posed by Fermat's principle: light follows the path of shortest optical length connecting two points, where the optical length depends upon the material of the medium. One corresponding concept in mechanics is the principle of least/stationary action.
 Many important problems involve functions of several variables. Solutions of boundary value problems for the Laplace equation satisfy the Dirichlet principle. Plateau's problem requires finding a surface of minimal area that spans a given contour in space: a solution can often be found by dipping a frame in a solution of soap suds. Although such experiments are relatively easy to perform, their mathematical interpretation is far from simple: there may be more than one locally minimizing surface, and they may have non-trivial topology.
"
Catastrophe_theory,Mathematics,3,"In mathematics, catastrophe theory is a branch of bifurcation theory in the study of dynamical systems; it is also a particular special case of more general singularity theory in geometry.
 Bifurcation theory studies and classifies phenomena characterized by sudden shifts in behavior arising from small changes in circumstances, analysing how the qualitative nature of equation solutions depends on the parameters that appear in the equation.  This may lead to sudden and dramatic changes, for example the unpredictable timing and magnitude of a landslide.
 Catastrophe theory originated with the work of the French mathematician René Thom in the 1960s, and became very popular due to the efforts of Christopher Zeeman in the 1970s. It considers the special case where the long-run stable equilibrium can be identified as the minimum of a smooth, well-defined potential function (Lyapunov function).
 Small changes in certain parameters of a nonlinear system can cause equilibria to appear or disappear, or to change from attracting to repelling and vice versa, leading to large and sudden changes of the behaviour of the system.  However, examined in a larger parameter space, catastrophe theory reveals that such bifurcation points tend to occur as part of well-defined qualitative geometrical structures.
"
Categorical_logic,Mathematics,3,"Categorical logic is the branch of mathematics in which tools and concepts from category theory are applied to the study of mathematical logic. It is also notable for its connections to theoretical computer science. In broad terms, categorical logic represents both syntax and semantics by a category, and an interpretation by a functor.  The categorical framework provides a rich conceptual background for logical and type-theoretic constructions. The subject has been recognisable in these terms since around 1970.
"
Category_theory,Mathematics,3,"Category theory formalizes mathematical structure and its concepts in terms of a labeled directed graph called a category, whose nodes are called  objects, and whose labelled directed edges are called  arrows (or morphisms).[1] A category has two basic properties: the ability to compose the arrows associatively, and the existence of an identity arrow for each object. The language of category theory has been used to formalize concepts of other high-level abstractions such as sets, rings, and groups. Informally, category theory is a general theory of functions.
 Several terms used in category theory, including the term ""morphism"", are used differently from their uses in the rest of mathematics. In category theory, morphisms obey conditions specific to category theory itself.
 Samuel Eilenberg and Saunders Mac Lane introduced the concepts of categories, functors, and natural transformations from 1942–45 in their study of algebraic topology, with the goal of understanding the processes that preserve mathematical structure.
 Category theory has practical applications in programming language theory, for example the usage of monads in functional programming.   It may also be used as an axiomatic foundation for mathematics, as an alternative to set theory and other proposed foundations.
"
Chaos_theory,Mathematics,3,"Chaos theory is a branch of mathematics focusing on the study of chaos — dynamical systems whose apparently random states of disorder and irregularities are actually governed by underlying patterns and deterministic laws that are highly sensitive to initial conditions.[1][2] Chaos theory is an interdisciplinary theory stating that, within the apparent randomness of chaotic complex systems, there are underlying patterns, interconnectedness, constant feedback loops, repetition, self-similarity, fractals, and self-organization.[3] The butterfly effect, an underlying principle of chaos, describes how a small change in one state of a deterministic nonlinear system can result in large differences in a later state (meaning that there is sensitive dependence on initial conditions).[4] A metaphor for this behavior is that a butterfly flapping its wings in Texas can cause a hurricane in China.[5] Small differences in initial conditions, such as those due to errors in measurements or due to rounding errors in numerical computation, can yield widely diverging outcomes for such dynamical systems, rendering long-term prediction of their behavior impossible in general.[6] This can happen even though these systems are deterministic, meaning that their future behavior follows a unique evolution[7] and is fully determined by their initial conditions, with no random elements involved.[8] In other words, the deterministic nature of these systems does not make them predictable.[9][10] This behavior is known as deterministic chaos, or simply chaos. The theory was summarized by Edward Lorenz as:[11] Chaos: When the present determines the future, but the approximate present does not approximately determine the future. Chaotic behavior exists in many natural systems, including fluid flow, heartbeat irregularities, weather and climate.[12][13][7] It also occurs spontaneously in some systems with artificial components, such as the stock market and road traffic.[14][3] This behavior can be studied through the analysis of a chaotic mathematical model, or through analytical techniques such as recurrence plots and Poincaré maps. Chaos theory has applications in a variety of disciplines, including meteorology,[7] anthropology,[15] sociology, physics,[16] environmental science, computer science, engineering, economics, biology, ecology, pandemic crisis management,[17][18] and philosophy. The theory formed the basis for such fields of study as complex dynamical systems, edge of chaos theory, and self-assembly processes.
"
Character_theory,Mathematics,3,"In mathematics, more specifically in group theory, the character of a group representation is a function on the group that associates to each group element the trace of the corresponding matrix. The character carries the essential information about the representation in a more condensed form. Georg Frobenius initially developed representation theory of finite groups entirely based on the characters, and without any explicit matrix realization of representations themselves. This is possible because a complex representation of a finite group is determined (up to isomorphism) by its character. The situation with representations over a field of positive characteristic, so-called ""modular representations"", is more delicate, but Richard Brauer developed a powerful theory of characters in this case as well. Many deep theorems on the structure of finite groups use characters of modular representations.
"
Class_field_theory,Mathematics,3,"In mathematics, class field theory is the branch of algebraic number theory concerned with the abelian extensions of number fields, global fields of positive characteristic, and local fields. The theory had its origins in the proof of quadratic reciprocity by Gauss at the end of the 18th century. These ideas were developed over the next century, giving rise to a set of conjectures by Hilbert that were subsequently proved by Takagi and Artin. These conjectures and their proofs constitute the main body of class field theory. 
 One major result states that, given a number field F, and writing K for the maximal abelian unramified extension of F, the Galois group of K over F is canonically isomorphic to the ideal class group of F. This statement can be generalized to the Artin reciprocity law; writing CF for the idele class group of F, and taking L to be any finite abelian extension of F, this law gives a canonical isomorphism
 where 




N

L

/

F




{  N_{L/F}}
 denotes the idelic norm map from L to F. This isomorphism is then called the reciprocity map. The existence theorem states that the reciprocity map can be used to give a bijection between the set of abelian extensions of F and the set of closed subgroups of finite index of 




C

F


.


{  C_{F}.}

 A standard method for developing global class field theory since the 1930s is to develop local class field theory, which describes abelian extensions of local fields, and then use it to construct global class field theory. This was first done by Artin and Tate using the theory of group cohomology, and in particular by developing the notion of class formations. Later, Neukirch has found a proof of the main statements of global class field theory without using cohomological ideas.
 Class field theory also encompasses the explicit construction of maximal abelian extensions of number fields in the few cases where such constructions are known. Currently, this portion of the theory consists of Kronecker-Weber theorem, which can be used to construct the abelian extensions of 




Q



{  \mathbb {Q} }
 and the theory of complex multiplication, which can be used to construct the abelian extensions of CM-fields.
 The Langlands program gives one approach for generalizing class field theory to non-abelian extensions. This generalization is mostly still conjectural. For number fields, class field theory and the results related to the modularity theorem are the only cases known.
"
Differential_geometry,Mathematics,3,"Differential geometry is a mathematical discipline that uses the techniques of differential calculus, integral calculus, linear algebra and multilinear algebra to study problems in geometry. The theory of plane and space curves and surfaces in the three-dimensional Euclidean space formed the basis for development of differential geometry during the 18th century and the 19th century.
 Since the late 19th century, differential geometry has grown into a field concerned more generally with the geometric structures on differentiable manifolds. Differential geometry is closely related to differential topology and the geometric aspects of the theory of differential equations. The differential geometry of surfaces captures many of the key ideas and techniques endemic to this field.
"
Algebraic_topology,Mathematics,3,"Algebraic topology is a branch of mathematics that uses tools from abstract algebra to study topological spaces. The basic goal is to find algebraic invariants that classify topological spaces up to homeomorphism, though usually most classify up to homotopy equivalence.
 Although algebraic topology primarily uses algebra to study topological problems, using topology to solve algebraic problems is sometimes also possible. Algebraic topology, for example, allows for a convenient proof that any subgroup of a free group is again a free group.
"
Classical_analysis,Mathematics,3,"Mathematical analysis is the branch of mathematics dealing with limits
and related theories, such as differentiation, integration, measure, infinite series, and analytic functions.[1][2] These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis.
Analysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects  (a metric space).
"
Analytic_number_theory,Mathematics,3,"In mathematics, analytic number theory is a branch of number theory that uses methods from mathematical analysis to solve problems about the integers.[1] It is often said to have begun with Peter Gustav Lejeune Dirichlet's 1837 introduction of Dirichlet L-functions to give the first proof of Dirichlet's theorem on arithmetic progressions.[1][2] It is well known for its results on prime numbers (involving the Prime Number Theorem and Riemann zeta function) and additive number theory (such as the Goldbach conjecture and Waring's problem).
"
Differential_calculus,Mathematics,3,"In mathematics, differential calculus is a subfield of calculus that studies the rates at which quantities change.[1] It is one of the two traditional divisions of calculus, the other being integral calculus—the study of the area beneath a curve.[2] The primary objects of study in differential calculus are the derivative of a function, related notions such as the differential, and their applications. The derivative of a function at a chosen input value describes the rate of change of the function near that input value. The process of finding a derivative is called differentiation. Geometrically, the derivative at a point is the slope of the tangent line to the graph of the function at that point, provided that the derivative exists and is defined at that point. For a real-valued function of a single real variable, the derivative of a function at a point generally determines the best linear approximation to the function at that point.
 Differential calculus and integral calculus are connected by the fundamental theorem of calculus, which states that differentiation is the reverse process to integration.
 Differentiation has applications to nearly all quantitative disciplines. For example, in physics, the derivative of the displacement of a moving body with respect to time is the velocity of the body, and the derivative of velocity with respect to time is acceleration. The derivative of the momentum of a body with respect to time equals the force applied to the body; rearranging this derivative statement leads to the famous F = ma equation associated with Newton's second law of motion. The reaction rate of a chemical reaction is a derivative. In operations research, derivatives determine the most efficient ways to transport materials and design factories.
 Derivatives are frequently used to find the maxima and minima of a function. Equations involving derivatives are called differential equations and are fundamental in describing natural phenomena. Derivatives and their generalizations appear in many fields of mathematics, such as complex analysis, functional analysis, differential geometry, measure theory, and abstract algebra.
"
Diophantine_geometry,Mathematics,3,"In mathematics, Diophantine geometry is the study of points of algebraic varieties with coordinates in the integers, rational numbers, and their generalizations. These generalizations typically are fields that are not algebraically closed, such as number fields, finite fields, function fields, and p-adic fields (but not the real numbers which are used in real algebraic geometry). It is a sub-branch of arithmetic geometry and is one approach to the theory of Diophantine equations, formulating questions about such equations in terms of algebraic geometry.
 A single equation defines a hypersurface, and simultaneous Diophantine equations give rise to a general algebraic variety V over K; the typical question is about the nature of the set V(K) of points on V with co-ordinates in K, and by means of height functions quantitative questions about the ""size"" of these solutions may be posed, as well as the qualitative issues of whether any points exist, and if so whether there are an infinite number. Given the geometric approach, the consideration of homogeneous equations and homogeneous co-ordinates is fundamental, for the same reasons that projective geometry is the dominant approach in algebraic geometry. Rational number solutions therefore are the primary consideration; but integral solutions (i.e. lattice points) can be treated in the same way as an affine variety may be considered inside a projective variety that has extra points at infinity.
 The general approach of Diophantine geometry is illustrated by Faltings's theorem (a conjecture of L. J. Mordell) stating that an algebraic curve C of genus g > 1 over the rational numbers has only finitely many rational points. The first result of this kind may have been the theorem of Hilbert and Hurwitz dealing with the case g = 0. The theory consists both of theorems and many conjectures and open questions.
"
Euclidean_geometry,Mathematics,3,"Euclidean geometry is a mathematical system attributed to Alexandrian Greek mathematician Euclid, which he described in his textbook on geometry: the Elements. Euclid's method consists in assuming a small set of intuitively appealing axioms, and deducing many other propositions (theorems) from these. Although many of Euclid's results had been stated by earlier mathematicians,[1] Euclid was the first to show how these propositions could fit into a comprehensive deductive and logical system.[2] The Elements begins with plane geometry, still taught in secondary school (high school) as the first axiomatic system and the first examples of formal proof. It goes on to the solid geometry of three dimensions. Much of the Elements states results of what are now called algebra and number theory, explained in geometrical language.[1] For more than two thousand years, the adjective ""Euclidean"" was unnecessary because no other sort of geometry had been conceived. Euclid's axioms seemed so intuitively obvious (with the possible exception of the parallel postulate) that any theorem proved from them was deemed true in an absolute, often metaphysical, sense. Today, however, many other self-consistent non-Euclidean geometries are known, the first ones having been discovered in the early 19th century. An implication of Albert Einstein's theory of general relativity is that physical space itself is not Euclidean, and Euclidean space is a good approximation for it only over short distances (relative to the strength of the  gravitational field).[3] Euclidean geometry is an example of synthetic geometry, in that it proceeds logically from axioms describing basic properties of geometric objects such as points and lines, to propositions about those objects, all without the use of coordinates to specify those objects. This is in contrast to analytic geometry, which uses coordinates to translate geometric propositions into algebraic formulas.
"
Invariant_theory,Mathematics,3,"Invariant theory is a branch of abstract algebra dealing with actions of groups on algebraic varieties, such as vector spaces, from the point of view of their effect on functions. Classically, the theory dealt with the question of explicit description of polynomial functions that do not change, or are invariant, under the transformations from a given linear group. For example, if we consider the action of the special linear group SLn on the space of n by n matrices by left multiplication, then the determinant is an invariant of this action because the determinant of A X equals the determinant of X, when  A is in SLn.
"
Classical_mathematics,Mathematics,3,"In the foundations of mathematics, classical mathematics refers generally to the mainstream approach to mathematics, which is based on classical logic and ZFC set theory.[1]  It stands in contrast to other types of mathematics such as constructive mathematics or predicative mathematics.  In practice, the most common non-classical systems are used in constructive mathematics.[2] Classical mathematics is sometimes attacked on philosophical grounds, due to constructivist and other objections to the logic, set theory, etc., chosen as its foundations, such as have been expressed by L. E. J. Brouwer. Almost all mathematics, however, is done in the classical tradition, or in ways compatible with it.
 Defenders of classical mathematics, such as David Hilbert, have argued that it is easier to work in, and is most fruitful; although they acknowledge non-classical mathematics has at times led to fruitful results that classical mathematics could not (or could not so easily) attain, they argue that on the whole, it is the other way round.[citation needed]"
Projective_geometry,Mathematics,3,"
 
 In mathematics, projective geometry is the study of geometric properties that are invariant with respect to projective transformations. This means that, compared to elementary Euclidean geometry, projective geometry has a different setting, projective space, and a selective set of basic geometric concepts. The basic intuitions are that projective space has more points than Euclidean space, for a given dimension, and that geometric transformations are permitted that transform the extra points (called ""points at infinity"") to Euclidean points, and vice-versa.
 Properties meaningful for projective geometry are respected by this new idea of transformation, which is more radical in its effects than can be expressed by a transformation matrix and translations (the affine transformations). The first issue for geometers is what kind of geometry is adequate for a novel situation. It is not possible to refer to angles in projective geometry as it is in Euclidean geometry, because angle is an example of a concept not invariant with respect to projective transformations, as is seen in perspective drawing. One source for projective geometry was indeed the theory of perspective. Another difference from elementary geometry is the way in which parallel lines can be said to meet in a point at infinity, once the concept is translated into projective geometry's terms. Again this notion has an intuitive basis, such as railway tracks meeting at the horizon in a perspective drawing. See projective plane for the basics of projective geometry in two dimensions.
 While the ideas were available earlier, projective geometry was mainly a development of the 19th century. This included the theory of complex projective space, the coordinates used (homogeneous coordinates) being complex numbers. Several major types of more abstract mathematics (including invariant theory, the Italian school of algebraic geometry, and Felix Klein's Erlangen programme resulting in the study of the classical groups) were based on projective geometry. It was also a subject with many practitioners for its own sake, as synthetic geometry. Another topic that developed from axiomatic studies of projective geometry is finite geometry.
 The topic of projective geometry is itself now divided into many research subtopics, two examples of which are projective algebraic geometry (the study of projective varieties) and projective differential geometry (the study of differential invariants of the projective transformations).
"
Tensor,Mathematics,3,"In mathematics, a tensor is an algebraic object that describes a (multilinear) relationship between sets of algebraic objects related to a vector space. Objects that tensors may map between include vectors and scalars, and even other tensors. Tensors can take several different forms – for example: scalars and vectors (which are the simplest tensors), dual vectors, multilinear maps between vector spaces, and even some operations such as the dot product. Tensors are defined independent of any basis, although they are often referred to by their components in a basis related to a particular coordinate system.
 Tensors are important in physics because they provide a concise mathematical framework for formulating and solving physics problems in areas such as mechanics (stress, elasticity, fluid mechanics, moment of inertia, ...), electrodynamics (electromagnetic tensor, Maxwell tensor, permittivity, magnetic susceptibility, ...), or general relativity (stress–energy tensor, curvature tensor, ... ) and others. In applications, it is common to study situations in which a different tensor can occur at each point of an object; for example the stress within an object may vary from one location to another. This leads to the concept of a tensor field. In some areas, tensor fields are so ubiquitous that they are often simply called ""tensors"".
 Tensors were conceived in 1900 by Tullio Levi-Civita and Gregorio Ricci-Curbastro, who continued the earlier work of Bernhard Riemann and Elwin Bruno Christoffel and others, as part of the absolute differential calculus. The concept enabled an alternative formulation of the intrinsic differential geometry of a manifold in the form of the Riemann curvature tensor.[1]"
Clifford_analysis,Mathematics,3,"Clifford analysis, using Clifford algebras named after William Kingdon Clifford, is the study of Dirac operators, and Dirac type operators in analysis and geometry, together with their applications. Examples of Dirac type operators include, but are not limited to, the Hodge–Dirac operator, 



d
+

⋆

d

⋆



{  d+{\star }d{\star }}
 on a Riemannian manifold, the Dirac operator in euclidean space and its inverse on 




C

0


∞


(


R


n


)


{  C_{0}^{\infty }(\mathbf {R} ^{n})}
 and their conformal equivalents on the sphere, the Laplacian in euclidean n-space and the Atiyah–Singer–Dirac operator on a spin manifold, Rarita–Schwinger/Stein–Weiss type operators, conformal Laplacians, spinorial Laplacians and Dirac operators on SpinC manifolds, systems of Dirac operators, the Paneitz operator, Dirac operators on hyperbolic space, the hyperbolic Laplacian and Weinstein equations.
"
Clifford_theory,Mathematics,3,"In mathematics, Clifford theory, introduced by Alfred H. Clifford (1937) harvtxt error: no target: CITEREFAlfred_H._Clifford1937 (help), describes the relation between representations of a group and those of a normal subgroup.
"
Cobordism_theory,Mathematics,3,"In mathematics, cobordism is a fundamental  equivalence relation on the class of compact manifolds of the same dimension, set up using the concept of the boundary (French bord, giving cobordism) of a manifold. Two manifolds of the same dimension are cobordant if their disjoint union is the boundary of a compact manifold one dimension higher.
 The boundary of an (n + 1)-dimensional manifold W is an n-dimensional manifold ∂W that is closed, i.e., with empty boundary. In general, a closed manifold need not be a boundary: cobordism theory is the study of the difference between all closed manifolds and those that are boundaries. The theory was originally developed by René Thom for smooth manifolds (i.e., differentiable), but there are now also versions for
piecewise linear and topological manifolds.
 A cobordism between manifolds M and N is a compact manifold W whose boundary is the disjoint union of M and N, 



∂
W
=
M
⊔
N


{  \partial W=M\sqcup N}
.
 Cobordisms are studied both for the equivalence relation that they generate, and as objects in their own right. Cobordism is a much coarser equivalence relation than diffeomorphism or homeomorphism of manifolds, and is significantly easier to study and compute. It is not possible to classify manifolds up to diffeomorphism or homeomorphism in dimensions ≥ 4 – because the word problem for groups cannot be solved – but it is possible to classify manifolds up to cobordism. Cobordisms are central objects of study in geometric topology and algebraic topology. In geometric topology, cobordisms are intimately connected with Morse theory, and h-cobordisms are fundamental in the study of high-dimensional manifolds, namely surgery theory. In algebraic topology, cobordism theories are fundamental extraordinary cohomology theories, and categories of cobordisms are the domains of topological quantum field theories.
"
Coding_theory,Mathematics,3,"Coding theory is the study of the properties of codes and their respective fitness for specific applications. Codes are used for data compression, cryptography, error detection and correction, data transmission and data storage. Codes are studied by various scientific disciplines—such as information theory, electrical engineering,  mathematics, linguistics, and computer science—for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.
 There are four types of coding:[1] Data compression attempts to remove redundancy from the data from a source in order to transmit it more efficiently. For example, ZIP data compression makes data files smaller, for purposes such as to reduce Internet traffic. Data compression and error correction may be studied in combination.
 Error correction adds extra data bits to make the transmission of data more robust to disturbances present on the transmission channel. The ordinary user may not be aware of many applications using error correction. A typical music compact disc (CD) uses the Reed-Solomon code to correct for scratches and dust. In this application the transmission channel is the CD itself. Cell phones also use coding techniques to correct for the fading and noise of high frequency radio transmission. Data modems, telephone transmissions, and the NASA Deep Space Network all employ channel coding techniques to get the bits through, for example the turbo code and LDPC codes.
"
Cohomology_theory,Mathematics,3,"In mathematics, specifically in homology theory and algebraic topology, cohomology is a general term for a sequence of abelian groups associated to a topological space, often defined from a cochain complex. Cohomology can be viewed as a method of assigning richer algebraic invariants to a space than homology. Some versions of cohomology arise by dualizing the construction of homology. In other words, cochains are functions on the group of chains in homology theory.
 From its beginning in topology, this idea became a dominant method in the mathematics of the second half of the twentieth century. From the initial idea of homology as a method of constructing algebraic invariants of topological spaces, the range of applications of homology and cohomology theories has spread throughout geometry and algebra. The terminology tends to hide the fact that cohomology, a contravariant theory, is more natural than homology in many applications. At a basic level, this has to do with functions and pullbacks in geometric situations: given spaces X and Y, and some kind of function F on Y, for any mapping f : X → Y, composition with f gives rise to a function F ∘ f on X. The most important cohomology theories have a product, the cup product, which gives them a ring structure.  Because of this feature, cohomology is usually a stronger invariant than homology.
"
Combinatorial_analysis,Mathematics,3,"Combinatorics is an area of mathematics primarily concerned with counting, both as a means and an end in obtaining results, and certain properties of finite structures.  It is closely related to many other areas of mathematics and has many applications ranging from logic to statistical physics, from evolutionary biology to computer science, etc.
 The full scope of combinatorics is not universally agreed upon.[1] According to H.J. Ryser, a definition of the subject is difficult because it crosses so many mathematical subdivisions.[2] Insofar as an area can be described by the types of problems it addresses, combinatorics is involved with
 Leon Mirsky has said: ""combinatorics is a range of linked studies which have something in common and yet diverge widely in their objectives, their methods, and the degree of coherence they have attained.""[3] One way to define combinatorics is, perhaps, to describe its subdivisions with their problems and techniques. This is the approach that is used below. However, there are also purely historical reasons for including or not including some topics under the combinatorics umbrella.[4] Although primarily concerned with finite systems, some combinatorial questions and techniques can be extended to an infinite (specifically, countable) but discrete setting.
 Combinatorics is well known for the breadth of the problems it tackles. Combinatorial problems arise in many areas of pure mathematics, notably in algebra, probability theory, topology, and geometry,[5] as well as in its many application areas. Many combinatorial questions have historically been considered in isolation, giving an ad hoc solution to a problem arising in some mathematical context. In the later twentieth century, however, powerful and general theoretical methods were developed, making combinatorics into an independent branch of mathematics in its own right.[6] One of the oldest and most accessible parts of combinatorics is graph theory, which by itself has numerous natural connections to other areas. Combinatorics is used frequently in computer science to obtain formulas and estimates in the analysis of algorithms.
 A mathematician who studies combinatorics is called a  combinatorialist.
"
Combinatorial_commutative_algebra,Mathematics,3,"Combinatorial commutative algebra is a relatively new, rapidly developing mathematical discipline. As the name implies, it lies at the intersection of two more established fields, commutative algebra and combinatorics, and frequently uses methods of one to address problems arising in the other. Less obviously, polyhedral geometry plays a significant role.
 One of the milestones in the development of the subject was Richard Stanley's 1975 proof of the Upper Bound Conjecture for simplicial spheres, which was based on earlier work of Melvin Hochster and Gerald Reisner. While the problem can be formulated purely in geometric terms, the methods of the proof drew on commutative algebra techniques.
 A signature theorem in combinatorial commutative algebra is the characterization of h-vectors of simplicial polytopes conjectured in 1970 by Peter McMullen. Known as the g-theorem, it was proved in 1979 by Stanley (necessity of the conditions, algebraic argument) and by Louis Billera and Carl W. Lee (sufficiency, combinatorial and geometric construction). A major open question was the extension of this characterization from simplicial polytopes to simplicial spheres, the g-conjecture, which was resolved in 2018 by Karim Adiprasito.
"
Combinatorial_design_theory,Mathematics,3,"Combinatorial design theory is the part of combinatorial mathematics that deals with the existence,  construction and properties of systems of finite sets whose arrangements satisfy generalized concepts of balance and/or symmetry. These concepts are not made precise so that a wide range of objects can be thought of as being under the same umbrella. At times this might involve the numerical sizes of set intersections as in block designs, while at other times it could involve the spatial arrangement of entries in an array as in sudoku grids.
 Combinatorial design theory can be applied to the area of design of experiments.  Some of the basic theory of combinatorial designs originated in the statistician Ronald Fisher's work on the design of biological experiments. Modern applications are also found in a wide gamut of areas including finite geometry, tournament scheduling, lotteries, mathematical chemistry, mathematical biology, algorithm design and analysis, networking, group testing and cryptography.[1]"
Combinatorial_game_theory,Mathematics,3,"Combinatorial game theory (CGT) is a branch of mathematics and theoretical computer science that typically studies sequential games with perfect information. Study has been largely confined to two-player games that have a position in which the players take turns changing in defined ways or moves to achieve a defined winning condition. CGT has not traditionally studied games of chance or those that use imperfect or incomplete information, favoring games that offer perfect information in which the state of the game and the set of available moves is always known by both players.[1] However, as mathematical techniques advance, the types of game that can be mathematically analyzed expands, thus the boundaries of the field are ever changing.[2] Scholars will generally define what they mean by a ""game"" at the beginning of a paper, and these definitions often vary as they are specific to the game being analyzed and are not meant to represent the entire scope of the field.
 Combinatorial games include well-known games such as chess, checkers, and Go, which are regarded as non-trivial, and tic-tac-toe, which is considered as trivial in the sense of being ""easy to solve"". Some combinatorial games may also have an unbounded playing area, such as infinite chess. In CGT, the moves in these and other games are represented as a game tree.
 Combinatorial games also include one-player combinatorial puzzles such as Sudoku, and no-player automata, such as Conway's Game of Life, (although in the strictest definition, ""games"" can be said to require more than one participant, thus the designations of ""puzzle"" and ""automata"".[3])
 Game theory in general includes games of chance, games of imperfect knowledge, and games in which players can move simultaneously, and they tend to represent real-life decision making situations.
 CGT has a different emphasis than ""traditional"" or ""economic"" game theory, which was initially developed to study games with simple combinatorial structure, but with elements of chance (although it also considers sequential moves, see extensive-form game). Essentially, CGT has contributed new methods for analyzing game trees, for example using surreal numbers, which are a subclass of all two-player perfect-information games.[3] The type of games studied by CGT is also of interest in artificial intelligence, particularly for automated planning and scheduling. In CGT there has been less emphasis on refining practical search algorithms (such as the alpha–beta pruning heuristic included in most artificial intelligence textbooks), but more emphasis on descriptive theoretical results (such as measures of game complexity or proofs of optimal solution existence without necessarily specifying an algorithm, such as the strategy-stealing argument).
 An important notion in CGT is that of the solved game. For example, tic-tac-toe is considered a solved game, as it can be proven that any game will result in a draw if both players play optimally. Deriving similar results for games with rich combinatorial structures is difficult. For instance, in 2007 it was announced that checkers has been weakly solved—optimal play by both sides also leads to a draw—but this result was a computer-assisted proof.[4] Other real world games are mostly too complicated to allow complete analysis today, although the theory has had some recent successes in analyzing Go endgames. Applying CGT to a position attempts to determine the optimum sequence of moves for both players until the game ends, and by doing so discover the optimum move in any position. In practice, this process is torturously difficult unless the game is very simple.
 It can be helpful to distinguish between combinatorial ""mathgames"" of interest primarily to mathematicians and scientists to ponder and solve, and combinatorial ""playgames"" of interest to the general population as a form of entertainment and competition.[5] However, a number of games fall into both categories.  Nim, for instance, is a playgame instrumental in the foundation of CGT, and one of the first computerized games.[6] Tic-tac-toe is still used to teach basic principles of game AI design to computer science students.
"
Combinatorial_geometry,Mathematics,3,"Discrete geometry and combinatorial geometry are branches of geometry that study combinatorial properties and constructive methods of discrete geometric objects.  Most questions in discrete geometry involve finite or discrete sets of basic geometric objects, such as points, lines, planes, circles, spheres, polygons, and so forth.  The subject focuses on the combinatorial properties of these objects, such as how they intersect one another, or how they may be arranged to cover a larger object.
 Discrete geometry has a large overlap with convex geometry and computational geometry, and is closely related to subjects such as finite geometry, combinatorial optimization, digital geometry, discrete differential geometry, geometric graph theory, toric geometry, and combinatorial topology.
"
Combinatorial_group_theory,Mathematics,3,"In mathematics, combinatorial group theory is the theory of free groups, and the concept of a presentation of a group by generators and relations. It is much used in geometric topology, the fundamental group of a simplicial complex having in a natural and geometric way such a presentation.
A very closely related topic is geometric group theory, which today largely subsumes combinatorial group theory, using techniques from outside combinatorics besides.
 It also comprises a number of algorithmically insoluble problems, most notably the word problem for groups; and the classical Burnside problem.
"
Combinatorial_mathematics,Mathematics,3,"Combinatorics is an area of mathematics primarily concerned with counting, both as a means and an end in obtaining results, and certain properties of finite structures.  It is closely related to many other areas of mathematics and has many applications ranging from logic to statistical physics, from evolutionary biology to computer science, etc.
 The full scope of combinatorics is not universally agreed upon.[1] According to H.J. Ryser, a definition of the subject is difficult because it crosses so many mathematical subdivisions.[2] Insofar as an area can be described by the types of problems it addresses, combinatorics is involved with
 Leon Mirsky has said: ""combinatorics is a range of linked studies which have something in common and yet diverge widely in their objectives, their methods, and the degree of coherence they have attained.""[3] One way to define combinatorics is, perhaps, to describe its subdivisions with their problems and techniques. This is the approach that is used below. However, there are also purely historical reasons for including or not including some topics under the combinatorics umbrella.[4] Although primarily concerned with finite systems, some combinatorial questions and techniques can be extended to an infinite (specifically, countable) but discrete setting.
 Combinatorics is well known for the breadth of the problems it tackles. Combinatorial problems arise in many areas of pure mathematics, notably in algebra, probability theory, topology, and geometry,[5] as well as in its many application areas. Many combinatorial questions have historically been considered in isolation, giving an ad hoc solution to a problem arising in some mathematical context. In the later twentieth century, however, powerful and general theoretical methods were developed, making combinatorics into an independent branch of mathematics in its own right.[6] One of the oldest and most accessible parts of combinatorics is graph theory, which by itself has numerous natural connections to other areas. Combinatorics is used frequently in computer science to obtain formulas and estimates in the analysis of algorithms.
 A mathematician who studies combinatorics is called a  combinatorialist.
"
Combinatorial_number_theory,Mathematics,3,"Number theory (or arithmetic or higher arithmetic in older usage) is a branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions. German mathematician Carl Friedrich Gauss (1777–1855) said, ""Mathematics is the queen of the sciences—and number theory is the queen of mathematics.""[1] Number theorists study prime numbers as well as the properties of mathematical objects made out of integers (for example, rational numbers) or defined as generalizations of the integers (for example, algebraic integers). 
 Integers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory are often best understood through the study of analytical objects (for example, the Riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers, for example, as approximated by the latter (Diophantine approximation).
 The older term for number theory is arithmetic. By the early twentieth century, it had been superseded by ""number theory"".[note 1] (The word ""arithmetic"" is used by the general public to mean ""elementary calculations""; it has also acquired other meanings in mathematical logic, as in Peano arithmetic, and computer science, as in floating point arithmetic.) The use of the term arithmetic for number theory regained some ground in the second half of the 20th century, arguably in part due to French influence.[note 2] In particular, arithmetical is preferred as an adjective to number-theoretic.[by whom?]"
Combinatorial_optimization,Mathematics,3,"Combinatorial optimization is a subfield of mathematical optimization that is related to operations research, algorithm theory, and computational complexity theory. It has important applications in several fields, including artificial intelligence, machine learning, auction theory, software engineering, applied mathematics and theoretical computer science.
 Combinatorial optimization  is a topic that consists of finding an optimal object from a finite set of objects.[1] In many such problems, exhaustive search is not tractable. It operates on the domain of those optimization problems in which the set of feasible solutions is discrete or can be reduced to discrete, and in which the goal is to find the best solution.  Typical problems are the travelling salesman problem (""TSP""), the minimum spanning tree problem (""MST""), and the knapsack problem.
 Some research literature[2] considers discrete optimization to consist of integer programming together with combinatorial optimization (which in turn is composed of optimization problems dealing with graph structures) although all of these topics have closely intertwined research literature. It often involves determining the way to efficiently allocate resources used to find solutions to mathematical problems.
"
Combinatorial_set_theory,Mathematics,3,"In mathematics, infinitary combinatorics, or combinatorial set theory, is an extension of ideas in combinatorics to infinite sets.
Some of the things studied include continuous graphs and trees, extensions of Ramsey's theorem, and Martin's axiom.
Recent developments concern combinatorics of the continuum[1] and combinatorics on successors of singular cardinals.[2]"
Combinatorial_theory,Mathematics,3,"Combinatorics is an area of mathematics primarily concerned with counting, both as a means and an end in obtaining results, and certain properties of finite structures.  It is closely related to many other areas of mathematics and has many applications ranging from logic to statistical physics, from evolutionary biology to computer science, etc.
 The full scope of combinatorics is not universally agreed upon.[1] According to H.J. Ryser, a definition of the subject is difficult because it crosses so many mathematical subdivisions.[2] Insofar as an area can be described by the types of problems it addresses, combinatorics is involved with
 Leon Mirsky has said: ""combinatorics is a range of linked studies which have something in common and yet diverge widely in their objectives, their methods, and the degree of coherence they have attained.""[3] One way to define combinatorics is, perhaps, to describe its subdivisions with their problems and techniques. This is the approach that is used below. However, there are also purely historical reasons for including or not including some topics under the combinatorics umbrella.[4] Although primarily concerned with finite systems, some combinatorial questions and techniques can be extended to an infinite (specifically, countable) but discrete setting.
 Combinatorics is well known for the breadth of the problems it tackles. Combinatorial problems arise in many areas of pure mathematics, notably in algebra, probability theory, topology, and geometry,[5] as well as in its many application areas. Many combinatorial questions have historically been considered in isolation, giving an ad hoc solution to a problem arising in some mathematical context. In the later twentieth century, however, powerful and general theoretical methods were developed, making combinatorics into an independent branch of mathematics in its own right.[6] One of the oldest and most accessible parts of combinatorics is graph theory, which by itself has numerous natural connections to other areas. Combinatorics is used frequently in computer science to obtain formulas and estimates in the analysis of algorithms.
 A mathematician who studies combinatorics is called a  combinatorialist.
"
Combinatorial_topology,Mathematics,3,"In mathematics, combinatorial topology was an older name for algebraic topology, dating from the time when topological invariants of spaces (for example the Betti numbers) were regarded as derived from combinatorial decompositions of spaces, such as decomposition into simplicial complexes. After the proof of the simplicial approximation theorem this approach provided rigour.
 The change of name reflected the move to organise topological classes such as cycles-modulo-boundaries explicitly into abelian groups. This point of view is often attributed to Emmy Noether,[1] and so the change of title may reflect her influence. The transition is also attributed to the work of Heinz Hopf,[2] who was influenced by Noether, and to Leopold Vietoris and Walther Mayer, who independently defined homology.[3] A fairly precise date can be supplied in the internal notes of the Bourbaki group. While topology was still combinatorial in 1942, it had become algebraic by 1944.[4] Azriel Rosenfeld (1973) proposed digital topology for a type of image processing that can be considered as a new development of combinatorial topology. The digital forms of the Euler characteristic theorem and the Gauss–Bonnet theorem were obtained by Li Chen and Yongwu Rong.[5][6] A 2D grid cell topology already appeared in the Alexandrov–Hopf book Topologie I (1935).
"
Combinatorics,Mathematics,3,"Combinatorics is an area of mathematics primarily concerned with counting, both as a means and an end in obtaining results, and certain properties of finite structures.  It is closely related to many other areas of mathematics and has many applications ranging from logic to statistical physics, from evolutionary biology to computer science, etc.
 The full scope of combinatorics is not universally agreed upon.[1] According to H.J. Ryser, a definition of the subject is difficult because it crosses so many mathematical subdivisions.[2] Insofar as an area can be described by the types of problems it addresses, combinatorics is involved with
 Leon Mirsky has said: ""combinatorics is a range of linked studies which have something in common and yet diverge widely in their objectives, their methods, and the degree of coherence they have attained.""[3] One way to define combinatorics is, perhaps, to describe its subdivisions with their problems and techniques. This is the approach that is used below. However, there are also purely historical reasons for including or not including some topics under the combinatorics umbrella.[4] Although primarily concerned with finite systems, some combinatorial questions and techniques can be extended to an infinite (specifically, countable) but discrete setting.
 Combinatorics is well known for the breadth of the problems it tackles. Combinatorial problems arise in many areas of pure mathematics, notably in algebra, probability theory, topology, and geometry,[5] as well as in its many application areas. Many combinatorial questions have historically been considered in isolation, giving an ad hoc solution to a problem arising in some mathematical context. In the later twentieth century, however, powerful and general theoretical methods were developed, making combinatorics into an independent branch of mathematics in its own right.[6] One of the oldest and most accessible parts of combinatorics is graph theory, which by itself has numerous natural connections to other areas. Combinatorics is used frequently in computer science to obtain formulas and estimates in the analysis of algorithms.
 A mathematician who studies combinatorics is called a  combinatorialist.
"
Commutative_algebra,Mathematics,3,"Commutative algebra is the branch of algebra that studies commutative rings, their ideals, and modules over such rings. Both algebraic geometry and algebraic number theory build on commutative algebra. Prominent examples of commutative rings include polynomial rings; rings of algebraic integers, including the ordinary integers 




Z



{  \mathbb {Z} }
; and p-adic integers.[1] Commutative algebra is the main technical tool in the local study of schemes.
 The study of rings that are not necessarily commutative is known as noncommutative algebra; it includes ring theory, representation theory, and the theory of Banach algebras.
"
Complex_algebra,Mathematics,3,"In mathematics, a field of sets is a mathematical structure consisting of a pair 



⟨
X
,


F


⟩


{  \langle X,{\mathcal {F}}\rangle }
 where 



X


{  X}
 is a set and 





F




{  {\mathcal {F}}}
 is a family of subsets of 



X


{  X}
 called an algebra over 



X


{  X}
 that contains the empty set as an element, and is closed under the operations of taking complements in 



X
,


{  X,}
 finite unions, and finite intersections. Equivalently, an algebra over 



X


{  X}
 is a subset 





F




{  {\mathcal {F}}}
 of the power set of 



X


{  X}
 such that
 By De Morgan's laws, if 





F




{  {\mathcal {F}}}
 has the first two properties then 





F




{  {\mathcal {F}}}
 has property (3) if and only if the intersection of any two of its members is again a member of 





F


,


{  {\mathcal {F}},}
 which is why the last condition (3) is sometimes replaced with: 
 In other words, 





F




{  {\mathcal {F}}}
 forms a subalgebra of the power set Boolean algebra of 



X


{  X}
 (with the same identity element 



X
∈


F




{  X\in {\mathcal {F}}}
). 
Many authors refer to 





F




{  {\mathcal {F}}}
 itself as a field of sets. Elements of 



X


{  X}
 are called points while elements of 





F




{  {\mathcal {F}}}
 are called complexes and are said to be the admissible sets of 



X
.


{  X.}

 Fields of sets should not be confused with fields in ring theory nor with fields in physics. Similarly the term ""algebra over 



X


{  X}
"" is used in the sense of a Boolean algebra and should not be confused with algebras over fields or rings in ring theory.
 Fields of sets play an essential role in the representation theory of Boolean algebras. Every Boolean algebra can be represented as a field of sets.
"
Complex_algebraic_geometry,Mathematics,3,"Algebraic geometry is a branch of mathematics, classically studying zeros of multivariate polynomials. Modern algebraic geometry is based on the use of abstract algebraic techniques, mainly from commutative algebra, for solving  geometrical problems about these sets of zeros.
 The fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are: plane algebraic curves, which include lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves, and quartic curves like lemniscates and Cassini ovals. A point of the plane belongs to an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of the points of special interest like the singular points, the inflection points and the points at infinity. More advanced questions involve the topology of the curve and relations between the curves given by different equations.
 Algebraic geometry occupies a central place in modern mathematics and has multiple conceptual connections with such diverse fields as complex analysis, topology and number theory. Initially a study of systems of polynomial equations in several variables, the subject of algebraic geometry starts where equation solving leaves off, and it becomes even more important to understand the intrinsic properties of the totality of solutions of a system of equations, than to find a specific solution; this leads into some of the deepest areas in all of mathematics, both conceptually and in terms of technique.
 In the 20th century, algebraic geometry split into several subareas.
 Much of the development of the mainstream of algebraic geometry in the 20th century occurred within an abstract algebraic framework, with increasing emphasis being placed on ""intrinsic"" properties of algebraic varieties not dependent on any particular way of embedding the variety in an ambient coordinate space; this parallels developments in topology, differential and complex geometry. One key achievement of this abstract algebraic geometry is Grothendieck's scheme theory which allows one to use sheaf theory to study algebraic varieties in a way which is very similar to its use in the study of differential and analytic manifolds. This is obtained by extending the notion of point: In classical algebraic geometry, a point of an affine variety may be identified, through Hilbert's Nullstellensatz, with a maximal ideal of the coordinate ring, while the points of the corresponding affine scheme are all prime ideals of this ring. This means that a point of such a scheme may be either a usual point or a subvariety. This approach also enables a unification of the language and the tools of classical algebraic geometry, mainly concerned with complex points, and of algebraic number theory. Wiles' proof of the longstanding conjecture called Fermat's last theorem is an example of the power of this approach.
"
Complex_analysis,Mathematics,3,"Complex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers. It is useful in many branches of mathematics, including algebraic geometry, number theory, analytic combinatorics, applied mathematics; as well as in physics, including the branches of hydrodynamics, thermodynamics, and particularly quantum mechanics. By extension, use of complex analysis also has applications in engineering fields such as nuclear, aerospace, mechanical and electrical engineering.[citation needed] As a differentiable function of a complex variable is equal to its Taylor series (that is, it is analytic), complex analysis is particularly concerned with analytic functions of a complex variable (that is, holomorphic functions).
"
Complex_analytic_dynamics,Mathematics,3,"
 Complex dynamics is the study of dynamical systems defined by iteration of functions on complex number spaces. Complex analytic dynamics is the study of the dynamics of specifically analytic functions.
"
Complex_analytic_geometry,Mathematics,3,"
 In mathematics, complex geometry is the study of complex manifolds, complex algebraic varieties, and functions of several complex variables. Application of transcendental methods to algebraic geometry falls in this category, together with more geometric aspects of complex analysis.
"
Differential_geometry,Mathematics,3,"Differential geometry is a mathematical discipline that uses the techniques of differential calculus, integral calculus, linear algebra and multilinear algebra to study problems in geometry. The theory of plane and space curves and surfaces in the three-dimensional Euclidean space formed the basis for development of differential geometry during the 18th century and the 19th century.
 Since the late 19th century, differential geometry has grown into a field concerned more generally with the geometric structures on differentiable manifolds. Differential geometry is closely related to differential topology and the geometric aspects of the theory of differential equations. The differential geometry of surfaces captures many of the key ideas and techniques endemic to this field.
"
Complex_dynamics,Mathematics,3,"
 Complex dynamics is the study of dynamical systems defined by iteration of functions on complex number spaces. Complex analytic dynamics is the study of the dynamics of specifically analytic functions.
"
Complex_geometry,Mathematics,3,"
 In mathematics, complex geometry is the study of complex manifolds, complex algebraic varieties, and functions of several complex variables. Application of transcendental methods to algebraic geometry falls in this category, together with more geometric aspects of complex analysis.
"
Complexity_theory_(disambiguation),Mathematics,3,"Complexity theory (or complexity science) is the study of complexity and of complex systems. It may also refer to:
"
Computable_analysis,Mathematics,3,"In mathematics and computer science, computable analysis is the study of mathematical analysis from the perspective of computability theory. It is concerned with the parts of real analysis and functional analysis that can be carried out in a computable manner.  The field is closely related to constructive analysis and numerical analysis.
"
Computable_model_theory,Mathematics,3,"Computable model theory is a branch of model theory which deals with questions of computability as they apply to model-theoretical structures. 
Computable model theory introduces the ideas of computable and decidable models and theories and one of the basic problems is discovering whether or not computable or decidable models fulfilling certain model-theoretic conditions can be shown to exist.
 Computable model theory was developed almost simultaneously by mathematicians in the West, primarily located in the United States and Australia, and Soviet Russia during the middle of the 20th century.  Because of the Cold War there was little communication between these two groups and so a number of important results were discovered independently.
"
Computability_theory,Mathematics,3,"Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability[disambiguation needed]. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory.
 Basic questions addressed by recursion theory include:
 Although there is considerable overlap in terms of knowledge and methods, mathematical recursion theorists study the theory of relative computability, reducibility notions, and degree structures; those in the computer science field focus on the theory of subrecursive hierarchies, formal methods, and formal languages.
"
Computational_algebraic_geometry,Mathematics,3,"Algebraic geometry is a branch of mathematics, classically studying zeros of multivariate polynomials. Modern algebraic geometry is based on the use of abstract algebraic techniques, mainly from commutative algebra, for solving  geometrical problems about these sets of zeros.
 The fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are: plane algebraic curves, which include lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves, and quartic curves like lemniscates and Cassini ovals. A point of the plane belongs to an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of the points of special interest like the singular points, the inflection points and the points at infinity. More advanced questions involve the topology of the curve and relations between the curves given by different equations.
 Algebraic geometry occupies a central place in modern mathematics and has multiple conceptual connections with such diverse fields as complex analysis, topology and number theory. Initially a study of systems of polynomial equations in several variables, the subject of algebraic geometry starts where equation solving leaves off, and it becomes even more important to understand the intrinsic properties of the totality of solutions of a system of equations, than to find a specific solution; this leads into some of the deepest areas in all of mathematics, both conceptually and in terms of technique.
 In the 20th century, algebraic geometry split into several subareas.
 Much of the development of the mainstream of algebraic geometry in the 20th century occurred within an abstract algebraic framework, with increasing emphasis being placed on ""intrinsic"" properties of algebraic varieties not dependent on any particular way of embedding the variety in an ambient coordinate space; this parallels developments in topology, differential and complex geometry. One key achievement of this abstract algebraic geometry is Grothendieck's scheme theory which allows one to use sheaf theory to study algebraic varieties in a way which is very similar to its use in the study of differential and analytic manifolds. This is obtained by extending the notion of point: In classical algebraic geometry, a point of an affine variety may be identified, through Hilbert's Nullstellensatz, with a maximal ideal of the coordinate ring, while the points of the corresponding affine scheme are all prime ideals of this ring. This means that a point of such a scheme may be either a usual point or a subvariety. This approach also enables a unification of the language and the tools of classical algebraic geometry, mainly concerned with complex points, and of algebraic number theory. Wiles' proof of the longstanding conjecture called Fermat's last theorem is an example of the power of this approach.
"
Computational_complexity_theory,Mathematics,3,"
Computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.
 A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millennium Prize Problems, is dedicated to the field of computational complexity.[1] Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kinds of problems can, in principle, be solved algorithmically.
"
Computational_geometry,Mathematics,3,"Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with a history stretching back to antiquity.
 Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.
 The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.
 Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction).
 The main branches of computational geometry are:
"
Computational_group_theory,Mathematics,3,"In mathematics, computational group theory is the study of
groups by means of computers. It is concerned
with designing and analysing algorithms and
data structures to compute information about groups. The subject
has attracted interest because for many interesting groups
(including most of the sporadic groups) it is impractical
to perform calculations by hand.
 Important algorithms in computational group theory include:
 Two important computer algebra systems (CAS) used for group theory are
GAP and Magma. Historically, other systems such as CAS (for character theory) and Cayley (a predecessor of Magma) were important.
 Some achievements of the field include:
"
Computational_mathematics,Mathematics,3,"
Computational mathematics involves mathematical research in mathematics as well as in areas of science where computing plays a central and essential role, and emphasizes algorithms, numerical methods, and symbolic computations.[1] Computational applied mathematics consists roughly of using mathematics for allowing and improving computer computation in applied mathematics. Computational mathematics may also refer to the use of computers for mathematics itself. This includes the use of computers for mathematical computations (computer algebra), the study of what can (and cannot) be computerized in mathematics (effective methods), which computations may be done with present technology (complexity theory), and which proofs can be done on computers (proof assistants).
"
Computational_number_theory,Mathematics,3,"In mathematics and computer science, computational number theory, also known as algorithmic number theory, is the study of 
computational methods for investigating and solving problems in number theory and arithmetic geometry, including algorithms for primality testing and integer factorization, finding solutions to diophantine equations, and explicit methods in arithmetic geometry.[1]
Computational number theory has applications to cryptography, including RSA, elliptic curve cryptography and post-quantum cryptography, and is used to investigate conjectures and open problems in number theory, including the Riemann hypothesis, the Birch and Swinnerton-Dyer conjecture, the ABC conjecture, the modularity conjecture, the Sato-Tate conjecture, and explicit aspects of the Langlands program.[1][2][3]"
Computational_statistics,Mathematics,3,"Computational statistics, or statistical computing, is the interface between statistics and computer science. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education.[1] As in traditional statistics the goal is to transform raw data into knowledge,[2] but the focus lies on computer intensive statistical methods, such as cases with very large sample size and non-homogeneous data sets.[2] The terms 'computational statistics' and 'statistical computing' are often used interchangeably, although Carlo Lauro (a former president of the International Association for Statistical Computing) proposed making a distinction, defining 'statistical computing' as ""the application of computer science to statistics"",
and 'computational statistics' as ""aiming at the design of algorithm for implementing
statistical methods on computers, including the ones unthinkable before the computer
age (e.g. bootstrap, simulation), as well as to cope with analytically intractable problems"" [sic].[3] The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models.
"
Computational_synthetic_geometry,Mathematics,3,"Synthetic geometry (sometimes referred to as axiomatic geometry or even pure geometry) is the study of geometry without the use of coordinates or formulae. It relies on the axiomatic method and the tools directly related to them, that is, compass and straightedge, to draw conclusions and solve problems. 
 Only after the introduction of coordinate methods was there a reason to introduce the term ""synthetic geometry"" to distinguish this approach to geometry from other approaches.
Other approaches to geometry are embodied in analytic and algebraic geometries, where one would use analysis and algebraic techniques to obtain geometric results.
 According to Felix Klein
 Synthetic geometry is that which studies figures as such, without recourse to formulae, whereas analytic geometry consistently makes use of such formulae as can be written down after the adoption of an appropriate system of coordinates.[1] Geometry as presented by Euclid in the Elements is the quintessential example of the use of the synthetic method. It was the favoured method of Isaac Newton for the solution of geometric problems.[2] Synthetic methods were most prominent during the 19th century when geometers rejected coordinate methods in establishing the foundations of projective geometry and non-Euclidean geometries. For example the geometer Jakob Steiner (1796 – 1863) hated analytic geometry, and always gave preference to synthetic methods.[3]"
Computational_topology,Mathematics,3,"Algorithmic topology, or computational topology, is a subfield of topology with an overlap with areas of computer science, in particular, computational geometry and computational complexity theory.
 A primary concern of algorithmic topology, as its name suggests, is to develop efficient algorithms for solving problems that arise naturally in fields such as computational geometry, graphics, robotics, structural biology and chemistry, using methods from computable topology.[1][2]"
Computer_algebra,Mathematics,3,"In mathematics and computer science,[1] computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols.
 Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.
 Computer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, as in public key cryptography, or for some non-linear problems.
"
Conformal_geometry,Mathematics,3,"In mathematics, conformal geometry is the study of the set of angle-preserving (conformal) transformations on a space. 
 In a real two dimensional space, conformal geometry is precisely the geometry of Riemann surfaces. In space higher than two dimensions, conformal geometry may refer either to the study of conformal transformations of what are called ""flat spaces"" (such as Euclidean spaces or spheres), or to the study of conformal manifolds which are Riemannian or pseudo-Riemannian manifolds with a class of metrics that are defined up to scale. Study of the flat structures is sometimes termed Möbius geometry, and is a type of Klein geometry.
"
Constructive_analysis,Mathematics,3,"In mathematics, constructive analysis is mathematical analysis done according to some principles of constructive mathematics.
This contrasts with classical analysis, which (in this context) simply means analysis done according to the (more common) principles of classical mathematics.
 Generally speaking, constructive analysis can reproduce theorems of classical analysis, but only in application to separable spaces; also, some theorems may need to be approached by approximations.
Furthermore, many classical theorems can be stated in ways that are logically equivalent according to classical logic, but not all of these forms will be valid in constructive analysis, which uses intuitionistic logic.
"
Constructive_function_theory,Mathematics,3,"In mathematical analysis, constructive function theory is a field which studies the connection between the smoothness of a function and its degree of approximation.[1][2] It is closely related to approximation theory. The term was coined by Sergei Bernstein.
"
Constructive_mathematics,Mathematics,3,"In the philosophy of mathematics, constructivism asserts that it is necessary to find (or ""construct"") a mathematical object to prove that it exists. In classical mathematics, one can prove the existence of a mathematical object without ""finding"" that object explicitly, by assuming its non-existence and then deriving a contradiction from that assumption. This proof by contradiction is not constructively valid. The constructive viewpoint involves a verificational interpretation of the existential quantifier, which is at odds with its classical interpretation.
 There are many forms of constructivism.[1] These include the program of intuitionism founded by Brouwer, the finitism of Hilbert and Bernays, the constructive recursive mathematics of Shanin and Markov, and Bishop's program of constructive analysis. Constructivism also includes the study of constructive set theories such as CZF and the study of topos theory.
 Constructivism is often identified with intuitionism, although intuitionism is only one constructivist program. Intuitionism maintains that the foundations of mathematics lie in the individual mathematician's intuition, thereby making mathematics into an intrinsically subjective activity.[2] Other forms of constructivism are not based on this viewpoint of intuition, and are compatible with an objective viewpoint on mathematics.
"
Constructive_quantum_field_theory,Mathematics,3,"In mathematical physics, constructive quantum field theory is the field devoted to showing that quantum theory is mathematically compatible with special relativity.  This demonstration requires new mathematics, in a sense analogous to Newton developing calculus in order to understand planetary motion and classical gravity.  Weak, strong, and electromagnetic forces of nature are believed to have their natural description in terms of quantum fields.
 Attempts to put quantum field theory on a basis of completely defined concepts have involved most branches of mathematics, including functional analysis, differential equations, probability theory, representation theory, geometry, and topology. It is known that a quantum field is inherently hard to handle using conventional mathematical techniques like explicit estimates.  This is because a quantum field has the general nature of an operator-valued distribution, a type of object from mathematical analysis.  The existence theorems for quantum fields can be expected to be very difficult to find, if indeed they are possible at all. 
 One discovery of the theory, that can be related in non-technical terms, is that the dimension d of the spacetime involved is crucial. In spite of these impediments, tremendous progress occurred, spurred on by a long collaboration and extensive work of James Glimm and Arthur Jaffe who showed that with d < 4 many examples can be found. Along with work of their students, coworkers, and others, constructive field theory resulted in giving a mathematical foundation and exact interpretation to what previously was only a set of recipes, also in the case d < 4. 
 Theoretical physicists had given these rules the name ""renormalization,"" but most physicists had been skeptical about whether they could be turned into a mathematical theory.  Today one of the most important open problems, both in theoretical physics and in mathematics, is to establish similar results for gauge theory in the realistic case d = 4.   
 The traditional basis of constructive quantum field theory is the set of Wightman axioms.  Osterwalder and Schrader showed that there is an equivalent problem in mathematical probability theory.  The examples with d < 4 satisfy the Wightman axioms as well as the Osterwalder-Schrader axioms.  They also fall in the related framework introduced by Haag and Kastler, called algebraic quantum field theory.  There is a firm belief in the physics community that the gauge theory of Yang and Mills can lead to a tractable theory, but new ideas and new methods will be required to actually establish this, and this could take many years.
"
Constructive_set_theory,Mathematics,3,"Constructive set theory is an approach to mathematical constructivism following the program of axiomatic set theory.
The same first-order language with ""



=


{  =}
"" and ""



∈


{  \in }
"" of classical set theory is usually used, so this is not to be confused with a constructive types approach.
On the other hand, some constructive theories are indeed motivated by their interpretability in type theories.
 Apart from rejecting the law of excluded middle (





L
E
M




{  {\mathrm {LEM} }}
), constructive set theories often require some logical quantifiers in the axioms to be  bounded, motivated by results tied to impredicativity.
"
Contact_geometry,Mathematics,3,"In mathematics,  contact geometry is the study of a geometric structure on smooth manifolds given by a hyperplane distribution in the tangent bundle satisfying a condition called 'complete non-integrability'.  Equivalently, such a distribution may be given (at least locally) as the kernel of a differential one-form, and the non-integrability condition translates into a maximal non-degeneracy condition on the form. These conditions are opposite to two equivalent conditions for 'complete integrability' of a hyperplane distribution, i.e. that it be tangent to a codimension one foliation on the manifold, whose equivalence is the content of the Frobenius theorem.
 Contact geometry is in many ways an odd-dimensional counterpart of symplectic geometry, a structure on certain even-dimensional manifolds. Both contact and symplectic geometry are motivated by the mathematical formalism of classical mechanics, where one can consider either the even-dimensional phase space of a mechanical system or constant-energy hypersurface, which, being codimension one, has odd dimension.
"
Convex_analysis,Mathematics,3,"Convex analysis is the branch of mathematics devoted to the study of properties of convex functions and convex sets, often with applications in convex minimization, a subdomain of optimization theory.
"
Convex_geometry,Mathematics,3,"In mathematics, convex geometry is the branch of geometry studying convex sets, mainly in Euclidean space. Convex sets occur naturally in many areas: computational geometry, convex analysis, discrete geometry, functional analysis, geometry of numbers, integral geometry, linear programming, probability theory, game theory, etc.
"
Coordinate_geometry,Mathematics,3,"In classical mathematics, analytic geometry, also known as coordinate geometry or Cartesian geometry, is the study of geometry using a coordinate system. This contrasts with synthetic geometry.
 Analytic geometry is used in physics and engineering, and also in aviation, rocketry, space science, and spaceflight. It is the foundation of most modern fields of geometry, including algebraic, differential, discrete and computational geometry.
 Usually the Cartesian coordinate system is applied to manipulate equations for planes, straight lines, and squares, often in two and sometimes three dimensions. Geometrically, one studies the Euclidean plane (two dimensions) and Euclidean space (three dimensions). As taught in school books, analytic geometry can be explained more simply: it is concerned with defining and representing geometrical shapes in a numerical way and extracting numerical information from shapes' numerical definitions and representations. That the algebra of the real numbers can be employed to yield results about the linear continuum of geometry relies on the Cantor–Dedekind axiom.
"
CR_geometry,Mathematics,3,"In mathematics, a CR manifold is a differentiable manifold together with a geometric structure modeled on that of a real hypersurface in a complex vector space, or more generally modeled on an edge of a wedge.
 Formally, a CR manifold is a differentiable manifold M together with a preferred complex distribution L, or in other words a complex subbundle of the complexified tangent bundle 




C

T
M
=
T
M

⊗


R




C



{  \mathbb {C} TM=TM\otimes _{\mathbb {R} }\mathbb {C} }
 such that
 The subbundle L is called a CR structure on the manifold M.
 The abbreviation CR stands for Cauchy–Riemann or Complex-Real.
"
Cryptography,Mathematics,3,"
 Cryptography, or cryptology (from Ancient Greek: κρυπτός, romanized: kryptós ""hidden, secret""; and γράφειν graphein, ""to write"", or -λογία -logia, ""study"", respectively[1]), is the practice and study of techniques for secure communication in the presence of third parties called adversaries.[2] More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages;[3] various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation[4] are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.
 Cryptography prior to the modern age was effectively synonymous with encryption, the conversion of information from a readable state to apparent nonsense. The originator of an encrypted message shares the decoding technique only with intended recipients to preclude access from adversaries. The cryptography literature often uses the names Alice (""A"") for the sender, Bob (""B"") for the intended recipient, and Eve (""eavesdropper"") for the adversary.[5] Since the development of rotor cipher machines in World War I and the advent of computers in World War II, the methods used to carry out cryptology have become increasingly complex and its application more widespread.
 Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power—an example is the one-time pad—but these schemes are more difficult to use in practice than the best theoretically breakable but computationally secure mechanisms.
 The growth of cryptographic technology has raised a number of legal issues in the information age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export.[6] In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation.[7][8] Cryptography also plays a major role in digital rights management and copyright infringement of digital media.[9]"
Decision_analysis,Mathematics,3,"Decision analysis (DA) is the discipline comprising the philosophy, methodology, and professional practice necessary to address important decisions in a formal manner. Decision analysis includes many procedures, methods, and tools for identifying, clearly representing, and formally assessing important aspects of a decision, for prescribing a recommended course of action by applying the maximum expected utility action axiom to a well-formed representation of the decision, and for translating the formal representation of a decision and its corresponding recommendation into insight for the decision maker and other stakeholders.
"
Decision_theory,Mathematics,3,"Decision theory (or the theory of choice not to be confused with choice theory) is the study of an agent's choices.[1] Decision theory can be broken into two branches: normative decision theory, which analyzes the outcomes of decisions or determines the optimal decisions given constraints and assumptions, and descriptive decision theory, which analyzes how agents actually make the decisions they do.
 Decision theory is closely related to the field of game theory[2] and is an interdisciplinary topic, studied by economists, statisticians, data scientists, psychologists, biologists,[3] political and other social scientists, philosophers,[4] and computer scientists.
 Empirical applications of this rich theory are usually done with the help of statistical and econometric methods.
"
Noncommutative_algebraic_geometry,Mathematics,3,"Noncommutative algebraic geometry is a branch of mathematics, and more specifically a direction in noncommutative geometry, that studies the geometric properties of formal duals of non-commutative algebraic objects such as rings as well as geometric objects derived from them (e.g. by gluing along localizations or taking noncommutative stack quotients).
 For example, noncommutative algebraic geometry is supposed to extend a notion of an algebraic scheme by suitable gluing of spectra of noncommutative rings; depending on how literally and how generally this aim (and a notion of spectrum) is understood in noncommutative setting, this has been achieved in various level of success. The noncommutative ring generalizes here a commutative ring of regular functions on a commutative scheme. Functions on usual spaces in the traditional (commutative) algebraic geometry have a product defined by pointwise multiplication; as the values of these functions commute, the functions also commute: a times b equals b times a. It is remarkable that viewing noncommutative associative algebras as algebras of functions on ""noncommutative"" would-be space is a far-reaching geometric intuition, though it formally looks like a fallacy.[citation needed] Much of the motivation for noncommutative geometry, and in particular for the noncommutative algebraic geometry, is from physics; especially from quantum physics, where the algebras of observables are indeed viewed as noncommutative analogues of functions, hence having the ability to observe their geometric aspects is desirable.
 One of the values of the field is that it also provides new techniques to study objects in commutative algebraic geometry such as Brauer groups.
 The methods of noncommutative algebraic geometry are analogs of the methods of commutative algebraic geometry, but frequently the foundations are different. Local behavior in commutative algebraic geometry is captured by commutative algebra and especially the study of local rings. These do not have a ring-theoretic analogue in the noncommutative setting; though in a categorical setup one can talk about stacks of local categories of quasicoherent sheaves over noncommutative spectra. Global properties such as those arising from homological algebra and K-theory more frequently carry over to the noncommutative setting.
"
Descriptive_set_theory,Mathematics,3,"In mathematical logic, descriptive set theory (DST) is the study of certain classes of ""well-behaved"" subsets of the real line and other Polish spaces. As well as being one of the primary areas of research in set theory, it has applications to other areas of mathematics such as functional analysis, ergodic theory, the study of operator algebras and group actions, and mathematical logic.
"
Differential_algebraic_geometry,Mathematics,3,"Differential algebraic geometry is an area of differential algebra that adapts concepts and methods from algebraic geometry and applies them to systems of differential equations/algebraic differential equations.
 Another way of generalizing ideas from algebraic geometry is diffiety theory.
"
Differential_calculus,Mathematics,3,"In mathematics, differential calculus is a subfield of calculus that studies the rates at which quantities change.[1] It is one of the two traditional divisions of calculus, the other being integral calculus—the study of the area beneath a curve.[2] The primary objects of study in differential calculus are the derivative of a function, related notions such as the differential, and their applications. The derivative of a function at a chosen input value describes the rate of change of the function near that input value. The process of finding a derivative is called differentiation. Geometrically, the derivative at a point is the slope of the tangent line to the graph of the function at that point, provided that the derivative exists and is defined at that point. For a real-valued function of a single real variable, the derivative of a function at a point generally determines the best linear approximation to the function at that point.
 Differential calculus and integral calculus are connected by the fundamental theorem of calculus, which states that differentiation is the reverse process to integration.
 Differentiation has applications to nearly all quantitative disciplines. For example, in physics, the derivative of the displacement of a moving body with respect to time is the velocity of the body, and the derivative of velocity with respect to time is acceleration. The derivative of the momentum of a body with respect to time equals the force applied to the body; rearranging this derivative statement leads to the famous F = ma equation associated with Newton's second law of motion. The reaction rate of a chemical reaction is a derivative. In operations research, derivatives determine the most efficient ways to transport materials and design factories.
 Derivatives are frequently used to find the maxima and minima of a function. Equations involving derivatives are called differential equations and are fundamental in describing natural phenomena. Derivatives and their generalizations appear in many fields of mathematics, such as complex analysis, functional analysis, differential geometry, measure theory, and abstract algebra.
"
Differential_Galois_theory,Mathematics,3,"
 In mathematics, differential Galois theory studies the Galois groups of differential equations.
"
Differential_geometry,Mathematics,3,"Differential geometry is a mathematical discipline that uses the techniques of differential calculus, integral calculus, linear algebra and multilinear algebra to study problems in geometry. The theory of plane and space curves and surfaces in the three-dimensional Euclidean space formed the basis for development of differential geometry during the 18th century and the 19th century.
 Since the late 19th century, differential geometry has grown into a field concerned more generally with the geometric structures on differentiable manifolds. Differential geometry is closely related to differential topology and the geometric aspects of the theory of differential equations. The differential geometry of surfaces captures many of the key ideas and techniques endemic to this field.
"
Differential_geometry_of_curves,Mathematics,3,"Differential geometry of curves is the branch of geometry that deals with smooth curves in the plane and the Euclidean space by methods of differential and integral calculus.
 Many specific curves have been thoroughly investigated using the synthetic approach. Differential geometry takes another path: curves are represented in a parametrized form, and their geometric properties and various quantities associated with them, such as the curvature and the arc length, are expressed via derivatives and integrals using vector calculus. One of the most important tools used to analyze a curve is the Frenet frame, a moving frame that provides a coordinate system at each point of the curve that is ""best adapted"" to the curve near that point.
 The theory of curves is much simpler and narrower in scope than the theory of surfaces and its higher-dimensional generalizations because a regular curve in a Euclidean space has no intrinsic geometry. Any regular curve may be parametrized by the arc length (the natural parametrization). From the point of view of a theoretical point particle on the curve that does not know anything about the ambient space, all curves would appear the same. Different space curves are only distinguished by how they bend and twist. Quantitatively, this is measured by the differential-geometric invariants called the curvature and the torsion of a curve. The fundamental theorem of curves asserts that the knowledge of these invariants completely determines the curve.
"
Differential_geometry_of_surfaces,Mathematics,3,"In mathematics, the differential geometry of surfaces deals with the differential geometry of smooth surfaces with various additional structures, most often, a Riemannian metric.
Surfaces have been extensively studied from various perspectives: extrinsically, relating to their embedding in Euclidean space and intrinsically, reflecting their properties determined solely by the distance within the surface as measured along curves on the surface. One of the fundamental concepts investigated is the Gaussian curvature, first studied in depth by Carl Friedrich Gauss,[1] who showed that curvature was an intrinsic property of a surface, independent of its isometric embedding in Euclidean space.
 Surfaces naturally arise as graphs of functions of a pair of variables, and sometimes appear in parametric form or as loci associated to space curves. An important role in their study has been played by Lie groups (in the spirit of the Erlangen program), namely the symmetry groups of the Euclidean plane, the sphere and the hyperbolic plane. These Lie groups can be used to describe surfaces of constant Gaussian curvature; they also provide an essential ingredient in the modern approach to intrinsic differential geometry through connections. On the other hand, extrinsic properties relying on an embedding of a surface in Euclidean space have also been extensively studied. This is well illustrated by the non-linear Euler–Lagrange equations in the calculus of variations: although Euler developed the one variable equations to understand geodesics, defined independently of an embedding, one of Lagrange's main applications of the two variable equations was to minimal surfaces, a concept that can only be defined in terms of an embedding.
"
Differential_topology,Mathematics,3,"
 In mathematics, differential topology is the field dealing with differentiable functions on differentiable manifolds. It is closely related to differential geometry and together they make up the geometric theory of differentiable manifolds.
"
Diffiety,Mathematics,3,"In mathematics, a diffiety is a geometrical object introduced by Alexandre Mikhailovich Vinogradov (see Vinogradov (1984a) harvtxt error: no target: CITEREFVinogradov1984a (help)) playing the same role in the modern theory of partial differential equations as algebraic varieties play for algebraic equations.
"
Diophantine_geometry,Mathematics,3,"In mathematics, Diophantine geometry is the study of points of algebraic varieties with coordinates in the integers, rational numbers, and their generalizations. These generalizations typically are fields that are not algebraically closed, such as number fields, finite fields, function fields, and p-adic fields (but not the real numbers which are used in real algebraic geometry). It is a sub-branch of arithmetic geometry and is one approach to the theory of Diophantine equations, formulating questions about such equations in terms of algebraic geometry.
 A single equation defines a hypersurface, and simultaneous Diophantine equations give rise to a general algebraic variety V over K; the typical question is about the nature of the set V(K) of points on V with co-ordinates in K, and by means of height functions quantitative questions about the ""size"" of these solutions may be posed, as well as the qualitative issues of whether any points exist, and if so whether there are an infinite number. Given the geometric approach, the consideration of homogeneous equations and homogeneous co-ordinates is fundamental, for the same reasons that projective geometry is the dominant approach in algebraic geometry. Rational number solutions therefore are the primary consideration; but integral solutions (i.e. lattice points) can be treated in the same way as an affine variety may be considered inside a projective variety that has extra points at infinity.
 The general approach of Diophantine geometry is illustrated by Faltings's theorem (a conjecture of L. J. Mordell) stating that an algebraic curve C of genus g > 1 over the rational numbers has only finitely many rational points. The first result of this kind may have been the theorem of Hilbert and Hurwitz dealing with the case g = 0. The theory consists both of theorems and many conjectures and open questions.
"
Discrepancy_theory,Mathematics,3,"In mathematics, discrepancy theory describes the deviation of a situation from the state one would like it to be in. It is also called the theory of irregularities of distribution. This refers to the theme of classical discrepancy theory, namely distributing points in some space such that they are evenly distributed with respect to some (mostly geometrically defined) subsets. The discrepancy (irregularity) measures how far a given distribution deviates from an ideal one.
 Discrepancy theory can be described as the study of inevitable irregularities of distributions, in measure-theoretic and combinatorial settings. Just as Ramsey theory elucidates the impossibility of total disorder, discrepancy theory studies the deviations from total uniformity.
 A significant event in the history of discrepancy theory was the 1916 paper of Weyl on the uniform distribution of sequences in the unit interval.[1] "
Discrete_differential_geometry,Mathematics,3,"Discrete differential geometry is the study of discrete counterparts of notions in differential geometry. Instead of smooth curves and surfaces, there are polygons, meshes, and simplicial complexes. It is used in the study of computer graphics, geometry processing and topological combinatorics.
"
Discrete_exterior_calculus,Mathematics,3,"In mathematics, the discrete exterior calculus (DEC) is the extension of the exterior calculus to discrete spaces including graphs and finite element meshes. DEC methods have proved to be very powerful in improving and analyzing finite element methods: for instance, DEC-based methods allow the use of highly non-uniform meshes to obtain accurate results. Non-uniform meshes are advantageous because they allow the use of large elements where the process to be simulated is relatively simple, as opposed to a fine resolution where the process may be complicated (e.g., near an obstruction to a fluid flow), while using less computational power than if a uniformly fine mesh were used.
"
Discrete_geometry,Mathematics,3,"Discrete geometry and combinatorial geometry are branches of geometry that study combinatorial properties and constructive methods of discrete geometric objects.  Most questions in discrete geometry involve finite or discrete sets of basic geometric objects, such as points, lines, planes, circles, spheres, polygons, and so forth.  The subject focuses on the combinatorial properties of these objects, such as how they intersect one another, or how they may be arranged to cover a larger object.
 Discrete geometry has a large overlap with convex geometry and computational geometry, and is closely related to subjects such as finite geometry, combinatorial optimization, digital geometry, discrete differential geometry, geometric graph theory, toric geometry, and combinatorial topology.
"
Discrete_mathematics,Mathematics,3,"Discrete mathematics is the study of mathematical structures that are fundamentally discrete rather than continuous. In contrast to real numbers that have the property of varying ""smoothly"", the objects studied in discrete mathematics – such as integers, graphs, and statements in logic[1] – do not vary smoothly in this way, but have distinct, separated values.[2][3] Discrete mathematics therefore excludes topics in ""continuous mathematics"" such as calculus or Euclidean geometry. Discrete objects can often be enumerated by integers. More formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets[4] (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term ""discrete mathematics.""[5] Indeed, discrete mathematics is described less by what is included than by what is excluded: continuously varying quantities and related notions.
 The set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.
 Research in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in discrete steps and store data in discrete bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, programming languages, cryptography, automated theorem proving, and software development. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems, such as in operations research.
 Although the main objects of study in discrete mathematics are discrete objects, analytic methods from continuous mathematics are often employed as well.
 In university curricula, ""Discrete Mathematics"" appeared in the 1980s, initially as a computer science support course; its contents were somewhat haphazard at the time. The curriculum has thereafter developed in conjunction with efforts by ACM and MAA into a course that is basically intended to develop mathematical maturity in first-year students; therefore it is nowadays a prerequisite for mathematics majors in some universities as well.[6][7] Some high-school-level discrete mathematics textbooks have appeared as well.[8] At this level, discrete mathematics is sometimes seen as a preparatory course, not unlike precalculus in this respect.[9] The Fulkerson Prize is awarded for outstanding papers in discrete mathematics.
"
Discrete_Morse_theory,Mathematics,3,"Discrete Morse theory is a combinatorial adaptation of Morse theory developed by Robin Forman. The theory has various practical applications in diverse fields of applied mathematics and computer science, such as configuration spaces,[1] homology computation,[2][3] denoising,[4] mesh compression,[5] and topological data analysis.[6]"
Distance_geometry,Mathematics,3,"Distance geometry is the  characterization and study of sets of points based only on given values of the distances between member pairs.[1][2][3] More abstractly, it is the study of semimetric spaces and the isometric transformations between them. In this view, it can be considered as a subject within general topology.[4] Historically, the first result in distance geometry is Heron's formula in 1st century AD. The modern theory began in 19th century with work by Arthur Cayley, followed by more extensive developments in the 20th century by Karl Menger and others.
 Distance geometry problems arise whenever one needs to infer the shape of a configuration of points (relative positions) from the distances between them, such as in biology,[4] sensor network,[5] surveying, navigation, cartography, and physics.
"
Domain_theory,Mathematics,3,"Domain theory is a branch of mathematics that studies special kinds of partially ordered sets (posets) commonly called domains. Consequently, domain theory can be considered as a branch of order theory. The field has major applications in computer science, where it is used to specify denotational semantics, especially for functional programming languages. Domain theory formalizes the intuitive ideas of approximation and convergence in a very general way and is closely related to topology.
"
Donaldson_theory,Mathematics,3,"In mathematics, and especially gauge theory, Donaldson theory is the study of the topology of smooth 4-manifolds using moduli spaces of anti-self-dual instantons. It was started by Simon Donaldson (1983) who proved Donaldson's theorem restricting the possible quadratic forms on the second cohomology group of a compact simply connected 4-manifold.  Important consequences of this theorem include the existence of an Exotic R4 and the failure of the smooth h-cobordism theorem in 4 dimensions. The results of Donaldson theory depend therefore on the manifold having a differential structure, and are largely false for topological 4-manifolds. 
 Many of the theorems in Donaldson theory can now be proved more easily using Seiberg–Witten theory, though there are a number of open problems remaining in Donaldson theory, such as the Witten conjecture and the Atiyah–Floer conjecture.
"
Dynamical_systems_theory,Mathematics,3,"
Dynamical systems theory is an area of mathematics used to describe the behavior of complex dynamical systems, usually by employing differential equations or difference equations. When differential equations are employed, the theory is called continuous dynamical systems. From a physical point of view, continuous dynamical systems is a generalization of classical mechanics, a generalization where the equations of motion are postulated directly and are not constrained to be Euler–Lagrange equations of a least action principle. When difference equations are employed, the theory is called discrete dynamical systems. When the time variable runs over a set that is discrete over some intervals and continuous over other intervals or is any arbitrary time-set such as a Cantor set, one gets dynamic equations on time scales. Some situations may also be modeled by mixed operators, such as differential-difference equations.
 This theory deals with the long-term qualitative behavior of dynamical systems, and studies the nature of, and when possible the solutions of, the equations of motion of systems that are often primarily mechanical or otherwise physical in nature, such as planetary orbits and the behaviour of electronic circuits, as well as systems that arise in biology, economics, and elsewhere. Much of modern research is focused on the study of chaotic systems.
 This field of study is also called just dynamical systems, mathematical dynamical systems theory or the mathematical theory of dynamical systems.
"
Econometrics,Mathematics,3,"
 Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships.[1] More precisely, it is ""the quantitative analysis of actual economic phenomena based on the concurrent development of theory and observation, related by appropriate methods of inference"".[2] An introductory economics textbook describes econometrics as allowing economists ""to sift through mountains of data to extract simple relationships"".[3] The first known use of the term ""econometrics"" (in cognate form) was by Polish economist Paweł Ciompa in 1910.[4] Jan Tinbergen is considered by many to be one of the founding fathers of econometrics.[5][6][7] Ragnar Frisch is credited with coining the term in the sense in which it is used today.[8] A basic tool for econometrics is the multiple linear regression model.[9] Econometric theory uses statistical theory and mathematical statistics to evaluate and develop econometric methods.[10][11] Econometricians try to find estimators that have desirable statistical properties including unbiasedness, efficiency, and consistency. Applied econometrics uses theoretical econometrics and real-world data for assessing economic theories, developing econometric models, analysing economic history, and forecasting.
"
Effective_descriptive_set_theory,Mathematics,3,"Effective descriptive set theory is the branch of descriptive set theory dealing with sets of reals having lightface definitions; that is, definitions that do not require an arbitrary real parameter (Moschovakis 1980). Thus effective descriptive set theory combines descriptive set theory with recursion theory.
"
Elementary_algebra,Mathematics,3,"Elementary algebra encompasses some of the basic concepts of algebra, one of the main branches of mathematics. It is typically taught to secondary school students and builds on their understanding of arithmetic. Whereas arithmetic deals with specified numbers,[1] algebra introduces quantities without fixed values, known as variables.[2] This use of variables entails use of algebraic notation and an understanding of the general rules of the operators introduced in arithmetic.  Unlike abstract algebra, elementary algebra is not concerned with algebraic structures outside the realm of real and complex numbers.
 The use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Many quantitative relationships in science and mathematics are expressed as algebraic equations.
"
Elementary_arithmetic,Mathematics,3,"Elementary arithmetic is the simplified portion of arithmetic that includes the operations of addition, subtraction, multiplication, and division. It should not be confused with elementary function arithmetic.
 Elementary arithmetic starts with the natural numbers and the written symbols (digits) that represent them. The process for combining a pair of these numbers with the four basic operations traditionally relies on memorized results for small values of numbers, including the contents of a multiplication table to assist with multiplication and division.
 Elementary arithmetic also includes fractions and negative numbers, which can be represented on a number line.
"
Elementary_mathematics,Mathematics,3,"Elementary mathematics consists of mathematics topics frequently taught at the primary or secondary school levels.
 In the Canadian curriculum, there are six basic strands in Elementary Mathematics: Number, Algebra, Data, Spatial Sense, Financial Literacy, and Social emotional learning skills and math processes. These six strands are the focus of Mathematics education from grade 1 through grade 8.[2] In secondary school, the main topics in elementary mathematics from grade nine until grade ten are: Number Sense and algebra, Linear Relations, Measurement and Geometry.[3] Once students enter grade eleven and twelve students begin university and college preparation classes, which include: Functions, Calculus & Vectors, Advanced Functions, and Data Management.[4]"
Elementary_group_theory,Mathematics,3,"In mathematics, a group is a set equipped with a binary operation that combines any two elements to form a third element in such a way that four conditions called group axioms are satisfied, namely closure, associativity, identity and invertibility. One of the most familiar examples of a group is the set of integers together with the addition operation, but groups are encountered in numerous areas within and outside mathematics, and help focusing on essential structural aspects, by detaching them from the concrete nature of the subject of the study.[1][2] Groups share a fundamental kinship with the notion of symmetry. For example, a symmetry group encodes symmetry features of a geometrical object: the group consists of the set of transformations that leave the object unchanged and the operation of combining two such transformations by performing one after the other. Lie groups are the symmetry groups used in the Standard Model of particle physics; Poincaré groups, which are also Lie groups, can express the physical symmetry underlying special relativity; and point groups are used to help understand symmetry phenomena in molecular chemistry.
 The concept of a group arose from the study of polynomial equations, starting with Évariste Galois in the 1830s, who introduced the term of group (groupe, in French) for the symmetry group of the roots of an equation, now called a Galois group.  After contributions from other fields such as number theory and geometry, the group notion was generalized and firmly established around 1870.  Modern group theory—an active mathematical discipline—studies groups in their own right.[a] To explore groups, mathematicians have devised various notions to break groups into smaller, better-understandable pieces, such as subgroups, quotient groups and simple groups. In addition to their abstract properties, group theorists also study the different ways in which a group can be expressed concretely, both from a point of view of representation theory (that is, through the representations of the group) and of computational group theory.  A theory has been developed for finite groups, which culminated with the classification of finite simple groups, completed in 2004.[aa] Since the mid-1980s, geometric group theory, which studies finitely generated groups as geometric objects, has become an active area in group theory.
"
Elimination_theory,Mathematics,3,"In commutative algebra and algebraic geometry, elimination theory is the classical name for algorithmic approaches to eliminating some variables between polynomials of several variables, in order to solve systems of polynomial equations.
 The classical elimination theory culminated with the work of Macaulay on multivariate resultants, and its description in chapter Elimination theory of the first editions (1930) of van der Waerden's Moderne Algebra. After that, elimination theory was ignored by most algebraic geometers for almost thirty years, until the introduction of new methods for solving polynomial equations, such as Gröbner bases, which were needed for computer algebra.
"
Elliptic_geometry,Mathematics,3,"Elliptic geometry is an example of a geometry in which Euclid's parallel postulate does not hold. Instead, as in spherical geometry, there are no parallel lines since any two lines must intersect. However, unlike in spherical geometry, two lines are usually assumed to intersect at a single point (rather than two). Because of this, the elliptic geometry described in this article is sometimes referred to as single elliptic geometry whereas spherical geometry is sometimes referred to as double elliptic geometry.
 The appearance of this geometry in the nineteenth century stimulated the development of non-Euclidean geometry generally, including hyperbolic geometry.
 Elliptic geometry has a variety of properties that differ from those of classical Euclidean plane geometry.  For example, the sum of the interior angles of any triangle is always greater than 180°.
"
Enumerative_combinatorics,Mathematics,3,"Enumerative combinatorics is an area of combinatorics that deals with the number of ways that certain patterns can be formed. Two examples of this type of problem are counting combinations and counting permutations.  More generally, given an infinite collection of finite sets Si indexed by the natural numbers, enumerative combinatorics seeks to describe a counting function which counts the number of objects in Sn for each n. Although counting the number of elements in a set is a rather broad mathematical problem, many of the problems that arise in applications have a relatively simple combinatorial description. The twelvefold way provides a unified framework for counting permutations, combinations and partitions.
 The simplest such functions are closed formulas, which can be expressed as a composition of elementary functions such as factorials, powers, and so on.  For instance, as shown below, the number of different possible orderings of a deck of n cards is f(n) = n!.  The problem of finding a closed formula is known as algebraic enumeration, and frequently involves deriving a recurrence relation or generating function and using this to arrive at the desired closed form.
 Often, a complicated closed formula yields little insight into the behavior of the counting function as the number of counted objects grows. 
In these cases, a simple asymptotic approximation may be preferable.  A function 



g
(
n
)


{  g(n)}
 is an asymptotic approximation to 



f
(
n
)


{  f(n)}
 if 



f
(
n
)

/

g
(
n
)
→
1


{  f(n)/g(n)\rightarrow 1}
 as 



n
→
∞


{  n\rightarrow \infty }
. In this case, we write 



f
(
n
)
∼
g
(
n
)
.



{  f(n)\sim g(n).\,}

"
Enumerative_geometry,Mathematics,3,"In mathematics, enumerative geometry is the branch of algebraic geometry concerned with counting numbers of solutions to geometric questions, mainly by means of intersection theory.
"
Epidemiology,Mathematics,3,"
Epidemiology is the study and analysis of the distribution (who, when, and where), patterns and determinants of health and disease conditions in defined populations.
 It is a cornerstone of public health, and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare. Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review).  Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences.[1] Major areas of epidemiological study include disease causation, transmission, outbreak investigation, disease surveillance, environmental epidemiology, forensic epidemiology, occupational epidemiology, screening, biomonitoring, and comparisons of treatment effects such as in clinical trials. Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment.
 Epidemiology, literally meaning ""the study of what is upon the people"", is derived from Greek  epi 'upon, among',  demos  'people, district', and  logos 'study, word, discourse', suggesting that it applies only to human populations. However, the term is widely used in studies of zoological populations (veterinary epidemiology), although the term ""epizoology"" is available, and it has also been applied to studies of plant populations (botanical or plant disease epidemiology).[2] The distinction between ""epidemic"" and ""endemic"" was first drawn by Hippocrates,[3] to distinguish between diseases that are ""visited upon"" a population (epidemic) from those that ""reside within"" a population (endemic).[4] The term ""epidemiology"" appears to have first been used to describe the study of epidemics in 1802 by the Spanish physician Villalba in Epidemiología Española.[4] Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic.
 The term epidemiology is now widely applied to cover the description and causation of not only epidemic disease, but of disease in general, and even many non-disease, health-related conditions, such as high blood pressure, depression and obesity. Therefore, this epidemiology is based upon how the pattern of the disease causes change in the function of human beings.
"
Noncommutative_algebraic_geometry,Mathematics,3,"Noncommutative algebraic geometry is a branch of mathematics, and more specifically a direction in noncommutative geometry, that studies the geometric properties of formal duals of non-commutative algebraic objects such as rings as well as geometric objects derived from them (e.g. by gluing along localizations or taking noncommutative stack quotients).
 For example, noncommutative algebraic geometry is supposed to extend a notion of an algebraic scheme by suitable gluing of spectra of noncommutative rings; depending on how literally and how generally this aim (and a notion of spectrum) is understood in noncommutative setting, this has been achieved in various level of success. The noncommutative ring generalizes here a commutative ring of regular functions on a commutative scheme. Functions on usual spaces in the traditional (commutative) algebraic geometry have a product defined by pointwise multiplication; as the values of these functions commute, the functions also commute: a times b equals b times a. It is remarkable that viewing noncommutative associative algebras as algebras of functions on ""noncommutative"" would-be space is a far-reaching geometric intuition, though it formally looks like a fallacy.[citation needed] Much of the motivation for noncommutative geometry, and in particular for the noncommutative algebraic geometry, is from physics; especially from quantum physics, where the algebras of observables are indeed viewed as noncommutative analogues of functions, hence having the ability to observe their geometric aspects is desirable.
 One of the values of the field is that it also provides new techniques to study objects in commutative algebraic geometry such as Brauer groups.
 The methods of noncommutative algebraic geometry are analogs of the methods of commutative algebraic geometry, but frequently the foundations are different. Local behavior in commutative algebraic geometry is captured by commutative algebra and especially the study of local rings. These do not have a ring-theoretic analogue in the noncommutative setting; though in a categorical setup one can talk about stacks of local categories of quasicoherent sheaves over noncommutative spectra. Global properties such as those arising from homological algebra and K-theory more frequently carry over to the noncommutative setting.
"
Ergodic_Ramsey_theory,Mathematics,3,"Ergodic Ramsey theory is a branch of mathematics where problems motivated by additive combinatorics are proven using ergodic theory.
"
Ergodic_theory,Mathematics,3,"
Ergodic theory (Greek: ἔργον ergon ""work"", ὁδός hodos ""way"") is a branch of mathematics that studies statistical properties of deterministic dynamical systems; it is the study of ergodicity. In this context, statistical properties means properties which are expressed through the behavior of time averages of various functions along trajectories of dynamical systems. The notion of deterministic dynamical systems assumes that the equations determining the dynamics do not contain any random perturbations, noise, etc. Thus, the statistics with which we are concerned are properties of the dynamics.
 Ergodic theory, like probability theory, is based on general notions of measure theory.  Its initial development was motivated by problems of statistical physics.
 A central concern of ergodic theory is the behavior of a dynamical system when it is allowed to run for a long time. The first result in this direction is the Poincaré recurrence theorem, which claims that almost all points in any subset of the phase space eventually revisit the set. Systems for which the Poincaré recurrence theorem holds are conservative systems; thus all ergodic systems are conservative.
 More precise information is provided by various ergodic theorems which assert that, under certain conditions, the time average of a function along the trajectories exists almost everywhere and is related to the space average. Two of the most important theorems are those of Birkhoff (1931) and von Neumann which assert the existence of a time average along each trajectory. For the special class of ergodic systems, this time average is the same for almost all initial points: statistically speaking, the system that evolves for a long time ""forgets"" its initial state. Stronger properties, such as mixing and equidistribution, have also been extensively studied.
 The problem of metric classification of systems is another important part of the abstract ergodic theory. An outstanding role in ergodic theory and its applications to stochastic processes is played by the various notions of entropy for dynamical systems.
 The concepts of ergodicity and the ergodic hypothesis are central to applications of ergodic theory. The underlying idea is that for certain systems the time average of their properties is equal to the average over the entire space. Applications of ergodic theory to other parts of mathematics usually involve establishing ergodicity properties for systems of special kind. In geometry, methods of ergodic theory have been used to study the geodesic flow on Riemannian manifolds, starting with the results of Eberhard Hopf for Riemann surfaces of negative curvature. Markov chains form a common context for applications in probability theory. Ergodic theory has fruitful connections with harmonic analysis, Lie theory (representation theory, lattices in algebraic groups), and number theory (the theory of diophantine approximations, L-functions).
"
Euclidean_geometry,Mathematics,3,"Euclidean geometry is a mathematical system attributed to Alexandrian Greek mathematician Euclid, which he described in his textbook on geometry: the Elements. Euclid's method consists in assuming a small set of intuitively appealing axioms, and deducing many other propositions (theorems) from these. Although many of Euclid's results had been stated by earlier mathematicians,[1] Euclid was the first to show how these propositions could fit into a comprehensive deductive and logical system.[2] The Elements begins with plane geometry, still taught in secondary school (high school) as the first axiomatic system and the first examples of formal proof. It goes on to the solid geometry of three dimensions. Much of the Elements states results of what are now called algebra and number theory, explained in geometrical language.[1] For more than two thousand years, the adjective ""Euclidean"" was unnecessary because no other sort of geometry had been conceived. Euclid's axioms seemed so intuitively obvious (with the possible exception of the parallel postulate) that any theorem proved from them was deemed true in an absolute, often metaphysical, sense. Today, however, many other self-consistent non-Euclidean geometries are known, the first ones having been discovered in the early 19th century. An implication of Albert Einstein's theory of general relativity is that physical space itself is not Euclidean, and Euclidean space is a good approximation for it only over short distances (relative to the strength of the  gravitational field).[3] Euclidean geometry is an example of synthetic geometry, in that it proceeds logically from axioms describing basic properties of geometric objects such as points and lines, to propositions about those objects, all without the use of coordinates to specify those objects. This is in contrast to analytic geometry, which uses coordinates to translate geometric propositions into algebraic formulas.
"
Differential_geometry,Mathematics,3,"Differential geometry is a mathematical discipline that uses the techniques of differential calculus, integral calculus, linear algebra and multilinear algebra to study problems in geometry. The theory of plane and space curves and surfaces in the three-dimensional Euclidean space formed the basis for development of differential geometry during the 18th century and the 19th century.
 Since the late 19th century, differential geometry has grown into a field concerned more generally with the geometric structures on differentiable manifolds. Differential geometry is closely related to differential topology and the geometric aspects of the theory of differential equations. The differential geometry of surfaces captures many of the key ideas and techniques endemic to this field.
"
Euler_calculus,Mathematics,3,"Euler calculus is a methodology from applied algebraic topology and integral geometry that integrates constructible functions and more recently definable functions[1] by integrating with respect to the Euler characteristic as a finitely-additive measure. In the presence of a metric, it can be extended to continuous integrands via the Gauss–Bonnet theorem.[2] It was introduced independently by Pierre Schapira[3][4][5] and Oleg Viro[6] in 1988, and is useful for enumeration problems in computational geometry and sensor networks.[7]"
Experimental_mathematics,Mathematics,3,"Experimental mathematics is an approach to mathematics in which computation is used to investigate mathematical objects and identify properties and patterns.[1] It has been defined as ""that branch of mathematics that concerns itself ultimately with the codification and transmission of insights within the mathematical community through the use of experimental (in either the Galilean, Baconian, Aristotelian or Kantian sense) exploration of conjectures and more informal beliefs and a careful analysis of the data acquired in this pursuit.""[2] As expressed by Paul Halmos: ""Mathematics is not a deductive science—that's a cliché. When you try to prove a theorem, you don't just list the hypotheses, and then start to reason. What you do is trial and error, experimentation, guesswork. You want to find out what the facts are, and what you do is in that respect similar to what a laboratory technician does.""[3]"
Extraordinary_cohomology_theory,Mathematics,3,"In mathematics, specifically in homology theory and algebraic topology, cohomology is a general term for a sequence of abelian groups associated to a topological space, often defined from a cochain complex. Cohomology can be viewed as a method of assigning richer algebraic invariants to a space than homology. Some versions of cohomology arise by dualizing the construction of homology. In other words, cochains are functions on the group of chains in homology theory.
 From its beginning in topology, this idea became a dominant method in the mathematics of the second half of the twentieth century. From the initial idea of homology as a method of constructing algebraic invariants of topological spaces, the range of applications of homology and cohomology theories has spread throughout geometry and algebra. The terminology tends to hide the fact that cohomology, a contravariant theory, is more natural than homology in many applications. At a basic level, this has to do with functions and pullbacks in geometric situations: given spaces X and Y, and some kind of function F on Y, for any mapping f : X → Y, composition with f gives rise to a function F ∘ f on X. The most important cohomology theories have a product, the cup product, which gives them a ring structure.  Because of this feature, cohomology is usually a stronger invariant than homology.
"
Extremal_combinatorics,Mathematics,3,"Extremal combinatorics is a field of combinatorics, which is itself a part of mathematics. Extremal combinatorics studies how large or how small a collection of finite objects (numbers, graphs, vectors, sets, etc.) can be, if it has to satisfy certain restrictions.
 Much of extremal combinatorics concerns classes of sets; this is called extremal set theory.  For instance, in an n-element set, what is the largest number of k-element subsets that can pairwise intersect one another?  What is the largest number of subsets of which none contains any other?  The latter question is answered by Sperner's theorem, which gave rise to much of extremal set theory.
 Another kind of example:  How many people can we invite to a party where among each three people there are two who know each other and two who don't know each other? Ramsey theory shows that at most five persons can attend such a party. Or, suppose we are given a finite set of nonzero integers, and are asked to mark as large a subset as possible of this set under the restriction that the sum of any two marked integers cannot be marked. It appears that (independent of what the given integers actually are!) we can always mark at least one-third of them.
"
Extremal_graph_theory,Mathematics,3,"Extremal graph theory is a branch of mathematics that studies how global properties of a graph influence local substructure.[1] It encompasses a vast number of results that describe how do certain graph properties - number of vertices (size), number of edges, edge density, chromatic number, and girth, for example - guarantee the existence of certain local substructures. One of the main objects of study in this area of graph theory are extremal graphs, which are maximal or minimal with respect to some global parameter, and such that they contain (or do not contain) a local substructure- such as a clique, or an edge coloring. 
"
Field_theory_(mathematics),Mathematics,3,"
 In mathematics, a field is a set on which addition, subtraction, multiplication, and division are defined and behave as the corresponding operations on rational and real numbers do. A field is thus a fundamental algebraic structure which is widely used in algebra, number theory, and many other areas of mathematics.
 The best known fields are the field of rational numbers, the field of real numbers and the field of complex numbers. Many other fields, such as fields of rational functions, algebraic function fields, algebraic number fields, and p-adic fields are commonly used and studied in mathematics, particularly in number theory and algebraic geometry. Most cryptographic protocols rely on finite fields, i.e., fields with finitely many elements.
 The relation of two fields is expressed by the notion of a field extension. Galois theory, initiated by Évariste Galois in the 1830s, is devoted to understanding the symmetries of field extensions. Among other results, this theory shows that angle trisection and squaring the circle cannot be done with a compass and straightedge. Moreover, it shows that quintic equations are algebraically unsolvable.
 Fields serve as foundational notions in several mathematical domains. This includes different branches of mathematical analysis, which are based on fields with additional structure. Basic theorems in analysis hinge on the structural properties of the field of real numbers. Most importantly for algebraic purposes, any field may be used as the scalars for a vector space, which is the standard general context for linear algebra. Number fields, the siblings of the field of rational numbers, are studied in depth in number theory. Function fields can help describe properties of geometric objects.
"
Finite_geometry,Mathematics,3,"A finite geometry is any geometric system that has only a finite number of points.
The familiar Euclidean geometry is not finite, because a Euclidean line contains infinitely many points. A geometry based on the graphics displayed on a computer screen, where the pixels are considered to be the points, would be a finite geometry. While there are many systems that could be called finite geometries, attention is mostly paid to the finite projective and affine spaces because of their regularity and simplicity.  Other significant types of finite geometry are finite Möbius or inversive planes and Laguerre planes, which are examples of a general type called Benz planes, and their higher-dimensional analogs such as higher finite inversive geometries.
 Finite geometries may be constructed via linear algebra, starting from vector spaces over a finite field; the affine and projective planes so constructed are called Galois geometries.  Finite geometries can also be defined purely axiomatically. Most common finite geometries are Galois geometries, since any finite projective space of dimension three or greater is isomorphic to a projective space over a finite field (that is, the projectivization of a vector space over a finite field). However, dimension two has affine and projective planes that are not isomorphic to Galois geometries, namely the non-Desarguesian planes.  Similar results hold for other kinds of finite geometries.
"
Finite_model_theory,Mathematics,3,"Finite model theory (FMT) is a subarea of model theory (MT). MT is the branch of mathematical logic which deals with the relation between a formal language (syntax) and its interpretations (semantics). FMT is a restriction of MT to interpretations on finite structures, which have a finite universe.
"
Finsler_geometry,Mathematics,3,"In mathematics, particularly differential geometry, a Finsler manifold is a differentiable manifold M where a (possibly asymmetric) Minkowski functional F(x,−) is provided on each tangent space TxM, that enables one to define the length of any smooth curve γ : [a,b] → M as
 Finsler manifolds are more general than Riemannian manifolds since the tangent norms need not be induced by inner products.
 Every Finsler manifold becomes an intrinsic quasimetric space when the distance between two points is defined as the infimum length of the curves that join them.
 Élie Cartan (1933) named Finsler manifolds after Paul Finsler, who studied this geometry in his dissertation (Finsler 1918).
"
Arithmetic,Mathematics,3,"Arithmetic (from the Greek ἀριθμός arithmos, 'number' and τική [τέχνη], tiké [téchne], 'art') is a branch of mathematics that consists of the study of numbers, especially the properties of the traditional operations on them—addition, subtraction, multiplication, division, exponentiation and extraction of roots.[1][2][3] Arithmetic is an elementary part of number theory, and number theory is considered to be one of the top-level divisions of modern mathematics, along with algebra, geometry, and analysis. The terms arithmetic and higher arithmetic were used until the beginning of the 20th century as synonyms for number theory, and are sometimes still used to refer to a wider part of number theory.[4]"
Fourier_analysis,Mathematics,3,"
 In mathematics, Fourier analysis (/ˈfʊrieɪ, -iər/)[1] is the study of the way general functions may be represented or approximated by sums of simpler trigonometric functions. Fourier analysis grew from the study of Fourier series, and is named after Joseph Fourier, who showed that representing a function as a sum of trigonometric functions greatly simplifies the study of heat transfer.
 
 Today, the subject of Fourier analysis encompasses a vast spectrum of mathematics. In the sciences and engineering, the process of decomposing a function into oscillatory components is often called Fourier analysis, while the operation of rebuilding the function from these pieces is known as Fourier synthesis. For example, determining what component frequencies are present in a musical note would involve computing the Fourier transform of a sampled musical note. One could then re-synthesize the same sound by including the frequency components as revealed in the Fourier analysis. In mathematics, the term Fourier analysis often refers to the study of both operations.
 The decomposition process itself is called a Fourier transformation. Its output, the Fourier transform, is often given a more specific name, which depends on the domain and other properties of the function being transformed. Moreover, the original concept of Fourier analysis has been extended over time to apply to more and more abstract and general situations, and the general field is often known as harmonic analysis. Each transform used for analysis (see list of Fourier-related transforms) has a corresponding inverse transform that can be used for synthesis.
"
Fractal_geometry,Mathematics,3,"In mathematics, a fractal is a self-similar subset of Euclidean space whose fractal dimension strictly exceeds its topological dimension. Fractals appear the same at different levels, as illustrated in successive magnifications of the Mandelbrot set.[1][2][3][4] Fractals exhibit similar patterns at increasingly small scales called self-similarity, also known as expanding symmetry or unfolding symmetry; if this replication is exactly the same at every scale, as in the Menger sponge,[5] it is called affine self-similar.  Fractal geometry lies within the mathematical branch of measure theory.
 

 One way that fractals are different from finite geometric figures is the way in which they scale. Doubling the edge lengths of a polygon multiplies its area by four, which is two (the ratio of the new to the old side length) raised to the power of two (the dimension of the space the polygon resides in). Likewise, if the radius of a sphere is doubled, its volume scales by eight, which is two (the ratio of the new to the old radius) to the power of three (the dimension that the sphere resides in). However, if a fractal's one-dimensional lengths are all doubled, the spatial content of the fractal scales by a power that is not necessarily an integer.[1] This power is called the fractal dimension of the fractal, and it usually exceeds the fractal's topological dimension.[6] Analytically, fractals are usually nowhere differentiable.[1][4][7] An infinite fractal curve can be conceived of as winding through space differently from an ordinary line – although it is still 1-dimensional, its fractal dimension indicates that it also resembles a surface.[1][6] Starting in the 17th century with notions of recursion, fractals have moved through increasingly rigorous mathematical treatment of the concept to the study of continuous but not differentiable functions in the 19th century by the seminal work of Bernard Bolzano, Bernhard Riemann, and Karl Weierstrass,[8] and on to the coining of the word fractal in the 20th century with a subsequent burgeoning of interest in fractals and computer-based modelling in the 20th century.[9][10] The term ""fractal"" was first used by mathematician Benoit Mandelbrot in 1975. Mandelbrot based it on the Latin frāctus, meaning ""broken"" or ""fractured"", and used it to extend the concept of theoretical fractional dimensions to geometric patterns in nature.[1][11] There is some disagreement among mathematicians about how the concept of a fractal should be formally defined. Mandelbrot himself summarized it as ""beautiful, damn hard, increasingly useful. That's fractals.""[12] More formally, in 1982 Mandelbrot stated that ""A fractal is by definition a set for which the Hausdorff–Besicovitch dimension strictly exceeds the topological dimension.""[13] Later, seeing this as too restrictive, he simplified and expanded the definition to: ""A fractal is a shape made of parts similar to the whole in some way.""[14] Still later, Mandelbrot settled on this use of the language: ""...to use fractal without a pedantic definition, to use fractal dimension as a generic term applicable to all the variants"".[15] The consensus is that theoretical fractals are infinitely self-similar, iterated, and detailed mathematical constructs having fractal dimensions, of which many examples have been formulated and studied in great depth.[1][2][3] Fractals are not limited to geometric patterns, but can also describe processes in time.[5][4][16][17][18][19] Fractal patterns with various degrees of self-similarity have been rendered or studied in images, structures and sounds[20] and found in nature,[21][22][23][24][25] technology,[26][27][28][29] art,[30][31] architecture[32] and law.[33] Fractals are of particular relevance in the field of chaos theory, since the graphs of most chaotic processes are fractals.[34] Many real and model networks have been found to have fractal features such as self similarity.[35][36][37] "
Fractional_calculus,Mathematics,3,"Fractional calculus is a branch of mathematical analysis that studies the several different possibilities of defining real number powers or complex number powers of the differentiation operator D
 and of the integration operator J [Note 1] and developing a calculus for such operators generalizing the classical one.
 In this context, the term powers refers to iterative application of a linear operator D to a function f, that is, repeatedly composing D with itself, as in 




D

n


(
f
)
=
(




D
∘
D
∘
D
∘
⋯
D

⏟



n


)
(
f
)
=




D
(
D
(
D
⋯

⏟



n


(
f
)


{  D^{n}(f)=(\underbrace {D\circ D\circ D\circ \cdots D} _{n})(f)=\underbrace {D(D(D\cdots } _{n}(f)}
.
 For example, one may ask for a meaningful interpretation of
 as an analogue of the functional square root for the differentiation operator, that is, an expression for some linear operator that when applied twice to any function will have the same effect as differentiation. More generally, one can look at the question of defining a linear functional
 for every real number a in such a way that, when a takes an integer value n ∈ ℤ, it coincides with the usual n-fold differentiation D if n > 0, and with the −n-th power of J when n < 0.
 One of the motivations behind the introduction and study of these sorts of extensions of the differentiation operator D is that the sets of operator powers { Da | a ∈ ℝ } defined in this way are continuous semigroups with parameter a, of which the original discrete semigroup of { Dn | n ∈ ℤ } for integer n is a denumerable subgroup: since continuous semigroups have a well developed mathematical theory, they can be applied to other branches of mathematics.
 Fractional differential equations, also known as extraordinary differential equations,[1] are a generalization of differential equations through the application of fractional calculus.
"
Fractional_dynamics,Mathematics,3,"In the fields of dynamical systems and control theory, a fractional-order system is a dynamical system that can be modeled by a fractional differential equation containing derivatives of non-integer order.[1] Such systems are said to have fractional dynamics. Derivatives and integrals of fractional orders are used to describe objects that can be characterized by power-law nonlocality,[2] power-law long-range dependence or fractal properties. Fractional-order systems are useful in studying the anomalous behavior of dynamical systems in physics, electrochemistry, biology, viscoelasticity and chaotic systems.[1]"
Fredholm_theory,Mathematics,3,"In mathematics, Fredholm theory is a theory of integral equations. In the narrowest sense, Fredholm theory concerns itself with the solution of the Fredholm integral equation. In a broader sense, the abstract structure of Fredholm's theory is given in terms of the spectral theory of Fredholm operators and Fredholm kernels on Hilbert space. The theory is named in honour of Erik Ivar Fredholm.
"
Functional_analysis,Mathematics,3,"Functional analysis is a branch of mathematical analysis which studies the transformations of functions and their algebraic and topological properties. The field builds upon and abstracts the results of Joseph Fourier's 1822 paper, Théorie analytique de la chaleur (The Analytical Theory of Heat), which demonstrated how a change of basis by means of the Fourier transform could be used to permit manipulations of a function in the frequency domain to obtain insights that were previously unobtainable. Functional analysis has modern applications in many areas of algebra, in particular associative algebra, in probability, operator theory, wavelets and wavelet transforms. The functional data analysis (FDA) paradigm of James O. Ramsay and Bernard Silverman ties functional analysis into principal component analysis and dimensionality reduction. 
 Functional analysis has strong parallels with linear algebra, as both fields are based on vector spaces as the core algebraic structure. Functional analysis endows linear algebra with concepts from topology (e.g. inner product, norm, topological space) in defining the topological vector space (TVS)[speculation?], which strengthens notions of continuity and limit, and supports generalization to infinite-dimensional spaces. The core operation within a TVS is linear transformation. 
 An important part of functional analysis is the extension of the theory of measure, integration, and probability to infinite dimensional spaces, also known as infinite dimensional analysis. Additionally, functional analysis generalizes the concept of an orthonormal basis—as found in Fourier analysis—to arbitrary inner product spaces, including those of infinite dimension. Important theoretical results include the Banach–Steinhaus theorem, spectral theorem (central to operator theory), Hahn–Banach theorem, open mapping theorem, and closed graph theorem. 
 The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations. 
 The usage of the word functional as a noun goes back to the calculus of variations, implying a function whose argument is a function. The term was first used in Leçons sur le calcul des variations (1910) by Jacques Hadamard. However, the general concept of a functional had previously been introduced in 1887 by the Italian mathematician and physicist Vito Volterra.[1][2] The theory of nonlinear functionals was continued by students of Hadamard, in particular Maurice René Fréchet and Paul Lévy. Hadamard also founded the modern school of linear functional analysis further developed by Frigyes Riesz and the Polish Lwów School of Mathematics centered around Stefan Banach.
"
Functional_calculus,Mathematics,3,"In mathematics, a functional calculus is a theory allowing one to apply mathematical functions to mathematical operators. It is now a branch (more accurately, several related areas) of the field of functional analysis, connected with spectral theory. (Historically, the term was also used synonymously with calculus of variations; this usage is obsolete, except for functional derivative. Sometimes it is used in relation to types of functional equations, or in logic for systems of predicate calculus.)
 If f is a function, say a numerical function of a real number, and M is an operator, there is no particular reason why the expression
 should make sense. If it does, then we are no longer using f on its original function domain. In the tradition of operational calculus, algebraic expressions in operators are handled irrespective of their meaning.  This passes nearly unnoticed if we talk about 'squaring a matrix', though, which is the case of f(x) = x2 and M an n×n matrix. The idea of a functional calculus is to create a principled approach to this kind of overloading of the notation. 
 The most immediate case is to apply polynomial functions to a square matrix, extending what has just been discussed. In the finite-dimensional case, the polynomial functional calculus yields quite a bit of information about the operator. For example, consider the family of polynomials which annihilates an operator T. This family is an ideal in the ring of polynomials. Furthermore, it is a nontrivial ideal: let n be the finite dimension of the algebra of matrices, then {I, T, T2...Tn} is linearly dependent. So ∑ αi Ti = 0 for some scalars αi, not all equal to 0. This implies that the polynomial ∑ αi xi lies in the ideal. Since the ring of polynomials is a principal ideal domain, this ideal is generated by some polynomial m. Multiplying by a unit if necessary, we can choose m to be monic. When this is done, the polynomial m is precisely the minimal polynomial of T. This polynomial gives deep information about T. For instance, a scalar α is an eigenvalue of T if and only if α is a root of m. Also, sometimes m can be used to calculate the exponential of T efficiently. 
 The polynomial calculus is not as informative in the infinite-dimensional case. Consider the unilateral shift with the polynomials calculus; the ideal defined above is now trivial. Thus one is interested in functional calculi more general than polynomials. The subject is closely linked to spectral theory, since for a diagonal matrix or multiplication operator, it is rather clear what the definitions should be.
"
Fuzzy_mathematics,Mathematics,3,Fuzzy mathematics  forms a branch of mathematics including fuzzy set theory and fuzzy logic. It started in 1965 after the publication of Lotfi Asker Zadeh's seminal work Fuzzy sets.[1]
Fuzzy_measure_theory,Mathematics,3,"In mathematics, fuzzy measure theory considers generalized measures in which the additive property is replaced by the weaker property of monotonicity.  The central concept of fuzzy measure theory is the fuzzy measure (also capacity, see [1]) which was introduced by Choquet in 1953 and independently defined by Sugeno in 1974 in the context of fuzzy integrals.  There exists a number of different classes of fuzzy measures including plausibility/belief measures; possibility/necessity measures; and probability measures which are a subset of classical measures.
"
Fuzzy_set_theory,Mathematics,3,"In mathematics, fuzzy sets (a.k.a. uncertain sets) are somewhat like sets whose elements have degrees of membership. Fuzzy sets were introduced independently by Lotfi A. Zadeh and Dieter Klaua [de] in 1965 as an extension of the classical notion of set.[1][2]
At the same time, Salii (1965) defined a more general kind of structure called an L-relation, which he studied in an abstract algebraic context. Fuzzy relations, which are now used throughout fuzzy mathematics and have applications in areas such as linguistics (De Cock, Bodenhofer & Kerre 2000), decision-making (Kuzmin 1982), and clustering (Bezdek 1978), are special cases of L-relations when L is the unit interval [0, 1].
 In classical set theory, the membership of elements in a set is assessed in binary terms according to a bivalent condition — an element either belongs or does not belong to the set. By contrast, fuzzy set theory permits the gradual assessment of the membership of elements in a set; this is described with the aid of a membership function valued in the real unit interval [0, 1]. Fuzzy sets generalize classical sets, since the indicator functions (aka characteristic functions) of classical sets are special cases of the membership functions of fuzzy sets, if the latter only take values 0 or 1.[3] In fuzzy set theory, classical bivalent sets are usually called crisp sets. The fuzzy set theory can be used in a wide range of domains in which information is incomplete or imprecise, such as bioinformatics.[4]"
Galois_cohomology,Mathematics,3,"In mathematics, Galois cohomology is the study of the group cohomology of Galois modules, that is, the application of homological algebra to modules for Galois groups. A Galois group G associated to a field extension L/K acts in a natural way on some abelian groups, for example those constructed directly from L, but also through other Galois representations that may be derived by more abstract means. Galois cohomology accounts for the way in which taking Galois-invariant elements fails to be an exact functor.
"
Galois_theory,Mathematics,3,"In mathematics, Galois theory provides a connection between field theory and group theory. Using Galois theory, certain problems in field theory can be reduced to group theory, which is in some sense simpler and better understood. It has been used to solve classic problems including showing that two problems of antiquity cannot be solved as they were stated (doubling the cube and trisecting the angle); showing that there is no quintic formula; and showing which polygons are constructible.
 The subject is named after Évariste Galois, who introduced it for studying the roots of a polynomial and characterizing the polynomial equations that are solvable by radicals in terms of properties of the permutation group of their roots—an equation is solvable by radicals if its roots may be expressed by a formula involving only integers, nth roots, and the four basic arithmetic operations. 
 The theory has been popularized among mathematicians and developed by Richard Dedekind, Leopold Kronecker, Emil Artin, and others who interpreted the permutation group of the roots as the automorphism group of a field extension.
 Galois theory has been generalized to Galois connections and Grothendieck's Galois theory.
"
Galois_geometry,Mathematics,3,"Galois geometry (so named after the 19th-century French mathematician Évariste Galois) is the branch of finite geometry that is concerned with algebraic and analytic geometry over a finite field (or Galois field).[1] More narrowly, a Galois geometry may be defined as a projective space over a finite field.[2] Objects of study include affine and projective spaces over finite fields and various structures that are contained in them. In particular, arcs, ovals, hyperovals, unitals, blocking sets, ovoids, caps, spreads and all finite analogues of structures found in non-finite geometries. Vector spaces defined over finite fields play a significant role, especially in construction methods.
"
Game_theory,Mathematics,3,"
 Game theory is the study of mathematical models of strategic interaction among rational decision-makers.[1] It has applications in all fields of social science, as well as in logic, systems science and computer science. Originally, it addressed zero-sum games, in which each participant's gains or losses are exactly balanced by those of the other participants. In the 21st century, game theory applies to a wide range of behavioral relations, and is now an umbrella term for the science of logical decision making in humans, animals, and computers.
 Modern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum games and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by the 1944 book Theory of Games and Economic Behavior, co-written with Oskar Morgenstern, which considered cooperative games of several players. The second edition of this book provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.
 Game theory was developed extensively in the 1950s by many scholars. It was explicitly applied to biology in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. As of 2014[update], with the Nobel Memorial Prize in Economic Sciences going to game theorist Jean Tirole, eleven game theorists have won the economics Nobel Prize. John Maynard Smith was awarded the Crafoord Prize for his application of game theory to biology.
"
Gauge_theory,Mathematics,3,"In physics, a gauge theory is a type of field theory in which the Lagrangian does not change (is invariant) under local transformations from certain Lie groups.
 The term gauge refers to any specific mathematical formalism to regulate redundant degrees of freedom in the Lagrangian. The transformations between possible gauges, called gauge transformations, form a Lie group—referred to as the symmetry group or the gauge group of the theory. Associated with any Lie group is the Lie algebra of group generators. For each group generator there necessarily arises a corresponding field (usually a vector field) called the gauge field. Gauge fields are included in the Lagrangian to ensure its invariance under the local group transformations (called gauge invariance). When such a theory is quantized, the quanta of the gauge fields are called gauge bosons. If the symmetry group is non-commutative, then the gauge theory is referred to as non-abelian gauge theory, the usual example being the Yang–Mills theory.
 Many powerful theories in physics are described by Lagrangians that are invariant under some symmetry transformation groups. When they are invariant under a transformation identically performed at every point in the spacetime in which the physical processes occur, they are said to have a global symmetry. Local symmetry, the cornerstone of gauge theories, is a stronger constraint. In fact, a global symmetry is just a local symmetry whose group's parameters are fixed in spacetime (the same way a constant value can be understood as a function of a certain parameter, the output of which is always the same).
 Gauge theories are important as the successful field theories explaining the dynamics of elementary particles.  Quantum electrodynamics is an abelian gauge theory with the symmetry group U(1) and has one gauge field, the electromagnetic four-potential, with the photon being the gauge boson. The Standard Model is a non-abelian gauge theory with the symmetry group U(1) × SU(2) × SU(3) and has a total of twelve gauge bosons: the photon, three weak bosons and eight gluons.
 Gauge theories are also important in explaining gravitation in the theory of general relativity.  Its case is somewhat unusual in that the gauge field is a tensor, the Lanczos tensor.  Theories of quantum gravity, beginning with gauge gravitation theory, also postulate the existence of a gauge boson known as the graviton.  Gauge symmetries can be viewed as analogues of the principle of general covariance of general relativity in which the coordinate system can be chosen freely under arbitrary diffeomorphisms of spacetime.  Both gauge invariance and diffeomorphism invariance reflect a redundancy in the description of the system.  An alternative theory of gravitation, gauge theory gravity, replaces the principle of general covariance with a true gauge principle with new gauge fields.
 Historically, these ideas were first stated in the context of classical electromagnetism and later in general relativity.  However, the modern importance of gauge symmetries appeared first in the relativistic quantum mechanics of electrons – quantum electrodynamics, elaborated on below. Today, gauge theories are useful in condensed matter, nuclear and high energy physics among other subfields.
"
General_topology,Mathematics,3,"In mathematics, general topology is the branch of topology that deals with the basic set-theoretic definitions and constructions used in topology. It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology. Another name for general topology is point-set topology.
 The fundamental concepts in point-set topology are continuity, compactness, and connectedness: 
 The words 'nearby', 'arbitrarily small', and 'far apart' can all be made precise by using the concept of open sets. If we change the definition of 'open set', we change what continuous functions, compact sets, and connected sets are. Each choice of definition for 'open set' is called a topology. A set with a topology is called a topological space.
 Metric spaces are an important class of topological spaces where a real, non-negative distance, also called a metric, can be defined on pairs of points in the set. Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.
"
Generalized_trigonometry,Mathematics,3,"Ordinary trigonometry studies triangles in the Euclidean plane R2. There are a number of ways of defining the ordinary Euclidean geometric trigonometric functions on real numbers: right-angled triangle definitions, unit-circle definitions, series definitions, definitions via differential equations, definitions using functional equations.  Generalizations of trigonometric functions are often developed by starting with one of the above methods and adapting it to a situation other than the real numbers of Euclidean geometry. Generally, trigonometry can be the study of triples of points in any kind of geometry or space. A triangle is the polygon with the smallest number of vertices, so one direction to generalize is to study higher-dimensional analogs of angles and polygons: solid angles and polytopes such as tetrahedrons and n-simplices.
"
Geometric_algebra,Mathematics,3,"The geometric algebra (GA) of a vector space is an algebra over a field, noted for its multiplication operation called the geometric product on a space of elements called multivectors, which contains both the scalars 



F


{  F}
 and the vector space 



V


{  V}
. Mathematically, a geometric algebra may be defined as the Clifford algebra of a vector space with a quadratic form. Clifford's contribution was to define a new product, the geometric product, that united the Grassmann and Hamilton algebras into a single structure. Adding the dual of the Grassmann exterior product (the ""meet"") allows the use of the Grassmann–Cayley algebra, and a conformal version of the latter together with a conformal Clifford algebra yields a conformal geometric algebra (CGA) providing a framework for classical geometries.[1] In practice, these and several derived operations allow a correspondence of elements, subspaces and operations of the algebra with geometric interpretations.
 The scalars and vectors have their usual interpretation, and make up distinct subspaces of a GA. Bivectors provide a more natural representation of the pseudovector quantities in vector algebra such as oriented area, oriented angle of rotation, torque, angular momentum, electromagnetic field and the Poynting vector. A trivector can represent an oriented volume, and so on. An element called a blade may be used to represent a subspace of 



V


{  V}
 and orthogonal projections onto that subspace. Rotations and reflections are represented as elements. Unlike vector algebra, a GA naturally accommodates any number of dimensions and any quadratic form such as in relativity.
 Examples of geometric algebras applied in physics include the spacetime algebra (and the less common algebra of physical space) and the conformal geometric algebra. Geometric calculus, an extension of GA that incorporates differentiation and integration, can be used to formulate other theories such as complex analysis and differential geometry, e.g. by using the Clifford algebra instead of differential forms. Geometric algebra has been advocated, most notably by David Hestenes[2] and Chris Doran,[3] as the preferred mathematical framework for physics. Proponents claim that it provides compact and intuitive descriptions in many areas including classical and quantum mechanics, electromagnetic theory and relativity.[4] GA has also found use as a computational tool in computer graphics[5] and robotics.
 The geometric product was first briefly mentioned by Hermann Grassmann,[6] who was chiefly interested in developing the closely related exterior algebra. In 1878, William Kingdon Clifford greatly expanded on Grassmann's work to form what are now usually called Clifford algebras in his honor (although Clifford himself chose to call them ""geometric algebras""). For several decades, geometric algebras went somewhat ignored, greatly eclipsed by the vector calculus then newly developed to describe electromagnetism. The term ""geometric algebra"" was repopularized in the 1960s by Hestenes, who advocated its importance to relativistic physics.[7]"
Geometric_analysis,Mathematics,3,"Geometric analysis is a mathematical discipline where tools from differential equations, especially elliptic partial differential equations are used to establish new results in differential geometry and differential topology. The use of linear elliptic PDEs dates at least as far back as Hodge theory. More recently, it refers largely
to the use of nonlinear partial differential equations to study geometric and topological properties of spaces, such as submanifolds of Euclidean space, Riemannian manifolds, and symplectic manifolds. This approach dates back to the work by Tibor Radó and Jesse Douglas on minimal surfaces, John Forbes Nash Jr. on isometric embeddings of Riemannian manifolds into Euclidean space, work by Louis Nirenberg on the Minkowski problem and the Weyl problem, and work by Aleksandr Danilovich Aleksandrov and Aleksei Pogorelov on convex hypersurfaces. In the 1980s fundamental contributions by Karen Uhlenbeck,[1] Clifford Taubes, Shing-Tung Yau, Richard Schoen, and Richard Hamilton launched a particularly exciting and productive era of geometric analysis that continues to this day. A celebrated achievement was the solution to the Poincaré conjecture by Grigori Perelman, completing a program initiated and largely carried out by Richard Hamilton.
"
Geometric_calculus,Mathematics,3,"In mathematics, geometric calculus extends the geometric algebra to include differentiation and integration.  The formalism is powerful and can be shown to encompass other mathematical theories including differential geometry and differential forms.[1]"
Geometric_combinatorics,Mathematics,3,"Geometric combinatorics is a branch of mathematics in general and combinatorics in particular.  It includes a number of subareas such as polyhedral combinatorics (the study of faces of convex polyhedra), convex geometry (the study of convex sets, in particular combinatorics of their intersections), and discrete geometry, which in turn has many applications to computational geometry. Other important areas include metric geometry of polyhedra, such as the Cauchy theorem on rigidity of convex polytopes.  The study of regular polytopes, Archimedean solids, and kissing numbers is also a part of geometric combinatorics.  Special polytopes are also considered, such as the permutohedron, associahedron and Birkhoff polytope. 
"
Geometric_function_theory,Mathematics,3,"Geometric function theory is the study of geometric properties of analytic functions.  A fundamental result in the theory is the Riemann mapping theorem.
"
Geometric_invariant_theory,Mathematics,3,"In mathematics, geometric invariant theory (or GIT) is a method for constructing quotients by group actions in algebraic geometry, used to construct moduli spaces. It was developed by David Mumford in  1965, using ideas from the paper (Hilbert 1893)  in  classical invariant theory.
 Geometric invariant theory studies an action of a group G on an algebraic variety (or scheme) X and provides techniques for forming the 'quotient' of X by G as a scheme with reasonable properties. One motivation was to construct moduli spaces in algebraic geometry as quotients of schemes parametrizing marked objects. In the 1970s and 1980s the theory developed interactions with symplectic geometry and equivariant topology, and was used to construct moduli spaces of objects in differential geometry, such as instantons and monopoles.
"
Geometric_graph_theory,Mathematics,3,"Geometric graph theory in the broader sense is a large and amorphous subfield of graph theory, concerned with graphs defined by geometric means. In a stricter sense, geometric graph theory studies combinatorial and
geometric properties of geometric graphs, meaning graphs drawn in the Euclidean plane with possibly intersecting straight-line edges, and topological graphs, where the edges are allowed to be arbitrary continuous curves connecting the vertices, thus it is ""the theory of geometric and topological graphs"" (Pach 2013).
"
Geometric_group_theory,Mathematics,3,"Geometric group theory is an area in mathematics devoted to the study of finitely generated groups via exploring the connections between algebraic properties of such groups and topological and geometric properties of spaces on which these groups act (that is, when the groups in question are realized as geometric symmetries or continuous transformations of some spaces).
 Another important idea in geometric group theory is to consider finitely generated groups themselves as geometric objects. This is usually done by studying the Cayley graphs of groups, which, in addition to the graph structure, are endowed with the structure of a metric space, given by the so-called word metric.
 Geometric group theory, as a distinct area, is relatively new, and became a clearly identifiable branch of mathematics in the late 1980s and early 1990s. Geometric group theory closely interacts with low-dimensional topology, hyperbolic geometry, algebraic topology, computational group theory and differential geometry. There are also substantial connections with complexity theory, mathematical logic, the study of Lie groups and their discrete subgroups, dynamical systems, probability theory, K-theory, and other areas of mathematics.
 In the introduction to his book Topics in Geometric Group Theory, Pierre de la Harpe wrote: ""One of my personal beliefs is that fascination with symmetries and groups is one way of coping with frustrations of life's limitations: we like to recognize symmetries which allow us to recognize more than what we can see. In this sense the study of geometric group theory is a part of culture, and reminds me of several things that Georges de Rham practiced on many occasions, such as teaching mathematics, reciting Mallarmé, or greeting a friend"".[1]:3"
Geometric_measure_theory,Mathematics,3,"In mathematics, geometric measure theory (GMT) is the study of geometric properties of sets (typically in Euclidean space) through measure theory. It allows mathematicians to extend tools from differential geometry to a much larger class of surfaces that are not necessarily smooth.
"
Geometric_topology,Mathematics,3,"
 In mathematics, geometric topology is the study of manifolds and maps between them, particularly embeddings of one manifold into another.
"
Geometry,Mathematics,3,"
 Geometry (from the Ancient Greek: γεωμετρία; geo- ""earth"", -metron ""measurement"") is, with arithmetic, one of the oldest branches of mathematics. It is concerned with properties of space that are related with distance, shape, size, and relative position of figures.[1] A mathematician who works in the field of geometry is called a geometer.
 Until the 19th century, geometry was almost exclusively devoted to Euclidean geometry,[a] which includes the notions of point, line, plane, distance, angle, surface, and curve, as fundamental concepts.[2] During the 19th century several discoveries enlarged dramatically the scope of geometry. One of the oldest such discoveries is Gauss' Theorema Egregium (remarkable theorem) that asserts roughly that the Gaussian curvature of a surface is independent from any specific embedding in an Euclidean space. This implies that surfaces can be studied intrinsically, that is as stand alone spaces, and has been expanded into the theory of manifolds and Riemannian geometry.
 Later in the 19th century, it appeared that geometries without the parallel postulate (non-Euclidean geometries) can be developed without introducing any contradiction. The geometry that underlies general relativity is a famous application of non-Euclidean geometry. 
 Since then, the scope of geometry has been greatly expanded, and the field has been split in many subfields that depend on the underlying methods—differential geometry, algebraic geometry, computational geometry, algebraic topology, discrete geometry (also known as combinatorial geometry), etc.—or on the properties of Euclidean spaces that are disregarded—projective geometry that consider only alignment of points but not distance and parallelism, affine geometry that omits the concept of angle and distance, finite geometry that that omits continuity, etc.
 Often developed with the aim to model the physical world, geometry has applications to almost all sciences, and also to art, architecture, and other activities that are related to graphics.[3] Geometry has also applications to areas of mathematics that are apparently unrelated. For example, methods of algebraic geometry are fundamental for Wiles's proof of Fermat's Last Theorem, a problem that was stated in terms of elementary arithmetic, and remainded unsolved for several centuries.
"
Geometry_of_numbers,Mathematics,3,"Geometry of numbers is the part of number theory which uses geometry for the study of algebraic numbers. Typically, a ring of algebraic integers is viewed as a lattice in 





R


n


,


{  \mathbb {R} ^{n},}
 and the study of these lattices provides fundamental information on algebraic numbers.[1] The geometry of numbers was initiated by Hermann Minkowski (1910).
 The geometry of numbers has a close relationship with other fields of mathematics, especially functional analysis and Diophantine approximation, the problem of finding rational numbers  that  approximate an irrational quantity.[2]"
Global_analysis,Mathematics,3,"In mathematics, global analysis, also called analysis on manifolds, is the study of the global and topological properties of differential equations on manifolds and vector bundles.[1][2]  Global analysis uses techniques in infinite-dimensional manifold theory and topological spaces of mappings to classify behaviors of differential equations, particularly nonlinear differential equations.[3] These spaces can include singularities and hence catastrophe theory is a part of global analysis. Optimization problems, such as finding geodesics on Riemannian manifolds, can be solved using differential equations so that the calculus of variations overlaps with global analysis. Global analysis finds application in physics in the study of dynamical systems[4] and topological quantum field theory.
"
Arithmetic_dynamics,Mathematics,3,"Arithmetic dynamics[1] is a field that amalgamates two areas of mathematics, dynamical systems and number theory. Classically, discrete dynamics refers to the study of the iteration of self-maps of the complex plane or real line. Arithmetic dynamics is the study of the number-theoretic properties of integer, rational, p-adic, and/or algebraic points under repeated application of a polynomial or rational function. A fundamental goal is to describe arithmetic properties in terms of underlying geometric structures.
 Global arithmetic dynamics is the study of analogues of classical diophantine geometry  in the setting of discrete dynamical systems, while local arithmetic dynamics, also called p-adic or nonarchimedean dynamics, is an analogue of classical dynamics in which one replaces the complex numbers C by a p-adic field such as Qp or Cp and studies chaotic behavior and the Fatou and Julia sets.
 The following table describes a rough correspondence between Diophantine equations, especially abelian varieties, and dynamical systems:
"
Graph_theory,Mathematics,3,"In mathematics, graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines). A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically; see Graph (discrete mathematics) for more detailed definitions and for other variations in the types of graph that are commonly considered. Graphs are one of the prime objects of study in discrete mathematics.
 Refer to the glossary of graph theory for basic definitions in graph theory.
"
Character_theory,Mathematics,3,"In mathematics, more specifically in group theory, the character of a group representation is a function on the group that associates to each group element the trace of the corresponding matrix. The character carries the essential information about the representation in a more condensed form. Georg Frobenius initially developed representation theory of finite groups entirely based on the characters, and without any explicit matrix realization of representations themselves. This is possible because a complex representation of a finite group is determined (up to isomorphism) by its character. The situation with representations over a field of positive characteristic, so-called ""modular representations"", is more delicate, but Richard Brauer developed a powerful theory of characters in this case as well. Many deep theorems on the structure of finite groups use characters of modular representations.
"
Group_representation_theory,Mathematics,3,"In the mathematical field of representation theory, group representations describe abstract groups in terms of bijective linear transformations (i.e. automorphisms) of vector spaces; in particular, they can be used to represent group elements as invertible matrices so that the group operation can be represented by matrix multiplication. Representations of groups are important because they allow many group-theoretic problems to be reduced to problems in linear algebra, which is well understood. They are also important in physics because, for example, they describe how the symmetry group of a physical system affects the solutions of equations describing that system.
 The term representation of a group is also used in a more general sense to mean any ""description"" of a group as a group of transformations of some mathematical object. More formally, a ""representation"" means a homomorphism from the group to the automorphism group of an object. If the object is a vector space we have a linear representation. Some people use realization for the general notion and reserve the term representation for the special case of linear representations. The bulk of this article describes linear representation theory; see the last section for generalizations.
"
Group_theory,Mathematics,3,"In mathematics and abstract algebra, group theory studies the algebraic structures known as groups.  
The concept of a group is central to abstract algebra: other well-known algebraic structures, such as rings, fields, and vector spaces, can all be seen as groups endowed with additional operations and axioms. Groups recur throughout mathematics, and the methods of group theory have influenced many parts of algebra. Linear algebraic groups and Lie groups are two branches of group theory that have experienced advances and have become subject areas in their own right.
 Various physical systems, such as crystals and the hydrogen atom, may be modelled by symmetry groups. Thus group theory and the closely related representation theory have many important applications in physics, chemistry, and materials science. Group theory is also central to public key cryptography.
 The early history of group theory dates from the 19th century. One of the most important mathematical achievements of the 20th century[1] was the collaborative effort, taking up more than 10,000 journal pages and mostly published between 1960 and 1980, that culminated in a complete classification of finite simple groups.
"
Gyrotrigonometry,Mathematics,3,"A gyrovector space is a mathematical concept proposed by Abraham A. Ungar for studying hyperbolic geometry in analogy to the way vector spaces are used in Euclidean geometry.[1]  Ungar introduced the concept of  gyrovectors that have addition based on gyrogroups instead of vectors which have  addition based on groups.  Ungar developed his concept as a tool for the formulation of special relativity as an alternative to  the use of Lorentz transformations to represent compositions of velocities (also called boosts - ""boosts"" are aspects of relative velocities, and should not be conflated with ""translations""). This is achieved by introducing ""gyro operators""; two 3d velocity vectors are used to construct an operator, which acts on another 3d velocity.
"
Hard_analysis,Mathematics,3,"Mathematical analysis is the branch of mathematics dealing with limits
and related theories, such as differentiation, integration, measure, infinite series, and analytic functions.[1][2] These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis.
Analysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects  (a metric space).
"
Harmonic_analysis,Mathematics,3,"
 Harmonic analysis is a branch of mathematics concerned with the representation of functions or signals as the superposition of basic waves, and the study of and generalization of the notions of Fourier series and Fourier transforms (i.e. an extended form of Fourier analysis). In the past two centuries, it has become a vast subject with applications in areas as diverse as number theory, representation theory, signal processing, quantum mechanics, tidal analysis and neuroscience.
 The term ""harmonics"" originated as the Ancient Greek word harmonikos, meaning ""skilled in music"".[1] In physical eigenvalue problems, it began to mean waves whose frequencies are integer multiples of one another, as are the frequencies of the harmonics of music notes, but the term has been generalized beyond its original meaning.
 The classical Fourier transform on Rn is still an area of ongoing research, particularly concerning Fourier transformation on more general objects such as tempered distributions. For instance, if we impose some requirements on a distribution f, we can attempt to translate these requirements in terms of the Fourier transform of f. The Paley–Wiener theorem is an example of this. The Paley–Wiener theorem immediately implies that if f is a nonzero distribution of compact support (these include functions of compact support), then its Fourier transform is never compactly supported. This is a very elementary form of an uncertainty principle in a harmonic-analysis setting.
 Fourier series can be conveniently studied in the context of Hilbert spaces, which provides a connection between harmonic analysis and functional analysis.
"
Arithmetic,Mathematics,3,"Arithmetic (from the Greek ἀριθμός arithmos, 'number' and τική [τέχνη], tiké [téchne], 'art') is a branch of mathematics that consists of the study of numbers, especially the properties of the traditional operations on them—addition, subtraction, multiplication, division, exponentiation and extraction of roots.[1][2][3] Arithmetic is an elementary part of number theory, and number theory is considered to be one of the top-level divisions of modern mathematics, along with algebra, geometry, and analysis. The terms arithmetic and higher arithmetic were used until the beginning of the 20th century as synonyms for number theory, and are sometimes still used to refer to a wider part of number theory.[4]"
Higher_category_theory,Mathematics,3,"In mathematics, higher category theory is the part of category theory at a higher order, which means that some equalities are replaced by explicit arrows in order to be able to explicitly study the structure behind those equalities. Higher category theory is often applied in algebraic topology (especially in homotopy theory), where one studies algebraic invariants of spaces, such as their fundamental weak ∞-groupoid.
"
Higher-dimensional_algebra,Mathematics,3,"In mathematics, especially (higher) category theory, higher-dimensional algebra is the study of categorified structures. It has applications in nonabelian algebraic topology, and generalizes abstract algebra.
"
Hodge_theory,Mathematics,3,"In mathematics, Hodge theory, named after W. V. D. Hodge, is a method for studying the cohomology groups of a smooth manifold M using partial differential equations.  The key observation is that, given a Riemannian metric on M, every cohomology class has a canonical representative, a differential form which vanishes under the Laplacian operator of the metric.  Such forms are called harmonic.
 The theory was developed by Hodge in the 1930s to study algebraic geometry, and it built on the work of Georges de Rham on de Rham cohomology.  It has major applications in two settings: Riemannian manifolds and Kähler manifolds.  Hodge's primary motivation, the study of complex projective varieties, is encompassed by the latter case.  Hodge theory has become an important tool in algebraic geometry, particularly through its connection to the study of algebraic cycles.
 While Hodge theory is intrinsically dependent upon the real and complex numbers, it can be applied to questions in number theory.  In arithmetic situations, the tools of p-adic Hodge theory have given alternative proofs of, or analogous results to, classical Hodge theory.
"
Holomorphic_functional_calculus,Mathematics,3,"In mathematics, holomorphic functional calculus is functional calculus with holomorphic functions. That is to say, given a holomorphic function f of a complex argument z and an operator T, the aim is to construct an operator, f(T), which naturally extends the function f from complex argument to operator argument. More precisely, the functional calculus defines a continuous algebra homomorphism from the holomorphic functions on a neighbourhood of the spectrum of T to the bounded operators. 
 This article will discuss the case where T is a bounded linear operator on some Banach space. In particular, T can be a square matrix with complex entries, a case which will be used to illustrate functional calculus and provide some heuristic insights for the assumptions involved in the general construction.
"
Homological_algebra,Mathematics,3,"Homological algebra is the branch of mathematics that studies homology in a general algebraic setting. It is a relatively young discipline, whose origins can be traced to investigations in combinatorial topology (a precursor to algebraic topology) and abstract algebra (theory of modules and syzygies) at the end of the 19th century, chiefly by Henri Poincaré and David Hilbert. 
 The development of homological algebra was closely intertwined with the emergence of category theory. By and large, homological algebra is the study of homological functors and the intricate algebraic structures that they entail. One quite useful and ubiquitous concept in mathematics is that of chain complexes, which can be studied through both their homology and cohomology. Homological algebra affords the means to extract information contained in these complexes and present it in the form of homological invariants of rings, modules, topological spaces, and other 'tangible' mathematical objects. A powerful tool for doing this is provided by spectral sequences.
 From its very origins, homological algebra has played an enormous role in algebraic topology. Its influence has gradually expanded and presently includes commutative algebra, algebraic geometry, algebraic number theory, representation theory, mathematical physics, operator algebras, complex analysis, and the theory of partial differential equations. K-theory is an independent discipline which draws upon methods of homological algebra, as does the noncommutative geometry of Alain Connes.
"
Homology_theory,Mathematics,3,"In mathematics, homology[1] is a general way of associating a sequence of algebraic objects, such as abelian groups or modules, to other mathematical objects such as topological spaces.  Homology groups were originally defined in algebraic topology.  Similar constructions are available in a wide variety of other contexts, such as abstract algebra, groups, Lie algebras, Galois theory, and algebraic geometry.
 The original motivation for defining homology groups was the observation that two shapes can be distinguished by examining their holes.  For instance, a circle is not a disk because the circle has a hole through it while the disk is solid, and the ordinary sphere is not a circle because the sphere encloses a two-dimensional hole while the circle encloses a one-dimensional hole.  However, because a hole is ""not there"", it is not immediately obvious how to define a hole or how to distinguish different kinds of holes.  Homology was originally a rigorous mathematical method for defining and categorizing holes in a manifold.  Loosely speaking, a cycle is a closed submanifold, a boundary is a cycle which is also the boundary of a submanifold, and a homology class (which represents a hole) is an equivalence class of cycles modulo boundaries. A homology class is thus represented by a cycle which is not the boundary of any submanifold: the cycle represents a hole, namely a hypothetical manifold whose boundary would be that cycle, but which is ""not there"".
 There are many different homology theories.  A particular type of mathematical object, such as a topological space or a group, may have one or more associated homology theories.  When the underlying object has a geometric interpretation as topological spaces do, the nth homology group represents behavior in dimension n.  Most homology groups or modules may be formulated as derived functors on appropriate abelian categories, measuring the failure of a functor to be exact.  From this abstract perspective, homology groups are determined by objects of a derived category.
"
Homotopy_theory,Mathematics,3,"In mathematics, homotopy theory is a systematic study of situations in which maps come with homotopies between them. It originated as a topic in algebraic topology but nowadays it is studied as an independent discipline. Besides algebraic topology, the theory has also been in used in other areas of mathematics such as algebraic geometry (e.g., A¹ homotopy theory) and category theory (specifically the study of higher categories).
"
Hyperbolic_geometry,Mathematics,3,"In mathematics, hyperbolic geometry (also called Lobachevskian geometry or Bolyai–Lobachevskian geometry) is a non-Euclidean geometry. The parallel postulate of Euclidean geometry is replaced with:
 Hyperbolic plane geometry is also the geometry of saddle surfaces and pseudospherical surfaces, surfaces with a constant negative Gaussian curvature.
 A modern use of hyperbolic geometry is in the theory of special relativity, particularly Minkowski spacetime and gyrovector space.
 When geometers first realised they were working with something other than the standard Euclidean geometry, they described their geometry under many different names; Felix Klein finally gave the subject the name hyperbolic geometry to include it in the now rarely used sequence elliptic geometry (spherical geometry), parabolic geometry (Euclidean geometry), and hyperbolic geometry.
In the former Soviet Union, it is commonly called Lobachevskian geometry, named after one of its discoverers, the Russian geometer Nikolai Lobachevsky.
 This page is mainly about the 2-dimensional (planar) hyperbolic geometry and the differences and similarities between Euclidean and hyperbolic geometry.
 Hyperbolic geometry can be extended to three and more dimensions; see hyperbolic space for more on the three and higher dimensional cases.
"
Hyperbolic_geometry,Mathematics,3,"In mathematics, hyperbolic geometry (also called Lobachevskian geometry or Bolyai–Lobachevskian geometry) is a non-Euclidean geometry. The parallel postulate of Euclidean geometry is replaced with:
 Hyperbolic plane geometry is also the geometry of saddle surfaces and pseudospherical surfaces, surfaces with a constant negative Gaussian curvature.
 A modern use of hyperbolic geometry is in the theory of special relativity, particularly Minkowski spacetime and gyrovector space.
 When geometers first realised they were working with something other than the standard Euclidean geometry, they described their geometry under many different names; Felix Klein finally gave the subject the name hyperbolic geometry to include it in the now rarely used sequence elliptic geometry (spherical geometry), parabolic geometry (Euclidean geometry), and hyperbolic geometry.
In the former Soviet Union, it is commonly called Lobachevskian geometry, named after one of its discoverers, the Russian geometer Nikolai Lobachevsky.
 This page is mainly about the 2-dimensional (planar) hyperbolic geometry and the differences and similarities between Euclidean and hyperbolic geometry.
 Hyperbolic geometry can be extended to three and more dimensions; see hyperbolic space for more on the three and higher dimensional cases.
"
Hypercomplex_analysis,Mathematics,3,"In mathematics, hypercomplex analysis is the extension of real analysis and complex analysis to the study of functions where the argument is a hypercomplex number. The first instance is functions of a quaternion variable, where the argument is a quaternion.  A second instance involves functions of a motor variable where arguments are split-complex numbers.
 In mathematical physics, there are hypercomplex systems called Clifford algebras. The study of functions with arguments from a Clifford algebra is called Clifford analysis.
 A matrix may be considered a hypercomplex number. For example, study of functions of 2 × 2 real matrices shows that the topology of the space of hypercomplex numbers determines the function theory. Functions such as square root of a matrix, matrix exponential, and logarithm of a matrix are basic examples of hypercomplex analysis.[1] 
The function theory of diagonalizable matrices is particularly transparent since they have eigendecompositions.[2] Suppose 




T
=

∑

i
=
1


N



λ

i



E

i





{  \textstyle T=\sum _{i=1}^{N}\lambda _{i}E_{i}}
 where the Ei are projections. Then for any polynomial 




f
,

f
(
T
)
=

∑

i
=
1


N


f
(

λ

i


)

E

i


.



{  \textstyle f,\quad f(T)=\sum _{i=1}^{N}f(\lambda _{i})E_{i}.}

 Modern terminology is algebra for ""system of hypercomplex numbers"", and the  algebras used in applications are often Banach algebras since Cauchy sequences can be taken to be convergent. Then the function theory is enriched by sequences and series. In this context the extension of holomorphic functions of a complex variable is developed as the holomorphic functional calculus.  Hypercomplex analysis on Banach algebras is called functional analysis.
"
Hyperfunction,Mathematics,3,"In mathematics, hyperfunctions are generalizations of functions, as a 'jump' from one holomorphic function to another at a boundary, and can be thought of informally as distributions of infinite order. Hyperfunctions were introduced by Mikio Sato in 1958 in Japanese, (1959, 1960 in English), building upon earlier work by Laurent Schwartz, Grothendieck and others.
"
Ideal_theory,Mathematics,3,"In mathematics, ideal theory is the theory of ideals in commutative rings; and is the precursor name for the contemporary subject of commutative algebra. The name grew out of the central considerations, such as the Lasker–Noether theorem in algebraic geometry, and the ideal class group in algebraic number theory, of the commutative algebra of the first quarter of the twentieth century. It was used in the influential van der Waerden text on abstract algebra from around 1930.
 The ideal theory in question had been based on elimination theory, but in line with David Hilbert's taste moved away from algorithmic methods. Gröbner basis theory has now reversed the trend, for computer algebra.
 The importance of the idea of a module, more general than an ideal, probably led to the perception that ideal theory was too narrow a description. Valuation theory, too, was an important technical extension, and was used by Helmut Hasse and Oscar Zariski.  Bourbaki used commutative algebra; sometimes local algebra is applied to the theory of local rings. Douglas Northcott's 1953 Cambridge Tract Ideal Theory (reissued 2004 under the same title) was one of the final appearances of the name.
"
Idempotent_analysis,Mathematics,3,"In mathematical analysis, idempotent analysis is the study of idempotent semirings, such as the tropical semiring. The lack of an additive inverse in the semiring is compensated somewhat by the idempotent rule 



A
⊕
A
=
A


{  A\oplus A=A}
.
"
Incidence_geometry,Mathematics,3,"In mathematics, incidence geometry is the study of incidence structures. A geometric structure such as the Euclidean plane is a complicated object that involves concepts such as length, angles, continuity, betweenness, and incidence. An incidence structure is what is obtained when all other concepts are removed and all that remains is the data about which points lie on which lines. Even with this severe limitation, theorems can be proved and interesting facts emerge concerning this structure. Such fundamental results remain valid when additional concepts are added to form a richer geometry. It sometimes happens that authors blur the distinction between a study and the objects of that study, so it is not surprising to find that some authors refer to incidence structures as incidence geometries.[1] Incidence structures arise naturally and have been studied in various areas of mathematics. Consequently, there are different terminologies to describe these objects. In graph theory they are called hypergraphs, and in combinatorial design theory they are called block designs. Besides the difference in terminology, each area approaches the subject differently and is interested in questions about these objects relevant to that discipline. Using geometric language, as is done in incidence geometry, shapes the topics and examples that are normally presented. It is, however, possible to translate the results from one discipline into the terminology of another, but this often leads to awkward and convoluted statements that do not appear to be natural outgrowths of the topics. In the examples selected for this article we use only those with a natural geometric flavor.
 A special case that has generated much interest deals with finite sets of points in the Euclidean plane and what can be said about the number and types of (straight) lines they determine. Some results of this situation can extend to more general settings since only incidence properties are considered.
"
Inconsistent_mathematics,Mathematics,3,"Paraconsistent mathematics, sometimes called inconsistent mathematics, represents an attempt to develop the classical infrastructure of mathematics (e.g. analysis) based on a foundation of paraconsistent logic instead of classical logic. A number of reformulations of analysis can be developed, for example functions which both do and do not have a given value simultaneously.
 Chris Mortensen claims (see references):
"
Infinitary_combinatorics,Mathematics,3,"In mathematics, infinitary combinatorics, or combinatorial set theory, is an extension of ideas in combinatorics to infinite sets.
Some of the things studied include continuous graphs and trees, extensions of Ramsey's theorem, and Martin's axiom.
Recent developments concern combinatorics of the continuum[1] and combinatorics on successors of singular cardinals.[2]"
Infinitesimal_calculus,Mathematics,3,"
 Calculus, originally called infinitesimal calculus or ""the calculus of infinitesimals"", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations.
 It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while  integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Today, calculus has widespread uses in science, engineering, and economics.[4] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly  devoted to the study of functions and limits. The word calculus (plural calculi) is a  Latin word, meaning originally ""small pebble"" (this meaning is kept in medicine – see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.
"
Infinitesimal_calculus,Mathematics,3,"
 Calculus, originally called infinitesimal calculus or ""the calculus of infinitesimals"", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations.
 It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while  integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Today, calculus has widespread uses in science, engineering, and economics.[4] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly  devoted to the study of functions and limits. The word calculus (plural calculi) is a  Latin word, meaning originally ""small pebble"" (this meaning is kept in medicine – see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.
"
Information_geometry,Mathematics,3,"Information geometry is an interdisciplinary field that applies the techniques of differential geometry to study probability theory and statistics. It studies statistical manifolds, which are Riemannian manifolds whose points correspond to probability distributions.
"
Integral_calculus,Mathematics,3,"In mathematics, an integral assigns numbers to functions in a way that can describe displacement, area, volume, and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus; its inverse operation, differentiation, is the other. Given a function f of a real variable x and an interval [a, b] of the real line, the definite integral of f from a to b can be interpreted informally as the signed area of the region in the xy-plane that is bounded by the graph of f, the x-axis and the vertical lines x = a and x = b. It is denoted
 The operation of integration, up to an additive constant, is the inverse of the operation of differentiation. For this reason, the term integral may also refer to the related notion of the antiderivative, called an indefinite integral, a function F whose derivative is the given function f. In this case, it is written:
 The integrals discussed in this article are those termed definite integrals. It is the fundamental theorem of calculus that connects differentiation with the definite integral: if f is a continuous real-valued function defined on a closed interval [a, b], then once an antiderivative F of f is known, the definite integral of f over that interval is given by
 The principles of integration were formulated independently by Isaac Newton and Gottfried Wilhelm Leibniz in the late 17th century, who thought of the integral as an infinite sum of rectangles of infinitesimal width. Bernhard Riemann later gave a rigorous mathematical definition of integrals, which is based on a limiting procedure that approximates the area of a curvilinear region by breaking the region into thin vertical slabs. Beginning in the 19th century, more sophisticated notions of integrals began to appear, where the type of the function as well as the domain over which the integration is performed has been generalized. A line integral is defined for functions of two or more variables, and the interval of integration [a, b] is replaced by a curve connecting the two endpoints. In a surface integral, the curve is replaced by a piece of a surface in three-dimensional space.
"
Integral_geometry,Mathematics,3,"In mathematics, integral geometry is the theory of measures on a geometrical space invariant under the symmetry group of that space.  In more recent times, the meaning has been broadened to include a view of invariant (or equivariant) transformations from the space of functions on one geometrical space to the space of functions on another geometrical space.  Such transformations often take the form of integral transforms such as the Radon transform and its generalizations.
"
Intersection_theory,Mathematics,3,"In mathematics, intersection theory is a branch of algebraic geometry, where subvarieties are intersected on an algebraic variety, and of algebraic topology, where intersections are computed within the cohomology ring. The theory for varieties is older, with roots in Bézout's theorem on curves and elimination theory. On the other hand, the topological theory more quickly reached a definitive form.
"
Intuitionistic_type_theory,Mathematics,3,"Intuitionistic type theory (also known as constructive type theory, or Martin-Löf type theory) is a type theory and an alternative foundation of mathematics.
Intuitionistic type theory was created by Per Martin-Löf, a Swedish mathematician and philosopher, who first published it in 1972.  There are multiple versions of the type theory:  Martin-Löf proposed both intensional and extensional variants of the theory and early impredicative versions, shown to be inconsistent by Girard's paradox, gave way to predicative versions. However, all versions keep the core design of constructive logic using dependent types.
"
Invariant_theory,Mathematics,3,"Invariant theory is a branch of abstract algebra dealing with actions of groups on algebraic varieties, such as vector spaces, from the point of view of their effect on functions. Classically, the theory dealt with the question of explicit description of polynomial functions that do not change, or are invariant, under the transformations from a given linear group. For example, if we consider the action of the special linear group SLn on the space of n by n matrices by left multiplication, then the determinant is an invariant of this action because the determinant of A X equals the determinant of X, when  A is in SLn.
"
Inventory_theory,Mathematics,3,"Material theory (or more formally the mathematical theory of inventory and production) is the sub-specialty within operations research and operations management that is concerned with the design of production/inventory systems to minimize costs: it studies the decisions faced by firms and the military in connection with manufacturing, warehousing, supply chains, spare part allocation and so on and provides the mathematical foundation for logistics. The inventory control problem is the problem faced by a firm that must decide how much to order in each time period to meet demand for its products. The problem can be modeled using mathematical techniques of optimal control, dynamic programming and network optimization.  The study of such models is part of inventory theory.
"
Inversive_geometry,Mathematics,3,"In geometry, inversive geometry is the study of inversion, a transformation of the Euclidean plane that maps circles or lines to other circles or lines and that preserves the angles between crossing curves. Many difficult problems in geometry become much more tractable when an inversion is applied.
 The concept of inversion can be generalized to higher-dimensional spaces.
"
Inversive_geometry,Mathematics,3,"In geometry, inversive geometry is the study of inversion, a transformation of the Euclidean plane that maps circles or lines to other circles or lines and that preserves the angles between crossing curves. Many difficult problems in geometry become much more tractable when an inversion is applied.
 The concept of inversion can be generalized to higher-dimensional spaces.
"
Inversive_ring_geometry,Mathematics,3,"In mathematics, the projective line over a ring is an extension of the concept of projective line over a field. Given a ring A with 1, the projective line P(A) over A consists of points identified by projective coordinates.  Let U be the group of units of A; pairs (a, b) and (c, d) from A × A are related when there is a u in U such that ua = c and ub = d. This relation is an equivalence relation. A typical equivalence class is written U[a, b].
 P(A) = { U[a, b] : aA + bA = A }, that is, U[a, b] is in the projective line if the ideal generated by a and b is all of A.
 The projective line P(A) is equipped with a group of homographies. The homographies are expressed through use of the matrix ring over A and its group of units V as follows: If c is in Z(U), the center of U, then the group action of matrix 





(



c


0




0


c



)




{  {\begin{pmatrix}c&0\\0&c\end{pmatrix}}}
 on P(A) is the same as the action of the identity matrix. Such matrices represent a normal subgroup N of V. The homographies of P(A) correspond to elements of the quotient group V / N .
 P(A) is considered an extension of the ring A since it contains a copy of A due to the embedding 
E : a → U[a, 1].  The multiplicative inverse mapping u → 1/u, ordinarily restricted to the group of units U of A, is expressed by a homography on P(A):
 Furthermore, for u,v ∈ U, the mapping a → uav can be extended to a homography:
 Since u is arbitrary, it may be substituted for u−1.
Homographies on P(A) are called linear-fractional transformations since
"
It%C3%B4_calculus,Mathematics,3,"Itô calculus, named after Kiyoshi Itô, extends the methods of calculus to stochastic processes such as Brownian motion (see Wiener process). It has important applications in mathematical finance and stochastic differential equations.
 The central concept is the Itô stochastic integral, a stochastic generalization of the Riemann–Stieltjes integral in analysis. The integrands and the integrators are now stochastic processes:
 where H is a locally square-integrable process adapted to the filtration generated by X (Revuz & Yor 1999, Chapter IV), which is a Brownian motion or, more generally, a semimartingale. The result of the integration is then another stochastic process. Concretely, the integral from 0 to any particular t is a random variable, defined as a limit of a certain sequence of random variables. The paths of Brownian motion fail to satisfy the requirements to be able to apply the standard techniques of calculus. So with the integrand a stochastic process, the Itô stochastic integral amounts to an integral with respect to a function which is not differentiable at any point and has infinite variation over every time interval. 
The main insight is that the integral can be defined as long as the integrand H is adapted, which loosely speaking means that its value at time t can only depend on information available up until this time. Roughly speaking, one chooses a sequence of partitions of the interval from 0 to t and construct Riemann sums. Every time we are computing a Riemann sum, we are using a particular instantiation of the integrator. It is crucial which point in each of the small intervals is used to compute the value of the function. The limit then is taken in probability as the mesh of the partition is going to zero. Numerous technical details have to be taken care of to show that this limit exists and is independent of the particular sequence of partitions. Typically, the left end of the interval is used.
 Important results of Itô calculus include the integration by parts formula and Itô's lemma, which is a change of variables formula. These differ from the formulas of standard calculus, due to quadratic variation terms.
 In mathematical finance, the described evaluation strategy of the integral is conceptualized as that we are first deciding what to do, then observing the change in the prices. The integrand is how much stock we hold, the integrator represents the movement of the prices, and the integral is how much money we have in total  including what our stock is worth, at any given moment. The prices of stocks and other traded financial assets can be modeled by stochastic processes such as Brownian motion or, more often, geometric Brownian motion (see Black–Scholes). Then, the Itô stochastic integral represents the payoff of a continuous-time trading strategy consisting of holding an amount Ht of the stock at time t. In this situation, the condition that H is adapted corresponds to the necessary restriction that the trading strategy can only make use of the available information at any time. This prevents the possibility of unlimited gains through high-frequency trading: buying the stock just before each uptick in the market and selling before each downtick. Similarly, the condition that H is adapted implies that the stochastic integral will not diverge when calculated as a limit of Riemann sums (Revuz & Yor 1999, Chapter IV).
"
Iwasawa_theory,Mathematics,3,"In number theory, Iwasawa theory is the study of objects of arithmetic interest over infinite towers of number fields. It began as a Galois module theory of ideal class groups, initiated by Kenkichi Iwasawa (1959) (岩澤 健吉), as part of the theory of cyclotomic fields. In the early 1970s, Barry Mazur considered generalizations of Iwasawa theory to abelian varieties. More recently (early 1990s), Ralph Greenberg has proposed an Iwasawa theory for motives.
"
Job_shop_scheduling,Mathematics,3,"Job shop scheduling or the job-shop problem (JSP) is an optimization problem in computer science and operations research in which jobs are assigned to resources at particular times.  The most basic version is as follows: We are given n jobs J1, J2, ..., Jn of varying processing times, which need to be scheduled on m machines with varying processing power, while trying to minimize the makespan. The makespan is the total length of the schedule (that is, when all the jobs have finished processing). 
 The standard version of the problem is where you have n jobs J1, J2, ..., Jn. Within each job there is a set of operations O1, O2, ..., On which need to be processed in a specific order (known as Precedence constraints). Each operation has a specific machine that it needs to be processed on and only one operation in a job can be processed at a given time. A common relaxation is the flexible job shop where each operation can be processed on any machine of a given set (the machines in the set are identical). 
 This problem is one of the best known combinatorial optimization problems, and was the first problem for which competitive analysis was presented, by Graham in 1966.[1]
Best problem instances for basic model with makespan objective are due to Taillard.[2] The name originally came from the scheduling of jobs in a job shop, but the theme has wide applications beyond that type of instance.
 A systematic notation was introduced to present the different variants of this scheduling problem and related problems, called the three-field notation.
"
K-theory,Mathematics,3,"In mathematics, K-theory is, roughly speaking, the study of a ring generated by vector bundles over a topological space or scheme. In algebraic topology, it is a cohomology theory known as topological K-theory. In algebra and algebraic geometry, it is referred to as algebraic K-theory. It is also a fundamental tool in the field of operator algebras. It can be seen as the study of certain kinds of invariants of large matrices.[1] K-theory involves the construction of families of K-functors that map from topological spaces or schemes to associated rings; these rings reflect some aspects of the structure of the original spaces or schemes. As with functors to groups in algebraic topology, the reason for this functorial mapping is that it is easier to compute some topological properties from the mapped rings than from the original spaces or schemes. Examples of results gleaned from the K-theory approach include the Grothendieck–Riemann–Roch theorem, Bott periodicity, the Atiyah–Singer index theorem, and the Adams operations.
 In high energy physics, K-theory and in particular twisted K-theory have appeared in Type II string theory where it has been conjectured that they classify D-branes, Ramond–Ramond field strengths and also certain spinors on generalized complex manifolds. In condensed matter physics K-theory has been used to classify topological insulators, superconductors and stable Fermi surfaces. For more details, see K-theory (physics).
"
K-homology,Mathematics,3,"In mathematics, K-homology is a homology theory on the category of locally compact Hausdorff spaces. It classifies the elliptic pseudo-differential operators acting on the vector bundles over a space. In terms of 




C

∗




{  C^{*}}
-algebras, it classifies the Fredholm modules over an algebra. 
 An operator homotopy between two Fredholm modules  



(


H


,

F

0


,
Γ
)


{  ({\mathcal {H}},F_{0},\Gamma )}
 and  



(


H


,

F

1


,
Γ
)


{  ({\mathcal {H}},F_{1},\Gamma )}
 is a norm continuous path of Fredholm modules,  



t
↦
(


H


,

F

t


,
Γ
)


{  t\mapsto ({\mathcal {H}},F_{t},\Gamma )}
,  



t
∈
[
0
,
1
]
.


{  t\in [0,1].}
 Two Fredholm modules are then equivalent if they are related by unitary transformations or operator homotopies. The 




K

0


(
A
)


{  K^{0}(A)}
 group is the abelian group of equivalence classes of even Fredholm modules over A. The 




K

1


(
A
)


{  K^{1}(A)}
 group is the abelian group of equivalence classes of odd Fredholm modules over A. Addition is given by direct summation of Fredholm modules, and the inverse of  



(


H


,
F
,
Γ
)


{  ({\mathcal {H}},F,\Gamma )}
 is  



(


H


,
−
F
,
−
Γ
)
.


{  ({\mathcal {H}},-F,-\Gamma ).}

"
K%C3%A4hler_manifold,Mathematics,3,"
 In mathematics and especially differential geometry, a Kähler manifold is a manifold with three mutually compatible structures: a complex structure, a Riemannian structure, and a symplectic structure. The concept was first studied by Jan Arnoldus Schouten and David van Dantzig in 1930, and then introduced by Erich Kähler in 1933. The terminology has been fixed by André Weil.
 Every smooth complex projective variety is a Kähler manifold. Hodge theory is a central part of algebraic geometry, proved using Kähler metrics.
"
KK-theory,Mathematics,3,"In mathematics, KK-theory is a common generalization both of K-homology and K-theory as an additive bivariant functor on separable C*-algebras.  This notion was introduced by the Russian mathematician Gennadi Kasparov[1] in 1980.
 It was influenced by Atiyah's concept of Fredholm modules for the Atiyah–Singer index theorem, and the classification of extensions of C*-algebras by Lawrence G. Brown, Ronald G. Douglas, and Peter Arthur Fillmore in 1977.[2]  In turn, it has had great success in operator algebraic formalism toward the index theory and the classification of nuclear C*-algebras, as it was the key to the solutions of many problems in operator K-theory, such as, for instance, the mere calculation of K-groups.  Furthermore, it was essential in the development of the Baum–Connes conjecture and plays a crucial role in noncommutative topology.
 KK-theory was followed by a series of similar bifunctor constructions such as the E-theory and the bivariant periodic cyclic theory, most of them having more category-theoretic flavors, or concerning another class of algebras rather than that of the separable C*-algebras, or incorporating group actions.
"
Klein_geometry,Mathematics,3,"In mathematics, a Klein geometry is a type of geometry motivated by Felix Klein in his influential Erlangen program. More specifically, it is a homogeneous space X together with a transitive action on X by a Lie group G, which acts as the symmetry group of the geometry.
 For background and motivation see the article on the Erlangen program.
"
Knot_theory,Mathematics,3,"In topology, knot theory is the study of mathematical knots. While inspired by knots which appear in daily life, such as those in shoelaces and rope, a mathematical knot differs in that the ends are joined together so that it cannot be undone, the simplest knot being a ring (or ""unknot""). In mathematical language, a knot is an embedding of a circle in 3-dimensional Euclidean space, 





R


3




{  \mathbb {R} ^{3}}
 (in topology, a circle isn't bound to the classical geometric concept, but to all of its homeomorphisms). Two mathematical knots are equivalent if one can be transformed into the other via a deformation of 





R


3




{  \mathbb {R} ^{3}}
 upon itself (known as an ambient isotopy); these transformations correspond to manipulations of a knotted string that do not involve cutting the string or passing the string through itself.
 Knots can be described in various ways. Given a method of description, however, there may be more than one description that represents the same knot. For example, a common method of describing a knot is a planar diagram called a knot diagram. Any given knot can be drawn in many different ways using a knot diagram. Therefore, a fundamental problem in knot theory is determining when two descriptions represent the same knot.
 A complete algorithmic solution to this problem exists, which has unknown complexity. In practice, knots are often distinguished by using a knot invariant, a ""quantity"" which is the same when computed from different descriptions of a knot. Important invariants include knot polynomials, knot groups, and hyperbolic invariants.
 The original motivation for the founders of knot theory was to create a table of knots and links, which are knots of several components entangled with each other. More than six billion knots and links have been tabulated since the beginnings of knot theory in the 19th century.
 To gain further insight, mathematicians have generalized the knot concept in several ways. Knots can be considered in other three-dimensional spaces and objects other than circles can be used; see knot (mathematics). Higher-dimensional knots are n-dimensional spheres in m-dimensional Euclidean space.
"
Kummer_theory,Mathematics,3,"In abstract algebra and number theory, Kummer theory provides a description of certain types of field extensions involving the adjunction of nth roots of elements of the base field. The theory was originally developed by Ernst Eduard Kummer around the 1840s in his pioneering work on Fermat's last theorem. The main statements do not depend on the nature of the field – apart from its characteristic, which should not divide the integer n – and therefore belong to abstract algebra. The theory of cyclic extensions of the field K when the characteristic of K does divide n is called Artin–Schreier theory.
 Kummer theory is basic, for example, in class field theory and in general in understanding abelian extensions; it says that in the presence of enough roots of unity, cyclic extensions can be understood in terms of extracting roots. The main burden in class field theory is to dispense with extra roots of unity ('descending' back to smaller fields); which is something much more serious.
"
L-theory,Mathematics,3,"In mathematics, algebraic L-theory is the K-theory of quadratic forms; the term was coined by C. T. C. Wall, 
with L being used as the letter after K. Algebraic L-theory, also known as ""Hermitian K-theory"",
is important in surgery theory.[1]"
Large_deviations_theory,Mathematics,3,"
In probability theory, the theory of large deviations concerns the asymptotic behaviour of remote tails of sequences of probability distributions. While some basic ideas of the theory can be traced to Laplace, the formalization started with insurance mathematics, namely ruin theory with Cramér and Lundberg. A unified formalization of large deviation theory was developed in 1966, in a paper by Varadhan.[1] Large deviations theory formalizes the heuristic ideas of concentration of measures and widely generalizes the notion of convergence of probability measures.
 Roughly speaking, large deviations theory concerns itself with the exponential decline of the probability measures of certain kinds of extreme or tail events.
"
Large_sample_theory,Mathematics,3,"In statistics: asymptotic theory, or large sample theory, is a framework for assessing properties of estimators and statistical tests. Within this framework, it is often assumed that the sample size n may grow indefinitely; the properties of estimators and tests are then evaluated under the limit of n → ∞. In practice, a limit evaluation is considered to be approximately valid for large finite sample sizes too.[1]"
Lattice_theory,Mathematics,3,"A lattice is an abstract structure studied in the mathematical subdisciplines of order theory and abstract algebra. It consists of a partially ordered set in which every two elements have a unique supremum (also called a least upper bound or join) and a unique infimum (also called a greatest lower bound or meet). An example is given by the natural numbers, partially ordered by divisibility, for which the unique supremum is the least common multiple and the unique infimum is the greatest common divisor.
 Lattices can also be characterized as algebraic structures satisfying certain axiomatic identities. Since the two definitions are equivalent, lattice theory draws on both order theory and universal algebra. Semilattices include lattices, which in turn include Heyting and Boolean algebras. These ""lattice-like"" structures all admit order-theoretic as well as algebraic descriptions.
"
Lie_group_theory,Mathematics,3,"In mathematics, a Lie group (pronounced /liː/ ""Lee"") is a group that is also a differentiable manifold. A manifold is a space that locally resembles Euclidean space, whereas groups define the abstract, generic concept of multiplication and the taking of inverses (division). Combining these two ideas, one obtains a continuous group where points can be multiplied together, and their inverse can be taken. If, in addition, the multiplication and taking of inverses are defined to be smooth (differentiable), one obtains a Lie group.
 Lie groups provide a natural model for the concept of continuous symmetry, a celebrated example of which is the rotational symmetry in three dimensions (given by the special orthogonal group 




SO

(
3
)


{  {\text{SO}}(3)}
). Lie groups are widely used in many parts of modern mathematics and physics.
 Lie groups were first found by studying matrix subgroups 



G


{  G}
 contained in 





GL


n


(

R

)


{  {\text{GL}}_{n}(\mathbb {R} )}
 or 





GL


n


(

C

)


{  {\text{GL}}_{n}(\mathbb {C} )}
, the groups of 



n
×
n


{  n\times n}
 invertible matrices over 




R



{  \mathbb {R} }
 or 




C



{  \mathbb {C} }
. These are now called the classical groups, as the concept has been extended far beyond these origins. Lie groups are named after Norwegian mathematician Sophus Lie (1842–1899), who laid the foundations of the theory of continuous transformation groups. Lie's original motivation for introducing Lie groups was to model the continuous symmetries of differential equations, in much the same way that finite groups are used in Galois theory to model the discrete symmetries of algebraic equations.
"
Lie_sphere_geometry,Mathematics,3,"Lie sphere geometry is a geometrical theory of planar or spatial geometry in which the fundamental concept is the circle or sphere. It was introduced by Sophus Lie in the nineteenth century.[1] The main idea which leads to Lie sphere geometry is that lines (or planes) should be regarded as circles (or spheres) of infinite radius and that points in the plane (or space) should be regarded as circles (or spheres) of zero radius.
 The space of circles in the plane (or spheres in space), including points and lines (or planes) turns out to be a manifold known as the Lie quadric (a quadric hypersurface in projective space). Lie sphere geometry is the geometry of the Lie quadric and the Lie transformations which preserve it. This geometry can be difficult to visualize because Lie transformations do not preserve points in general: points can be transformed into circles (or spheres).
 To handle this, curves in the plane and surfaces in space are studied using their contact lifts, which are determined by their tangent spaces. This provides a natural realisation of the osculating circle to a curve, and the curvature spheres of a surface. It also allows for a natural treatment of Dupin cyclides and a conceptual solution of the problem of Apollonius.
 Lie sphere geometry can be defined in any dimension, but the case of the plane and 3-dimensional space are the most important. In the latter case, Lie noticed a remarkable similarity between the Lie quadric of spheres in 3-dimensions, and the space of lines in 3-dimensional projective space, which is also a quadric hypersurface in a 5-dimensional projective space, called the Plücker or Klein quadric.  This similarity led Lie to his famous ""line-sphere correspondence"" between the space of lines and the space of spheres in 3-dimensional space.[2]"
Lie_theory,Mathematics,3,"
 In mathematics, the researcher Sophus Lie (/ˈliː/ LEE) initiated lines of study involving integration of differential equations, transformation groups, and contact of spheres that have come to be called Lie theory.[1]  For instance, the latter subject is Lie sphere geometry. This article addresses his approach to transformation groups, which is one of the areas of mathematics, and was worked out by Wilhelm Killing and Élie Cartan.
 The foundation of Lie theory is the exponential map relating Lie algebras to Lie groups which is called the Lie group–Lie algebra correspondence. The subject is part of differential geometry since Lie groups are differentiable manifolds. Lie groups evolve out of the identity (1) and the tangent vectors to one-parameter subgroups generate the Lie algebra. The structure of a Lie group is implicit in its algebra, and the structure of the Lie algebra is expressed by root systems and root data.
 Lie theory has been particularly useful in mathematical physics since it describes the standard transformation groups: the Galilean group, the Lorentz group, the Poincaré group and the conformal group of spacetime.
"
Line_geometry,Mathematics,3,"In geometry, line coordinates are used to specify the position of a line just as point coordinates (or simply coordinates) are used to specify the position of a point.
"
Linear_algebra,Mathematics,3,"Linear algebra is the branch of mathematics concerning linear equations such as: 
 linear maps such as:
 and their representations in vector spaces and through matrices.[1][2][3] Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as basically the application of linear algebra to spaces of functions.
 Linear algebra is also used in most sciences and fields of engineering, because it allows modeling many natural phenomena, and computing efficiently with such models. For nonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing with first-order approximations, using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point.
"
Functional_analysis,Mathematics,3,"Functional analysis is a branch of mathematical analysis which studies the transformations of functions and their algebraic and topological properties. The field builds upon and abstracts the results of Joseph Fourier's 1822 paper, Théorie analytique de la chaleur (The Analytical Theory of Heat), which demonstrated how a change of basis by means of the Fourier transform could be used to permit manipulations of a function in the frequency domain to obtain insights that were previously unobtainable. Functional analysis has modern applications in many areas of algebra, in particular associative algebra, in probability, operator theory, wavelets and wavelet transforms. The functional data analysis (FDA) paradigm of James O. Ramsay and Bernard Silverman ties functional analysis into principal component analysis and dimensionality reduction. 
 Functional analysis has strong parallels with linear algebra, as both fields are based on vector spaces as the core algebraic structure. Functional analysis endows linear algebra with concepts from topology (e.g. inner product, norm, topological space) in defining the topological vector space (TVS)[speculation?], which strengthens notions of continuity and limit, and supports generalization to infinite-dimensional spaces. The core operation within a TVS is linear transformation. 
 An important part of functional analysis is the extension of the theory of measure, integration, and probability to infinite dimensional spaces, also known as infinite dimensional analysis. Additionally, functional analysis generalizes the concept of an orthonormal basis—as found in Fourier analysis—to arbitrary inner product spaces, including those of infinite dimension. Important theoretical results include the Banach–Steinhaus theorem, spectral theorem (central to operator theory), Hahn–Banach theorem, open mapping theorem, and closed graph theorem. 
 The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations. 
 The usage of the word functional as a noun goes back to the calculus of variations, implying a function whose argument is a function. The term was first used in Leçons sur le calcul des variations (1910) by Jacques Hadamard. However, the general concept of a functional had previously been introduced in 1887 by the Italian mathematician and physicist Vito Volterra.[1][2] The theory of nonlinear functionals was continued by students of Hadamard, in particular Maurice René Fréchet and Paul Lévy. Hadamard also founded the modern school of linear functional analysis further developed by Frigyes Riesz and the Polish Lwów School of Mathematics centered around Stefan Banach.
"
Linear_programming,Mathematics,3,"Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).
 More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polytope where this function has the smallest (or largest) value if such a point exists.
 Linear programs are problems that can be expressed in canonical form as
 where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and 



(
⋅

)


T





{  (\cdot )^{\mathrm {T} }}
 is the matrix transpose. The expression to be maximized or minimized is called the objective function (cTx in this case). The inequalities Ax ≤ b and x ≥ 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second, then it can be said that the first vector is less-than or equal-to the second vector.
 Linear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics, and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.
"
List_of_graphical_methods,Mathematics,3,"This is a list of graphical methods with a mathematical basis.
Included are diagram techniques, chart techniques, plot techniques, and other forms of visualization.
 There is also a list of computer graphics and descriptive geometry topics.
"
Local_algebra,Mathematics,3,"In abstract algebra, more specifically ring theory, local rings are certain rings that are comparatively simple, and serve to describe what is called ""local behaviour"", in the sense of functions defined on varieties or manifolds, or of algebraic number fields examined at a particular place, or prime. Local algebra is the branch of commutative algebra that studies commutative local rings and their modules.
 In practice, a commutative local ring often arises as the result of the localization of a ring at a prime ideal.
 The concept of local rings was introduced by Wolfgang Krull in 1938 under the name Stellenringe.[1]  The English term local ring is due to Zariski.[2]"
Local_class_field_theory,Mathematics,3,"In mathematics, local class field theory, introduced by Helmut Hasse,[1] is the study of abelian extensions of local fields; here, ""local field"" means a field which is complete with respect to an absolute value or a discrete valuation with a finite residue field: hence every local field is isomorphic (as a topological field) to the real numbers R, the complex numbers C, a finite extension of the p-adic numbers Qp (where p is any prime number), or a finite extension of the field of formal Laurent series Fq((T)) over a finite field Fq.
"
Low-dimensional_topology,Mathematics,3,"In mathematics, low-dimensional topology is the branch of topology that studies manifolds, or more generally topological spaces, of four or fewer dimensions. Representative topics are the structure theory of 3-manifolds and 4-manifolds, knot theory, and braid groups. This can be regarded as a part of geometric topology. It may also be used to refer to the study of topological spaces of dimension 1, though this is more typically considered part of continuum theory.
"
Malliavin_calculus,Mathematics,3,"In probability theory and related fields, Malliavin calculus is a set of mathematical techniques and ideas that extend the mathematical field of calculus of variations from  deterministic functions to stochastic processes. In particular, it allows the computation of derivatives of random variables. Malliavin calculus is also called the stochastic calculus of variations. P. Malliavin first initiated the calculus on infinite dimensional space. Then, the significant contributors such as S. Kusuoka, D. Stroock, Bismut, S. Watanabe, I. Shigekawa, and so on finally completed the foundations.
 Malliavin calculus is named after Paul Malliavin whose ideas led to a proof that Hörmander's condition implies the existence and smoothness of a density for the solution of a stochastic differential equation; Hörmander's original proof was based on the theory of  partial differential equations. The calculus has been applied to stochastic partial differential equations as well.
 The calculus allows integration by parts with random variables; this operation is used in mathematical finance to compute the sensitivities of financial derivatives. The calculus has applications in, for example, stochastic filtering.
"
Mathematical_biology,Mathematics,3,"Mathematical and theoretical biology is a branch of biology which employs theoretical analysis, mathematical models and abstractions of the living organisms to investigate the principles that govern the structure, development and behavior of the systems, as opposed to experimental biology which deals with the conduction of experiments to prove and validate the scientific theories.[1] The field is sometimes called mathematical biology or biomathematics to stress the mathematical side, or theoretical biology to stress the biological side.[2] Theoretical biology focuses more on the development of theoretical principles for biology while mathematical biology focuses on the use of mathematical tools to study biological systems, even though the two terms are sometimes interchanged.[3][4] Mathematical biology aims at the mathematical representation and modeling of biological processes, using techniques and tools of applied mathematics and it can be useful in both theoretical and practical research. Describing systems in a quantitative manner means their behavior can be better simulated, and hence properties can be predicted that might not be evident to the experimenter. This requires precise mathematical models.
 Because of the complexity of the living systems, theoretical biology employs several fields of mathematics,[5] and has contributed to the development of new techniques.
"
Mathematical_chemistry,Mathematics,3,"Mathematical chemistry[1] is the area of research engaged in novel applications of mathematics to chemistry; it concerns itself principally with the mathematical modeling of chemical phenomena.[2] Mathematical chemistry has also sometimes been called computer chemistry, but should not be confused with computational chemistry.
 Major areas of research in mathematical chemistry include chemical graph theory, which deals with topology such as the mathematical study of isomerism and the development of topological descriptors or indices which find application in quantitative structure-property relationships; and chemical aspects of group theory, which finds applications in stereochemistry and quantum chemistry.
 The history of the approach may be traced back to the 19th century. Georg Helm published a treatise titled ""The Principles of Mathematical Chemistry: The Energetics of Chemical Phenomena"" in 1894.[3] Some of the more contemporary periodical publications specializing in the field are MATCH Communications in Mathematical and in Computer Chemistry, first published in 1975, and the Journal of Mathematical Chemistry, first published in 1987. In 1986 a series of annual conferences MATH/CHEM/COMP taking place in Dubrovnik was
initiated by the late Ante Graovac.
 The basic models for mathematical chemistry are molecular graph and topological index.
 In 2005 the International Academy of Mathematical Chemistry (IAMC) was founded in Dubrovnik (Croatia) by Milan Randić. The Academy has 82 members (2009) from all over the world, including six scientists awarded with a Nobel Prize.
"
Mathematical_economics,Mathematics,3,"Mathematical economics is the application of mathematical methods to represent theories and analyze problems in economics.  By convention, these applied methods are beyond simple geometry, such as differential and integral calculus, difference and differential equations, matrix algebra, mathematical programming, and other computational methods.[1][2] Proponents of this approach claim that it allows the formulation of theoretical relationships with rigor, generality, and simplicity.[3] Mathematics allows economists to form meaningful, testable propositions about  wide-ranging and complex subjects which could less easily  be expressed informally. Further, the language of mathematics allows economists to make specific, positive claims about controversial or contentious subjects that would be impossible without mathematics.[4] Much of economic theory is currently presented in terms of mathematical economic models, a set of stylized and simplified mathematical relationships asserted to clarify assumptions and implications.[5] Broad applications include:
 Formal economic modeling began in the 19th century with the use of differential calculus to represent and explain economic behavior, such as utility maximization, an early economic application of mathematical optimization. Economics became more mathematical as a discipline throughout the first half of the 20th century, but introduction of new and generalized techniques in the period around the Second World War, as in game theory, would greatly broaden the use of mathematical formulations in economics.[8][7] This rapid systematizing of economics alarmed critics of the discipline as well as some noted economists.  John Maynard Keynes, Robert Heilbroner, Friedrich Hayek and others have criticized the broad use of mathematical models for human behavior, arguing that some human choices are irreducible to mathematics.
"
Mathematical_finance,Mathematics,3,"Mathematical finance, also known as quantitative finance and financial mathematics, is a field of applied mathematics, concerned with mathematical modeling of financial markets. Generally, mathematical finance will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. Mathematical consistency is required, not compatibility with economic theory. Thus, for example, while a financial economist might study the structural reasons why a company may have a certain share price, a financial mathematician may take the share price as a given, and attempt to use stochastic calculus to obtain the corresponding value of derivatives of the stock (see: Valuation of options; Financial modeling; Asset pricing). The fundamental theorem of arbitrage-free pricing is one of the key theorems in mathematical finance, while the Black–Scholes equation and formula are amongst the key results.[1] Mathematical finance also overlaps heavily with the fields of computational finance and financial engineering. The latter focuses on applications and modeling, often by help of stochastic asset models (see: Quantitative analyst), while the former focuses, in addition to analysis, on building tools of implementation for the models. In general, there exist two separate branches of finance that require advanced quantitative techniques: derivatives pricing on the one hand, and risk- and portfolio management on the other.[2] French mathematician Louis Bachelier is considered the author of the first scholarly work on mathematical finance, published in 1900. But mathematical finance emerged as a discipline in the 1970s, following the work of Fischer Black, Myron Scholes and Robert Merton on option pricing theory.
 Today many universities offer degree and research programs in mathematical finance.
"
Mathematical_logic,Mathematics,3,"
 Mathematical logic is a subfield of mathematics exploring the applications of formal logic to mathematics.  It bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science.[1]  The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.
 Mathematical logic is often divided into the fields of set theory, model theory, recursion theory, and proof theory.  These areas share basic results on logic, particularly first-order logic, and definability. In computer science (particularly in the ACM Classification) mathematical logic encompasses additional topics not detailed in this article; see Logic in computer science for those.
 Since its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.
"
Mathematical_optimization,Mathematics,3,"Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element (with regard to some criterion) from some set of available alternatives.[1] Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.[2] In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding ""best available"" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.
"
Mathematical_physics,Mathematics,3,"Mathematical physics refers to the development of mathematical methods for application to problems in physics. The Journal of Mathematical Physics defines the field as ""the application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories"".[1]"
Mathematical_psychology,Mathematics,3,"Mathematical psychology is an approach to psychological research that is based on mathematical modeling of perceptual, thought, cognitive and motor processes, and on the establishment of law-like rules that relate quantifiable stimulus characteristics with quantifiable behavior. The mathematical approach is used with the goal of deriving hypotheses that are more exact and thus yield stricter empirical validations. Quantifiable behavior is in practice often constituted by task performance.
 The application of math in psychology could be traced back to at least seventeenth century when scientists like Kepler and Galileo were investigating the laws of mental process.[1] At that time, psychology had not even been recognized as an independent subject of science. The applications of mathematics in psychology can be roughly classified into two areas: one is the mathematical modeling of psychological theories and experimental phenomena which leads to mathematical psychology, the other is the statistical approach of quantitative measurement practices in psychology which leads to psychometrics.[2][3] There are five major research areas in mathematical psychology: learning and memory, perception and psychophysics, choice and decision making, language and thinking, and measurement and scaling.[3][4] As quantification of behavior is fundamental in this endeavor, the theory of measurement is a central topic in mathematical psychology.
Mathematical psychology is therefore closely related to psychometrics. However, where psychometrics is concerned with individual differences (or population structure) in mostly static variables, mathematical psychology focuses on process models of perceptual, cognitive and motor processes as inferred from the 'average individual'. Furthermore, where psychometrics investigates the stochastic dependence structure between variables as observed in the population, mathematical psychology almost exclusively focuses on the modeling of data obtained from experimental paradigms and is therefore even more closely related to experimental psychology/cognitive psychology/psychonomics.  Like computational neuroscience and econometrics, mathematical psychology theory often uses statistical optimality as a guiding principle, assuming that the human brain has evolved to solve problems in an optimized way. Central themes from cognitive psychology; limited vs. unlimited processing capacity, serial vs. parallel processing, etc., and their implications, are central in rigorous analysis in mathematical psychology. 
 Mathematical psychologists are active in many fields of psychology, especially in psychophysics, sensation and perception, problem solving, decision-making, learning, memory, and language, collectively known as cognitive psychology, and the quantitative analysis of behavior but also, e.g., in clinical psychology, social psychology, and psychology of music.
"
Mathematical_sciences,Mathematics,3,"The mathematical sciences are a group of areas of study that includes, in addition to mathematics, those academic disciplines that are primarily mathematical in nature but may not be universally considered subfields of mathematics proper.
 Statistics, for example, is mathematical in its methods but grew out of scientific observations[1] which merged with inverse probability and grew through applications in the social sciences, some areas of physics, and biometrics to become its own separate, though closely allied field. Computer science, computational science, data science, population genetics, operations research, control theory, cryptology, econometrics, theoretical physics, fluid mechanics, chemical reaction network theory and actuarial science are other fields that may be considered part of mathematical sciences.
 Some institutions offer degrees in mathematical sciences (e.g. the United States Military Academy, Stanford University, and University of Khartoum) or applied mathematical sciences (e.g. the University of Rhode Island).
"
Mathematical_sociology,Mathematics,3,"Mathematical sociology is the area of sociology that uses mathematics to construct social theories.  Mathematical sociology aims to take sociological theory and to express it in mathematical terms.  The benefits of this approach include increased clarity and the ability to use mathematics to derive implications of a theory that cannot be arrived at intuitively.  In mathematical sociology, the preferred style is encapsulated in the phrase ""constructing a mathematical model.""   This means making specified assumptions about some social phenomenon, expressing them in formal mathematics, and providing an empirical interpretation for the ideas.  It also means deducing properties of the model and comparing these with relevant empirical data.  Social network analysis is the best-known contribution of this subfield to sociology as a whole and to the scientific community at large. The models typically used in mathematical sociology allow sociologists to understand how predictable local interactions are and they are often able to elicit global patterns of social structure.[1]"
Mathematical_statistics,Mathematics,3,"Mathematical statistics is the application of probability theory, a branch of mathematics, to statistics, as opposed to techniques for collecting statistical data. Specific mathematical techniques which are used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure theory.[1][2]"
Mathematical_system_theory,Mathematics,3,"
Dynamical systems theory is an area of mathematics used to describe the behavior of complex dynamical systems, usually by employing differential equations or difference equations. When differential equations are employed, the theory is called continuous dynamical systems. From a physical point of view, continuous dynamical systems is a generalization of classical mechanics, a generalization where the equations of motion are postulated directly and are not constrained to be Euler–Lagrange equations of a least action principle. When difference equations are employed, the theory is called discrete dynamical systems. When the time variable runs over a set that is discrete over some intervals and continuous over other intervals or is any arbitrary time-set such as a Cantor set, one gets dynamic equations on time scales. Some situations may also be modeled by mixed operators, such as differential-difference equations.
 This theory deals with the long-term qualitative behavior of dynamical systems, and studies the nature of, and when possible the solutions of, the equations of motion of systems that are often primarily mechanical or otherwise physical in nature, such as planetary orbits and the behaviour of electronic circuits, as well as systems that arise in biology, economics, and elsewhere. Much of modern research is focused on the study of chaotic systems.
 This field of study is also called just dynamical systems, mathematical dynamical systems theory or the mathematical theory of dynamical systems.
"
Matrix_ring,Mathematics,3,"In abstract algebra, a matrix ring is any collection of matrices over some ring R that form a ring under matrix addition and matrix multiplication (Lam 1999). The set of n × n matrices with entries from R is a matrix ring denoted Mn(R), as well as some subsets of infinite matrices which form infinite matrix rings. Any subring of a matrix ring is a matrix ring.
 When R is a commutative ring, the matrix ring Mn(R) is an associative algebra, and may be called a matrix algebra.  For this case, if M is a matrix and r is in R, then the matrix Mr is the matrix M with each of its entries multiplied by r.
 This article assumes that R is an associative ring with a unit 1 ≠ 0, although matrix rings can be formed over rings without unity.
"
Matrix_calculus,Mathematics,3,"In mathematics, matrix calculus is a specialized notation for doing multivariable calculus, especially over spaces of matrices.  It collects the various partial derivatives of a single function with respect to many variables, and/or of a multivariate function with respect to a single variable, into vectors and matrices that can be treated as single entities.  This greatly simplifies operations such as finding the maximum or minimum of a multivariate function and solving systems of differential equations. The notation used here is commonly used in statistics and engineering, while the tensor index notation is preferred in physics.
 Two competing notational conventions split the field of matrix calculus into two separate groups. The two groups can be distinguished by whether they write the derivative of a scalar with respect to a vector as a column vector or a row vector. Both of these conventions are possible even when the common assumption is made that vectors should be treated as column vectors when combined with matrices (rather than row vectors). A single convention can be somewhat standard throughout a single field that commonly uses matrix calculus (e.g. econometrics, statistics, estimation theory and machine learning). However, even within a given field different authors can be found using competing conventions. Authors of both groups often write as though their specific convention were standard. Serious mistakes can result when combining results from different authors without carefully verifying that compatible notations have been used. Definitions of these two conventions and comparisons between them are collected in the layout conventions section.
"
Matrix_(mathematics),Mathematics,3,"In mathematics, a matrix (plural matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns.[1][2] For example, the dimension of the matrix below is 2 × 3 (read ""two by three""), because there are two rows and three columns:
 Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for an (m×n)-matrix times an (n×p)-matrix, resulting in an (m×p)-matrix). There is no product the other way round—a first hint that matrix multiplication is not commutative. Any matrix can be multiplied element-wise by a scalar from its associated field. Matrices are often denoted using capital roman letters such as 



A


{  A}
, 



B


{  B}
 and 



C


{  C}
.[3] The individual items in an m×n matrix A, often denoted by ai,j, where i and j usually vary from 1 to m and n, respectively, are called its elements or entries.[4][5] For conveniently expressing an element of the results of matrix operations, the indices of the element are often attached to the parenthesized or bracketed matrix expression (e.g., (AB)i,j refers to an element of a matrix product). In the context of abstract index notation, this ambiguously refers also to the whole matrix product.
 A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. 
 If the matrix is square, then it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.
 Applications of matrices are found in most scientific fields.[6] In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. 
 In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[7] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.
 A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.
"
Matroid_theory,Mathematics,3,"In combinatorics, a branch of mathematics, a matroid /ˈmeɪtrɔɪd/ is a structure that abstracts and generalizes the notion of linear independence in vector spaces. There are many equivalent ways to define a matroid axiomatically, the most significant being in terms of: independent sets; bases or circuits; rank functions; closure operators; and closed sets or flats. In the language of partially ordered sets, a finite matroid is equivalent to a geometric lattice.
 Matroid theory borrows extensively from the terminology of linear algebra and graph theory, largely because it is the abstraction of various notions of central importance in these fields. Matroids have found applications in geometry, topology, combinatorial optimization, network theory and coding theory.[1][2]"
Measure_theory,Mathematics,3,"In mathematical analysis, a measure on a set is a systematic way to assign a number to each suitable subset of that set, intuitively interpreted as its size. In this sense, a measure is a generalization of the concepts of length, area, and volume. A particularly important example is the Lebesgue measure on a Euclidean space, which assigns the conventional length, area, and volume of Euclidean geometry to suitable subsets of the n-dimensional Euclidean space Rn. For instance, the Lebesgue measure of the interval [0, 1] in the real numbers is its length in the everyday sense of the word, specifically, 1.
 Technically, a measure is a function that assigns a non-negative real number or +∞ to (certain) subsets of a set X (see Definition below). It must further be countably additive: the measure of a 'large' subset that can be decomposed into a finite (or countably infinite) number of 'smaller' disjoint subsets is equal to the sum of the measures of the ""smaller"" subsets. In general, if one wants to associate a consistent size to each subset of a given set while satisfying the other axioms of a measure, one only finds trivial examples like the counting measure. This problem was resolved by defining measure only on a sub-collection of all subsets; the so-called measurable subsets, which are required to form a σ-algebra. This means that countable unions, countable intersections and complements of measurable subsets are measurable. Non-measurable sets in a Euclidean space, on which the Lebesgue measure cannot be defined consistently, are necessarily complicated in the sense of being badly mixed up with their complement.[1] Indeed, their existence is a non-trivial consequence of the axiom of choice.
 Measure theory was developed in successive stages during the late 19th and early 20th centuries by Émile Borel, Henri Lebesgue, Johann Radon, and Maurice Fréchet, among others. The main applications of measures are in the foundations of the Lebesgue integral, in Andrey Kolmogorov's axiomatisation of probability theory and in ergodic theory. In integration theory, specifying a measure allows one to define integrals on spaces more general than subsets of Euclidean space; moreover, the integral with respect to the Lebesgue measure on Euclidean spaces is more general and has a richer theory than its predecessor, the Riemann integral. Probability theory considers measures that assign to the whole set the size 1, and considers measurable subsets to be events whose probability is given by the measure. Ergodic theory considers measures that are invariant under, or arise naturally from, a dynamical system.
"
Metric_geometry,Mathematics,3,"In mathematics, a metric space is a set together with a metric on the set.  The metric is a function that defines a concept of distance between any two members of the set, which are usually called points.  The metric satisfies a few simple properties.  Informally:
 A metric on a space induces topological properties like open and closed sets, which lead to the study of more abstract topological spaces.
 The most familiar metric space is 3-dimensional Euclidean space. In fact, a ""metric"" is the generalization of the Euclidean metric arising from the four long-known properties of the Euclidean distance. The Euclidean metric  defines the distance between two points as the length of the straight line segment connecting them. Other metric spaces occur for example in elliptic geometry and hyperbolic geometry, where distance on a sphere measured by angle is a metric, and the hyperboloid model of hyperbolic geometry is used by special relativity as a metric space of velocities.
"
Microlocal_analysis,Mathematics,3,"In mathematical analysis, microlocal analysis comprises techniques developed from the 1950s onwards based on Fourier transforms related to the study of variable-coefficients-linear and nonlinear partial differential equations.  This includes generalized functions, pseudo-differential operators, wave front sets, Fourier integral operators, oscillatory integral operators, and paradifferential operators.
 The term microlocal implies localisation not only with respect to location in the space, but also with respect to cotangent space directions at a given point. This gains in importance on manifolds of dimension greater than one.
"
Model_theory,Mathematics,3,"In mathematics, model theory is the study of the relationship between formal theories (a collection of sentences in a formal language expressing statements about a mathematical structure), and their models, taken as interpretations that satisfy the sentences of that theory.[1]"
Abstract_algebra,Mathematics,3,"In algebra, which is a broad division of mathematics, abstract algebra (occasionally called modern algebra) is the study of algebraic structures. Algebraic structures include groups, rings, fields, modules, vector spaces, lattices, and algebras. The term abstract algebra was coined in the early 20th century to distinguish this area of study from the other parts of algebra.
 Algebraic structures, with their associated homomorphisms, form mathematical categories. Category theory is a formalism that allows a unified  way for expressing properties and constructions that are similar for various structures.
 Universal algebra is a related subject that studies types of algebraic structures as single objects. For example, the structure of groups is a single object in universal algebra, which is called variety of groups.
"
Scheme_(mathematics),Mathematics,3,"In mathematics, a scheme is a mathematical structure that enlarges the notion of algebraic variety in several ways, such as taking account of multiplicities (the equations x = 0 and x2 = 0 define the same algebraic variety and different schemes) and allowing ""varieties"" defined over any commutative ring (for example, Fermat curves are defined over the integers).
 Schemes were introduced by Alexander Grothendieck in 1960 in his treatise ""Éléments de géométrie algébrique""; one of its aims was developing the formalism needed to solve deep problems of algebraic geometry, such as the Weil conjectures (the last of which was proved by Pierre Deligne).[1] Strongly based on commutative algebra, scheme theory allows a systematic use of methods of topology and homological algebra. Scheme theory also unifies algebraic geometry with much of number theory, which eventually led to Wiles's proof of Fermat's Last Theorem.
 Formally, a scheme is a topological space together with commutative rings for all of its open sets, which arises from gluing together spectra (spaces of prime ideals) of commutative rings along their open subsets. In other words, it is a ringed space which is locally a spectrum of a commutative ring.
 The relative point of view is that much of algebraic geometry should be developed for a morphism X → Y of schemes (called a scheme X over Y), rather than for an individual scheme. For example, in studying algebraic surfaces, it can be useful to consider families of algebraic surfaces over any scheme Y. In many cases, the family of all varieties of a given type can itself be viewed as a variety or scheme, known as a moduli space.
 For some of the detailed definitions in the theory of schemes, see the glossary of scheme theory.
"
Invariant_theory,Mathematics,3,"Invariant theory is a branch of abstract algebra dealing with actions of groups on algebraic varieties, such as vector spaces, from the point of view of their effect on functions. Classically, the theory dealt with the question of explicit description of polynomial functions that do not change, or are invariant, under the transformations from a given linear group. For example, if we consider the action of the special linear group SLn on the space of n by n matrices by left multiplication, then the determinant is an invariant of this action because the determinant of A X equals the determinant of X, when  A is in SLn.
"
Modular_representation_theory,Mathematics,3,"Modular representation theory is a branch of mathematics, and that part of representation theory that studies linear representations of finite groups over a field K of positive characteristic p, necessarily a prime number. As well as having applications to group theory, modular representations arise naturally in other branches of mathematics, such as algebraic geometry, coding theory[citation needed], combinatorics and number theory.
 Within finite group theory, character-theoretic results proved by Richard Brauer using modular representation theory played an important role in early progress towards the classification of finite simple groups, especially for simple groups whose characterization was not amenable to purely group-theoretic methods because their Sylow 2-subgroups were too small in an appropriate sense. Also, a general result on embedding of elements of order 2 in finite groups called the Z* theorem, proved by George Glauberman using the theory developed by Brauer, was particularly useful in the classification program.
 If the characteristic p of K does not divide the order |G|, then modular representations are completely reducible, as with ordinary (characteristic 0) representations, by virtue of  Maschke's theorem. In the other case, when |G| ≡ 0 mod p, the process of averaging over the group needed to prove Maschke's theorem breaks down, and representations need not be completely reducible. Much of the discussion below implicitly assumes that the field K is sufficiently large (for example, K algebraically closed suffices), otherwise some statements need refinement.
"
Module_theory,Mathematics,3,"In mathematics, a module is one of the fundamental algebraic structures used in abstract algebra. A module over a ring is a generalization of the notion of vector space over a field, wherein the corresponding scalars are the elements of an arbitrary given ring (with identity) and a multiplication (on the left and/or on the right) is defined between elements of the ring and elements of the module. A module taking its scalars from a ring R is called an R-module.
 Thus, a module, like a vector space, is an additive abelian group; a product is defined between elements of the ring and elements of the module that is distributive over the addition operation of each parameter and is compatible with the ring multiplication.
 Modules are very closely related to the representation theory of groups. They are also one of the central notions of commutative algebra and homological algebra, and are used widely in algebraic geometry and algebraic topology.
"
Molecular_geometry,Mathematics,3,"
 Molecular geometry is the three-dimensional arrangement of the atoms that constitute a molecule. It includes the general shape of the molecule as well as bond lengths, bond angles, torsional angles and any other geometrical parameters that determine the position of each atom.
 Molecular geometry influences several properties of a substance including its reactivity, polarity, phase of matter, color, magnetism and biological activity.[1][2][3] The angles between bonds that an atom forms depend only weakly on the rest of molecule, i.e. they can be understood as approximately local and hence transferable properties.
"
Morse_theory,Mathematics,3,"In mathematics, specifically in differential topology, Morse theory enables one to analyze the topology of a manifold by studying differentiable functions on that manifold. According to the basic insights of Marston Morse, a typical differentiable function on a manifold will reflect the topology quite directly. Morse theory allows one to find CW structures and handle decompositions on manifolds and to obtain substantial information about their homology.
 Before Morse, Arthur Cayley and James Clerk Maxwell had developed some of the ideas of Morse theory in the context of topography. Morse originally applied his theory to geodesics (critical points of the energy functional on paths). These techniques were used in Raoul Bott's proof of his periodicity theorem.
 The analogue of Morse theory for complex manifolds is Picard–Lefschetz theory.
"
Motivic_cohomology,Mathematics,3,"Motivic cohomology is an invariant of algebraic varieties and of more general schemes. It includes the Chow ring of algebraic cycles as a special case. Some of the deepest problems in algebraic geometry and number theory are attempts to understand motivic cohomology.
"
Multilinear_algebra,Mathematics,3,"
 In mathematics, multilinear algebra extends the methods of linear algebra. Just as linear algebra is built on the concept of a vector and develops the theory of vector spaces, multilinear algebra builds on the concepts of p-vectors and multivectors with Grassmann algebra.
"
Multiplicative_number_theory,Mathematics,3,"Multiplicative number theory is a subfield of analytic number theory that deals with prime numbers and with factorization and divisors. The focus is usually on developing approximate formulas for counting these objects in various contexts. The prime number theorem is a key result in this subject. The Mathematics Subject Classification for multiplicative number theory is 11Nxx.
"
Multivariable_calculus,Mathematics,3,"Multivariable calculus (also known as multivariate calculus) is the extension of calculus in one variable to calculus with functions of several variables: the differentiation and integration of functions involving several variables, rather than just one.[1]"
Multiple-scale_analysis,Mathematics,3,"In mathematics and physics, multiple-scale analysis (also called the method of multiple scales) comprises techniques used to construct uniformly valid approximations to the solutions of perturbation problems, both for small as well as large values of the independent variables. This is done by introducing fast-scale and slow-scale variables for an independent variable, and subsequently treating these variables, fast and slow, as if they are independent. In the solution process of the perturbation problem thereafter, the resulting additional freedom – introduced by the new independent variables – is used to remove (unwanted) secular terms. The latter puts constraints on the approximate solution, which are called solvability conditions.
 Mathematics research from about the 1980s proposes that coordinate transforms and invariant manifolds provide a sounder support for multiscale modelling (for example, see center manifold and slow manifold).
"
Neutral_geometry,Mathematics,3,"Absolute geometry is a geometry based on an axiom system for Euclidean geometry without the parallel postulate or any of its alternatives. Traditionally, this has meant using only the first four of Euclid's postulates, but since these are not sufficient as a basis of Euclidean geometry, other systems, such as  Hilbert's axioms without the parallel axiom, are used.[1] The term was introduced by János Bolyai in 1832.[2] It is sometimes referred to as neutral geometry,[3] as it is neutral with respect to the parallel postulate.
"
Nevanlinna_theory,Mathematics,3,"In the mathematical field of complex analysis, Nevanlinna theory is part of the
theory of meromorphic functions. It was devised in 1925, by Rolf Nevanlinna.  Hermann Weyl has called it ""one of the few great mathematical events of (the twentieth) century.""[1] The theory describes the asymptotic distribution of solutions of the equation f(z) = a, as a varies. A fundamental tool is the Nevanlinna characteristic T(r, f) which measures the rate of growth of a meromorphic function.
 Other main contributors in the first half of the 20th century were Lars Ahlfors, André Bloch, Henri Cartan, Edward Collingwood, Otto Frostman, Frithiof Nevanlinna, Henrik Selberg, Tatsujiro Shimizu, Oswald Teichmüller,
and Georges Valiron.  In its original form, Nevanlinna theory deals with meromorphic functions of one complex variable defined in a disc |z| ≤ R or in the whole complex plane (R = ∞). Subsequent generalizations extended Nevanlinna theory to algebroid functions, holomorphic curves, holomorphic maps between complex manifolds of arbitrary dimension, quasiregular maps and minimal surfaces.
 This article describes mainly the classical version for meromorphic functions of one variable, with emphasis on functions meromorphic in the complex plane. General references for this theory are Goldberg & Ostrovskii,[2] Hayman[3] and Lang (1987).
"
Nielsen_theory,Mathematics,3,"Nielsen theory is a branch of mathematical research with its origins in topological fixed point theory. Its central ideas were developed by Danish mathematician Jakob Nielsen, and bear his name.
 The theory developed in the study of the so-called minimal number of a map f from a compact space to itself, denoted MF[f]. This is defined as:
 where ~ indicates homotopy of mappings, and #Fix(g) indicates the number of fixed points of g. The minimal number was very difficult to compute in Nielsen's time, and remains so today. Nielsen's approach is to group the fixed point set into classes, which are judged ""essential"" or ""nonessential"" according to whether or not they can be ""removed"" by a homotopy.
 Nielsen's original formulation is equivalent to the following:
We define an equivalence relation on the set of fixed points of a self-map f on a space X. We say that x is equivalent to y if and only if there exists a path c from x to y with f(c) homotopic to c as paths. The equivalence classes with respect to this relation are called the Nielsen classes of f, and the Nielsen number N(f) is defined as the number of Nielsen classes having non-zero fixed point index sum.
 Nielsen proved that
 making his invariant a good tool for estimating the much more difficult MF[f]. This leads immediately to what is now known as the Nielsen fixed point theorem: Any map f has at least N(f) fixed points.
 Because of its definition in terms of the fixed point index, the Nielsen number is closely related to the Lefschetz number. Indeed, shortly after Nielsen's initial work, the two invariants were combined into a single ""generalized Lefschetz number"" (more recently called the Reidemeister trace) by Wecken and Reidemeister.
"
Non-abelian_class_field_theory,Mathematics,3,"In mathematics, non-abelian class field theory is a catchphrase, meaning the extension of the results of class field theory, the relatively complete and classical set of results on abelian extensions of any number field K, to the general Galois extension L/K. While class field theory was essentially known by 1930, the corresponding non-abelian theory has never been formulated in a definitive and accepted sense.[1]"
Non-classical_analysis,Mathematics,3,"In mathematics, non-classical analysis is any system of analysis, other than classical real analysis, and complex, vector, tensor, etc., analysis based upon it.
 Such systems include:
 Non-standard analysis and the calculus it involves, non-standard calculus, are considered part of classical mathematics (i.e. The concept of ""hyperreal number"" it uses, can be constructed in the framework of Zermelo–Fraenkel set theory).
 Non-Newtonian calculus is also a part of classical mathematics.
"
Non-Euclidean_geometry,Mathematics,3,"In mathematics, non-Euclidean geometry consists of two geometries based on axioms closely related to those that specify Euclidean geometry. As Euclidean geometry lies at the intersection of metric geometry and affine geometry, non-Euclidean geometry arises by either relaxing the metric requirement, or replacing the parallel postulate with an alternative. In the latter case one obtains hyperbolic geometry and elliptic geometry, the traditional non-Euclidean geometries. When the metric requirement is relaxed, then there are affine planes associated with the planar algebras, which give rise to kinematic geometries that have also been called non-Euclidean geometry.
 The essential difference between the metric geometries is the nature of parallel lines. Euclid's fifth postulate, the parallel postulate, is equivalent to Playfair's postulate, which states that, within a two-dimensional plane, for any given line  l and a point A, which is not on l, there is exactly one line through A that does not intersect l. In hyperbolic geometry, by contrast, there are infinitely many lines through A not intersecting l, while in elliptic geometry, any line through A intersects l.
 Another way to describe the differences between these geometries is to consider two straight lines indefinitely extended in a two-dimensional plane that are both perpendicular to a third line (in the same plane):
"
Non-standard_analysis,Mathematics,3,"
 The history of calculus is fraught with philosophical debates about the meaning and logical validity of fluxions or infinitesimal numbers. The standard way to resolve these debates is to define the operations of calculus using epsilon–delta procedures rather than infinitesimals. Nonstandard analysis[1][2][3] instead reformulates the calculus using a logically rigorous notion of infinitesimal numbers.
 Nonstandard analysis was originated in the early 1960s by the mathematician Abraham Robinson.[4][5] He wrote:
 ... the idea of infinitely small or infinitesimal quantities seems to appeal naturally to our intuition. At any rate, the use of infinitesimals was widespread during the formative stages of the Differential and Integral Calculus. As for the objection ... that the distance between two distinct real numbers cannot be infinitely small, Gottfried Wilhelm Leibniz argued that the theory of infinitesimals implies the introduction of ideal numbers which might be infinitely small or infinitely large compared with the real numbers but which were to possess the same properties as the latter. Robinson argued that this law of continuity of Leibniz's is a precursor of the transfer principle. Robinson continued:
 However, neither he nor his disciples and successors were able to give a rational development leading up to a system of this sort. As a result, the theory of infinitesimals gradually fell into disrepute and was replaced eventually by the classical theory of limits.[6] Robinson continues:
 ... Leibniz's ideas can be fully vindicated and ... they lead to a novel and fruitful approach to classical Analysis and to many other branches of mathematics. The key to our method is provided by the detailed analysis of the relation between mathematical languages and mathematical structures which lies at the bottom of contemporary model theory. In 1973, intuitionist Arend Heyting praised nonstandard analysis as ""a standard model of important mathematical research"".[7]"
Non-standard_calculus,Mathematics,3,"In mathematics, nonstandard calculus is the modern application of infinitesimals, in the sense of nonstandard analysis, to infinitesimal calculus. It provides a rigorous justification for some arguments in calculus that were previously considered merely heuristic.
 Non-rigorous calculations with infinitesimals were widely used before Karl Weierstrass sought to replace them with the (ε, δ)-definition of limit starting in the 1870s.  (See history of calculus.) For almost one hundred years thereafter, mathematicians like Richard Courant viewed infinitesimals as being naive and vague or meaningless.[1] Contrary to such views, Abraham Robinson showed in 1960 that infinitesimals are precise, clear, and meaningful, building upon work by Edwin Hewitt and Jerzy Łoś. According to Howard Keisler, ""Robinson solved a three hundred year old problem by giving a precise treatment of infinitesimals.  Robinson's achievement will probably rank as one of the major mathematical advances of the twentieth century.""[2]"
Arithmetic_dynamics,Mathematics,3,"Arithmetic dynamics[1] is a field that amalgamates two areas of mathematics, dynamical systems and number theory. Classically, discrete dynamics refers to the study of the iteration of self-maps of the complex plane or real line. Arithmetic dynamics is the study of the number-theoretic properties of integer, rational, p-adic, and/or algebraic points under repeated application of a polynomial or rational function. A fundamental goal is to describe arithmetic properties in terms of underlying geometric structures.
 Global arithmetic dynamics is the study of analogues of classical diophantine geometry  in the setting of discrete dynamical systems, while local arithmetic dynamics, also called p-adic or nonarchimedean dynamics, is an analogue of classical dynamics in which one replaces the complex numbers C by a p-adic field such as Qp or Cp and studies chaotic behavior and the Fatou and Julia sets.
 The following table describes a rough correspondence between Diophantine equations, especially abelian varieties, and dynamical systems:
"
Noncommutative_algebraic_geometry,Mathematics,3,"Noncommutative algebraic geometry is a branch of mathematics, and more specifically a direction in noncommutative geometry, that studies the geometric properties of formal duals of non-commutative algebraic objects such as rings as well as geometric objects derived from them (e.g. by gluing along localizations or taking noncommutative stack quotients).
 For example, noncommutative algebraic geometry is supposed to extend a notion of an algebraic scheme by suitable gluing of spectra of noncommutative rings; depending on how literally and how generally this aim (and a notion of spectrum) is understood in noncommutative setting, this has been achieved in various level of success. The noncommutative ring generalizes here a commutative ring of regular functions on a commutative scheme. Functions on usual spaces in the traditional (commutative) algebraic geometry have a product defined by pointwise multiplication; as the values of these functions commute, the functions also commute: a times b equals b times a. It is remarkable that viewing noncommutative associative algebras as algebras of functions on ""noncommutative"" would-be space is a far-reaching geometric intuition, though it formally looks like a fallacy.[citation needed] Much of the motivation for noncommutative geometry, and in particular for the noncommutative algebraic geometry, is from physics; especially from quantum physics, where the algebras of observables are indeed viewed as noncommutative analogues of functions, hence having the ability to observe their geometric aspects is desirable.
 One of the values of the field is that it also provides new techniques to study objects in commutative algebraic geometry such as Brauer groups.
 The methods of noncommutative algebraic geometry are analogs of the methods of commutative algebraic geometry, but frequently the foundations are different. Local behavior in commutative algebraic geometry is captured by commutative algebra and especially the study of local rings. These do not have a ring-theoretic analogue in the noncommutative setting; though in a categorical setup one can talk about stacks of local categories of quasicoherent sheaves over noncommutative spectra. Global properties such as those arising from homological algebra and K-theory more frequently carry over to the noncommutative setting.
"
Noncommutative_geometry,Mathematics,3,"Noncommutative geometry (NCG) is a branch of mathematics concerned with a geometric approach to noncommutative algebras, and with the construction of spaces that are locally presented by noncommutative algebras of functions (possibly in some generalized sense). A noncommutative algebra is an associative algebra in which the multiplication is not commutative, that is, for which 



x
y


{  xy}
 does not always equal 



y
x


{  yx}
; or more generally an algebraic structure in which one of the principal binary operations is not commutative; one also allows additional structures, e.g. topology or norm, to be possibly carried by the noncommutative algebra of functions.
"
Noncommutative_harmonic_analysis,Mathematics,3,"In mathematics, noncommutative harmonic analysis is the field in which results from Fourier analysis are extended to topological groups that are not commutative.[1] Since locally compact abelian groups have a well-understood theory, Pontryagin duality, which includes the basic structures of Fourier series and Fourier transforms, the major business of non-commutative harmonic analysis is usually taken to be the extension of the theory to all groups G that are locally compact. The case of compact groups is understood, qualitatively and after the Peter–Weyl theorem from the 1920s, as being generally analogous to that of finite groups and their character theory.
 The main task is therefore the case of G that is locally compact, not compact and not commutative. The interesting examples include many Lie groups, and also algebraic groups over p-adic fields. These examples are of interest and frequently applied in mathematical physics, and contemporary number theory, particularly automorphic representations.
 What to expect is known as the result of basic work of John von Neumann. He showed that if the von Neumann group algebra of G is of type I, then L2(G) as a unitary representation of G is a direct integral of irreducible representations. It is parametrized therefore by the unitary dual, the set of isomorphism classes of such representations, which is given the hull-kernel topology. The analogue of the Plancherel theorem is abstractly given by identifying a measure on the unitary dual, the Plancherel measure, with respect to which the direct integral is taken. (For Pontryagin duality the Plancherel measure is some Haar measure on the dual group to G, the only issue therefore being its normalization.) For general locally compact groups, or even countable discrete groups, the von Neumann group algebra need not be of type I and the regular representation of G cannot be written in terms of irreducible representations, even though it is unitary and completely reducible. An example where this happens is the infinite symmetric group, where the von Neumann group algebra is the hyperfinite type II1 factor. The further theory divides up the Plancherel measure into a discrete and a continuous part. For semisimple groups, and classes of solvable Lie groups, a very detailed theory is available.[2]"
Noncommutative_topology,Mathematics,3,"In mathematics, noncommutative topology is a term used for the relationship between topological and C*-algebraic concepts. The term has its origins in the Gelfand–Naimark theorem, which implies the duality of the category of locally compact Hausdorff spaces and the category of commutative C*-algebras. Noncommutative topology is related to analytic noncommutative geometry.
"
Nonlinear_analysis,Mathematics,3,"Nonlinear functional analysis is a branch of mathematical analysis that deals with nonlinear mappings.
"
Nonlinear_functional_analysis,Mathematics,3,"Nonlinear functional analysis is a branch of mathematical analysis that deals with nonlinear mappings.
"
Number_theory,Mathematics,3,"Number theory (or arithmetic or higher arithmetic in older usage) is a branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions. German mathematician Carl Friedrich Gauss (1777–1855) said, ""Mathematics is the queen of the sciences—and number theory is the queen of mathematics.""[1] Number theorists study prime numbers as well as the properties of mathematical objects made out of integers (for example, rational numbers) or defined as generalizations of the integers (for example, algebraic integers). 
 Integers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory are often best understood through the study of analytical objects (for example, the Riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers, for example, as approximated by the latter (Diophantine approximation).
 The older term for number theory is arithmetic. By the early twentieth century, it had been superseded by ""number theory"".[note 1] (The word ""arithmetic"" is used by the general public to mean ""elementary calculations""; it has also acquired other meanings in mathematical logic, as in Peano arithmetic, and computer science, as in floating point arithmetic.) The use of the term arithmetic for number theory regained some ground in the second half of the 20th century, arguably in part due to French influence.[note 2] In particular, arithmetical is preferred as an adjective to number-theoretic.[by whom?]"
Numerical_analysis,Mathematics,3,"
 Numerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). Numerical analysis naturally finds application in all fields of engineering and the physical sciences, but in the 21st century also the life sciences, social sciences, medicine, business and even the arts have adopted elements of scientific computations. The growth in computing power has revolutionized the use of realistic mathematical models in science and engineering, and subtle numerical analysis is required to implement these detailed models of the world. For example, ordinary differential equations appear in celestial mechanics (predicting the motions of planets, stars and galaxies); numerical linear algebra is important for data analysis;[2][3][4] stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.
 Before the advent of modern computers, numerical methods often depended on hand interpolation formulas applied to data from large printed tables. Since the mid 20th century, computers calculate the required functions instead, but many of the same formulas nevertheless continue to be used as part of the software algorithms.[5] The numerical point of view goes back to the earliest mathematical writings. A tablet from the Yale Babylonian Collection (YBC 7289), gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square.
 Numerical analysis continues this long tradition: rather than exact symbolic answers, which can only be applied to real-world measurements by translation into digits, it gives approximate solutions within specified error bounds.
"
Numerical_linear_algebra,Mathematics,3,"Numerical linear algebra, sometimes called applied linear algebra, is the study of how matrix operations can be used to create computer algorithms which efficiently and accurately provide approximate answers to questions in continuous mathematics. It is a subfield of numerical analysis, and a type of linear algebra. Computers use floating-point arithmetic and cannot exactly represent irrational data, so when a computer algorithm is applied to a matrix of data, it can sometimes increase the difference between a number stored in the computer and the true number that it is an approximation of. Numerical linear algebra uses properties of vectors and matrices to develop computer algorithms that minimize the error introduced by the computer, and is also concerned with ensuring that the algorithm is as efficient as possible.
 Numerical linear algebra aims to solve problems of continuous mathematics using finite precision computers, so its applications to the natural and social sciences are as vast as the applications of continuous mathematics. It is often a fundamental part of engineering and computational science problems, such as image and signal processing, telecommunication, computational finance, materials science simulations, structural biology, data mining, bioinformatics, and fluid dynamics. Matrix methods are particularly used in finite difference methods, finite element methods, and the modeling of differential equations. Noting the broad applications of numerical linear algebra, Lloyd N. Trefethen and David Bau, III argue that it is ""as fundamental to the mathematical sciences as calculus and differential equations"",[1]:x even though it is a comparatively small field.[2] Because many properties of matrices and vectors also apply to functions and operators, numerical linear algebra can also be viewed as a type of functional analysis which has a particular emphasis on practical algorithms.[1]:ix Common problems in numerical linear algebra include obtaining matrix decompositions like the singular value decomposition, the QR factorization, the LU factorization, or the eigendecomposition, which can then be used to answer common linear algebraic problems like solving linear systems of equations, locating eigenvalues, or least squares optimisation. Numerical linear algebra's central concern with developing algorithms that do not introduce errors when applied to real data on a finite precision computer is often achieved by iterative methods rather than direct ones.
"
Operad_theory,Mathematics,3,"
In mathematics, an operad is concerned with prototypical algebras that model properties such as commutativity or anticommutativity as well as various amounts of associativity. Operads generalize the various associativity properties already observed in algebras and coalgebras such as Lie algebras or Poisson algebras by modeling computational trees within the algebra. Algebras are to operads as group representations are to groups. An operad can be seen as a set of operations, each one having a fixed finite number of inputs (arguments) and one output, which can be composed one with others. They form a category-theoretic analog of universal algebra.[dubious  – discuss]"
Operation_research,Mathematics,3,"
Operations research (British English: operational research) (OR) is a discipline that deals with the application of advanced analytical methods to help make better decisions.[1] Further, the term operational analysis is used in the British (and some British Commonwealth) military as an intrinsic part of capability development, management and assurance.  In particular, operational analysis forms part of the Combined Operational Effectiveness and Investment Appraisals, which support British defence capability acquisition decision-making.
 It is often considered to be a sub-field of mathematical sciences.[2] The terms management science and decision science are sometimes used as synonyms.[3] Employing techniques from other mathematical sciences, such as mathematical modeling, statistical analysis, and mathematical optimization, operations research arrives at optimal or near-optimal solutions to complex decision-making problems. Because of its emphasis on human-technology interaction and because of its focus on practical applications, operations research has overlap with other disciplines, notably industrial engineering and operations management, and draws on psychology and organization science. Operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.[4]"
Operator_K-theory,Mathematics,3,"In mathematics, operator K-theory is a noncommutative analogue of topological K-theory for Banach algebras with most applications used for C*-algebras.
"
Operator_theory,Mathematics,3,"In mathematics, operator theory is the study of linear operators on function spaces, beginning with differential operators and integral operators. The operators may be presented abstractly by their characteristics, such as bounded linear operators or closed operators, and consideration may be given to nonlinear operators. The study, which depends heavily on the topology of function spaces, is a branch of functional analysis.
 If a collection of operators forms an algebra over a field, then it is an operator algebra. The description of operator algebras is part of operator theory.
"
Optimal_control_theory,Mathematics,3,"Optimal control theory is a branch of mathematical optimization that deals with finding a control for a dynamical system over a period of time such that an objective function is optimized.[1] It has numerous applications in both science and engineering. For example, the dynamical system might be a spacecraft with controls corresponding to rocket thrusters, and the objective might be to reach the moon with minimum fuel expenditure.[2] Or the dynamical system could be a nation's economy, with the objective to minimize unemployment; the controls in this case could be fiscal and monetary policy.[3] Optimal control is an extension of the calculus of variations, and is a mathematical optimization method for deriving control policies.[4] The method is largely due to the work of Lev Pontryagin and Richard Bellman in the 1950s, after contributions to calculus of variations by Edward J. McShane.[5] Optimal control can be seen as a control strategy in control theory.
"
Optimal_maintenance,Mathematics,3,"Optimal maintenance is the discipline within operations research concerned with maintaining a system in a manner that maximizes profit or minimizes cost. Cost functions depending on the reliability, availability and maintainability characteristics of the system of interest determine the parameters to minimize. Parameters often considered are the cost of failure, the cost per time unit of ""downtime"" (for example: revenue losses), the cost (per time unit) of corrective maintenance, the cost per time unit of preventive maintenance and the cost of repairable system replacement [Cassady and Pohl]. The foundation of any maintenance model relies on the correct description of the underlying deterioration process and failure behavior of the component, and on the relationships between maintained components in the product breakdown (system / sub-system / assembly / sub-assembly...).
 Optimal Maintenance strategies are often constructed using stochastic models and focus on finding an optimal inspection time or the optimal acceptable degree of system degradation before maintenance and/or replacement. Cost considerations on an Asset scale may also lead to select a ""run-to-failure"" approach for specific components.
 There are four main survey papers available accomplished to cover the spectrum of optimal maintenance:
"
Order_theory,Mathematics,3,"
 Order theory is a branch of mathematics which investigates the intuitive notion of order using binary relations. It provides a formal framework for describing statements such as ""this is less than that"" or ""this precedes that"". This article introduces the field and provides basic definitions. A list of order-theoretic terms can be found in the order theory glossary.
"
Ordered_geometry,Mathematics,3,"Ordered geometry is a form of geometry featuring the concept of intermediacy (or ""betweenness"") but, like projective geometry, omitting the basic notion of measurement.  Ordered geometry is a fundamental geometry forming a common framework for affine, Euclidean, absolute, and hyperbolic geometry (but not for projective geometry).
"
Oscillation_theory,Mathematics,3,"In mathematics, in the field of ordinary differential equations, a nontrivial solution to an ordinary differential equation
 is called oscillating if it has an infinite number of roots; otherwise it is called non-oscillating. The differential equation is called oscillating if it has an oscillating solution.
The number of roots carries also information on the spectrum of associated boundary value problems.
"
P-adic_analysis,Mathematics,3,"In mathematics, p-adic analysis is a branch of number theory that deals with the mathematical analysis of functions of p-adic numbers.
 The theory of complex-valued numerical functions on the p-adic numbers is part of the theory of locally compact groups. The usual meaning taken for p-adic analysis is the theory of p-adic-valued functions on spaces of interest.
 Applications of p-adic analysis have mainly been in number theory, where it has a significant role in diophantine geometry and diophantine approximation. Some applications have required the development of p-adic functional analysis and spectral theory. In many ways p-adic analysis is less subtle than classical analysis, since the ultrametric inequality means, for example, that convergence of infinite series of p-adic numbers is much simpler. Topological vector spaces over p-adic fields show distinctive features; for example aspects relating to convexity and the Hahn–Banach theorem are different.
"
P-adic_analysis,Mathematics,3,"In mathematics, p-adic analysis is a branch of number theory that deals with the mathematical analysis of functions of p-adic numbers.
 The theory of complex-valued numerical functions on the p-adic numbers is part of the theory of locally compact groups. The usual meaning taken for p-adic analysis is the theory of p-adic-valued functions on spaces of interest.
 Applications of p-adic analysis have mainly been in number theory, where it has a significant role in diophantine geometry and diophantine approximation. Some applications have required the development of p-adic functional analysis and spectral theory. In many ways p-adic analysis is less subtle than classical analysis, since the ultrametric inequality means, for example, that convergence of infinite series of p-adic numbers is much simpler. Topological vector spaces over p-adic fields show distinctive features; for example aspects relating to convexity and the Hahn–Banach theorem are different.
"
P-adic_Hodge_theory,Mathematics,3,"In mathematics, p-adic Hodge theory is a theory that provides a way to classify and study p-adic Galois representations of characteristic 0 local fields[1] with residual characteristic p (such as Qp). The theory has its beginnings in Jean-Pierre Serre and John Tate's study of Tate modules of abelian varieties and the notion of Hodge–Tate representation. Hodge–Tate representations are related to certain decompositions of p-adic cohomology theories analogous to the Hodge decomposition, hence the name p-adic Hodge theory. Further developments were inspired by properties of p-adic Galois representations arising from the étale cohomology of varieties. Jean-Marc Fontaine introduced many of the basic concepts of the field.
"
Parabolic_geometry_(disambiguation),Mathematics,3,"Parabolic geometry  may refer to:
"
Paraconsistent_mathematics,Mathematics,3,"Paraconsistent mathematics, sometimes called inconsistent mathematics, represents an attempt to develop the classical infrastructure of mathematics (e.g. analysis) based on a foundation of paraconsistent logic instead of classical logic. A number of reformulations of analysis can be developed, for example functions which both do and do not have a given value simultaneously.
 Chris Mortensen claims (see references):
"
Partition_theory,Mathematics,3,"In number theory and combinatorics, a partition of a positive integer n, also called an integer partition, is a way of writing n as a sum of positive integers. Two sums that differ only in the order of their summands are considered the same partition. (If order matters, the sum becomes a composition.)  For example, 4 can be partitioned in five distinct ways:
 The order-dependent composition 1 + 3 is the same partition as 3 + 1, while the two distinct compositions 1 + 2 + 1 and 1 + 1 + 2 represent the same partition 2 + 1 + 1.
 A summand in a partition is also called a part. The number of partitions of n is given by the partition function p(n). So p(4) = 5. The notation λ ⊢ n means that λ is a partition of n.
 Partitions can be graphically visualized with Young diagrams or Ferrers diagrams. They occur in a number of branches of mathematics and physics, including the study of symmetric polynomials and of the symmetric group and in group representation theory in general.
"
Perturbation_theory,Mathematics,3,"
In mathematics and physics, perturbation theory comprises mathematical methods for finding an approximate solution to a problem, by starting from the exact solution of a related, simpler problem. A critical feature of the technique is a middle step that breaks the problem into ""solvable"" and ""perturbative"" parts.[1] Perturbation theory is widely used when the problem at hand does not have a known exact solution, but can be expressed as a ""small"" change to a known solvable problem. Perturbation theory is used in a wide range of fields, and reaches its most sophisticated and advanced forms in quantum field theory. Perturbation theory for quantum mechanics imparts the first step on this path. The field in general remains actively and heavily researched across multiple disciplines.
"
Picard%E2%80%93Vessiot_theory,Mathematics,3,"
 In differential algebra, Picard–Vessiot theory is the study of the differential field extension generated by the solutions of a linear differential equation, using the differential Galois group of the field extension.  A major goal is to describe when the differential equation can be solved by quadratures in terms of properties of the differential Galois group. The theory was initiated by Émile Picard and Ernest Vessiot from about 1883 to 1904.
 Kolchin (1973) and van der Put & Singer (2003) give detailed accounts of Picard–Vessiot theory.
"
Plane_geometry,Mathematics,3,"Euclidean geometry is a mathematical system attributed to Alexandrian Greek mathematician Euclid, which he described in his textbook on geometry: the Elements. Euclid's method consists in assuming a small set of intuitively appealing axioms, and deducing many other propositions (theorems) from these. Although many of Euclid's results had been stated by earlier mathematicians,[1] Euclid was the first to show how these propositions could fit into a comprehensive deductive and logical system.[2] The Elements begins with plane geometry, still taught in secondary school (high school) as the first axiomatic system and the first examples of formal proof. It goes on to the solid geometry of three dimensions. Much of the Elements states results of what are now called algebra and number theory, explained in geometrical language.[1] For more than two thousand years, the adjective ""Euclidean"" was unnecessary because no other sort of geometry had been conceived. Euclid's axioms seemed so intuitively obvious (with the possible exception of the parallel postulate) that any theorem proved from them was deemed true in an absolute, often metaphysical, sense. Today, however, many other self-consistent non-Euclidean geometries are known, the first ones having been discovered in the early 19th century. An implication of Albert Einstein's theory of general relativity is that physical space itself is not Euclidean, and Euclidean space is a good approximation for it only over short distances (relative to the strength of the  gravitational field).[3] Euclidean geometry is an example of synthetic geometry, in that it proceeds logically from axioms describing basic properties of geometric objects such as points and lines, to propositions about those objects, all without the use of coordinates to specify those objects. This is in contrast to analytic geometry, which uses coordinates to translate geometric propositions into algebraic formulas.
"
Point-set_topology,Mathematics,3,"In mathematics, general topology is the branch of topology that deals with the basic set-theoretic definitions and constructions used in topology. It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology. Another name for general topology is point-set topology.
 The fundamental concepts in point-set topology are continuity, compactness, and connectedness: 
 The words 'nearby', 'arbitrarily small', and 'far apart' can all be made precise by using the concept of open sets. If we change the definition of 'open set', we change what continuous functions, compact sets, and connected sets are. Each choice of definition for 'open set' is called a topology. A set with a topology is called a topological space.
 Metric spaces are an important class of topological spaces where a real, non-negative distance, also called a metric, can be defined on pairs of points in the set. Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.
"
Pointless_topology,Mathematics,3,"In mathematics, pointless topology (also called point-free or pointfree topology, or locale theory) is an approach to topology that avoids mentioning points.
"
Poisson_geometry,Mathematics,3,"In geometry, a Poisson structure on a smooth manifold 



M


{  M}
 is a Lie bracket 



{
⋅
,
⋅
}


{  \{\cdot ,\cdot \}}
 (called a Poisson bracket in this special case) on the algebra 





C

∞



(
M
)


{  {C^{\infty }}(M)}
 of smooth functions on 



M


{  M}
, subject to the Leibniz rule
 Said in another manner, it is a Lie algebra structure on the vector space of smooth functions on 



M


{  M}
 such that 




X

f






=


df




{
f
,
⋅
}
:


C

∞



(
M
)
→


C

∞



(
M
)


{  X_{f}{\stackrel {\text{df}}{=}}\{f,\cdot \}:{C^{\infty }}(M)\to {C^{\infty }}(M)}
 is a vector field for each smooth function 



f


{  f}
, which we call the Hamiltonian vector field associated to 



f


{  f}
. These vector fields span a  completely integrable singular foliation, each of whose maximal integral sub-manifolds inherits a symplectic structure. One may thus informally view a Poisson structure on a smooth manifold as a smooth partition of the ambient manifold into even-dimensional symplectic leaves, which are not necessarily of the same dimension.
 Poisson structures are one instance of Jacobi structures introduced by André Lichnerowicz in 1977.[1] They were further studied in the classical paper of Alan Weinstein,[2] where many basic structure theorems were first proved, and which exerted a huge influence on the development of Poisson geometry — which today is deeply entangled with non-commutative geometry, integrable systems, topological field theories and representation theory, to name a few.
"
Polyhedral_combinatorics,Mathematics,3,"Polyhedral combinatorics is a branch of mathematics, within combinatorics and discrete geometry, that studies the problems of counting and describing the faces of convex polyhedra and higher-dimensional convex polytopes.
 Research in polyhedral combinatorics falls into two distinct areas. Mathematicians in this area study the combinatorics of polytopes; for instance, they seek inequalities that describe the relations between the numbers of vertices, edges, and faces of higher dimensions in arbitrary polytopes or in certain important subclasses of polytopes, and study other combinatorial properties of polytopes such as their connectivity and diameter (number of steps needed to reach any vertex from any other vertex). Additionally, many computer scientists use the phrase “polyhedral combinatorics” to describe research into precise descriptions of the faces of certain specific polytopes (especially 0-1 polytopes, whose vertices are subsets of a hypercube) arising from integer programming problems.
"
Possibility_theory,Mathematics,3,"Possibility theory is a mathematical theory for dealing with certain types of uncertainty and is an alternative to probability theory. It uses measures of possibility and necessity between 0 and 1, ranging from impossible to possible and unnecessary to necessary, respectively. Professor Lotfi Zadeh first introduced possibility theory in 1978 as an extension of his theory of fuzzy sets and fuzzy logic. Didier Dubois and Henri Prade further contributed to its development. Earlier in the 1950s, economist G. L. S. Shackle proposed the min/max algebra to describe degrees of potential surprise.
"
Potential_theory,Mathematics,3,"In mathematics and mathematical physics, potential theory is the study of harmonic functions.
 The term ""potential theory"" was coined in 19th-century physics when it was realized that two fundamental forces of nature known at the time, namely gravity and the electrostatic force, could be modeled using functions called the gravitational potential and electrostatic potential, both of which satisfy Poisson's equation—or in the vacuum, Laplace's equation.
 There is considerable overlap between potential theory and the theory of Poisson's equation to the extent that it is impossible to draw a distinction between these two fields. The difference is more one of emphasis than subject matter and rests on the following distinction: potential theory focuses on the properties of the functions as opposed to the properties of the equation. For example, a result about the singularities of harmonic functions would be said to belong to potential theory whilst a result on how the solution depends on the boundary data would be said to belong to the theory of the Laplace equation. This is not a hard and fast distinction, and in practice there is considerable overlap between the two fields, with methods and results from one being used in the other.
 Modern potential theory is also intimately connected with probability and the theory of Markov chains. In the continuous case, this is closely related to analytic theory. In the finite state space case, this connection can be introduced by introducing an electrical network on the state space, with resistance between points inversely proportional to transition probabilities and densities proportional to potentials. Even in the finite case, the analogue I-K of the Laplacian in potential theory has its own maximum principle, uniqueness principle, balance principle, and others.
"
Precalculus,Mathematics,3,"In mathematics education, precalculus or college algebra is a course, or a set of courses, that includes algebra and trigonometry at a  level which is designed to prepare students for the study of calculus. Schools often distinguish between algebra and trigonometry as two separate parts of the coursework.[1]"
Predicative_mathematics,Mathematics,3,"In mathematics, logic and philosophy of mathematics, something that is impredicative is a self-referencing definition. Roughly speaking, a definition is impredicative if it invokes (mentions or quantifies over) the set being defined, or (more commonly) another set that contains the thing being defined. There is no generally accepted precise definition of what it means to be predicative or impredicative. Authors have given different but related definitions.
 The opposite of impredicativity is predicativity, which essentially entails building stratified (or ramified) theories where quantification over lower levels results in variables of some new type, distinguished from the lower types that the variable ranges over. A prototypical example is intuitionistic type theory, which retains ramification so as to discard impredicativity.
 Russell's paradox is a famous example of an impredicative construction—namely the set of all sets that do not contain themselves. The paradox is that such a set cannot exist: If it would exist, the question could be  asked whether it contains itself or not — if it does then by definition it should not, and if it does not then by definition it should.
 The greatest lower bound of a set X, glb(X), also has an impredicative definition: y = glb(X) if and only if for all elements x of X, y is less than or equal to x, and any z less than or equal to all elements of X is less than or equal to y. This definition quantifies over the set (potentially infinite, depending on the order in question) whose members are the lower bounds of X, one of which being the glb itself. Hence predicativism would reject this definition.[1]"
Probability_theory,Mathematics,3,"Probability theory is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of these outcomes is called an event.
Central subjects in probability theory include discrete and continuous random variables, probability distributions, and stochastic processes, which provide mathematical abstractions of non-deterministic or uncertain processes or measured quantities that may either be single occurrences or evolve over time in a random fashion.
Although it is not possible to perfectly predict random events, much can be said about their behavior. Two major results in probability theory describing such behaviour are the law of large numbers and the central limit theorem.
 As a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of data.[1] Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics. A great discovery of twentieth-century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics.[2]"
Probabilistic_combinatorics,Mathematics,3,"The probabilistic method is a nonconstructive method, primarily used in combinatorics and pioneered by Paul Erdős, for proving the existence of a prescribed kind of mathematical object. It works by showing that if one randomly chooses objects from a specified class, the probability that the result is of the prescribed kind is strictly greater than zero. Although the proof uses probability, the final conclusion is determined for certain, without any possible error.
 This method has now been applied to other areas of mathematics such as number theory, linear algebra, and real analysis, as well as in computer science (e.g. randomized rounding), and information theory.
"
Probabilistic_graph_theory,Mathematics,3,"In mathematics, random graph is the general term to refer to probability distributions over graphs.  Random graphs may be described simply by a probability distribution, or by a random process which generates them.[1][2] The theory of random graphs lies at the intersection between graph theory and probability theory. From a mathematical perspective, random graphs are used to answer questions about the properties of typical graphs. Its practical applications are found in all areas in which complex networks need to be modeled – many random graph models are thus known, mirroring the diverse types of complex networks encountered in different areas. In a mathematical context, random graph refers almost exclusively to the Erdős–Rényi random graph model. In other contexts, any graph model may be referred to as a random graph.
"
Probabilistic_number_theory,Mathematics,3,"In mathematics, Probabilistic number theory is a subfield of number theory, which explicitly uses probability to answer questions about the integers and integer-valued functions. One basic idea underlying it is that different prime numbers are, in some serious sense, like independent random variables. This however is not an idea that has a unique useful formal expression.
 The founders of the theory were Paul Erdős, Aurel Wintner and Mark Kac during the 1930s, one of the periods of investigation in analytic number theory. Foundational results include the Erdős–Wintner theorem and the Erdős–Kac theorem on additive functions.
"
Projective_geometry,Mathematics,3,"
 
 In mathematics, projective geometry is the study of geometric properties that are invariant with respect to projective transformations. This means that, compared to elementary Euclidean geometry, projective geometry has a different setting, projective space, and a selective set of basic geometric concepts. The basic intuitions are that projective space has more points than Euclidean space, for a given dimension, and that geometric transformations are permitted that transform the extra points (called ""points at infinity"") to Euclidean points, and vice-versa.
 Properties meaningful for projective geometry are respected by this new idea of transformation, which is more radical in its effects than can be expressed by a transformation matrix and translations (the affine transformations). The first issue for geometers is what kind of geometry is adequate for a novel situation. It is not possible to refer to angles in projective geometry as it is in Euclidean geometry, because angle is an example of a concept not invariant with respect to projective transformations, as is seen in perspective drawing. One source for projective geometry was indeed the theory of perspective. Another difference from elementary geometry is the way in which parallel lines can be said to meet in a point at infinity, once the concept is translated into projective geometry's terms. Again this notion has an intuitive basis, such as railway tracks meeting at the horizon in a perspective drawing. See projective plane for the basics of projective geometry in two dimensions.
 While the ideas were available earlier, projective geometry was mainly a development of the 19th century. This included the theory of complex projective space, the coordinates used (homogeneous coordinates) being complex numbers. Several major types of more abstract mathematics (including invariant theory, the Italian school of algebraic geometry, and Felix Klein's Erlangen programme resulting in the study of the classical groups) were based on projective geometry. It was also a subject with many practitioners for its own sake, as synthetic geometry. Another topic that developed from axiomatic studies of projective geometry is finite geometry.
 The topic of projective geometry is itself now divided into many research subtopics, two examples of which are projective algebraic geometry (the study of projective varieties) and projective differential geometry (the study of differential invariants of the projective transformations).
"
Projective_differential_geometry,Mathematics,3,"In mathematics, projective differential geometry is the study of differential geometry, from the point of view of properties of mathematical objects such as functions, diffeomorphisms, and submanifolds, that are invariant under transformations of the projective group. This is a mixture of the approaches from Riemannian geometry of studying invariances, and of the Erlangen program of characterizing geometries according to their group symmetries. 
 The area was much studied by mathematicians from around 1890 for a generation (by J. G. Darboux, George Henri Halphen, Ernest Julius Wilczynski, E. Bompiani, G. Fubini, Eduard Čech, amongst others), without a comprehensive theory of differential invariants emerging. Élie Cartan formulated the idea of a general projective connection, as part of his method of moving frames; abstractly speaking, this is the level of generality at which the Erlangen program can be reconciled with differential geometry, while it also develops the oldest part of the theory (for the projective line), namely the Schwarzian derivative, the simplest projective differential invariant.[1] Further work from the 1930s onwards was carried out by J. Kanitani, Shiing-Shen Chern, A. P. Norden, G. Bol, S. P. Finikov and G. F. Laptev. Even the basic results on osculation of curves, a manifestly projective-invariant topic, lack any comprehensive theory. The ideas of projective differential geometry recur in mathematics and its applications, but the formulations given are still rooted in the language of the early twentieth century.
"
Proof_theory,Mathematics,3,"Proof theory is a major branch[1] of mathematical logic that represents proofs as formal mathematical objects, facilitating their analysis by mathematical techniques.  Proofs are typically presented as inductively-defined data structures such as plain lists, boxed lists, or trees, which are constructed according to the axioms and rules of inference of the logical system.  As such, proof theory is syntactic in nature, in contrast to model theory, which is semantic in nature.
 Some of the major areas of proof theory include structural proof theory, ordinal analysis, provability logic, reverse mathematics, proof mining, automated theorem proving, and proof complexity. Much research also focuses on applications in computer science, linguistics, and philosophy.
"
Pseudo-Riemannian_geometry,Mathematics,3,"
 In differential geometry, a pseudo-Riemannian manifold,[1][2] also called a semi-Riemannian manifold, is a differentiable manifold with a metric tensor that is everywhere nondegenerate.  This is a generalization of a Riemannian manifold in which the requirement of positive-definiteness is relaxed. 
 Every tangent space of a pseudo-Riemannian manifold is a pseudo-Euclidean vector space.
 A special case used in general relativity is a four-dimensional Lorentzian manifold for modeling spacetime, where tangent vectors can be classified as timelike, null, and spacelike.
"
Pure_mathematics,Mathematics,3,"Pure mathematics is the study of mathematical concepts independently of any application outside mathematics. These concepts may originate in real-world concerns, and the results obtained may later turn out to be useful for practical applications, but pure mathematicians are not primarily motivated by such applications. Instead, the appeal is attributed to the intellectual challenge and aesthetic beauty of working out the logical consequences of basic principles.
 While pure mathematics has existed as an activity since at least Ancient Greece, the concept was elaborated upon around the year 1900,[1] after the introduction of theories with counter-intuitive properties (such as non-Euclidean geometries and Cantor's theory of infinite sets), and the discovery of apparent paradoxes (such as continuous functions that are nowhere differentiable, and Russell's paradox). This introduced the need to renew the concept of mathematical rigor and rewrite all mathematics accordingly, with a systematic use of axiomatic methods. This led many mathematicians to focus on mathematics for its own sake, that is, pure mathematics.
 Nevertheless, almost all mathematical theories remained motivated by problems coming from the real world or from less abstract mathematical theories. Also, many mathematical theories, which had seemed to be totally pure mathematics, were eventually used in applied areas, mainly physics and computer science. A famous early example is Isaac Newton's demonstration that his law of universal gravitation implied that planets move in orbits that are conic sections, geometrical curves that had been studied in antiquity by Apollonius. Another example is the problem of factoring large integers, which is the basis of the RSA cryptosystem, widely used to secure internet communications.[2] It follows that, presently, the distinction between pure and applied mathematics is more a philosophical point of view or a mathematician's preference than a rigid subdivision of mathematics. In particular, it is not uncommon that some members of a department of applied mathematics describe themselves as pure mathematicians.
"
Quantum_calculus,Mathematics,3,"Quantum calculus, sometimes called calculus without limits, is equivalent to traditional infinitesimal calculus without the notion of limits. It defines ""q-calculus"" and ""h-calculus"", where h ostensibly stands for Planck's constant while q stands for quantum. The two parameters are related by the formula
 where 




ℏ
=


h

2
π







{  \scriptstyle \hbar ={\frac {h}{2\pi }}\,}
 is the reduced Planck constant.
"
Quantum_geometry,Mathematics,3,"In theoretical physics, quantum geometry is the set of mathematical concepts generalizing the concepts of geometry whose understanding is necessary to describe the physical phenomena at distance scales comparable to the Planck length. At these distances, quantum mechanics has a profound effect on physical phenomena.
"
Quaternion,Mathematics,3,"In mathematics, the quaternions are a number system that extends the complex numbers. They were first described by Irish mathematician William Rowan Hamilton in 1843[1][2] and applied to mechanics in three-dimensional space. A feature of quaternions is that multiplication of two quaternions is noncommutative. Hamilton defined a quaternion as the quotient of two directed lines in a three-dimensional space[3] or equivalently as the quotient of two vectors.[4] Quaternions are generally represented in the form:
 where a, b, c, and d are real numbers, and  i, j, and k are the fundamental quaternion units.
 Quaternions are used in pure mathematics, and also have practical uses in applied mathematics—in particular for calculations involving three-dimensional rotations such as in three-dimensional computer graphics, computer vision, and crystallographic texture analysis.[5] In practical applications, they can be used alongside other methods, such as Euler angles and rotation matrices, or as an alternative to them, depending on the application.
 In modern mathematical language, quaternions form a four-dimensional associative normed division algebra over the real numbers, and therefore also a domain. In fact, the quaternions were the first noncommutative division algebra to be discovered. The algebra of quaternions is often denoted by H (for Hamilton), or in blackboard bold by 




H



{  \mathbb {H} }
 (Unicode U+210D, ℍ). It can also be given by the Clifford algebra classifications Cl0,2(ℝ) ≅ Cl+3,0(ℝ). The algebra ℍ holds a special place in analysis since, according to the Frobenius theorem, it is one of only two finite-dimensional division rings containing the real numbers as a proper subring, the other being the complex numbers. These rings are also Euclidean Hurwitz algebras, of which quaternions are the largest associative algebra. Further extending the quaternions yields the non-associative octonions, which is the last normed division algebra over the reals (the extension of the octonions, sedenions, has zero divisors and so cannot be a normed division algebra).[6] The unit quaternions can be thought of as a choice of a group structure on the 3-sphere S3 that gives the group Spin(3), which is isomorphic to SU(2) and also to the universal cover of SO(3).
"
Ramsey_theory,Mathematics,3,"Ramsey theory, named after the British mathematician and philosopher Frank P. Ramsey, is a branch of mathematics that focuses on the appearance of order in a substructure given a structure of a known size. Problems in Ramsey theory typically ask a question of the form: ""how big must some substructure be to guarantee that a particular property holds?""  More specifically, Ron Graham described Ramsey theory as a ""branch of combinatorics"".[1]"
Rational_geometry,Mathematics,3,"Rational trigonometry is a proposed reformulation of metrical planar and solid geometries (which includes trigonometry) by Canadian mathematician Norman J. Wildberger, currently a professor of mathematics at the University of New South Wales. His ideas are set out in his 2005 book Divine Proportions: Rational Trigonometry to Universal Geometry.[1] According to New Scientist, part of his motivation for an alternative to traditional trigonometry was to avoid some problems that he claims occur when infinite series are used in mathematics.  Rational trigonometry avoids direct use of transcendental functions like sine and cosine by substituting their squared equivalents.[2] Wildberger draws inspiration from mathematicians predating Georg Cantor's infinite set-theory, like Gauss and Euclid, who he claims were far more wary of using infinite sets than modern mathematicians.[2][nb 1] To date, rational trigonometry is largely unmentioned in mainstream mathematical literature.
"
Rational_trigonometry,Mathematics,3,"Rational trigonometry is a proposed reformulation of metrical planar and solid geometries (which includes trigonometry) by Canadian mathematician Norman J. Wildberger, currently a professor of mathematics at the University of New South Wales. His ideas are set out in his 2005 book Divine Proportions: Rational Trigonometry to Universal Geometry.[1] According to New Scientist, part of his motivation for an alternative to traditional trigonometry was to avoid some problems that he claims occur when infinite series are used in mathematics.  Rational trigonometry avoids direct use of transcendental functions like sine and cosine by substituting their squared equivalents.[2] Wildberger draws inspiration from mathematicians predating Georg Cantor's infinite set-theory, like Gauss and Euclid, who he claims were far more wary of using infinite sets than modern mathematicians.[2][nb 1] To date, rational trigonometry is largely unmentioned in mainstream mathematical literature.
"
Real_algebraic_geometry,Mathematics,3,"In mathematics, real algebraic geometry is the sub-branch of algebraic geometry studying real algebraic sets, i.e. real-number solutions to algebraic equations with real-number coefficients, and mappings between them (in particular real polynomial mappings).
 Semialgebraic geometry is the study of semialgebraic sets, i.e. real-number solutions to algebraic inequalities with-real number coefficients, and mappings between them. The most natural mappings between semialgebraic sets are semialgebraic mappings, i.e., mappings whose graphs are semialgebraic sets.
"
Real_algebraic_geometry,Mathematics,3,"In mathematics, real algebraic geometry is the sub-branch of algebraic geometry studying real algebraic sets, i.e. real-number solutions to algebraic equations with real-number coefficients, and mappings between them (in particular real polynomial mappings).
 Semialgebraic geometry is the study of semialgebraic sets, i.e. real-number solutions to algebraic inequalities with-real number coefficients, and mappings between them. The most natural mappings between semialgebraic sets are semialgebraic mappings, i.e., mappings whose graphs are semialgebraic sets.
"
Real_analysis,Mathematics,3,"In mathematics, real analysis is the branch of mathematical analysis that studies the behavior of real numbers, sequences and series of real numbers, and real functions.[1] Some particular properties of real-valued sequences and functions that real analysis studies include convergence, limits, continuity, smoothness, differentiability and integrability.
 Real analysis is distinguished from complex analysis, which deals with the study of complex numbers and their functions.
"
Real_K-theory,Mathematics,3,"In mathematics, K-theory is, roughly speaking, the study of a ring generated by vector bundles over a topological space or scheme. In algebraic topology, it is a cohomology theory known as topological K-theory. In algebra and algebraic geometry, it is referred to as algebraic K-theory. It is also a fundamental tool in the field of operator algebras. It can be seen as the study of certain kinds of invariants of large matrices.[1] K-theory involves the construction of families of K-functors that map from topological spaces or schemes to associated rings; these rings reflect some aspects of the structure of the original spaces or schemes. As with functors to groups in algebraic topology, the reason for this functorial mapping is that it is easier to compute some topological properties from the mapped rings than from the original spaces or schemes. Examples of results gleaned from the K-theory approach include the Grothendieck–Riemann–Roch theorem, Bott periodicity, the Atiyah–Singer index theorem, and the Adams operations.
 In high energy physics, K-theory and in particular twisted K-theory have appeared in Type II string theory where it has been conjectured that they classify D-branes, Ramond–Ramond field strengths and also certain spinors on generalized complex manifolds. In condensed matter physics K-theory has been used to classify topological insulators, superconductors and stable Fermi surfaces. For more details, see K-theory (physics).
"
Recreational_mathematics,Mathematics,3,"Recreational mathematics is mathematics carried out for recreation (entertainment) rather than as a strictly research and application-based professional activity or as a part of a student's formal education. Although it is not necessarily limited to being an endeavor for amateurs, many topics in this field require no knowledge of advanced mathematics. Recreational mathematics involves mathematical puzzles and games, often appealing to children and untrained adults, inspiring their further study of the subject.[1] The Mathematical Association of America (MAA) includes Recreational Mathematics as one of its seventeen Special Interest Groups, commenting:
 Recreational mathematics is not easily defined because it is more than mathematics done as a diversion or playing games that involve mathematics. Recreational mathematics is inspired by deep ideas that are hidden in puzzles, games, and other forms of play. The aim of the SIGMAA on Recreational Mathematics (SIGMAA-Rec) is to bring together enthusiasts and researchers in the myriad of topics that fall under recreational math. We will share results and ideas from our work, show that real, deep mathematics is there awaiting those who look, and welcome those who wish to become involved in this branch of mathematics.[2] Mathematical competitions (such as those sponsored by mathematical associations) are also categorized under recreational mathematics.
"
Recursion_theory,Mathematics,3,"Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability[disambiguation needed]. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory.
 Basic questions addressed by recursion theory include:
 Although there is considerable overlap in terms of knowledge and methods, mathematical recursion theorists study the theory of relative computability, reducibility notions, and degree structures; those in the computer science field focus on the theory of subrecursive hierarchies, formal methods, and formal languages.
"
Representation_theory,Mathematics,3,"Representation theory is a branch of mathematics that studies abstract algebraic structures by representing their elements as linear transformations of vector spaces,[1] and studies modules over these abstract algebraic structures.[2][3] In essence, a representation makes an abstract algebraic object more concrete by describing its elements by matrices and their algebraic operations (for example, matrix addition, matrix multiplication). The theory of matrices and linear operators is well-understood, so representations of more abstract objects in terms of familiar linear algebra objects helps glean properties and sometimes simplify calculations on more abstract theories.
 The algebraic objects amenable to such a description include groups, associative algebras and Lie algebras. The most prominent of these (and historically the first) is the representation theory of groups, in which elements of a group are represented by invertible matrices in such a way that the group operation is matrix multiplication.[4][5] Representation theory is a useful method because it reduces problems in abstract algebra to problems in linear algebra, a subject that is well understood.[6] Furthermore, the vector space on which a group (for example) is represented can be infinite-dimensional, and by allowing it to be, for instance, a Hilbert space, methods of analysis can be applied to the theory of groups.[7][8] Representation theory is also important in physics because, for example, it describes how the symmetry group of a physical system affects the solutions of equations describing that system.[9] Representation theory is pervasive across fields of mathematics for two reasons. First, the applications of representation theory are diverse:[10] in addition to its impact on algebra, representation theory:
 Second, there are diverse approaches to representation theory. The same objects can be studied using methods from algebraic geometry, module theory, analytic number theory, differential geometry, operator theory, algebraic combinatorics and topology.[14] The success of representation theory has led to numerous generalizations. One of the most general is in category theory.[15] The algebraic objects to which representation theory applies can be viewed as particular kinds of categories, and the representations as functors from the object category to the category of vector spaces.[5] This description points to two obvious generalizations: first, the algebraic objects can be replaced by more general categories; second, the target category of vector spaces can be replaced by other well-understood categories.
"
Representation_theory_of_algebras,Mathematics,3,"In abstract algebra, a representation of an associative algebra is a module for that algebra. Here an associative algebra is a (not necessarily unital) ring. If the algebra is not unital, it may be made so in a standard way (see the adjoint functors page); there is no essential difference between modules for the resulting unital ring, in which the identity acts by the identity mapping, and representations of the algebra.
"
Representation_theory_of_diffeomorphism_groups,Mathematics,3,"In mathematics, a source for the representation theory of the group of diffeomorphisms of a smooth manifold M is the initial observation that (for M connected) that group acts transitively on M.
"
Group_representation,Mathematics,3,"In the mathematical field of representation theory, group representations describe abstract groups in terms of bijective linear transformations (i.e. automorphisms) of vector spaces; in particular, they can be used to represent group elements as invertible matrices so that the group operation can be represented by matrix multiplication. Representations of groups are important because they allow many group-theoretic problems to be reduced to problems in linear algebra, which is well understood. They are also important in physics because, for example, they describe how the symmetry group of a physical system affects the solutions of equations describing that system.
 The term representation of a group is also used in a more general sense to mean any ""description"" of a group as a group of transformations of some mathematical object. More formally, a ""representation"" means a homomorphism from the group to the automorphism group of an object. If the object is a vector space we have a linear representation. Some people use realization for the general notion and reserve the term representation for the special case of linear representations. The bulk of this article describes linear representation theory; see the last section for generalizations.
"
Representation_theory_of_Hopf_algebras,Mathematics,3,"In abstract algebra, a representation of a Hopf algebra is a representation of its underlying associative algebra. That is, a representation of a Hopf algebra H over a field K is a K-vector space V with an action H × V → V usually denoted by juxtaposition ( that is, the image of (h,v) is written hv ). The vector space V is called an H-module.
"
Representation_theory_of_Lie_algebras,Mathematics,3,"In the mathematical field of representation theory, a Lie algebra representation or representation of a Lie algebra is a way of writing a Lie algebra as a set of matrices (or endomorphisms of a vector space) in such a way that the Lie bracket is given by the commutator. In the language of physics, one looks for a vector space 



V


{  V}
 together with a collection of operators on 



V


{  V}
 satisfying some fixed set of commutation relations, such as the relations satisfied by the angular momentum operators.
 The notion is closely related to that of a representation of a Lie group. Roughly speaking, the representations of Lie algebras are the differentiated form of representations of Lie groups, while the representations of the universal cover of a Lie group are the integrated form of the representations of its Lie algebra.
 In the study of representations of a Lie algebra, a particular ring, called the universal enveloping algebra, associated with the Lie algebra plays an important role. The universality of this ring says that the category of representations of a Lie algebra is the same as the category of modules over its enveloping algebra.
"
Representation_theory_of_Lie_groups,Mathematics,3,"In mathematics and theoretical physics, a representation of a Lie group is a linear action of a Lie group on a vector space. Equivalently, a representation is a smooth homomorphism of the group into the group of invertible operators on the vector space. Representations play an important role in the study of continuous symmetry. A great deal is known about such representations, a basic tool in their study being the use of the corresponding 'infinitesimal' representations of Lie algebras.
"
Representation_theory_of_the_Galilean_group,Mathematics,3,"In nonrelativistic quantum mechanics, an account can be given of the existence of mass and spin (normally explained in Wigner's classification of relativistic mechanics)  in terms of the representation theory of the Galilean group, which is the spacetime symmetry group of nonrelativistic quantum mechanics.
 In 3 + 1 dimensions, this is the subgroup of the affine group on (t, x, y, z), whose linear part leaves invariant both the metric (gμν = diag(1, 0, 0, 0)) and the (independent) dual metric (gμν = diag(0, 1, 1, 1)). A similar definition applies for n + 1 dimensions.
 We are interested in projective representations of this group, which are equivalent to unitary representations of the nontrivial central extension of the universal covering group of the Galilean group by the one-dimensional Lie group R, cf. the article Galilean group for the central extension of its Lie algebra. The method of induced representations will be used to survey these.
 We focus on the (centrally extended, Bargmann) Lie algebra here, because it is simpler to analyze and we can always extend the results to the full Lie group through the Frobenius theorem.
 E is the generator of time translations (Hamiltonian), Pi is the generator of translations (momentum operator), Ci is the generator of Galilean boosts, and Lij stands for a generator of rotations (angular momentum operator). The central charge M is a Casimir invariant.
 The mass-shell invariant
 is an additional Casimir invariant.
 In 3 + 1 dimensions, a third Casimir invariant is W2,   where
 somewhat analogous to the Pauli–Lubanski pseudovector of relativistic mechanics.
 More generally, in n + 1 dimensions, invariants will be a function of 
 and 
 as well as of the above mass-shell invariant and central charge.
 Using Schur's lemma, in an irreducible unitary representation,  all these Casimir invariants are multiples of the identity. Call these coefficients m and mE0 and (in the case of 3 + 1 dimensions) w, respectively. Recalling that we are considering unitary representations here, we see that these eigenvalues have to be real numbers.
 Thus,  m > 0, m = 0 and m < 0. (The last case is similar to the first.)  In 3 + 1 dimensions, when In m > 0, we can write, w = ms  for the third invariant, where s  represents the spin, or intrinsic angular momentum. More generally, in n + 1 dimensions, the generators L and C will be related, respectively, to the total angular momentum and center-of-mass moment by
 From a purely representation-theoretic point of view, one would have to study all of the representations;  but, here,  we are only interested in applications to quantum mechanics. There, E represents the energy, which has to be bounded below, if thermodynamic stability is required. Consider first the case where m is nonzero.
 Considering the (E, P→) space with the constraint
 we see that the Galilean boosts act transitively on this hypersurface. In fact, treating the energy E as the Hamiltonian, differentiating with respect to P, and applying Hamilton's equations, we obtain the mass-velocity relation m v→ = P→.
 The hypersurface is parametrized by this velocity In v→. Consider the stabilizer of a point on the orbit, (E0, 0),  where the velocity is 0. Because of transitivity, we know the unitary irrep contains a nontrivial linear subspace with these energy-momentum eigenvalues. (This subspace only exists in a rigged Hilbert space, because the momentum spectrum is continuous.)
 The subspace is spanned by E,  P→, M and Lij. We already know how the subspace of the irrep transforms under all operators but the angular momentum. Note that the rotation subgroup is Spin(3). We have to look at its double cover, because we are considering projective representations. This is called the little group, a name given by Eugene Wigner. His method of induced representations specifies that the irrep is given by the direct sum of all the fibers in a vector bundle over the mE = mE0 + P2/2 hypersurface, whose fibers are a unitary irrep of Spin(3).
 Spin(3) is none other than SU(2). (See representation theory of SU(2), where it is shown that the unitary irreps of SU(2) are labeled by s, a non-negative integer multiple of one half. This is called spin,  for historical reasons.)
 is nonpositive. Suppose it is zero. Here, it is also the boosts as well as the rotations that constitute the little group. Any unitary irrep of this little group also gives rise to a projective irrep of the Galilean group. As far as we can tell, only the case which transforms trivially under the little group has any physical interpretation, and it corresponds to the no-particle state, the vacuum.
 The case where the invariant is negative requires additional comment. This corresponds to the representation class for m = 0 and non-zero P→.  Extending the bradyon, luxon, tachyon classification from the representation theory of the Poincaré group to an analogous classification, here, one may term these states as synchrons. They represent an instantaneous transfer of non-zero momentum across a (possibly large) distance. Associated with them, by above, is a ""time"" operator
 which may be identified with the time of transfer. These states are naturally interpreted as the carriers of instantaneous action-at-a-distance forces.
 N.B. In the 3 + 1-dimensional Galilei group, the boost generator may be decomposed into
 with W→ playing a role analogous to helicity.
"
Representation_theory_of_the_Lorentz_group,Mathematics,3,"
 The Lorentz group is a Lie group of symmetries of the spacetime of special relativity.  This group can be realized as a collection of matrices, linear transformations, or unitary operators on some Hilbert space; it has a variety of representations.[nb 1]  This group is significant because special relativity together with quantum mechanics are the two physical theories that are most thoroughly established,[nb 2] and the conjunction of these two theories is the study of the infinite-dimensional unitary representations of the Lorentz group.  These have both historical importance in mainstream physics, as well as connections to more speculative present-day theories.
"
Representation_theory_of_the_Poincar%C3%A9_group,Mathematics,3,"In mathematics, the representation theory of the Poincaré group is an example of the representation theory of a Lie group that is neither a compact group nor a semisimple group. It is fundamental in theoretical physics.
 In a physical theory having Minkowski space as the underlying spacetime, the space of physical states is typically a representation of the Poincaré group. (More generally, it may be a projective representation, which amounts to a representation of the double cover of the group.)
 In a classical field theory, the physical states are sections of a Poincaré-equivariant vector bundle over Minkowski space. The equivariance condition means that the group acts on the total space of the vector bundle, and the projection to Minkowski space is an equivariant map. Therefore, the Poincaré group also acts on the space of sections. Representations arising in this way (and their subquotients) are called covariant field representations, and are not usually unitary.
 For a discussion of such unitary representations, see Wigner's classification.
 In quantum mechanics, the state of the system is determined by the Schrödinger equation, which is invariant under Galilean transformations. Quantum field theory is the relativistic extension of quantum mechanics, where relativistic (Lorentz/Poincaré invariant) wave equations are solved, ""quantized"", and act on a Hilbert space composed of Fock states; eigenstates of the theory's Hamiltonian which are states with a definite number of particles with individual 4-momentum.  
 There are no finite unitary representations of the full Lorentz (and thus Poincaré) transformations due to the non-compact nature of Lorentz boosts (rotations in Minkowski space along a space and time axis). However, there are finite non-unitary indecomposible representations of the Poincaré algebra, which may be used for modelling of unstable particles.[1][2] In case of spin 1/2 particles, it is possible to find a construction that includes both a finite-dimensional representation and a scalar product preserved by this representation by associating a 4-component Dirac spinor 



ψ


{  \psi }
 with each particle. These spinors transform under Lorentz transformations generated by the gamma matrices (




γ

μ




{  \gamma _{\mu }}
). It can be shown that the scalar product
 is preserved. It is not, however, positive definite, so the representation is not unitary.
"
Representation_theory_of_the_symmetric_group,Mathematics,3,"In mathematics, the representation theory of the symmetric group is a particular case of the representation theory of finite groups, for which a concrete and detailed theory can be obtained. This has a large area of potential applications, from symmetric function theory to problems of quantum mechanics for a number of identical particles.
 The symmetric group Sn has order n!. Its conjugacy classes are labeled by partitions of n. Therefore according to the representation theory of a finite group, the number of inequivalent irreducible representations, over the complex numbers, is equal to the number of partitions of n. Unlike the general situation for finite groups, there is in fact a natural way to parametrize irreducible representations by the same set that parametrizes conjugacy classes, namely by partitions of n or equivalently Young diagrams of size n.
 Each such irreducible representation can in fact be realized over the integers (every permutation acting by a matrix with integer coefficients); it can be explicitly constructed by computing the Young symmetrizers acting on a space generated by the Young tableaux of shape given by the Young diagram. The dimension 




d

λ




{  d_{\lambda }}
 of the representation that corresponds to the Young diagram 



λ


{  \lambda }
 is given by the hook length formula.
 To each irreducible representation ρ we can associate an irreducible character, χρ.
To compute χρ(π) where π is a permutation, one can use the combinatorial Murnaghan–Nakayama rule
.[1] Note that χρ is constant on conjugacy classes,
that is, χρ(π) = χρ(σ−1πσ) for all permutations σ.
 Over other fields the situation can become much more complicated.  If the field K has characteristic equal to zero or greater than n then by Maschke's theorem the group algebra KSn is semisimple. In these cases the irreducible representations defined over the integers give the complete set of irreducible representations (after reduction modulo the characteristic if necessary).
 However, the irreducible representations of the symmetric group are not known in arbitrary characteristic. In this context it is more usual to use the language of modules rather than representations. The representation obtained from an irreducible representation defined over the integers by reducing modulo the characteristic will not in general be irreducible. The modules so constructed are called Specht modules, and every irreducible does arise inside some such module. There are now fewer irreducibles, and although they can be classified they are very poorly understood. For example, even their dimensions are not known in general.
 The determination of the irreducible modules for the symmetric group over an arbitrary field is widely regarded as one of the most important open problems in representation theory.
"
Ribbon_theory,Mathematics,3,Ribbon theory is a strand of mathematics within topology that has seen particular application as regards DNA.[1]
Riemannian_geometry,Mathematics,3,"Riemannian geometry is the branch of differential geometry that studies Riemannian manifolds, smooth manifolds with a Riemannian metric, i.e. with an inner product on the tangent space at each point that varies smoothly from point to point. This gives, in particular, local notions of angle, length of curves, surface area and volume. From those, some other global quantities can be derived by integrating local contributions.
 Riemannian geometry originated with the vision of Bernhard Riemann expressed in his inaugural lecture ""Ueber die Hypothesen, welche der Geometrie zu Grunde liegen"" (""On the Hypotheses on which Geometry is Based""). It is a very broad and abstract generalization of the differential geometry of surfaces in R3. Development of Riemannian geometry resulted in synthesis of diverse results concerning the geometry of surfaces and the behavior of geodesics on them, with techniques that can be applied to the study of differentiable manifolds of higher dimensions. It enabled the formulation of Einstein's general theory of relativity, made profound impact on group theory and representation theory, as well as analysis, and spurred the development of algebraic and differential topology.
"
Rough_set,Mathematics,3,"In computer science, a rough set, first described by Polish computer scientist Zdzisław I. Pawlak, is a formal approximation of a crisp set (i.e., conventional set) in terms of a pair of sets which give the lower and the upper approximation of the original set. In the standard version of rough set theory (Pawlak 1991), the lower- and upper-approximation sets are crisp sets, but in other variations, the approximating sets may be fuzzy sets.
"
Sampling_(statistics),Mathematics,3,"In statistics, quality assurance, and survey methodology, sampling is the selection of a subset (a statistical sample) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt for the samples to represent the population in question. Two advantages of sampling are lower cost and faster data collection than measuring the entire population.
 Each observation measures one or more properties (such as weight, location, colour) of observable bodies distinguished as independent objects or individuals. In survey sampling, weights can be applied to the data to adjust for the sample design, particularly in stratified sampling.[1] Results from probability theory and statistical theory are employed to guide the practice. In business and medical research, sampling is widely used for gathering information about a population.[2] Acceptance sampling is used to determine if a production lot of material meets the governing specifications.
"
Scheme_(mathematics),Mathematics,3,"In mathematics, a scheme is a mathematical structure that enlarges the notion of algebraic variety in several ways, such as taking account of multiplicities (the equations x = 0 and x2 = 0 define the same algebraic variety and different schemes) and allowing ""varieties"" defined over any commutative ring (for example, Fermat curves are defined over the integers).
 Schemes were introduced by Alexander Grothendieck in 1960 in his treatise ""Éléments de géométrie algébrique""; one of its aims was developing the formalism needed to solve deep problems of algebraic geometry, such as the Weil conjectures (the last of which was proved by Pierre Deligne).[1] Strongly based on commutative algebra, scheme theory allows a systematic use of methods of topology and homological algebra. Scheme theory also unifies algebraic geometry with much of number theory, which eventually led to Wiles's proof of Fermat's Last Theorem.
 Formally, a scheme is a topological space together with commutative rings for all of its open sets, which arises from gluing together spectra (spaces of prime ideals) of commutative rings along their open subsets. In other words, it is a ringed space which is locally a spectrum of a commutative ring.
 The relative point of view is that much of algebraic geometry should be developed for a morphism X → Y of schemes (called a scheme X over Y), rather than for an individual scheme. For example, in studying algebraic surfaces, it can be useful to consider families of algebraic surfaces over any scheme Y. In many cases, the family of all varieties of a given type can itself be viewed as a variety or scheme, known as a moduli space.
 For some of the detailed definitions in the theory of schemes, see the glossary of scheme theory.
"
Secondary_calculus,Mathematics,3,"In mathematics, secondary calculus is a proposed expansion of classical differential calculus on manifolds, to the ""space"" of solutions of a (nonlinear) partial differential equation. It is a sophisticated theory at the level of jet spaces and employing algebraic methods.
"
Self-similarity,Mathematics,3,"
 In mathematics, a self-similar object is exactly or approximately similar to a part of itself (i.e., the whole has the same shape as one or more of the parts). Many objects in the real world, such as coastlines, are statistically self-similar: parts of them show the same statistical properties at many scales.[2] Self-similarity is a typical property of fractals. Scale invariance is an exact form of self-similarity where at any magnification there is a smaller piece of the object that is similar to the whole. For instance, a side of the Koch snowflake is both symmetrical and scale-invariant; it can be continually magnified 3x without changing shape. The non-trivial similarity evident in fractals is distinguished by their fine structure, or detail on arbitrarily small scales. As a counterexample, whereas any portion of a straight line may resemble the whole, further detail is not revealed.
 A time developing phenomenon is said to exhibit self-similarity if the numerical value of certain observable quantity 




f
(
x
,
t
)


{  f(x,t)}
 measured at different times are different but the corresponding dimensionless quantity at given value of 



x

/


t

z




{  x/t^{z}}
 remain invariant. It happens if the quantity 



f
(
x
,
t
)


{  f(x,t)}
 exhibits dynamic scaling. The idea is just an extension of the idea of similarity of two triangles.[3][4][5] Note that two triangles are similar if the numerical values of their sides are different however the corresponding dimensionless quantities, such as their angles, coincide. 
 Peitgen et al. explain the concept as such: 
 If parts of a figure are small replicas of the whole, then the figure is called self-similar....A figure is strictly self-similar if the figure can be decomposed into parts which are exact replicas of the whole. Any arbitrary part contains an exact replica of the whole figure.[6] Since mathematically, a fractal may show self-similarity under indefinite magnification, it is impossible to recreate this physically. Peitgen et al. suggest studying self-similarity using approximations: In order to give an operational meaning to the property of self-similarity, we are necessarily restricted to dealing with finite approximations of the limit figure. This is done using the method which we will call box self-similarity where measurements are made on finite stages of the figure using grids of various sizes.[7] This vocabulary was introduced by Benoit Mandelbrot in 1964[8].
"
Real_algebraic_geometry,Mathematics,3,"In mathematics, real algebraic geometry is the sub-branch of algebraic geometry studying real algebraic sets, i.e. real-number solutions to algebraic equations with real-number coefficients, and mappings between them (in particular real polynomial mappings).
 Semialgebraic geometry is the study of semialgebraic sets, i.e. real-number solutions to algebraic inequalities with-real number coefficients, and mappings between them. The most natural mappings between semialgebraic sets are semialgebraic mappings, i.e., mappings whose graphs are semialgebraic sets.
"
Set-theoretic_topology,Mathematics,3,"In mathematics, set-theoretic topology is a subject that combines set theory and general topology. It focuses on topological questions that are independent of Zermelo–Fraenkel set theory (ZFC).
"
Set_theory,Mathematics,3,"Set theory is a branch of mathematical logic that studies sets, which informally are collections of objects. Although any type of object can be collected into a set, set theory is applied most often to objects that are relevant to mathematics. The language of set theory can be used to define nearly all mathematical objects.
 The modern study of set theory was initiated by Georg Cantor and Richard Dedekind in the 1870s. After the discovery of paradoxes in naive set theory, such as Russell's paradox, numerous axiom systems were proposed in the early twentieth century, of which the Zermelo–Fraenkel axioms, with or without the axiom of choice, are the best-known.
 Set theory is commonly employed as a foundational system for mathematics, particularly in the form of Zermelo–Fraenkel set theory with the axiom of choice.[1] Beyond its foundational role, set theory is a branch of mathematics in its own right, with an active research community. Contemporary research into set theory includes a diverse collection of topics, ranging from the structure of the real number line to the study of the consistency of large cardinals.
"
Sheaf_theory,Mathematics,3,"In mathematics, a sheaf is a tool for systematically tracking locally defined data attached to the open sets of a topological space. The data can be restricted to smaller open sets, and the data assigned to an open set is equivalent to all collections of compatible data assigned to collections of smaller open sets covering the original one. For example, such data can consist of the rings of continuous or smooth real-valued functions defined on each open set. Sheaves are by design quite general and abstract objects, and their correct definition is rather technical. They are variously defined, for example, as sheaves of sets or sheaves of rings, depending on the type of data assigned to open sets.
 There are also maps (or morphisms) from one sheaf to another; sheaves (of a specific type, such as sheaves of abelian groups) with their morphisms on a fixed topological space form a category. On the other hand, to each continuous map there is associated both a direct image functor, taking sheaves and their morphisms on the domain to sheaves and morphisms on the codomain, and an inverse image functor operating in the opposite direction. These functors, and certain variants of them, are essential parts of sheaf theory.
 Due to their general nature and versatility, sheaves have several applications in topology and especially in algebraic and differential geometry. First, geometric structures such as that of a differentiable manifold or a scheme can be expressed in terms of a sheaf of rings on the space. In such contexts several geometric constructions such as vector bundles or divisors are naturally specified in terms of sheaves. Second, sheaves provide the framework for a very general cohomology theory, which encompasses also the ""usual"" topological cohomology theories such as singular cohomology. Especially in algebraic geometry and the theory of complex manifolds, sheaf cohomology provides a powerful link between topological and geometric properties of spaces. Sheaves also provide the basis for the theory of D-modules, which provide applications to the theory of differential equations. In addition, generalisations of sheaves to more general settings than topological spaces, such as Grothendieck topology, have provided applications to mathematical logic and number theory.
"
Sheaf_cohomology,Mathematics,3,"In mathematics, sheaf cohomology is the application of homological algebra to analyze the global sections of a sheaf on a topological space. Broadly speaking, sheaf cohomology describes the obstructions to solving a geometric problem globally when it can be solved locally. The central work for the study of sheaf cohomology is Grothendieck's 1957 Tôhoku paper.
 Sheaves, sheaf cohomology, and spectral sequences were invented by Jean Leray at the prisoner-of-war camp Oflag XVII-A in Austria.[1]  From 1940 to 1945, Leray and other prisoners organized a ""université en captivité"" in the camp.
 Leray's definitions were simplified and clarified in the 1950s. It became clear that sheaf cohomology was not only a new approach to cohomology in algebraic topology, but also a powerful method in complex analytic geometry and algebraic geometry. These subjects often involve constructing global functions with specified local properties, and sheaf cohomology is ideally suited to such problems. Many earlier results such as the Riemann–Roch theorem and the Hodge theorem have been generalized or understood better using sheaf cohomology.
"
Sieve_theory,Mathematics,3,"Sieve theory is a set of general techniques in number theory, designed to count, or more realistically to estimate the size of, sifted sets of integers. The prototypical example of a sifted set is the set of prime numbers up to some prescribed limit X.  Correspondingly, the prototypical example of a sieve is the sieve of Eratosthenes, or the more general Legendre sieve. The direct attack on prime numbers using these methods soon reaches apparently insuperable obstacles, in the way of the accumulation of error terms.[citation needed] In one of the major strands of number theory in the twentieth century, ways were found of avoiding some of the difficulties of a frontal attack with a naive idea of what sieving should be.[citation needed] One successful approach is to approximate a specific sifted set of numbers (e.g. the set of
prime numbers) by another, simpler set (e.g. the set of almost prime numbers), which is typically somewhat larger than the original set, and easier to analyze. More sophisticated sieves also do not work directly with sets per se, but instead count them according to carefully chosen weight functions on these sets (options for giving some elements of these sets more ""weight"" than others). Furthermore, in some modern applications, sieves are used not to estimate the size of a sifted
set, but to produce a function that is large on the set and mostly small outside it, while being easier to analyze than
the characteristic function of the set.
"
Operator_theory,Mathematics,3,"In mathematics, operator theory is the study of linear operators on function spaces, beginning with differential operators and integral operators. The operators may be presented abstractly by their characteristics, such as bounded linear operators or closed operators, and consideration may be given to nonlinear operators. The study, which depends heavily on the topology of function spaces, is a branch of functional analysis.
 If a collection of operators forms an algebra over a field, then it is an operator algebra. The description of operator algebras is part of operator theory.
"
Singularity_theory,Mathematics,3,"In mathematics, singularity theory studies spaces that are almost manifolds, but not quite. A string can serve as an example of a one-dimensional manifold, if one neglects its thickness. A singularity can be made by balling it up, dropping it on the floor, and flattening it. In some places the flat string will cross itself in an approximate ""X"" shape. The points on the floor where it does this are one kind of singularity, the double point: one bit of the floor corresponds to more than one bit of string. Perhaps the string will also touch itself without crossing, like an underlined ""U"". This is another kind of singularity. Unlike the double point, it is not stable, in the sense that a small push will lift the bottom of the ""U"" away from the ""underline"".
 Vladimir Arnold defines the main goal of singularity theory as describing how objects depend on parameters, particularly in cases where the properties undergo sudden change under a small variation of the parameters. These situations are called perestroika (Russian: перестройка), bifurcations or catastrophes. Classifying the types of changes and characterizing sets of parameters which give rise to these changes are some of the main mathematical goals. Singularities can occur in a wide range of mathematical objects, from matrices depending on parameters to wavefronts.[1]"
Smooth_infinitesimal_analysis,Mathematics,3,"Smooth infinitesimal analysis is a modern reformulation of the calculus in terms of infinitesimals. Based on the ideas of F. W. Lawvere and employing the methods of category theory, it views all functions as being continuous and incapable of being expressed in terms of discrete entities.  As a theory, it is a subset of synthetic differential geometry.
 The nilsquare or nilpotent infinitesimals are numbers ε where ε² = 0 is true, but ε = 0 need not be true at the same time.
"
Solid_geometry,Mathematics,3,"In mathematics, solid geometry is the traditional name for the geometry of three-dimensional Euclidean space[1] (i.e., 3D geometry).
 Stereometry deals with the measurements of volumes of various solid figures (three-dimensional figures), including pyramids, prisms and other polyhedrons; cylinders; cones; truncated cones; and balls bounded by spheres.[2]"
Spatial_geometry,Mathematics,3,"Three-dimensional space (also: 3-space or, rarely, tri-dimensional space) is a geometric setting in which three values (called parameters) are required to determine the position of an element (i.e., point). This is the informal meaning of the term dimension.
 In physics and mathematics, a sequence of n numbers can be understood as a location in n-dimensional space. When n = 3, the set of all such locations is called three-dimensional Euclidean space (or simply Euclidean space when the context is clear). It is commonly represented by the symbol ℝ3.[1][2] This serves as a three-parameter model of the physical universe (that is, the spatial part, without considering time), in which all known matter exists. While this space remains the most compelling and useful way to model the world as it is experienced,[3] it is only one example of a large variety of spaces in three dimensions called 3-manifolds. In this classical example, when the three values refer to measurements in different directions (coordinates), any three directions can be chosen, provided that vectors in these directions do not all lie in the same 2-space (plane). Furthermore, in this case, these three values can be labeled by any combination of three chosen from the terms width, height, depth, and length.
"
Spectral_geometry,Mathematics,3,"Spectral geometry is a field in mathematics which concerns relationships between geometric structures of manifolds and spectra of canonically defined differential operators. The case of the Laplace–Beltrami operator on a closed Riemannian manifold has been most intensively studied, although other Laplace operators in differential geometry have also been examined.  The field concerns itself with two kinds of questions: direct problems and inverse problems.
 Inverse problems seek to identify features of the geometry from information about the eigenvalues of the Laplacian.  One of the earliest results of this kind was due to Hermann Weyl who used David Hilbert's theory of integral equation in 1911 to show that the volume of a bounded domain in Euclidean space can be determined from the asymptotic behavior of the eigenvalues for the Dirichlet boundary value problem of the Laplace operator. This question is usually expressed as ""Can one hear the shape of a drum?"", the popular phrase due to Mark Kac. A refinement of Weyl's asymptotic formula obtained by Pleijel and Minakshisundaram produces a series of local spectral invariants involving covariant differentiations of the curvature tensor, which can be used to establish spectral rigidity for a special class of manifolds. However as the example given by John Milnor tells us, the information of eigenvalues is not enough to determine the isometry class of a manifold (see isospectral). A general and systematic method due to Toshikazu Sunada gave rise to a veritable cottage industry of such examples which clarifies the phenomenon of isospectral manifolds.
 Direct problems attempt to infer the behavior of the eigenvalues of a Riemannian manifold from knowledge of the geometry. The solutions to direct problems are typified by the Cheeger inequality which gives a relation between the first positive eigenvalue and an isoperimetric constant (the Cheeger constant). Many versions of the inequality have been established since Cheeger's work (by R. Brooks and P. Buser for instance).
"
Spectral_graph_theory,Mathematics,3,"In mathematics, spectral graph theory is the study of the properties of a graph in relationship to the characteristic polynomial, eigenvalues, and eigenvectors of matrices associated with the graph, such as its adjacency matrix or Laplacian matrix.
 The adjacency matrix of a simple graph is a real symmetric matrix and is therefore orthogonally diagonalizable; its eigenvalues are real algebraic integers.
 While the adjacency matrix depends on the vertex labeling, its spectrum is a graph invariant, although not a complete one.
 Spectral graph theory is also concerned with graph parameters that are defined via multiplicities of eigenvalues of matrices associated to the graph, such as the Colin de Verdière number.
"
Spectral_theory,Mathematics,3,"In mathematics, spectral theory is an inclusive term for theories extending the eigenvector and eigenvalue theory of a single square matrix to a much broader theory of the structure of operators in a variety of mathematical spaces.[1] It is a result of studies of linear algebra and the solutions of systems of linear equations and their generalizations.[2] The theory is connected to that of analytic functions because the spectral properties of an operator are related to analytic functions of the spectral parameter.[3]"
Spectral_theory_of_ordinary_differential_equations,Mathematics,3,"In mathematics, the spectral theory of ordinary differential equations is the part of spectral theory concerned with the determination of the spectrum and eigenfunction expansion associated with a linear ordinary differential equation. In his dissertation Hermann Weyl generalized the classical Sturm–Liouville theory on a finite closed interval to second order differential operators with singularities at the endpoints of the interval, possibly semi-infinite or infinite. Unlike the classical case, the spectrum may no longer consist of just a countable set of eigenvalues, but may also contain a continuous part. In this case the eigenfunction expansion involves an integral over the continuous part with respect to a spectral measure, given by the Titchmarsh–Kodaira formula. The theory was put in its final simplified form for singular differential equations of even degree by Kodaira and others, using von Neumann's spectral theorem. It has had important applications in quantum mechanics, operator theory and harmonic analysis on semisimple Lie groups.
"
Spectrum_continuation_analysis,Mathematics,3,"Spectrum continuation analysis (SCA) is a generalization of the concept of Fourier series to non-periodic functions of which only a fragment has been sampled in the time domain.
 Recall that a Fourier series is only suitable to the analysis of periodic (or finite-domain) functions f(x) with period 2π. It can be expressed as an infinite series of sinusoids:
 where 




F

n




{  F_{n}}
 is the amplitude of the individual harmonics.
 In SCA however, one decomposes the spectrum into optimized discrete frequencies. As a consequence, and as the period of the sampled function is supposed to be infinite or not yet known, each of the discrete periodic functions that compose the sampled function fragment can not be considered to be a multiple of the fundamental frequency:
 As such, SCA does not necessarily deliver 



2
π


{  2\pi }
 periodic functions, as would have been the case in Fourier analysis.
For real-valued functions, the SCA series can be written as:
 where An and Bn are the series amplitudes. The amplitudes can only be solved if the series of values 




ω

n




{  \omega _{n}}
 is previously optimized for a desired objective function (usually least residuals).




C
(
x
)


{  C(x)}
 is not necessarily the average value over the sampled interval: one might prefer to include predominant information on the behavior of the offset value in the time domain.
"
Spherical_geometry,Mathematics,3,"Spherical geometry is the geometry of the two-dimensional surface of a sphere. It is an example of a geometry that is not Euclidean. Two practical applications of the principles of spherical geometry are navigation and astronomy.
"
Spherical_trigonometry,Mathematics,3,"
Spherical trigonometry is the branch of spherical geometry that deals with the relationships between trigonometric functions of the sides and angles of the spherical polygons (especially spherical triangles) defined by a number of intersecting great circles on the sphere. Spherical trigonometry is of great importance for calculations in astronomy, geodesy, and navigation.
 The origins of spherical trigonometry in Greek mathematics and the major developments in Islamic mathematics are discussed fully in History of trigonometry and Mathematics in medieval Islam. The subject came to fruition in Early Modern times with important developments by John Napier, Delambre and others, and attained an essentially complete form by the end of the nineteenth century with the publication of Todhunter's textbook Spherical trigonometry for the use of colleges and Schools.[1]
Since then, significant developments have been the application of vector methods, and the use of numerical methods.
"
Statistical_mechanics,Mathematics,3,"
 Statistical mechanics, one of the pillars of modern physics, describes how macroscopic observations (such as temperature and pressure) are related to microscopic parameters that fluctuate around an average. It connects thermodynamic quantities (such as heat capacity) to microscopic behavior, whereas, in classical thermodynamics, the only available option would be to measure and tabulate such quantities for various materials.[1] Statistical mechanics is necessary for the fundamental study of any physical system that has many degrees of freedom. The approach is based on statistical methods, probability theory and the microscopic physical laws.[1][2][3][note 1] It can be used to explain the thermodynamic behaviour of large systems. This branch of statistical mechanics, which treats and extends classical thermodynamics, is known as statistical thermodynamics or equilibrium statistical mechanics.
 Statistical mechanics can also be used to study systems that are out of equilibrium. An important sub-branch known as non-equilibrium statistical mechanics (sometimes called statistical dynamics) deals with the issue of microscopically modelling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions or flows of particles and heat. The fluctuation–dissipation theorem is the basic knowledge obtained from applying non-equilibrium statistical mechanics to study the simplest non-equilibrium situation of a steady state current flow in a system of many particles.
"
Statistical_modelling,Mathematics,3,"A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process.[1] A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables.  As such, a statistical model is ""a formal representation of a theory"" (Herman Adèr quoting Kenneth Bollen).[2] All statistical hypothesis tests and all statistical estimators are derived via statistical models. More generally, statistical models are part of the foundation of statistical inference.
"
Statistical_theory,Mathematics,3,"The theory of statistics provides a basis for the whole range of techniques, in both study design and data analysis, that are used within applications of statistics.[1][2] The theory covers approaches to statistical-decision problems and to statistical inference, and the actions and deductions that satisfy  the basic principles stated for these different approaches. Within a given approach, statistical theory gives ways of comparing statistical procedures; it can find a best possible procedure within a given context for given statistical problems, or can provide guidance on the choice between alternative procedures.[2][3] Apart from philosophical considerations about how to make statistical inferences and decisions, much of statistical theory consists of mathematical statistics, and is closely linked to probability theory, to utility theory, and to optimization.
"
Mathematical_statistics,Mathematics,3,"Mathematical statistics is the application of probability theory, a branch of mathematics, to statistics, as opposed to techniques for collecting statistical data. Specific mathematical techniques which are used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure theory.[1][2]"
Steganography,Mathematics,3,"Steganography (/ˌstɛɡəˈnɒɡrəfi/ (listen) STEG-ə-NOG-rə-fee) is the practice of concealing a file, message, image, or video within another file, message, image, or video. The word steganography comes from Greek steganographia, which combines the words steganós (στεγανός), meaning ""covered or concealed"", and -graphia (γραφή) meaning ""writing"".[1] The first recorded use of the term was in 1499 by Johannes Trithemius in his Steganographia, a treatise on cryptography and steganography, disguised as a book on magic. Generally, the hidden messages appear to be (or to be part of) something else: images, articles, shopping lists, or some other cover text. For example, the hidden message may be in invisible ink between the visible lines of a private letter. Some implementations of steganography that lack a shared secret are forms of security through obscurity, and key-dependent steganographic schemes adhere to Kerckhoffs's principle.[2] The advantage of steganography over cryptography alone is that the intended secret message does not attract attention to itself as an object of scrutiny. Plainly visible encrypted messages, no matter how unbreakable they are, arouse interest and may in themselves be incriminating in countries in which encryption is illegal.[3] Whereas cryptography is the practice of protecting the contents of a message alone, steganography is concerned both with concealing the fact that a secret message is being sent and its contents.
 Steganography includes the concealment of information within computer files. In digital steganography, electronic communications may include steganographic coding inside of a transport layer, such as a document file, image file, program or protocol. Media files are ideal for steganographic transmission because of their large size. For example, a sender might start with an innocuous image file and adjust the color of every hundredth pixel to correspond to a letter in the alphabet. The change is so subtle that someone who is not specifically looking for it is unlikely to notice the change.
"
Stochastic_calculus,Mathematics,3,"Stochastic calculus is a branch of mathematics that operates on stochastic processes. It allows a consistent theory of integration to be defined for integrals of stochastic processes with respect to stochastic processes. It is used to model systems that behave randomly.
 The best-known stochastic process to which stochastic calculus is applied is the Wiener process (named in honor of Norbert Wiener), which is used for modeling Brownian motion as described by Louis Bachelier in 1900 and by Albert Einstein in 1905 and other physical diffusion processes in space of particles subject to random forces.  Since the 1970s, the Wiener process has been widely applied in financial mathematics and economics to model the evolution in time of stock prices and bond interest rates.
 The main flavours of stochastic calculus are the Itô calculus and its variational relative the Malliavin calculus.  For technical reasons the Itô integral is the most useful for general classes of processes, but the related Stratonovich integral is frequently useful in problem formulation (particularly in engineering disciplines). The Stratonovich integral can readily be expressed in terms of the Itô integral.  The main benefit of the Stratonovich integral is that it obeys the usual chain rule and therefore does not require Itô's lemma. This enables problems to be expressed in a coordinate system invariant form, which is invaluable when developing stochastic calculus on manifolds other than Rn.
The dominated convergence theorem does not hold for the Stratonovich integral; consequently it is very difficult to prove results without re-expressing the integrals in Itô form.
"
Malliavin_calculus,Mathematics,3,"In probability theory and related fields, Malliavin calculus is a set of mathematical techniques and ideas that extend the mathematical field of calculus of variations from  deterministic functions to stochastic processes. In particular, it allows the computation of derivatives of random variables. Malliavin calculus is also called the stochastic calculus of variations. P. Malliavin first initiated the calculus on infinite dimensional space. Then, the significant contributors such as S. Kusuoka, D. Stroock, Bismut, S. Watanabe, I. Shigekawa, and so on finally completed the foundations.
 Malliavin calculus is named after Paul Malliavin whose ideas led to a proof that Hörmander's condition implies the existence and smoothness of a density for the solution of a stochastic differential equation; Hörmander's original proof was based on the theory of  partial differential equations. The calculus has been applied to stochastic partial differential equations as well.
 The calculus allows integration by parts with random variables; this operation is used in mathematical finance to compute the sensitivities of financial derivatives. The calculus has applications in, for example, stochastic filtering.
"
Stochastic_geometry,Mathematics,3,"In mathematics, stochastic geometry is the study of random spatial patterns. At the heart of the subject lies the study of random point patterns. This leads to the theory of spatial point processes, hence notions of Palm conditioning, which extend to the more abstract setting of random measures.
"
Stochastic_process,Mathematics,3,"In probability theory and related fields, a stochastic or random process is a mathematical object usually defined as a family of random variables. Many stochastic processes can be represented by time series. However, a stochastic process is by nature continuous while a time series is a set of observations indexed by integers. A stochastic process may involve several related random variables. 
 Common examples include the growth of a bacterial population, an electrical current fluctuating due to thermal noise, or the movement of a gas molecule.[1][4][5][6] Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner. They have applications in many disciplines such as biology,[7] chemistry,[8] ecology,[9] neuroscience,[10] physics,[11] image processing, signal processing,[12] control theory, [13] information theory,[14] computer science,[15] cryptography[16] and telecommunications.[17] Furthermore, seemingly random changes in financial markets have motivated the extensive use of stochastic processes in finance.[18][19][20] Applications and the study of phenomena have in turn inspired the proposal of new stochastic processes. Examples of such stochastic processes include the Wiener process or Brownian motion process,[a] used by Louis Bachelier to study price changes on the Paris Bourse,[23] and the Poisson process, used by A. K. Erlang to study the number of phone calls occurring in a certain period of time.[24] These two stochastic processes are considered the most important and central in the theory of stochastic processes,[1][4][25] and were discovered repeatedly and independently, both before and after Bachelier and Erlang, in different settings and countries.[23][26] The term random function is also used to refer to a stochastic or random process,[27][28] because a stochastic process can also be interpreted as a random element in a function space.[29][30] The terms stochastic process and random process are used interchangeably, often with no specific mathematical space for the set that indexes the random variables.[29][31] But often these two terms are used when the random variables are indexed by the integers or an interval of the real line.[5][31] If the random variables are indexed by the Cartesian plane or some higher-dimensional Euclidean space, then the collection of random variables is usually called a random field instead.[5][32] The values of a stochastic process are not always numbers and can be vectors or other mathematical objects.[5][30] Based on their mathematical properties, stochastic processes can be grouped into various categories, which include random walks,[33] martingales,[34] Markov processes,[35] Lévy processes,[36] Gaussian processes,[37] random fields,[38] renewal processes, and branching processes.[39] The study of stochastic processes uses mathematical knowledge and techniques from probability, calculus, linear algebra, set theory, and topology[40][41][42] as well as branches of mathematical analysis such as real analysis, measure theory, Fourier analysis, and functional analysis.[43][44][45] The theory of stochastic processes is considered to be an important contribution to mathematics[46] and it continues to be an active topic of research for both theoretical reasons and applications.[47][48][49]"
Stratified_Morse_theory,Mathematics,3,"In mathematics, stratified Morse theory is an analogue to Morse theory for general stratified spaces, originally developed by Mark Goresky and  Robert MacPherson.  The main point of the theory is to consider functions 



f
:
M
→

R



{  f:M\to \mathbb {R} }
 and consider how the stratified space 




f

−
1


(
−
∞
,
c
]


{  f^{-1}(-\infty ,c]}
 changes as the real number 



c
∈

R



{  c\in \mathbb {R} }
 changes.  Morse theory of stratified spaces has uses everywhere from pure mathematics topics such as braid groups and representations to robot motion planning and potential theory. A popular application in pure mathematics is Morse theory on manifolds with boundary, and manifolds with corners. 
"
Super_linear_algebra,Mathematics,3,"In mathematics, a super vector space is a 





Z


2




{  \mathbb {Z} _{2}}
-graded vector space, that is, a vector space over a field 




K



{  \mathbb {K} }
 with a given decomposition of subspaces of grade 



0


{  0}
 and grade 



1


{  1}
. The study of super vector spaces and their generalizations is sometimes called super linear algebra. These objects find their principal application in theoretical physics where they are used to describe the various algebraic aspects of supersymmetry.
"
Surgery_theory,Mathematics,3,"In mathematics, specifically in geometric topology, surgery theory is a collection of techniques used to produce one finite-dimensional manifold from another in a 'controlled' way, introduced by John Milnor (1961). Originally developed for differentiable (or, smooth) manifolds, surgery techniques also apply to piecewise linear (PL-) and topological manifolds. 
 Surgery refers to cutting out parts of the manifold and replacing it with a part of another manifold, matching up along the cut or boundary. This is closely related to, but not identical with, handlebody decompositions. It is a major tool in the study and classification of manifolds of dimension greater than 3.
 More technically, the idea is to start with a well-understood manifold M and perform surgery on it to produce a manifold M ′ having some desired property, in such a way that the effects on the homology, homotopy groups, or other invariants of the manifold are known.
 The classification of exotic spheres by Michel Kervaire and  Milnor (1963) led to the emergence of surgery theory as a major tool in high-dimensional topology.
"
Survey_sampling,Mathematics,3,"In statistics, survey sampling describes the process of selecting a sample of elements from a target population to conduct a survey. 
The term ""survey"" may refer to many different types or techniques of observation.  In survey sampling it most often involves a questionnaire used to measure the characteristics and/or attitudes of people. Different ways of contacting members of a sample once they have been selected is the subject of survey data collection. The purpose of sampling is to reduce the cost and/or the amount of work that it would take to survey the entire target population.  A survey that measures the entire target population is called a census. A sample refers to a group or section of a population from which information is to be obtained
 Survey samples can be broadly divided into two types: probability samples and super samples.  Probability-based samples implement a sampling plan with specified probabilities (perhaps adapted probabilities specified by an adaptive procedure). Probability-based sampling allows design-based inference about the target population. The inferences are based on a known objective probability distribution that was specified in the study protocol. Inferences from probability-based surveys may still suffer from many types of bias.
 Surveys that are not based on probability sampling have greater difficulty measuring their bias or sampling error.[1]  Surveys based on non-probability samples often fail to represent the people in the target population.[2] In academic and government survey research, probability sampling is a standard procedure. In the United States, the Office of Management and Budget's ""List of Standards for Statistical Surveys"" states that federally funded surveys must be performed:
 
selecting samples using generally accepted statistical methods (e.g., probabilistic methods that can provide estimates of sampling error). Any use of nonprobability sampling methods (e.g., cut-off or model-based samples) must be justified statistically and be able to measure estimation error.[3] Random sampling and design-based inference are supplemented by other statistical methods, such as model-assisted sampling and model-based sampling.[4][5] For example, many surveys have substantial amounts of nonresponse. Even though the units are initially chosen with known probabilities, the nonresponse mechanisms are unknown. For surveys with substantial nonresponse, statisticians have proposed statistical models with which the data sets are analyzed.
 Issues related to survey sampling are discussed in several sources, including Salant and Dillman (1994).[6]"
Survey_methodology,Mathematics,3,"Survey methodology is ""the study of  survey methods"".[1]
As a field of applied statistics concentrating on human-research surveys, survey methodology studies the sampling of individual units from a population and associated techniques of survey data collection, such as questionnaire construction and methods for improving the number and accuracy of responses to surveys. Survey methodology targets instruments or procedures that ask one or more questions that may or may not be answered.
 Researchers carry out statistical surveys with a view towards making statistical inferences about the population being studied; such inferences depend strongly on the survey questions used.  Polls about public opinion, public-health surveys,  market-research surveys, government surveys and censuses all exemplify quantitative research that uses survey methodology to answer questions about a  population. Although censuses do not include a ""sample"", they do include other aspects of survey methodology, like questionnaires, interviewers, and non-response follow-up techniques. Surveys provide important information for all kinds of  public-information and research fields, such as marketing research, psychology,  health-care provision and sociology.
"
Symbolic_computation,Mathematics,3,"In mathematics and computer science,[1] computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols.
 Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.
 Computer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, as in public key cryptography, or for some non-linear problems.
"
Symbolic_dynamics,Mathematics,3,"In mathematics, symbolic dynamics is the practice of modeling a topological or smooth dynamical system by a discrete space consisting of infinite sequences of abstract symbols, each of which corresponds to a state of the system, with the dynamics (evolution) given by the shift operator. Formally, a Markov partition is used to provide a finite cover for the smooth system; each set of the cover is associated with a single symbol, and the sequences of symbols result as a trajectory of the system moves from one covering set to another.
"
Symplectic_geometry,Mathematics,3,"Symplectic geometry is a branch of differential geometry and differential topology that studies symplectic manifolds; that is, differentiable manifolds equipped with a closed, nondegenerate 2-form. Symplectic geometry has its origins in the Hamiltonian formulation of classical mechanics where the phase space of certain classical systems takes on the structure of a symplectic manifold.[1]"
Symplectic_topology,Mathematics,3,"Symplectic geometry is a branch of differential geometry and differential topology that studies symplectic manifolds; that is, differentiable manifolds equipped with a closed, nondegenerate 2-form. Symplectic geometry has its origins in the Hamiltonian formulation of classical mechanics where the phase space of certain classical systems takes on the structure of a symplectic manifold.[1]"
Synthetic_differential_geometry,Mathematics,3,"In mathematics, synthetic differential geometry is a formalization of the theory of differential geometry in the language of topos theory.  There are several insights that allow for such a reformulation.  The first is that most of the analytic data for describing the class of smooth manifolds can be encoded into certain fibre bundles on manifolds: namely bundles of jets (see also jet bundle).  The second insight is that the operation of assigning a bundle of jets to a smooth manifold is functorial in nature.  The third insight is that over a certain category, these are representable functors.  Furthermore, their representatives are related to the algebras of dual numbers, so that smooth infinitesimal analysis may be used.
 Synthetic differential geometry can serve as a platform for formulating certain otherwise obscure or confusing notions from differential geometry.  For example, the meaning of what it means to be natural (or invariant) has a particularly simple expression, even though the formulation in classical differential geometry may be quite difficult.
"
Synthetic_geometry,Mathematics,3,"Synthetic geometry (sometimes referred to as axiomatic geometry or even pure geometry) is the study of geometry without the use of coordinates or formulae. It relies on the axiomatic method and the tools directly related to them, that is, compass and straightedge, to draw conclusions and solve problems. 
 Only after the introduction of coordinate methods was there a reason to introduce the term ""synthetic geometry"" to distinguish this approach to geometry from other approaches.
Other approaches to geometry are embodied in analytic and algebraic geometries, where one would use analysis and algebraic techniques to obtain geometric results.
 According to Felix Klein
 Synthetic geometry is that which studies figures as such, without recourse to formulae, whereas analytic geometry consistently makes use of such formulae as can be written down after the adoption of an appropriate system of coordinates.[1] Geometry as presented by Euclid in the Elements is the quintessential example of the use of the synthetic method. It was the favoured method of Isaac Newton for the solution of geometric problems.[2] Synthetic methods were most prominent during the 19th century when geometers rejected coordinate methods in establishing the foundations of projective geometry and non-Euclidean geometries. For example the geometer Jakob Steiner (1796 – 1863) hated analytic geometry, and always gave preference to synthetic methods.[3]"
Systolic_geometry,Mathematics,3,"In mathematics, systolic geometry is the study of systolic invariants of manifolds and polyhedra, as initially conceived by Charles Loewner and developed by Mikhail Gromov, Michael Freedman, Peter Sarnak, Mikhail Katz, Larry Guth, and others, in its arithmetical, ergodic, and topological manifestations.  See also a slower-paced Introduction to systolic geometry.
"
Systolic_geometry,Mathematics,3,"In mathematics, systolic geometry is the study of systolic invariants of manifolds and polyhedra, as initially conceived by Charles Loewner and developed by Mikhail Gromov, Michael Freedman, Peter Sarnak, Mikhail Katz, Larry Guth, and others, in its arithmetical, ergodic, and topological manifestations.  See also a slower-paced Introduction to systolic geometry.
"
Tensor_analysis,Mathematics,3,"In mathematics and physics,  a tensor field assigns a tensor to each point of a mathematical space (typically a Euclidean space or manifold). Tensor fields are used in differential geometry, algebraic geometry, general relativity, in the analysis of stress and strain in materials, and in numerous applications in the physical sciences.  As a tensor is a generalization of a scalar (a pure number representing a value, for example speed) and a vector (a pure number plus a direction, like velocity), a tensor field is a generalization of a scalar field or vector field that assigns, respectively, a scalar or vector to each point of space.
 Many mathematical structures called ""tensors"" are tensor fields. For example, the Riemann curvature tensor is not a tensor, as the name implies, but a tensor field: It is named after Bernhard Riemann, and associates a tensor to each point of a Riemannian manifold, which is a topological space.
"
Tensor_calculus,Mathematics,3,"In mathematics, tensor calculus, tensor analysis, or Ricci calculus is an extension of vector calculus to tensor fields (tensors that may vary over a manifold, e.g. in spacetime).
 Developed by Gregorio Ricci-Curbastro and his student Tullio Levi-Civita,[1] it was used by Albert Einstein to develop his general theory of relativity. Unlike the infinitesimal calculus, tensor calculus allows presentation of physics equations in a form that is independent of the choice of coordinates on the manifold.
 Tensor calculus has many applications in physics, engineering and computer science including elasticity, continuum mechanics, electromagnetism (see mathematical descriptions of the electromagnetic field), general relativity (see mathematics of general relativity), quantum field theory, and machine learning.
 
Working with a main proponent of the exterior calculus Elie Cartan, the influential geometer Shiing-Shen Chern summarizes the role of tensor calculus:[2] In our subject of differential geometry, where you talk about manifolds, one difficulty is that the geometry is described by coordinates, but the coordinates do not have meaning. They are allowed to undergo transformation. And in order to handle this kind of situation, an important tool is the so-called tensor analysis, or Ricci calculus, which was new to mathematicians. In mathematics you have a function, you write down the function, you calculate, or you add, or you multiply, or you can differentiate. You have something very concrete. In geometry the geometric situation is described by numbers, but you can change your numbers arbitrarily. So to handle this, you need the Ricci calculus."
Tensor,Mathematics,3,"In mathematics, a tensor is an algebraic object that describes a (multilinear) relationship between sets of algebraic objects related to a vector space. Objects that tensors may map between include vectors and scalars, and even other tensors. Tensors can take several different forms – for example: scalars and vectors (which are the simplest tensors), dual vectors, multilinear maps between vector spaces, and even some operations such as the dot product. Tensors are defined independent of any basis, although they are often referred to by their components in a basis related to a particular coordinate system.
 Tensors are important in physics because they provide a concise mathematical framework for formulating and solving physics problems in areas such as mechanics (stress, elasticity, fluid mechanics, moment of inertia, ...), electrodynamics (electromagnetic tensor, Maxwell tensor, permittivity, magnetic susceptibility, ...), or general relativity (stress–energy tensor, curvature tensor, ... ) and others. In applications, it is common to study situations in which a different tensor can occur at each point of an object; for example the stress within an object may vary from one location to another. This leads to the concept of a tensor field. In some areas, tensor fields are so ubiquitous that they are often simply called ""tensors"".
 Tensors were conceived in 1900 by Tullio Levi-Civita and Gregorio Ricci-Curbastro, who continued the earlier work of Bernhard Riemann and Elwin Bruno Christoffel and others, as part of the absolute differential calculus. The concept enabled an alternative formulation of the intrinsic differential geometry of a manifold in the form of the Riemann curvature tensor.[1]"
Tessellation,Mathematics,3,"
 A tiling or tessellation of a flat surface is the covering of a plane using one or more geometric shapes, called tiles, with no overlaps and no gaps. In mathematics, tessellations can be generalized to higher dimensions and a variety of geometries.
 A periodic tiling has a repeating pattern. Some special kinds include regular tilings with regular polygonal tiles all of the same shape, and semiregular tilings with regular tiles of more than one shape and with every corner identically arranged. The patterns formed by periodic tilings can be categorized into 17 wallpaper groups. A tiling that lacks a repeating pattern is called ""non-periodic"". An aperiodic tiling uses a small set of tile shapes that cannot form a repeating pattern. In the geometry of higher dimensions, a space-filling or honeycomb is also called a tessellation of space.
 A real physical tessellation is a tiling made of materials such as cemented ceramic squares or hexagons. Such tilings may be decorative patterns, or may have functions such as providing durable and water-resistant pavement, floor or wall coverings. Historically, tessellations were used in Ancient Rome and in Islamic art such as in the decorative geometric tiling of the Alhambra palace. In the twentieth century, the work of M. C. Escher often made use of tessellations, both in ordinary Euclidean geometry and in hyperbolic geometry, for artistic effect. Tessellations are sometimes employed for decorative effect in quilting. Tessellations form a class of patterns in nature, for example in the arrays of hexagonal cells found in honeycombs.
"
Theoretical_physics,Mathematics,3,"
 Theoretical physics is a branch of physics that employs mathematical models and abstractions of physical objects and systems to rationalize, explain and predict natural phenomena. This is in contrast to experimental physics, which uses experimental tools to probe these phenomena.
 The advancement of science generally depends on the interplay between experimental studies and theory. In some cases, theoretical physics adheres to standards of mathematical rigour while giving little weight to experiments and observations.[a] For example, while developing special relativity, Albert Einstein was concerned with the Lorentz transformation which left Maxwell's equations invariant, but was apparently uninterested in the Michelson–Morley experiment on Earth's drift through a luminiferous aether.[1] Conversely, Einstein was awarded the Nobel Prize for explaining the photoelectric effect, previously an experimental result lacking a theoretical formulation.[2]"
Theory_of_computation,Mathematics,3,"In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e.g., approximate solutions versus precise ones).  The field is divided into three major branches: automata theory and formal languages, computability theory, and computational complexity theory, which are linked by the question: ""What are the fundamental capabilities and limitations of computers?"".[1] In order to perform a rigorous study of computation, computer scientists work with a mathematical abstraction of computers called a model of computation.  There are several models in use, but the most commonly examined is the Turing machine.[2] Computer scientists study the Turing machine because it is simple to formulate, can be analyzed and used to prove results, and because it represents what many consider the most powerful possible ""reasonable"" model of computation (see Church–Turing thesis).[3] It might seem that the potentially infinite memory capacity is an unrealizable attribute, but any decidable problem[4] solved by a Turing machine will always require only a finite amount of memory. So in principle, any problem that can be solved (decided) by a Turing machine can be solved by a computer that has a finite amount of memory.
"
Time-scale_calculus,Mathematics,3,"In mathematics, time-scale calculus is a unification of the theory of difference equations with that of differential equations, unifying integral and differential calculus with the calculus of finite differences, offering a formalism for studying hybrid discrete–continuous dynamical systems.  It has applications in any field that requires simultaneous modelling of discrete and continuous data. It gives a new definition of a derivative such that if one differentiates a function defined on the real numbers then the definition is equivalent to standard differentiation, but if one uses a function defined on the integers then it is equivalent to the forward difference operator.
"
Topology,Mathematics,3,"
 In mathematics, topology (from the Greek words τόπος, 'place, location', and λόγος, 'study') is concerned with the properties of a geometric object that are preserved under continuous deformations, such as stretching, twisting, crumpling and bending, but not tearing or gluing.
 A topological space is a set endowed with a structure, called a topology, which allows defining continuous deformation of subspaces, and, more generally, all kinds of continuity. Euclidean spaces, and, more generally, metric spaces are examples of a topological space, as any distance or metric defines a topology. The deformations that are considered in topology are homeomorphisms and homotopies. A property that is invariant under such deformations is a topological property. Basic examples of topological properties are: the dimension, which allows distinguishing between a line and a surface; compactness, which allows distinguishing between a line and a circle; connectedness, which allows distinguishing a circle from two non-intersecting circles.
 The ideas underlying topology go back to Gottfried Leibniz, who in the 17th century envisioned the geometria situs and analysis situs. Leonhard Euler's Seven Bridges of Königsberg problem and polyhedron formula are arguably the field's first theorems. The term topology was introduced by Johann Benedict Listing in the 19th century, although it was not until the first decades of the 20th century that the idea of a topological space was developed.
"
Topological_combinatorics,Mathematics,3,"The mathematical discipline of topological combinatorics is the application of topological and algebraic topological methods to solving problems in combinatorics.
"
Topological_degree_theory,Mathematics,3,"In mathematics, topological degree theory is a generalization of the winding number of a curve in the complex plane. It can be used to estimate the number of solutions of an equation, and is closely connected to fixed-point theory. When one solution of an equation is easily found, degree theory can often be used to prove existence of a second, nontrivial, solution. There are different types of degree for different types of maps: e.g.  for maps between Banach spaces there is the Brouwer degree in Rn, the Leray-Schauder degree for compact mappings in normed spaces, the coincidence degree and various other types. There is also a degree for continuous maps between manifolds. 
 Topological degree theory has applications in complementarity problems, differential equations, differential inclusions and dynamical systems.
"
Topological_graph_theory,Mathematics,3,"In mathematics, topological graph theory is a branch of graph theory. It studies the embedding of graphs in surfaces, spatial embeddings of graphs, and graphs as topological spaces.[1] It also studies immersions of graphs.
 Embedding a graph in a surface means that we want to draw the graph on a surface, a sphere for example, without two edges intersecting. A basic embedding problem often presented as a mathematical puzzle is the three-cottage problem. Other applications can be found in printing electronic circuits where the aim is to print (embed) a circuit (the graph) on a circuit board (the surface) without two connections crossing each other and resulting in a short circuit.
"
Topological_K-theory,Mathematics,3,"In mathematics, topological K-theory is a branch of algebraic topology. It was founded to study vector bundles on topological spaces, by means of ideas now recognised as (general) K-theory that were introduced by Alexander Grothendieck. The early work on topological K-theory is due to Michael Atiyah and Friedrich Hirzebruch.
"
Topos_theory,Mathematics,3,"In mathematics, a topos (UK: /ˈtɒpɒs/, US: /ˈtoʊpoʊs, ˈtoʊpɒs/; plural topoi /ˈtoʊpɔɪ/ or /ˈtɒpɔɪ/, or toposes) is a category that behaves like the category of sheaves of sets on a topological space (or more generally: on a site). Topoi behave much like the category of sets and possess a notion of localization; they are a direct generalization of point-set topology.[1]  The Grothendieck topoi find applications in algebraic geometry; the more general elementary topoi are used in logic.
"
Toric_geometry,Mathematics,3,"In algebraic geometry, a toric variety or torus embedding is an algebraic variety containing an algebraic torus as an open dense subset, such that the action of the torus on itself extends to the whole variety. Some authors also require it to be normal. Toric varieties form an important and rich class of examples in algebraic geometry, which often provide a testing ground for theorems. The geometry of a toric variety is fully determined by the combinatorics of its associated fan, which often makes computations far more tractable. For a certain special, but still quite general class of toric varieties, this information is also encoded in a polytope, which creates a powerful connection of the subject with convex geometry. Familiar examples of toric varieties are affine space, projective spaces, products of projective spaces and bundles over projective space. 
"
Transcendental_number_theory,Mathematics,3,"
 Transcendental number theory is a branch of number theory that investigates transcendental numbers (numbers that are not solutions of any polynomial equation with integer coefficients), in both qualitative and quantitative ways.
"
Transformation_geometry,Mathematics,3,"In mathematics, transformation geometry (or transformational geometry) is the name of a mathematical and pedagogic take on the study of geometry by focusing on groups of geometric transformations, and properties that are invariant under them.  It is opposed to the classical synthetic geometry approach of Euclidean geometry, that focuses on proving theorems.
 For example, within transformation geometry, the properties of an isosceles triangle are deduced from the fact that it is mapped to itself by a reflection about a certain line. This contrasts with the classical proofs by the criteria for congruence of triangles.[1] The first systematic effort to use transformations as the foundation of geometry was made by Felix Klein in the 19th century, under the name Erlangen programme.  For nearly a century this approach remained confined to mathematics research circles.   In the 20th century efforts were made to exploit it for mathematical education.  Andrei Kolmogorov included  this approach (together with set theory) as part of a proposal for geometry teaching reform in Russia.[2]  These efforts culminated in the 1960s with the general reform of mathematics teaching known as the New Math movement.
"
Trigonometry,Mathematics,3,"
 Trigonometry (from Greek trigōnon, ""triangle"" and metron, ""measure""[1]) is a branch of mathematics that studies relationships between side lengths and angles of triangles. The field emerged in the Hellenistic world during the 3rd century BC from applications of geometry to astronomical studies.[2] The Greeks focused on the calculation of chords, while mathematicians in India created the earliest-known tables of values for  trigonometric ratios (also called trigonometric functions) such as sine.[3] Throughout history, trigonometry has been applied in areas such as geodesy, surveying, celestial mechanics, and navigation.[4] Trigonometry is known for its many identities,[5][6] which are equations used for rewriting trigonometrical expressions to solve equations, to find a more useful expression, or to discover new relationships.[7]"
Tropical_analysis,Mathematics,3,"In the mathematical discipline of idempotent analysis, tropical analysis is the study of the tropical semiring.
"
Tropical_geometry,Mathematics,3,"
 In mathematics, tropical geometry is the study of polynomials and their  geometric properties when addition is replaced with minimization and multiplication is replaced with ordinary addition:
 So for example, the classical polynomial 




x

3


+
2
x
y
+

y

4




{  x^{3}+2xy+y^{4}}
 would become 



min
{
x
+
x
+
x
,

2
+
x
+
y
,

y
+
y
+
y
+
y
}


{  \min\{x+x+x,\;2+x+y,\;y+y+y+y\}}
. Such polynomials and their solutions have important applications in optimization problems, for example the problem of optimizing departure times for a network of trains.
 Tropical geometry is a variant of algebraic geometry in which polynomial graphs resemble piecewise linear meshes, and in which numbers belong to the tropical semiring instead of a field. Because classical and tropical geometry are closely related, results and methods can be converted between them. Algebraic varieties can be mapped to a tropical counterpart and, since this process still retains some geometric information about the original variety, it can be used to help prove and generalize classical results from algebraic geometry, such as the Brill–Noether theorem, using the tools of tropical geometry.[1]"
Twisted_K-theory,Mathematics,3,"In mathematics, twisted K-theory (also called K-theory with local coefficients[1]) is a variation on K-theory, a mathematical theory from the 1950s that spans algebraic topology, abstract algebra and operator theory.
 More specifically, twisted K-theory with twist H is a particular variant of K-theory, in which the twist is given by an integral 3-dimensional cohomology class. It is special among the various twists that K-theory admits for two reasons.  First, it admits a geometric formulation. This was provided in two steps; the first one was done in 1970 (Publ. Math. de l'IHÉS) by Peter Donovan and Max Karoubi; the second one in 1988 by Jonathan Rosenberg in Continuous-Trace Algebras from the Bundle Theoretic Point of View.
 In physics, it has been conjectured to classify D-branes, Ramond-Ramond field strengths and in some cases even spinors in type II string theory. For more information on twisted K-theory in string theory, see K-theory (physics).
 In the broader context of K-theory, in each subject it has numerous isomorphic formulations and, in many cases, isomorphisms relating definitions in various subjects have been proven.  It also has numerous deformations, for example, in abstract algebra K-theory may be twisted by any integral cohomology class.
"
Type_theory,Mathematics,3,"In mathematics, logic, and computer science, a type system is a formal system in which every term has a ""type"" which defines its meaning and the operations that may be performed on it. Type theory is the academic study of type systems.
 Some type theories serve as alternatives to set theory as a foundation of mathematics. Two well-known such theories are Alonzo Church's typed λ-calculus and Per Martin-Löf's intuitionistic type theory.  
 Type theory was created to avoid paradoxes in previous foundations such as naive set theory, formal logics and rewrite systems.
 Type theory is closely related to, and in some cases overlaps with, computational type systems, which are a programming language feature used to reduce bugs.
"
Umbral_calculus,Mathematics,3,"
 In mathematics before the 1970s, the term umbral calculus referred to the surprising similarity between seemingly unrelated polynomial equations and certain shadowy techniques used to 'prove' them. These techniques were introduced by John Blissard (1861) and are sometimes called Blissard's symbolic method. They are often attributed to Édouard Lucas (or James Joseph Sylvester), who used the technique extensively.[1]"
Uncertainty_theory,Mathematics,3,"Uncertainty theory is a branch of mathematics based on normality, monotonicity, self-duality, countable subadditivity, and product measure axioms.[clarification needed] Mathematical measures of the likelihood of an event being true include probability theory, capacity, fuzzy logic, possibility, and credibility, as well as uncertainty.
"
Universal_algebra,Mathematics,3,"Universal algebra (sometimes called general algebra) is the field of mathematics that studies algebraic structures themselves, not examples (""models"") of algebraic structures.
For instance, rather than take particular groups as the object of study, in universal algebra one takes the class of groups as an object of study.
"
Rational_trigonometry,Mathematics,3,"Rational trigonometry is a proposed reformulation of metrical planar and solid geometries (which includes trigonometry) by Canadian mathematician Norman J. Wildberger, currently a professor of mathematics at the University of New South Wales. His ideas are set out in his 2005 book Divine Proportions: Rational Trigonometry to Universal Geometry.[1] According to New Scientist, part of his motivation for an alternative to traditional trigonometry was to avoid some problems that he claims occur when infinite series are used in mathematics.  Rational trigonometry avoids direct use of transcendental functions like sine and cosine by substituting their squared equivalents.[2] Wildberger draws inspiration from mathematicians predating Georg Cantor's infinite set-theory, like Gauss and Euclid, who he claims were far more wary of using infinite sets than modern mathematicians.[2][nb 1] To date, rational trigonometry is largely unmentioned in mainstream mathematical literature.
"
Valuation_theory,Mathematics,3,"In algebra (in particular in algebraic geometry or algebraic number theory), a valuation is a function on a field that provides a measure of size or multiplicity of elements of the field. It generalizes to commutative algebra the notion of size inherent in consideration of the degree of a pole or multiplicity of a zero in complex analysis, the degree of divisibility of a number by a prime number in number theory, and the geometrical concept of contact between two algebraic or analytic varieties in algebraic geometry. A field with a valuation on it is called a valued field.
"
Variational_analysis,Mathematics,3,"In mathematics, the term variational analysis usually denotes the combination and extension of methods from convex optimization and the classical calculus of variations to a more general theory.[1] This includes the more general problems of optimization theory, including topics in set-valued analysis, e.g. generalized derivatives.
 In the Mathematics Subject Classification scheme (MSC2010), the field of ""Set-valued and variational analysis""  is coded by ""49J53"".[2]"
Vector_analysis,Mathematics,3,"Vector calculus, or vector analysis, is  concerned with differentiation and integration of vector fields, primarily in 3-dimensional Euclidean space 





R


3


.


{  \mathbb {R} ^{3}.}
  The term ""vector calculus"" is sometimes used as a synonym for the broader subject of multivariable calculus, which includes vector calculus as well as partial differentiation and multiple integration.  Vector calculus plays an important role in differential geometry and in the study of partial differential equations.  It is used extensively in physics and engineering, especially in the description of
electromagnetic fields, gravitational fields, and fluid flow.
 Vector calculus was developed from quaternion analysis by J. Willard Gibbs and Oliver Heaviside near the end of the 19th century, and most of the notation and terminology was established by Gibbs and Edwin Bidwell Wilson in their 1901 book, Vector Analysis. In the conventional form using cross products, vector calculus does not generalize to higher dimensions, while the alternative approach of geometric algebra which uses exterior products does (see § Generalizations below for more).
"
Vector_calculus,Mathematics,3,"Vector calculus, or vector analysis, is  concerned with differentiation and integration of vector fields, primarily in 3-dimensional Euclidean space 





R


3


.


{  \mathbb {R} ^{3}.}
  The term ""vector calculus"" is sometimes used as a synonym for the broader subject of multivariable calculus, which includes vector calculus as well as partial differentiation and multiple integration.  Vector calculus plays an important role in differential geometry and in the study of partial differential equations.  It is used extensively in physics and engineering, especially in the description of
electromagnetic fields, gravitational fields, and fluid flow.
 Vector calculus was developed from quaternion analysis by J. Willard Gibbs and Oliver Heaviside near the end of the 19th century, and most of the notation and terminology was established by Gibbs and Edwin Bidwell Wilson in their 1901 book, Vector Analysis. In the conventional form using cross products, vector calculus does not generalize to higher dimensions, while the alternative approach of geometric algebra which uses exterior products does (see § Generalizations below for more).
"
Wavelet,Mathematics,3,"A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases, and then decreases back to zero. It can typically be visualized as a ""brief oscillation"" like one recorded by a seismograph or heart monitor. Generally, wavelets are intentionally crafted to have specific properties that make them useful for signal processing. Using convolution, wavelets can be combined with known portions of a damaged signal to extract information from the unknown portions.
 For example, a wavelet could be created to have a frequency of Middle C and a short duration of roughly a 32nd note. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the Middle C note was being played in the song. Mathematically, the wavelet will correlate with the signal if the unknown signal contains information of similar frequency. This concept of correlation is at the core of many practical applications of wavelet theory.
 As a mathematical tool, wavelets can be used to extract information from many different kinds of data, including – but not limited to – audio signals and images. Sets of wavelets are generally needed to analyze data fully. A set of ""complementary"" wavelets will decompose data without gaps or overlap so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet based compression/decompression algorithms where it is desirable to recover the original information with minimal loss.
 In formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square integrable functions. This is accomplished through coherent states.
"
Windowed_Fourier_transform,Mathematics,3,"The Short-time Fourier transform (STFT), is a Fourier-related transform used to determine the sinusoidal frequency and phase content of local sections of a signal as it changes over time.[1]  In practice, the procedure for computing STFTs is to divide a longer time signal into shorter segments of equal length and then compute the Fourier transform separately on each shorter segment.  This reveals the Fourier spectrum on each shorter segment.  One then usually plots the changing spectra as a function of time, known as a spectrogram or waterfall plot.
"
Window_function,Mathematics,3,"In signal processing and statistics, a window function (also known as an apodization function or tapering function[1]) is a mathematical function that is zero-valued outside of some chosen interval, normally symmetric around the middle of the interval, usually near a maximum in the middle, and usually tapering away from the middle. Mathematically, when another function or waveform/data-sequence is ""multiplied"" by a window function, the product is also zero-valued outside the interval: all that is left is the part where they overlap, the ""view through the window"".  Equivalently, and in actual practice, the segment of data within the window is first isolated, and then only that data is multiplied by the window function values.  Thus, tapering, not segmentation, is the main purpose of window functions.
 The reasons for examining segments of a longer function include detection of transient events and time-averaging of frequency spectra.  The duration of the segments is determined in each application by requirements like time and frequency resolution.  But that method also changes the frequency content of the signal by an effect called spectral leakage.  Window functions allow us to distribute the leakage spectrally in different ways, according to the needs of the particular application.  There are many choices detailed in this article, but many of the differences are so subtle as to be insignificant in practice.
 In typical applications, the window functions used are non-negative, smooth, ""bell-shaped"" curves.[2] Rectangle, triangle, and other functions can also be used.  A rectangular window does not modify the data segment at all.  It's only for modelling purposes that we say it multiplies by 1 inside the window and by 0 outside.  A more general definition of window functions does not require them to be identically zero outside an interval, as long as the product of the window multiplied by its argument is square integrable, and, more specifically, that the function goes sufficiently rapidly toward zero.[3]"