{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WikiAlgo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djch31ICRSE4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SLTXIw6FEBu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from lxml import html\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "import string\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzP1vOu8ITws"
      },
      "source": [
        "def wiki(search):\n",
        "  #search = \"_\".join(search.split(\" \"))\n",
        "  try:\n",
        "    url = \"https://en.m.wikipedia.org/wiki/\"+search\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.content,\"html.parser\")\n",
        "    mysec = soup.find(\"section\", {\"class\": \"mf-section-0\"})\n",
        "    #mysec = soup.select(\"section#mf-section-0.mf-section-0\")\n",
        "    sumtext = []\n",
        "    z = mysec.find_all(\"p\")\n",
        "    for y in z:\n",
        "      sumtext.append(y.get_text())\n",
        "    return \" \".join(sumtext).replace(\"\\displaystyle\",\" \")\n",
        "  except:\n",
        "    return print(search)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpbaU6u5RFqf"
      },
      "source": [
        "def preprocess(msg):\n",
        "  msg2 = word_tokenize(msg)\n",
        "  msg2 = [m for m in msg2 if m not in string.punctuation]\n",
        "  sw = stopwords.words('english')\n",
        "  msg2 = [m for m in msg2 if m not in sw]\n",
        "  ps = PorterStemmer()\n",
        "  msg2 = [ps.stem(m) for m in msg2]\n",
        "  return \" \".join(msg2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3XiY2rQRJvJ"
      },
      "source": [
        "Wiki_Dataset_updated = pd.read_csv(\"/content/drive/My Drive/ML/wiki_data.csv\")\n",
        "Wiki_Dataset_updated = Wiki_Dataset_updated.dropna()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXRLEkHNB0ux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b4cc08-70f5-47c4-a45b-ff111fd9c3c2"
      },
      "source": [
        "Variable = input()\n",
        "Wiki_Dataset_updated.loc[2637] = [Variable,'Cat',0,wiki(Variable)]\n",
        "#Wiki_Dataset_updated.tail()\n",
        "NewScrap = Wiki_Dataset_updated.ScrapData.apply(preprocess)\n",
        "#cv = CountVectorizer(analyzer=preprocess)\n",
        "spa_mat = CountVectorizer().fit_transform(NewScrap)\n",
        "x_train, x_test, y_train, y_test = train_test_split(spa_mat,Wiki_Dataset_updated.iloc[:,2],test_size=1/2638,shuffle=False)\n",
        "classifier = MultinomialNB()\n",
        "#classifier = LinearSVC(random_state=1,dual=False,max_iter=10000)\n",
        "#classifier = LogisticRegression()\n",
        "classifier.fit(x_train.toarray(),y_train)\n",
        "y_cap = classifier.predict(x_test.toarray())\n",
        "if y_cap==1:\n",
        "  result = \"Chemistry\"\n",
        "elif y_cap==2:\n",
        "  result = \"Physics\"\n",
        "elif y_cap==3:\n",
        "  result = \"Mathematics\"\n",
        "elif y_cap==4:\n",
        "  result = \"Computer Science\"\n",
        "elif y_cap==5:\n",
        "  result = \"Biology\"\n",
        "elif y_cap==6:\n",
        "  result = \"Economics\"\n",
        "else:\n",
        "  result = \"What\"\n",
        "print(result)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mathematics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v75j_x5h7LNY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}